[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2512.18741v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18741v2",
                "title": "Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorize-and-Generate: Towards Long-Term Consistency in Real-Time Video Generation"
                },
                "updated": "2025-12-23T16:47:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    47,
                    46,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18741v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18741v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Frame-level autoregressive (frame-AR) models have achieved significant progress, enabling real-time video generation comparable to bidirectional diffusion models and serving as a foundation for interactive world models and game engines. However, current approaches in long video generation typically rely on window attention, which naively discards historical context outside the window, leading to catastrophic forgetting and scene inconsistency; conversely, retaining full history incurs prohibitive memory costs. To address this trade-off, we propose Memorize-and-Generate (MAG), a framework that decouples memory compression and frame generation into distinct tasks. Specifically, we train a memory model to compress historical information into a compact KV cache, and a separate generator model to synthesize subsequent frames utilizing this compressed representation. Furthermore, we introduce MAG-Bench to strictly evaluate historical memory retention. Extensive experiments demonstrate that MAG achieves superior historical scene consistency while maintaining competitive performance on standard video generation benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Frame-level autoregressive (frame-AR) models have achieved significant progress, enabling real-time video generation comparable to bidirectional diffusion models and serving as a foundation for interactive world models and game engines. However, current approaches in long video generation typically rely on window attention, which naively discards historical context outside the window, leading to catastrophic forgetting and scene inconsistency; conversely, retaining full history incurs prohibitive memory costs. To address this trade-off, we propose Memorize-and-Generate (MAG), a framework that decouples memory compression and frame generation into distinct tasks. Specifically, we train a memory model to compress historical information into a compact KV cache, and a separate generator model to synthesize subsequent frames utilizing this compressed representation. Furthermore, we introduce MAG-Bench to strictly evaluate historical memory retention. Extensive experiments demonstrate that MAG achieves superior historical scene consistency while maintaining competitive performance on standard video generation benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-21T14:02:53Z",
                "published_parsed": [
                    2025,
                    12,
                    21,
                    14,
                    2,
                    53,
                    6,
                    355,
                    0
                ],
                "arxiv_comment": "Code will be released at https://github.com/Xilluill/MAG",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tianrui Zhu"
                    },
                    {
                        "name": "Shiyi Zhang"
                    },
                    {
                        "name": "Zhirui Sun"
                    },
                    {
                        "name": "Jingqi Tian"
                    },
                    {
                        "name": "Yansong Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Tang"
                },
                "author": "Yansong Tang"
            },
            {
                "id": "http://arxiv.org/abs/2512.20276v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20276v1",
                "title": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge"
                },
                "updated": "2025-12-23T11:29:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    29,
                    3,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20276v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T11:29:03Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    29,
                    3,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yuntao Dai"
                    },
                    {
                        "name": "Hang Gu"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Qianyu Cheng"
                    },
                    {
                        "name": "Yifei Zheng"
                    },
                    {
                        "name": "Zhiyong Qiu"
                    },
                    {
                        "name": "Lei Gong"
                    },
                    {
                        "name": "Wenqi Lou"
                    },
                    {
                        "name": "Xuehai Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xuehai Zhou"
                },
                "author": "Xuehai Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2512.20245v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20245v1",
                "title": "Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory as Resonance: A Biomimetic Architecture for Infinite Context Memory on Ergodic Phonetic Manifolds"
                },
                "updated": "2025-12-23T10:55:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    55,
                    32,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20245v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The memory of contemporary Large Language Models is bound by a physical paradox: as they learn, they fill up. The linear accumulation (O(N)) of Key-Value states treats context as a warehouse of static artifacts, eventually forcing a destructive choice between amnesia and latency. We challenge this discrete orthodoxy, proposing that long-term memory is not the storage of items, but the persistence of a trajectory. We introduce Phonetic Trajectory Memory (PTM), a neuro-symbolic architecture that encodes language not as a sequence of tensors, but as a continuous path on an ergodic manifold governed by irrational rotation matrices. By decoupling the navigation (an invariant O(1) geometric signal) from the reconstruction (a probabilistic generative act), PTM achieves a compression magnitude of greater than 3,000x relative to dense caches. We demonstrate that retrieval becomes a process of resonance: the phonetic trace stabilizes the model against hallucination via \"Signal Consensus\" mechanism, securing up to approximately 92% factual accuracy. While this aggressive abstraction alters generative texture, it unlocks immediate access latency (approximately 34ms) independent of depth. Our results suggest that infinite context does not require infinite silicon; it requires treating memory not as data to be stored, but as a reconstructive process acting on a conserved, undying physical signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The memory of contemporary Large Language Models is bound by a physical paradox: as they learn, they fill up. The linear accumulation (O(N)) of Key-Value states treats context as a warehouse of static artifacts, eventually forcing a destructive choice between amnesia and latency. We challenge this discrete orthodoxy, proposing that long-term memory is not the storage of items, but the persistence of a trajectory. We introduce Phonetic Trajectory Memory (PTM), a neuro-symbolic architecture that encodes language not as a sequence of tensors, but as a continuous path on an ergodic manifold governed by irrational rotation matrices. By decoupling the navigation (an invariant O(1) geometric signal) from the reconstruction (a probabilistic generative act), PTM achieves a compression magnitude of greater than 3,000x relative to dense caches. We demonstrate that retrieval becomes a process of resonance: the phonetic trace stabilizes the model against hallucination via \"Signal Consensus\" mechanism, securing up to approximately 92% factual accuracy. While this aggressive abstraction alters generative texture, it unlocks immediate access latency (approximately 34ms) independent of depth. Our results suggest that infinite context does not require infinite silicon; it requires treating memory not as data to be stored, but as a reconstructive process acting on a conserved, undying physical signal."
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T10:55:32Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    55,
                    32,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE"
                },
                "authors": [
                    {
                        "name": "Tarik Houichime"
                    },
                    {
                        "name": "Abdelghani Souhar"
                    },
                    {
                        "name": "Younes El Amrani"
                    }
                ],
                "author_detail": {
                    "name": "Younes El Amrani"
                },
                "author": "Younes El Amrani"
            },
            {
                "id": "http://arxiv.org/abs/2512.20201v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20201v1",
                "title": "Joint Design of Embedded Index Coding and Beamforming for MIMO-based Distributed Computing via Multi-Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Design of Embedded Index Coding and Beamforming for MIMO-based Distributed Computing via Multi-Agent Reinforcement Learning"
                },
                "updated": "2025-12-23T09:49:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    49,
                    25,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20201v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In distributed computing systems, reducing the communication load during the data shuffling phase is a critical challenge, as excessive inter-node transmissions are a major performance bottleneck. One promising approach to alleviate this burden is Embedded Index Coding (EIC), which exploits cached data at user nodes to encode transmissions more efficiently. However, most prior work on EIC has focused on minimizing code length in wired, error-free environments-an objective often suboptimal for wireless multiple-input multiple-output (MIMO) systems, where channel conditions and spatial multiplexing gains must be considered. This paper investigates the joint design of EIC and transmit beamforming in MIMO systems to minimize total transmission time, an NP-hard problem. We first present a conventional optimization method that determines the optimal EIC via exhaustive search. To address its prohibitive complexity and adapt to dynamic wireless environments, we propose a novel, low-complexity multi-agent reinforcement learning (MARL) framework. The proposed framework enables decentralized agents to act on local observations while effectively managing the hybrid action space of discrete EIC selection and continuous beamforming design. Simulation results demonstrate that the proposed MARL-based approach achieves near-optimal performance with significantly reduced complexity, underscoring its effectiveness and practicality for real-world wireless systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In distributed computing systems, reducing the communication load during the data shuffling phase is a critical challenge, as excessive inter-node transmissions are a major performance bottleneck. One promising approach to alleviate this burden is Embedded Index Coding (EIC), which exploits cached data at user nodes to encode transmissions more efficiently. However, most prior work on EIC has focused on minimizing code length in wired, error-free environments-an objective often suboptimal for wireless multiple-input multiple-output (MIMO) systems, where channel conditions and spatial multiplexing gains must be considered. This paper investigates the joint design of EIC and transmit beamforming in MIMO systems to minimize total transmission time, an NP-hard problem. We first present a conventional optimization method that determines the optimal EIC via exhaustive search. To address its prohibitive complexity and adapt to dynamic wireless environments, we propose a novel, low-complexity multi-agent reinforcement learning (MARL) framework. The proposed framework enables decentralized agents to act on local observations while effectively managing the hybrid action space of discrete EIC selection and continuous beamforming design. Simulation results demonstrate that the proposed MARL-based approach achieves near-optimal performance with significantly reduced complexity, underscoring its effectiveness and practicality for real-world wireless systems."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:49:25Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    49,
                    25,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Heekang Song"
                    },
                    {
                        "name": "Wan Choi"
                    }
                ],
                "author_detail": {
                    "name": "Wan Choi"
                },
                "author": "Wan Choi"
            },
            {
                "id": "http://arxiv.org/abs/2510.23649v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.23649v3",
                "title": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models"
                },
                "updated": "2025-12-23T08:47:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    47,
                    31,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.23649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.23649v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As the length of input text increases, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. In this work, Low Rank Query and Key attention (LRQK) is introduced, a two-stage framework that jointly decomposes full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then employs these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism where only missing full-precision KV pairs are transferred, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal accuracy loss. Our code is available at https://github.com/tenghuilee/LRQK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of input text increases, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. In this work, Low Rank Query and Key attention (LRQK) is introduced, a two-stage framework that jointly decomposes full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then employs these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism where only missing full-precision KV pairs are transferred, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal accuracy loss. Our code is available at https://github.com/tenghuilee/LRQK."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-25T11:43:27Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "arxiv_comment": "https://neurips.cc/virtual/2025/loc/san-diego/poster/118451",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "arxiv_journal_ref": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
                "authors": [
                    {
                        "name": "Tenghui Li"
                    },
                    {
                        "name": "Guoxu Zhou"
                    },
                    {
                        "name": "Xuyang Zhao"
                    },
                    {
                        "name": "Yuning Qiu"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.20072v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20072v1",
                "title": "Ultrahigh Charge-to-Spin Conversion and Tunneling Magnetoresistance in Quasi-Two-Dimensional d-wave Altermagnet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrahigh Charge-to-Spin Conversion and Tunneling Magnetoresistance in Quasi-Two-Dimensional d-wave Altermagnet"
                },
                "updated": "2025-12-23T05:52:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    5,
                    52,
                    45,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20072v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The emergence of altermagnets has driven groundbreaking advances in spintronics. Notably, d-wave altermagnets support non-relativistic spin transport, efficient charge-to-spin conversion, and T-odd spin currents. In addition, their integration as electrodes in antiferromagnetic tunnel junctions (AFMTJs) enables a tunneling magnetoresistance (TMR) effect, allowing electrical detection of Néel vectors for next-generation memory devices. In this work, we investigate the non-relativistic spin transport properties of the quasi-two-dimensional (quasi-2D) d-wave altermagnet KV\\textsubscript{2}Se\\textsubscript{2}O and the TMR effect in KV\\textsubscript{2}Se\\textsubscript{2}O-based AFMTJs via first-principles calculations. Our results reveal that KV\\textsubscript{2}Se\\textsubscript{2}O exhibits a non-relativistic longitudinal spin polarization and a spin Hall angle both exceeding 60\\% at room temperature, while KV\\textsubscript{2}Se\\textsubscript{2}O-based AFMTJs achieve a giant TMR reaching approximately $10^{12}$\\%, which remains robust against Fermi level shifts. These findings highlight the anisotropic spin polarization inherent to d-wave staggered magnetism and underscore the critical role of Fermi surface topology in enhancing T-odd spin transport and the TMR effect in AFMTJs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of altermagnets has driven groundbreaking advances in spintronics. Notably, d-wave altermagnets support non-relativistic spin transport, efficient charge-to-spin conversion, and T-odd spin currents. In addition, their integration as electrodes in antiferromagnetic tunnel junctions (AFMTJs) enables a tunneling magnetoresistance (TMR) effect, allowing electrical detection of Néel vectors for next-generation memory devices. In this work, we investigate the non-relativistic spin transport properties of the quasi-two-dimensional (quasi-2D) d-wave altermagnet KV\\textsubscript{2}Se\\textsubscript{2}O and the TMR effect in KV\\textsubscript{2}Se\\textsubscript{2}O-based AFMTJs via first-principles calculations. Our results reveal that KV\\textsubscript{2}Se\\textsubscript{2}O exhibits a non-relativistic longitudinal spin polarization and a spin Hall angle both exceeding 60\\% at room temperature, while KV\\textsubscript{2}Se\\textsubscript{2}O-based AFMTJs achieve a giant TMR reaching approximately $10^{12}$\\%, which remains robust against Fermi level shifts. These findings highlight the anisotropic spin polarization inherent to d-wave staggered magnetism and underscore the critical role of Fermi surface topology in enhancing T-odd spin transport and the TMR effect in AFMTJs."
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T05:52:45Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    5,
                    52,
                    45,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "15 pages,4 figures,Supplementary Material included",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci"
                },
                "authors": [
                    {
                        "name": "Qing Zhang"
                    },
                    {
                        "name": "Siyun Wang"
                    },
                    {
                        "name": "Jianting Dong"
                    },
                    {
                        "name": "Jia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jia Zhang"
                },
                "author": "Jia Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.19964v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19964v1",
                "title": "VNF-Cache: An In-Network Key-Value Store Cache Based on Network Function Virtualization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VNF-Cache: An In-Network Key-Value Store Cache Based on Network Function Virtualization"
                },
                "updated": "2025-12-23T01:25:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    1,
                    25,
                    21,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19964v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the exponential growth of the amount of data available on the Internet, optimizing the response time and resource usage for data access becomes essential. Caches are an effective solution that brings data closer to clients, eliminating repetitive requests to servers. This paper presents VNF-Cache, a caching service for geographically remote key-value databases. VNF-Cache is an NFV-COIN (Network Function Virtualization-Computing In The Network) service, a technology undergoing standardization by the IETF that enables the implementation of arbitrary services directly in the network. VNF-Cache intercepts network packets, processes, stores, and sends values directly to clients when possible. Through a proof-of-concept implementation and experiments conducted with geographically dispersed servers in Brazil, the United States, and Japan, significant reductions in response time and increases in the number of requests processed per second were observed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the exponential growth of the amount of data available on the Internet, optimizing the response time and resource usage for data access becomes essential. Caches are an effective solution that brings data closer to clients, eliminating repetitive requests to servers. This paper presents VNF-Cache, a caching service for geographically remote key-value databases. VNF-Cache is an NFV-COIN (Network Function Virtualization-Computing In The Network) service, a technology undergoing standardization by the IETF that enables the implementation of arbitrary services directly in the network. VNF-Cache intercepts network packets, processes, stores, and sends values directly to clients when possible. Through a proof-of-concept implementation and experiments conducted with geographically dispersed servers in Brazil, the United States, and Japan, significant reductions in response time and increases in the number of requests processed per second were observed."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T01:25:21Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    1,
                    25,
                    21,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Bruno E. Farias"
                    },
                    {
                        "name": "José Flauzino"
                    },
                    {
                        "name": "Elias P. Duarte"
                    }
                ],
                "author_detail": {
                    "name": "Elias P. Duarte"
                },
                "author": "Elias P. Duarte"
            },
            {
                "id": "http://arxiv.org/abs/2512.19678v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19678v1",
                "title": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion"
                },
                "updated": "2025-12-22T18:53:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    53,
                    50,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19678v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a \"fill-and-revise\" objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \\href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T18:53:50Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    18,
                    53,
                    50,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Project page: https://hyokong.github.io/worldwarp-page/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hanyang Kong"
                    },
                    {
                        "name": "Xingyi Yang"
                    },
                    {
                        "name": "Xiaoxu Zheng"
                    },
                    {
                        "name": "Xinchao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinchao Wang"
                },
                "author": "Xinchao Wang"
            },
            {
                "id": "http://arxiv.org/abs/2510.06415v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.06415v2",
                "title": "Neutron-Assisted Breakdown Enhancement in $β$-Ga$_2$O$_3$ Schottky Diodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neutron-Assisted Breakdown Enhancement in $β$-Ga$_2$O$_3$ Schottky Diodes"
                },
                "updated": "2025-12-22T16:32:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    16,
                    32,
                    7,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.06415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.06415v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This study demonstrates a substantial enhancement of breakdown voltage in $β$-Ga$_2$O$_3$ Schottky diodes through an approach that combines fast neutron irradiation with controlled post-irradiation electro-thermal annealing. Devices irradiated with 1 MeV neutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation, including a drastic reduction in on-current and an increase in on-resistance. Electrothermal testing, conducted through simultaneous current-voltage (J-V) measurements and thermal annealing, resulted in significant recovery. After four cycles of electro-thermal testing, the devices demonstrated significant improvements in performance, with a substantial recovery of on-current and a reduction in on-resistance compared to the post-radiation condition, approaching pre-radiation levels. Most recovery occurred during the first two cycles, with diminishing improvements in later cycles, indicating that most thermally recoverable traps were mitigated early. Capacitance-voltage (C-V) measurements revealed a substantial reduction in carrier concentration, decreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first electro-thermal testing cycle, indicating an over 82% reduction. Following the third cycle, the carrier concentration partially recovered to 9.9E15 cm^-3, reflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited a remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a ~325% improvement) after the first electro-thermal testing, attributed to the reduction in carrier concentration by compensating radiation-induced traps. Subsequent testing reduced breakdown voltage slightly to 940 V due to partial recovery of carrier concentration, but it remained significantly higher than pre-radiation levels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates a substantial enhancement of breakdown voltage in $β$-Ga$_2$O$_3$ Schottky diodes through an approach that combines fast neutron irradiation with controlled post-irradiation electro-thermal annealing. Devices irradiated with 1 MeV neutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation, including a drastic reduction in on-current and an increase in on-resistance. Electrothermal testing, conducted through simultaneous current-voltage (J-V) measurements and thermal annealing, resulted in significant recovery. After four cycles of electro-thermal testing, the devices demonstrated significant improvements in performance, with a substantial recovery of on-current and a reduction in on-resistance compared to the post-radiation condition, approaching pre-radiation levels. Most recovery occurred during the first two cycles, with diminishing improvements in later cycles, indicating that most thermally recoverable traps were mitigated early. Capacitance-voltage (C-V) measurements revealed a substantial reduction in carrier concentration, decreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first electro-thermal testing cycle, indicating an over 82% reduction. Following the third cycle, the carrier concentration partially recovered to 9.9E15 cm^-3, reflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited a remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a ~325% improvement) after the first electro-thermal testing, attributed to the reduction in carrier concentration by compensating radiation-induced traps. Subsequent testing reduced breakdown voltage slightly to 940 V due to partial recovery of carrier concentration, but it remained significantly higher than pre-radiation levels."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-07T19:50:52Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    19,
                    50,
                    52,
                    1,
                    280,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Saleh Ahmed Khan"
                    },
                    {
                        "name": "Sudipto Saha"
                    },
                    {
                        "name": "Ahmed Ibreljic"
                    },
                    {
                        "name": "Stephen Margiotta"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Walid Amir"
                    },
                    {
                        "name": "Surajit Chakraborty"
                    },
                    {
                        "name": "Uttam Singisetti"
                    },
                    {
                        "name": "A F M Anhar Uddin Bhuiyan"
                    }
                ],
                "author_detail": {
                    "name": "A F M Anhar Uddin Bhuiyan"
                },
                "author": "A F M Anhar Uddin Bhuiyan"
            },
            {
                "id": "http://arxiv.org/abs/2512.19437v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19437v1",
                "title": "Ultra-high precision high voltage system for PTOLEMY",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-high precision high voltage system for PTOLEMY"
                },
                "updated": "2025-12-22T14:34:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    34,
                    51,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19437v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19437v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The PTOLEMY project is prototyping a novel electromagnetic filter for high-precision $β$ spectroscopy, with the ultimate and ambitious long-term goal of detecting the cosmic neutrino background through electron capture on tritium bound to graphene. Intermediate small-scale prototypes can achieve competitive sensitivity to the effective neutrino mass, even with reduced energy resolution. To reach an energy resolution better than \\SI{500}{meV} at the tritium $β$-spectrum endpoint of \\SI{18.6}{keV}, and accounting for all uncertainties in the filtering chain, the electrode voltage must be controlled at the level of a few parts per million and monitored in real time. In this work, we present the first results obtained in this effort, using a chain of commercial ultra-high-precision voltage references, read out by precision multimeters and a \\emph{field mill} device. The currently available precision on high voltage is, in the conservative case, as low as \\SI{0.2}{ppm} per \\SI{1}{kV} single board and $\\lesssim$ \\SI{50}{mV} over the \\SI{10}{kV} series, presently limited by field mill read-out noise. However, assuming uncorrelated Gaussian noise extrapolation, the real precision could in principle be as low as \\SI{0.05}{ppm} over \\SI{20}{kV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The PTOLEMY project is prototyping a novel electromagnetic filter for high-precision $β$ spectroscopy, with the ultimate and ambitious long-term goal of detecting the cosmic neutrino background through electron capture on tritium bound to graphene. Intermediate small-scale prototypes can achieve competitive sensitivity to the effective neutrino mass, even with reduced energy resolution. To reach an energy resolution better than \\SI{500}{meV} at the tritium $β$-spectrum endpoint of \\SI{18.6}{keV}, and accounting for all uncertainties in the filtering chain, the electrode voltage must be controlled at the level of a few parts per million and monitored in real time. In this work, we present the first results obtained in this effort, using a chain of commercial ultra-high-precision voltage references, read out by precision multimeters and a \\emph{field mill} device. The currently available precision on high voltage is, in the conservative case, as low as \\SI{0.2}{ppm} per \\SI{1}{kV} single board and $\\lesssim$ \\SI{50}{mV} over the \\SI{10}{kV} series, presently limited by field mill read-out noise. However, assuming uncorrelated Gaussian noise extrapolation, the real precision could in principle be as low as \\SI{0.05}{ppm} over \\SI{20}{kV}."
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T14:34:51Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    34,
                    51,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "17 pages, 13 figures",
                "arxiv_primary_category": {
                    "term": "physics.ins-det"
                },
                "authors": [
                    {
                        "name": "R. Ammendola"
                    },
                    {
                        "name": "A. Apponi"
                    },
                    {
                        "name": "G. Benato"
                    },
                    {
                        "name": "M. G. Betti"
                    },
                    {
                        "name": "R. Biondim"
                    },
                    {
                        "name": "P. Bos"
                    },
                    {
                        "name": "G. Cavoto"
                    },
                    {
                        "name": "M. Cadeddu"
                    },
                    {
                        "name": "A. Casale"
                    },
                    {
                        "name": "O. Castellano"
                    },
                    {
                        "name": "E. Celasco"
                    },
                    {
                        "name": "L. Cecchini"
                    },
                    {
                        "name": "M. Chirico"
                    },
                    {
                        "name": "W. Chung"
                    },
                    {
                        "name": "A. G. Cocco"
                    },
                    {
                        "name": "A. P. Colijn"
                    },
                    {
                        "name": "B. Corcione"
                    },
                    {
                        "name": "N. D'Ambrosio"
                    },
                    {
                        "name": "M. D'Incecco"
                    },
                    {
                        "name": "G. De Bellis"
                    },
                    {
                        "name": "M. De Deo"
                    },
                    {
                        "name": "N. de Groot"
                    },
                    {
                        "name": "A. Esposito"
                    },
                    {
                        "name": "M. Farino"
                    },
                    {
                        "name": "S. Farinon"
                    },
                    {
                        "name": "A. D. Ferella"
                    },
                    {
                        "name": "L. Ferro"
                    },
                    {
                        "name": "L. Ficcadenti"
                    },
                    {
                        "name": "G. Galbato Muscio"
                    },
                    {
                        "name": "S. Gariazzo"
                    },
                    {
                        "name": "H. Garrone"
                    },
                    {
                        "name": "F. Gatti"
                    },
                    {
                        "name": "G. Korga"
                    },
                    {
                        "name": "F. Malnati"
                    },
                    {
                        "name": "G. Mangano"
                    },
                    {
                        "name": "L. E. Marcucci"
                    },
                    {
                        "name": "C. Mariani"
                    },
                    {
                        "name": "J. Mead"
                    },
                    {
                        "name": "G. Menichetti"
                    },
                    {
                        "name": "M. Messina"
                    },
                    {
                        "name": "E. Monticone"
                    },
                    {
                        "name": "M. Naafs"
                    },
                    {
                        "name": "V. Narcisi"
                    },
                    {
                        "name": "S. Nagorny"
                    },
                    {
                        "name": "G. Neri"
                    },
                    {
                        "name": "F. Pandolfi"
                    },
                    {
                        "name": "R. Pavarani"
                    },
                    {
                        "name": "C. Pèrez de los Heros"
                    },
                    {
                        "name": "O. Pisanti"
                    },
                    {
                        "name": "C. Pepe"
                    },
                    {
                        "name": "F. M. Pofi"
                    },
                    {
                        "name": "A. D. Polosa"
                    },
                    {
                        "name": "I. Rago"
                    },
                    {
                        "name": "M. Rajteri N. Rossi"
                    },
                    {
                        "name": "S. Ritarossi"
                    },
                    {
                        "name": "A. Ruocco"
                    },
                    {
                        "name": "G. Salina"
                    },
                    {
                        "name": "A. Santucci"
                    },
                    {
                        "name": "M. Sestu"
                    },
                    {
                        "name": "A. Tan"
                    },
                    {
                        "name": "V. Tozzini"
                    },
                    {
                        "name": "C. G. Tully"
                    },
                    {
                        "name": "I. van Rens"
                    },
                    {
                        "name": "F. Virzi"
                    },
                    {
                        "name": "G. Visser"
                    },
                    {
                        "name": "M. Vivian"
                    }
                ],
                "author_detail": {
                    "name": "M. Vivian"
                },
                "author": "M. Vivian"
            },
            {
                "id": "http://arxiv.org/abs/2512.19434v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19434v1",
                "title": "Hybrid Analytical-Machine Learning Framework for Ripple Factor Estimation in Cockcroft-Walton Voltage Multipliers with Residual Correction for Non-Ideal Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Analytical-Machine Learning Framework for Ripple Factor Estimation in Cockcroft-Walton Voltage Multipliers with Residual Correction for Non-Ideal Effects"
                },
                "updated": "2025-12-22T14:32:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    32,
                    19,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19434v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cockcroft-Walton (CW) voltage multipliers suffer from output ripple that classical analytical models underestimate due to neglected non-idealities like diode drops and capacitor ESR, particularly in high-stage, low-frequency and heavy-load regimes. This paper proposes a hybrid framework that generates a comprehensive 324-case MATLAB/Simulink dataset varying stages (2-8), input voltage (5-25 kV), capacitance (1-10 μF), frequency (50-500 Hz) and load (6-60 MΩ), then trains a Random Forest model to predict residuals between simulated and theoretical peak-to-peak ripple. The approach achieves 70.6% RMSE reduction (131 V vs. 448 V) globally and 66.7% in critical regimes, with near-zero bias, enabling physically interpretable design optimization while outperforming pure ML in extrapolation reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cockcroft-Walton (CW) voltage multipliers suffer from output ripple that classical analytical models underestimate due to neglected non-idealities like diode drops and capacitor ESR, particularly in high-stage, low-frequency and heavy-load regimes. This paper proposes a hybrid framework that generates a comprehensive 324-case MATLAB/Simulink dataset varying stages (2-8), input voltage (5-25 kV), capacitance (1-10 μF), frequency (50-500 Hz) and load (6-60 MΩ), then trains a Random Forest model to predict residuals between simulated and theoretical peak-to-peak ripple. The approach achieves 70.6% RMSE reduction (131 V vs. 448 V) globally and 66.7% in critical regimes, with near-zero bias, enabling physically interpretable design optimization while outperforming pure ML in extrapolation reliability."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T14:32:19Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    14,
                    32,
                    19,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "6 Pages, 2 figures, IEEE Conference Template used",
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Md. Tanvirul Islam"
                    }
                ],
                "author_detail": {
                    "name": "Md. Tanvirul Islam"
                },
                "author": "Md. Tanvirul Islam"
            },
            {
                "id": "http://arxiv.org/abs/2512.17452v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17452v2",
                "title": "Learning What to Write: Write-Gated KV for Efficient Long-Context Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning What to Write: Write-Gated KV for Efficient Long-Context Inference"
                },
                "updated": "2025-12-22T10:23:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    10,
                    23,
                    36,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17452v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17452v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\\times$ prefill and 1.89-2.56$\\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLM inference is bottlenecked by the quadratic attention complexity and linear KV cache growth. Prior approaches mitigate this via post-hoc selection or eviction but overlook the root inefficiency: indiscriminate writing to persistent memory. In this paper, we formalize KV cache management as a causal system of three primitives: KV Admission, Selection, and Eviction. We instantiate KV Admission via Write-Gated KV, a lightweight mechanism that learns to predict token utility before it enters the cache. By filtering out low-utility states early to maintain a compact global cache alongside a sliding local cache, Write-Gated KV reduces memory usage by 46-57% and delivers 3.03-3.45$\\times$ prefill and 1.89-2.56$\\times$ decode speedups on Llama model with negligible accuracy loss, all while remaining compatible with FlashAttention and paged-KV systems. These results demonstrate that learning what to write, is a principled and practical recipe for efficient long-context inference. Code is available at https://github.com/EMCLab-Sinica/WG-KV ."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T11:08:58Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    11,
                    8,
                    58,
                    4,
                    353,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yen-Chieh Huang"
                    },
                    {
                        "name": "Pi-Cheng Hsiu"
                    },
                    {
                        "name": "Rui Fang"
                    },
                    {
                        "name": "Ming-Syan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Syan Chen"
                },
                "author": "Ming-Syan Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.19206v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19206v1",
                "title": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixKVQ: Query-Aware Mixed-Precision KV Cache Quantization for Long-Context Reasoning"
                },
                "updated": "2025-12-22T09:44:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    44,
                    26,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19206v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long Chain-of-Thought (CoT) reasoning has significantly advanced the capabilities of Large Language Models (LLMs), but this progress is accompanied by substantial memory and latency overhead from the extensive Key-Value (KV) cache. Although KV cache quantization is a promising compression technique, existing low-bit quantization methods often exhibit severe performance degradation on complex reasoning tasks. Fixed-precision quantization struggles to handle outlier channels in the key cache, while current mixed-precision strategies fail to accurately identify components requiring high-precision representation. We find that an effective low-bit KV cache quantization strategy must consider two factors: a key channel's intrinsic quantization difficulty and its relevance to the query. Based on this insight, we propose MixKVQ, a novel plug-and-play method that introduces a lightweight, query-aware algorithm to identify and preserve critical key channels that need higher precision, while applying per-token quantization for value cache. Experiments on complex reasoning datasets demonstrate that our approach significantly outperforms existing low-bit methods, achieving performance comparable to a full-precision baseline at a substantially reduced memory footprint."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T09:44:26Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    9,
                    44,
                    26,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Ziqian Zeng"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Huiping Zhuang"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen"
            },
            {
                "id": "http://arxiv.org/abs/2510.11496v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.11496v3",
                "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model"
                },
                "updated": "2025-12-22T04:40:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    4,
                    40,
                    4,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.11496v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.11496v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient task adaptation and model compression during mobile-side deployment of AndesVL. Moreover, utilizing our cache eviction algorithm -- OKV -- along with customized speculative decoding and compression strategies, we achieve a 6.7x peak decoding speedup ratio, up to 30.9% memory reduction, and 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We release all models on https://huggingface.co/OPPOer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside a Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient task adaptation and model compression during mobile-side deployment of AndesVL. Moreover, utilizing our cache eviction algorithm -- OKV -- along with customized speculative decoding and compression strategies, we achieve a 6.7x peak decoding speedup ratio, up to 30.9% memory reduction, and 1.8 bits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We release all models on https://huggingface.co/OPPOer."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-13T15:04:38Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    4,
                    38,
                    0,
                    286,
                    0
                ],
                "arxiv_comment": "Tech report of OPPO AndesVL Team",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhiwei Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yafei Liu"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Ruichen Wang"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Qi Qi"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Dongze Hao"
                    },
                    {
                        "name": "Quanlong Zheng"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haobo Ji"
                    },
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Zhitong Zheng"
                    },
                    {
                        "name": "Zhenyi Lin"
                    },
                    {
                        "name": "Haolin Deng"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Xiaojie Yin"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Liankai Cai"
                    },
                    {
                        "name": "Haijing Liu"
                    },
                    {
                        "name": "Yuqing Qiu"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Zixian Li"
                    },
                    {
                        "name": "Chi Xie"
                    },
                    {
                        "name": "Huafei Li"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Chuangchuang Wang"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Zhiguang Zhu"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Wenmei Gao"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Qin Xie"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Lu"
                },
                "author": "Haonan Lu"
            },
            {
                "id": "http://arxiv.org/abs/2511.01385v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.01385v2",
                "title": "Memory-Efficient Training with In-Place FFT Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training with In-Place FFT Implementation"
                },
                "updated": "2025-12-22T02:25:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    22,
                    2,
                    25,
                    29,
                    0,
                    356,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.01385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.01385v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fast Fourier Transforms (FFT) are widely used to reduce memory and computational costs in deep learning. However, existing implementations, including standard FFT and real FFT (rFFT), cannot achieve true in-place computation. In particular, rFFT maps an input of size n to a complex output of size n/2+1, causing dimensional mismatch and requiring additional memory allocation. We propose the first real-domain, fully in-place FFT framework (rdFFT) that preserves input-output memory space consistency. By leveraging butterfly operation symmetry and conjugate properties in the frequency domain, we design an implicit complex encoding scheme that eliminates intermediate cache usage entirely. Experiments on multiple natural language understanding tasks demonstrate the method effectiveness in reducing training memory cost, offering a promising direction for frequency-domain lightweight adaptation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Fourier Transforms (FFT) are widely used to reduce memory and computational costs in deep learning. However, existing implementations, including standard FFT and real FFT (rFFT), cannot achieve true in-place computation. In particular, rFFT maps an input of size n to a complex output of size n/2+1, causing dimensional mismatch and requiring additional memory allocation. We propose the first real-domain, fully in-place FFT framework (rdFFT) that preserves input-output memory space consistency. By leveraging butterfly operation symmetry and conjugate properties in the frequency domain, we design an implicit complex encoding scheme that eliminates intermediate cache usage entirely. Experiments on multiple natural language understanding tasks demonstrate the method effectiveness in reducing training memory cost, offering a promising direction for frequency-domain lightweight adaptation."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-03T09:36:11Z",
                "published_parsed": [
                    2025,
                    11,
                    3,
                    9,
                    36,
                    11,
                    0,
                    307,
                    0
                ],
                "arxiv_comment": "Accepted at NeurIPS 2025. Version 2 adds links to the ongoing PyTorch upstreaming discussion",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xinyu Ding"
                    },
                    {
                        "name": "Bangtian Liu"
                    },
                    {
                        "name": "Siyu Liao"
                    },
                    {
                        "name": "Zhongfeng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongfeng Wang"
                },
                "author": "Zhongfeng Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.18674v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18674v1",
                "title": "Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remoe: Towards Efficient and Low-Cost MoE Inference in Serverless Computing"
                },
                "updated": "2025-12-21T10:27:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    21,
                    10,
                    27,
                    50,
                    6,
                    355,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18674v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture-of-Experts (MoE) has become a dominant architecture in large language models (LLMs) due to its ability to scale model capacity via sparse expert activation. Meanwhile, serverless computing, with its elasticity and pay-per-use billing, is well-suited for deploying MoEs with bursty workloads. However, the large number of experts in MoE models incurs high inference costs due to memory-intensive parameter caching. These costs are difficult to mitigate via simple model partitioning due to input-dependent expert activation. To address these issues, we propose Remoe, a heterogeneous MoE inference system tailored for serverless computing. Remoe assigns non-expert modules to GPUs and expert modules to CPUs, and further offloads infrequently activated experts to separate serverless functions to reduce memory overhead and enable parallel execution. We incorporate three key techniques: (1) a Similar Prompts Searching (SPS) algorithm to predict expert activation patterns based on semantic similarity of inputs; (2) a Main Model Pre-allocation (MMP) algorithm to ensure service-level objectives (SLOs) via worst-case memory estimation; and (3) a joint memory and replica optimization framework leveraging Lagrangian duality and the Longest Processing Time (LPT) algorithm. We implement Remoe on Kubernetes and evaluate it across multiple LLM benchmarks. Experimental results show that Remoe reduces inference cost by up to 57% and cold start latency by 47% compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has become a dominant architecture in large language models (LLMs) due to its ability to scale model capacity via sparse expert activation. Meanwhile, serverless computing, with its elasticity and pay-per-use billing, is well-suited for deploying MoEs with bursty workloads. However, the large number of experts in MoE models incurs high inference costs due to memory-intensive parameter caching. These costs are difficult to mitigate via simple model partitioning due to input-dependent expert activation. To address these issues, we propose Remoe, a heterogeneous MoE inference system tailored for serverless computing. Remoe assigns non-expert modules to GPUs and expert modules to CPUs, and further offloads infrequently activated experts to separate serverless functions to reduce memory overhead and enable parallel execution. We incorporate three key techniques: (1) a Similar Prompts Searching (SPS) algorithm to predict expert activation patterns based on semantic similarity of inputs; (2) a Main Model Pre-allocation (MMP) algorithm to ensure service-level objectives (SLOs) via worst-case memory estimation; and (3) a joint memory and replica optimization framework leveraging Lagrangian duality and the Longest Processing Time (LPT) algorithm. We implement Remoe on Kubernetes and evaluate it across multiple LLM benchmarks. Experimental results show that Remoe reduces inference cost by up to 57% and cold start latency by 47% compared to state-of-the-art baselines."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-21T10:27:50Z",
                "published_parsed": [
                    2025,
                    12,
                    21,
                    10,
                    27,
                    50,
                    6,
                    355,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Wentao Liu"
                    },
                    {
                        "name": "Yuhao Hu"
                    },
                    {
                        "name": "Ruiting Zhou"
                    },
                    {
                        "name": "Baochun Li"
                    },
                    {
                        "name": "Ne Wang"
                    }
                ],
                "author_detail": {
                    "name": "Ne Wang"
                },
                "author": "Ne Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.01802v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01802v4",
                "title": "JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford"
                },
                "updated": "2025-12-20T14:13:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    14,
                    13,
                    44,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01802v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01802v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from -31 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=10,000 nodes and 55,000,000 edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from -31 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=10,000 nodes and 55,000,000 edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential."
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T15:35:53Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    15,
                    35,
                    53,
                    0,
                    335,
                    0
                ],
                "arxiv_comment": "with editor,25 pages",
                "arxiv_primary_category": {
                    "term": "cs.DS"
                },
                "authors": [
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Xi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xi Chen"
                },
                "author": "Xi Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.18345v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18345v1",
                "title": "Theodosian: A Deep Dive into Memory-Hierarchy-Centric FHE Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theodosian: A Deep Dive into Memory-Hierarchy-Centric FHE Acceleration"
                },
                "updated": "2025-12-20T12:18:29Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    12,
                    18,
                    29,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18345v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18345v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Fully homomorphic encryption (FHE) enables secure computation on encrypted data, mitigating privacy concerns in cloud and edge environments. However, due to its high compute and memory demands, extensive acceleration research has been pursued across diverse hardware platforms, especially GPUs. In this paper, we perform a microarchitectural analysis of CKKS, a popular FHE scheme, on modern GPUs. We focus on on-chip cache behavior, and show that the dominant kernels remain bound by memory bandwidth despite a high-bandwidth L2 cache, exposing a persistent memory wall. We further discover that the overall CKKS pipeline throughput is constrained by low per-kernel hardware utilization, caused by insufficient intra-kernel parallelism. Motivated by these findings, we introduce Theodosian, a set of complementary, memory-aware optimizations that improve cache efficiency and reduce runtime overheads. Our approach delivers consistent speedups across various CKKS workloads. On an RTX 5090, we reduce the bootstrapping latency for 32,768 complex numbers to 15.2ms with Theodosian, and further to 12.8ms with additional algorithmic optimizations, establishing new state-of-the-art GPU performance to the best of our knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully homomorphic encryption (FHE) enables secure computation on encrypted data, mitigating privacy concerns in cloud and edge environments. However, due to its high compute and memory demands, extensive acceleration research has been pursued across diverse hardware platforms, especially GPUs. In this paper, we perform a microarchitectural analysis of CKKS, a popular FHE scheme, on modern GPUs. We focus on on-chip cache behavior, and show that the dominant kernels remain bound by memory bandwidth despite a high-bandwidth L2 cache, exposing a persistent memory wall. We further discover that the overall CKKS pipeline throughput is constrained by low per-kernel hardware utilization, caused by insufficient intra-kernel parallelism. Motivated by these findings, we introduce Theodosian, a set of complementary, memory-aware optimizations that improve cache efficiency and reduce runtime overheads. Our approach delivers consistent speedups across various CKKS workloads. On an RTX 5090, we reduce the bootstrapping latency for 32,768 complex numbers to 15.2ms with Theodosian, and further to 12.8ms with additional algorithmic optimizations, establishing new state-of-the-art GPU performance to the best of our knowledge."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-20T12:18:29Z",
                "published_parsed": [
                    2025,
                    12,
                    20,
                    12,
                    18,
                    29,
                    5,
                    354,
                    0
                ],
                "arxiv_comment": "11 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Wonseok Choi"
                    },
                    {
                        "name": "Hyunah Yu"
                    },
                    {
                        "name": "Jongmin Kim"
                    },
                    {
                        "name": "Hyesung Ji"
                    },
                    {
                        "name": "Jaiyoung Park"
                    },
                    {
                        "name": "Jung Ho Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Jung Ho Ahn"
                },
                "author": "Jung Ho Ahn"
            },
            {
                "id": "http://arxiv.org/abs/2512.18337v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18337v1",
                "title": "Towards Efficient Agents: A Co-Design of Inference Architecture and System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Efficient Agents: A Co-Design of Inference Architecture and System"
                },
                "updated": "2025-12-20T12:06:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    12,
                    6,
                    13,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18337v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-20T12:06:13Z",
                "published_parsed": [
                    2025,
                    12,
                    20,
                    12,
                    6,
                    13,
                    5,
                    354,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Weizhe Lin"
                    },
                    {
                        "name": "Hui-Ling Zhen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xian Wang"
                    },
                    {
                        "name": "Renxi Liu"
                    },
                    {
                        "name": "Hanting Chen"
                    },
                    {
                        "name": "Wangze Zhang"
                    },
                    {
                        "name": "Chuansai Zhou"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Zhiyuan Yang"
                    },
                    {
                        "name": "Xiaosong Li"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Zhenhua Dong"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.18300v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18300v1",
                "title": "BARD: Reducing Write Latency of DDR5 Memory by Exploiting Bank-Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BARD: Reducing Write Latency of DDR5 Memory by Exploiting Bank-Parallelism"
                },
                "updated": "2025-12-20T10:11:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    10,
                    11,
                    47,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18300v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper studies the impact of DRAM writes on DDR5-based system. To efficiently perform DRAM writes, modern systems buffer write requests and try to complete multiple write operations whenever the DRAM mode is switched from read to write. When the DRAM system is performing writes, it is not available to service read requests, thus increasing read latency and reducing performance. We observe that, given the presence of on-die ECC in DDR5 devices, the time to perform a write operation varies significantly: from 1x (for writes to banks of different bankgroups) to 6x (for writes to banks within the same bankgroup) to 24x (for conflicting requests to the same bank). If we can orchestrate the write stream to favor write requests that incur lower latency, then we can reduce the stall time from DRAM writes and improve performance. However, for current systems, the write stream is dictated by the cache replacement policy, which makes eviction decisions without being aware of the variable latency of DRAM writes. The key insight of our work is to improve performance by modifying the cache replacement policy to increase bank-parallelism of DRAM writes.\n  Our paper proposes {\\em BARD (Bank-Aware Replacement Decisions)}, which modifies the cache replacement policy to favor dirty lines that belong to banks without pending writes. We analyze two variants of BARD: BARD-E (Eviction-based), which changes the eviction policy to evict low-cost dirty lines, and BARD-C (Cleansing-Based), which proactively cleans low-cost dirty lines without modifying the eviction decisions. We develop a hybrid policy (BARD-H), which uses a selective combination of both eviction and writeback. Our evaluations across workloads from SPEC2017, LIGRA, STREAM, and Google server traces show that BARD-H improves performance by 4.3\\% on average and up-to 8.5\\%. BARD requires only 8 bytes of SRAM per LLC slice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies the impact of DRAM writes on DDR5-based system. To efficiently perform DRAM writes, modern systems buffer write requests and try to complete multiple write operations whenever the DRAM mode is switched from read to write. When the DRAM system is performing writes, it is not available to service read requests, thus increasing read latency and reducing performance. We observe that, given the presence of on-die ECC in DDR5 devices, the time to perform a write operation varies significantly: from 1x (for writes to banks of different bankgroups) to 6x (for writes to banks within the same bankgroup) to 24x (for conflicting requests to the same bank). If we can orchestrate the write stream to favor write requests that incur lower latency, then we can reduce the stall time from DRAM writes and improve performance. However, for current systems, the write stream is dictated by the cache replacement policy, which makes eviction decisions without being aware of the variable latency of DRAM writes. The key insight of our work is to improve performance by modifying the cache replacement policy to increase bank-parallelism of DRAM writes.\n  Our paper proposes {\\em BARD (Bank-Aware Replacement Decisions)}, which modifies the cache replacement policy to favor dirty lines that belong to banks without pending writes. We analyze two variants of BARD: BARD-E (Eviction-based), which changes the eviction policy to evict low-cost dirty lines, and BARD-C (Cleansing-Based), which proactively cleans low-cost dirty lines without modifying the eviction decisions. We develop a hybrid policy (BARD-H), which uses a selective combination of both eviction and writeback. Our evaluations across workloads from SPEC2017, LIGRA, STREAM, and Google server traces show that BARD-H improves performance by 4.3\\% on average and up-to 8.5\\%. BARD requires only 8 bytes of SRAM per LLC slice."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-20T10:11:47Z",
                "published_parsed": [
                    2025,
                    12,
                    20,
                    10,
                    11,
                    47,
                    5,
                    354,
                    0
                ],
                "arxiv_comment": "Accepted to HPCA 2026",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Suhas Vittal"
                    },
                    {
                        "name": "Moinuddin Qureshi"
                    }
                ],
                "author_detail": {
                    "name": "Moinuddin Qureshi"
                },
                "author": "Moinuddin Qureshi"
            },
            {
                "id": "http://arxiv.org/abs/2512.18194v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18194v1",
                "title": "TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TraCT: Disaggregated LLM Serving with CXL Shared Memory KV Cache at Rack-Scale"
                },
                "updated": "2025-12-20T03:42:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    3,
                    42,
                    19,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18194v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Disaggregated LLM serving improves resource efficiency by separating the compute-intensive prefill phase from the latency-critical decode phase. However, this architecture introduces a fundamental bottleneck: key/value (KV) tensors generated during prefill must be transferred to decode workers, and existing systems rely on RDMA-based network paths for this exchange. As model sizes and context lengths increase, KV transfer dominates both time-to-first-token (TTFT) and peak throughput, and remains highly sensitive to network contention even when prefix reuse is high. This paper presents TraCT, a rack-scale LLM serving system that uses CXL shared memory as both a KV-transfer substrate and a rack-wide prefix-aware KV cache. TraCT enables GPUs to write and read KV blocks directly through CXL load/store and DMA operations, eliminating the NIC hop that constrains existing disaggregated pipelines. However, to realize this design, multiple new challenges such as synchronization, consistency, and data management on non-coherent CXL memory need to be addressed. TraCT proposes various software solutions such as the two-tier inter-node synchronization mechanism to address these challenges. We implement TraCT on the Dynamo LLM inference framework and show that, across static and synthetic workloads, TraCT reduces average TTFT by up to 9.8x, lowers P99 latency by up to 6.2x, and improves peak throughput by up to 1.6x compared to RDMA and DRAM-based caching baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated LLM serving improves resource efficiency by separating the compute-intensive prefill phase from the latency-critical decode phase. However, this architecture introduces a fundamental bottleneck: key/value (KV) tensors generated during prefill must be transferred to decode workers, and existing systems rely on RDMA-based network paths for this exchange. As model sizes and context lengths increase, KV transfer dominates both time-to-first-token (TTFT) and peak throughput, and remains highly sensitive to network contention even when prefix reuse is high. This paper presents TraCT, a rack-scale LLM serving system that uses CXL shared memory as both a KV-transfer substrate and a rack-wide prefix-aware KV cache. TraCT enables GPUs to write and read KV blocks directly through CXL load/store and DMA operations, eliminating the NIC hop that constrains existing disaggregated pipelines. However, to realize this design, multiple new challenges such as synchronization, consistency, and data management on non-coherent CXL memory need to be addressed. TraCT proposes various software solutions such as the two-tier inter-node synchronization mechanism to address these challenges. We implement TraCT on the Dynamo LLM inference framework and show that, across static and synthetic workloads, TraCT reduces average TTFT by up to 9.8x, lowers P99 latency by up to 6.2x, and improves peak throughput by up to 1.6x compared to RDMA and DRAM-based caching baselines."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-20T03:42:19Z",
                "published_parsed": [
                    2025,
                    12,
                    20,
                    3,
                    42,
                    19,
                    5,
                    354,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Dongha Yoon"
                    },
                    {
                        "name": "Younghoon Min"
                    },
                    {
                        "name": "Hoshik Kim"
                    },
                    {
                        "name": "Sam H. Noh"
                    },
                    {
                        "name": "Jongryool Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jongryool Kim"
                },
                "author": "Jongryool Kim"
            },
            {
                "id": "http://arxiv.org/abs/2511.02230v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.02230v2",
                "title": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuum: Efficient and Robust Multi-Turn LLM Agent Scheduling with KV Cache Time-to-Live"
                },
                "updated": "2025-12-20T01:17:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    20,
                    1,
                    17,
                    3,
                    5,
                    354,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.02230v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.02230v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "KV cache management is essential for efficient LLM inference. To maximize utilization, existing inference engines evict finished requests' KV cache if new requests are waiting. This policy breaks for agentic workloads, which interleave LLM calls with tools, introducing pauses that prevent effective KV reuse across turns. Since some tool calls have much shorter durations than human response multi-turn chatbot, it would be promising to retain the KV cache in during these tools. However, there are many challenges. First, we need to consider both the potential cost of recomputation or reloading (if CPU offloading enabled) and the increasing queueing delays after eviction from GPU. Second, due to the internal variance of tool call durations, we need the method to remain robust under limited predictability of tool call durations.\n  We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by introducing time-to-live mechanism for KV cache retaining. For LLM request that generates a tool call, Continuum selectively pins the KV cache in GPU memory with a time-to-live value determined by considering both the reload cost and ordering preserve benefit of retaining KV cache. Moreover, when the TTL expires, the KV cache can be automatically evicted to free up GPU memory, providing robust performance under edge cases. When combined with program-level first-come-first-serve, Continuum preserves multi-turn continuity, and reduces delay for complex agentic workflows. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B shows that Continuum significantly improves the average job completion times and its improvement scales with turn number increase. We release a preview version at: https://github.com/Hanchenli/vllm-continuum",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache management is essential for efficient LLM inference. To maximize utilization, existing inference engines evict finished requests' KV cache if new requests are waiting. This policy breaks for agentic workloads, which interleave LLM calls with tools, introducing pauses that prevent effective KV reuse across turns. Since some tool calls have much shorter durations than human response multi-turn chatbot, it would be promising to retain the KV cache in during these tools. However, there are many challenges. First, we need to consider both the potential cost of recomputation or reloading (if CPU offloading enabled) and the increasing queueing delays after eviction from GPU. Second, due to the internal variance of tool call durations, we need the method to remain robust under limited predictability of tool call durations.\n  We present Continuum, a serving system to optimize job completion time for multi-turn agent workloads by introducing time-to-live mechanism for KV cache retaining. For LLM request that generates a tool call, Continuum selectively pins the KV cache in GPU memory with a time-to-live value determined by considering both the reload cost and ordering preserve benefit of retaining KV cache. Moreover, when the TTL expires, the KV cache can be automatically evicted to free up GPU memory, providing robust performance under edge cases. When combined with program-level first-come-first-serve, Continuum preserves multi-turn continuity, and reduces delay for complex agentic workflows. Our evaluation on real-world agentic workloads (SWE-Bench and BFCL) with Llama-3.1 8B/70B shows that Continuum significantly improves the average job completion times and its improvement scales with turn number increase. We release a preview version at: https://github.com/Hanchenli/vllm-continuum"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-04T03:43:05Z",
                "published_parsed": [
                    2025,
                    11,
                    4,
                    3,
                    43,
                    5,
                    1,
                    308,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS"
                },
                "authors": [
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Qiuyang Mang"
                    },
                    {
                        "name": "Runyuan He"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Huanzhi Mao"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Hangrui Zhou"
                    },
                    {
                        "name": "Alvin Cheung"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica"
            },
            {
                "id": "http://arxiv.org/abs/2512.18117v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18117v1",
                "title": "Factorized Transport Alignment for Multimodal and Multiview E-commerce Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Factorized Transport Alignment for Multimodal and Multiview E-commerce Representation Learning"
                },
                "updated": "2025-12-19T22:50:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    19,
                    22,
                    50,
                    49,
                    4,
                    353,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18117v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid growth of e-commerce requires robust multimodal representations that capture diverse signals from user-generated listings. Existing vision-language models (VLMs) typically align titles with primary images, i.e., single-view, but overlook non-primary images and auxiliary textual views that provide critical semantics in open marketplaces such as Etsy or Poshmark. To this end, we propose a framework that unifies multimodal and multi-view learning through Factorized Transport, a lightweight approximation of optimal transport, designed for scalability and deployment efficiency. During training, the method emphasizes primary views while stochastically sampling auxiliary ones, reducing training cost from quadratic in the number of views to constant per item. At inference, all views are fused into a single cached embedding, preserving the efficiency of two-tower retrieval with no additional online overhead. On an industrial dataset of 1M product listings and 0.3M interactions, our approach delivers consistent improvements in cross-view and query-to-item retrieval, achieving up to +7.9% Recall@500 over strong multimodal baselines. Overall, our framework bridges scalability with optimal transport-based learning, making multi-view pretraining practical for large-scale e-commerce search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of e-commerce requires robust multimodal representations that capture diverse signals from user-generated listings. Existing vision-language models (VLMs) typically align titles with primary images, i.e., single-view, but overlook non-primary images and auxiliary textual views that provide critical semantics in open marketplaces such as Etsy or Poshmark. To this end, we propose a framework that unifies multimodal and multi-view learning through Factorized Transport, a lightweight approximation of optimal transport, designed for scalability and deployment efficiency. During training, the method emphasizes primary views while stochastically sampling auxiliary ones, reducing training cost from quadratic in the number of views to constant per item. At inference, all views are fused into a single cached embedding, preserving the efficiency of two-tower retrieval with no additional online overhead. On an industrial dataset of 1M product listings and 0.3M interactions, our approach delivers consistent improvements in cross-view and query-to-item retrieval, achieving up to +7.9% Recall@500 over strong multimodal baselines. Overall, our framework bridges scalability with optimal transport-based learning, making multi-view pretraining practical for large-scale e-commerce search."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T22:50:49Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    22,
                    50,
                    49,
                    4,
                    353,
                    0
                ],
                "arxiv_comment": "Accepted by WSDM'26",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Xiwen Chen"
                    },
                    {
                        "name": "Yen-Chieh Lien"
                    },
                    {
                        "name": "Susan Liu"
                    },
                    {
                        "name": "María Castaños"
                    },
                    {
                        "name": "Abolfazl Razi"
                    },
                    {
                        "name": "Xiaoting Zhao"
                    },
                    {
                        "name": "Congzhe Su"
                    }
                ],
                "author_detail": {
                    "name": "Congzhe Su"
                },
                "author": "Congzhe Su"
            },
            {
                "id": "http://arxiv.org/abs/2512.17661v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17661v1",
                "title": "Vidarc: Embodied Video Diffusion Model for Closed-loop Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vidarc: Embodied Video Diffusion Model for Closed-loop Control"
                },
                "updated": "2025-12-19T15:04:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    19,
                    15,
                    4,
                    24,
                    4,
                    353,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17661v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robotic arm manipulation in data-scarce settings is a highly challenging task due to the complex embodiment dynamics and diverse contexts. Recent video-based approaches have shown great promise in capturing and transferring the temporal and physical interactions by pre-training on Internet-scale video data. However, such methods are often not optimized for the embodiment-specific closed-loop control, typically suffering from high latency and insufficient grounding. In this paper, we present Vidarc (Video Diffusion for Action Reasoning and Closed-loop Control), a novel autoregressive embodied video diffusion approach augmented by a masked inverse dynamics model. By grounding video predictions with action-relevant masks and incorporating real-time feedback through cached autoregressive generation, Vidarc achieves fast, accurate closed-loop control. Pre-trained on one million cross-embodiment episodes, Vidarc surpasses state-of-the-art baselines, achieving at least a 15% higher success rate in real-world deployment and a 91% reduction in latency. We also highlight its robust generalization and error correction capabilities across previously unseen robotic platforms."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T15:04:24Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    15,
                    4,
                    24,
                    4,
                    353,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Yao Feng"
                    },
                    {
                        "name": "Chendong Xiang"
                    },
                    {
                        "name": "Xinyi Mao"
                    },
                    {
                        "name": "Hengkai Tan"
                    },
                    {
                        "name": "Zuyue Zhang"
                    },
                    {
                        "name": "Shuhe Huang"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Haitian Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Jun Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhu"
                },
                "author": "Jun Zhu"
            },
            {
                "id": "http://arxiv.org/abs/2512.11529v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11529v2",
                "title": "xGR: Efficient Generative Recommendation Serving at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xGR: Efficient Generative Recommendation Serving at Scale"
                },
                "updated": "2025-12-19T11:20:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    19,
                    11,
                    20,
                    16,
                    4,
                    353,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11529v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11529v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommendation system delivers substantial economic benefits by providing personalized predictions. Generative recommendation (GR) integrates LLMs to enhance the understanding of long user-item sequences. Despite employing attention-based architectures, GR's workload differs markedly from that of LLM serving. GR typically processes long prompt while producing short, fixed-length outputs, yet the computational cost of each decode phase is especially high due to the large beam width. In addition, since the beam search involves a vast item space, the sorting overhead becomes particularly time-consuming. We propose xGR, a GR-oriented serving system that meets strict low-latency requirements under highconcurrency scenarios. First, xGR unifies the processing of prefill and decode phases through staged computation and separated KV cache. Second, xGR enables early sorting termination and mask-based item filtering with data structure reuse. Third, xGR reconstructs the overall pipeline to exploit multilevel overlap and multi-stream parallelism. Our experiments with real-world recommendation service datasets demonstrate that xGR achieves at least 3.49x throughput compared to the state-of-the-art baseline under strict latency constraints."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T12:59:38Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    12,
                    59,
                    38,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Qingxiao Sun"
                    },
                    {
                        "name": "Tongxuan Liu"
                    },
                    {
                        "name": "Shen Zhang"
                    },
                    {
                        "name": "Siyu Wu"
                    },
                    {
                        "name": "Peijun Yang"
                    },
                    {
                        "name": "Haotian Liang"
                    },
                    {
                        "name": "Menxin Li"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Zhiwei Liang"
                    },
                    {
                        "name": "Ziyi Ren"
                    },
                    {
                        "name": "Minchao Zhang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Depei Qian"
                    },
                    {
                        "name": "Hailong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Hailong Yang"
                },
                "author": "Hailong Yang"
            },
            {
                "id": "http://arxiv.org/abs/2512.12284v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12284v2",
                "title": "V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval"
                },
                "updated": "2025-12-19T08:02:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    19,
                    8,
                    2,
                    44,
                    4,
                    353,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12284v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12284v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.\n  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices."
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-13T11:02:04Z",
                "published_parsed": [
                    2025,
                    12,
                    13,
                    11,
                    2,
                    4,
                    5,
                    347,
                    0
                ],
                "arxiv_comment": "14 pages, 20 figures, conference",
                "arxiv_primary_category": {
                    "term": "eess.IV"
                },
                "authors": [
                    {
                        "name": "Donghyuk Kim"
                    },
                    {
                        "name": "Sejeong Yang"
                    },
                    {
                        "name": "Wonjin Shin"
                    },
                    {
                        "name": "Joo-Young Kim"
                    }
                ],
                "author_detail": {
                    "name": "Joo-Young Kim"
                },
                "author": "Joo-Young Kim"
            },
            {
                "id": "http://arxiv.org/abs/2512.17298v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17298v1",
                "title": "ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProCache: Constraint-Aware Feature Caching with Selective Computation for Diffusion Transformer Acceleration"
                },
                "updated": "2025-12-19T07:27:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    19,
                    7,
                    27,
                    19,
                    4,
                    353,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17298v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have achieved state-of-the-art performance in generative modeling, yet their high computational cost hinders real-time deployment. While feature caching offers a promising training-free acceleration solution by exploiting temporal redundancy, existing methods suffer from two key limitations: (1) uniform caching intervals fail to align with the non-uniform temporal dynamics of DiT, and (2) naive feature reuse with excessively large caching intervals can lead to severe error accumulation. In this work, we analyze the evolution of DiT features during denoising and reveal that both feature changes and error propagation are highly time- and depth-varying. Motivated by this, we propose ProCache, a training-free dynamic feature caching framework that addresses these issues via two core components: (i) a constraint-aware caching pattern search module that generates non-uniform activation schedules through offline constrained sampling, tailored to the model's temporal characteristics; and (ii) a selective computation module that selectively computes within deep blocks and high-importance tokens for cached segments to mitigate error accumulation with minimal overhead. Extensive experiments on PixArt-alpha and DiT demonstrate that ProCache achieves up to 1.96x and 2.90x acceleration with negligible quality degradation, significantly outperforming prior caching-based methods."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T07:27:19Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    7,
                    27,
                    19,
                    4,
                    353,
                    0
                ],
                "arxiv_comment": "Accepted for poster presentation at AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Fanpu Cao"
                    },
                    {
                        "name": "Yaofo Chen"
                    },
                    {
                        "name": "Zeng You"
                    },
                    {
                        "name": "Wei Luo"
                    },
                    {
                        "name": "Cen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Cen Chen"
                },
                "author": "Cen Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.17970v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17970v1",
                "title": "CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeGEMM: A Codebook-Centric Approach to Efficient GEMM in Quantized LLMs"
                },
                "updated": "2025-12-19T06:16:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    19,
                    6,
                    16,
                    32,
                    4,
                    353,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17970v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Weight-only quantization is widely used to mitigate the memory-bound nature of LLM inference. Codebook-based methods extend this trend by achieving strong accuracy in the extremely low-bit regime (e.g., 2-bit). However, current kernels rely on dequantization, which repeatedly fetches centroids and reconstructs weights, incurring substantial latency and cache pressure. We present CodeGEMM, a codebook-centric GEMM kernel that replaces dequantization with precomputed inner products between centroids and activations stored in a lightweight Psumbook. At inference, code indices directly gather these partial sums, eliminating per-element lookups and reducing the on-chip footprint. The kernel supports the systematic exploration of latency-memory-accuracy trade-offs under a unified implementation. On Llama-3 models, CodeGEMM delivers 1.83x (8B) and 8.93x (70B) speedups in the 2-bit configuration compared to state-of-the-art codebook-based quantization at comparable accuracy and further improves computing efficiency and memory subsystem utilization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weight-only quantization is widely used to mitigate the memory-bound nature of LLM inference. Codebook-based methods extend this trend by achieving strong accuracy in the extremely low-bit regime (e.g., 2-bit). However, current kernels rely on dequantization, which repeatedly fetches centroids and reconstructs weights, incurring substantial latency and cache pressure. We present CodeGEMM, a codebook-centric GEMM kernel that replaces dequantization with precomputed inner products between centroids and activations stored in a lightweight Psumbook. At inference, code indices directly gather these partial sums, eliminating per-element lookups and reducing the on-chip footprint. The kernel supports the systematic exploration of latency-memory-accuracy trade-offs under a unified implementation. On Llama-3 models, CodeGEMM delivers 1.83x (8B) and 8.93x (70B) speedups in the 2-bit configuration compared to state-of-the-art codebook-based quantization at comparable accuracy and further improves computing efficiency and memory subsystem utilization."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T06:16:32Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    6,
                    16,
                    32,
                    4,
                    353,
                    0
                ],
                "arxiv_comment": "NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Gunho Park"
                    },
                    {
                        "name": "Jeongin Bae"
                    },
                    {
                        "name": "Byeongwook Kim"
                    },
                    {
                        "name": "Baeseong park"
                    },
                    {
                        "name": "Jiwon Ryu"
                    },
                    {
                        "name": "Hoseung Kim"
                    },
                    {
                        "name": "Se Jung Kwon"
                    },
                    {
                        "name": "Dongsoo Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongsoo Lee"
                },
                "author": "Dongsoo Lee"
            },
            {
                "id": "http://arxiv.org/abs/2507.20116v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.20116v2",
                "title": "PeerSync: Accelerating Containerized Service Delivery at the Network Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PeerSync: Accelerating Containerized Service Delivery at the Network Edge"
                },
                "updated": "2025-12-19T02:09:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    19,
                    2,
                    9,
                    53,
                    4,
                    353,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.20116v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.20116v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Efficient container image distribution is crucial for enabling machine learning inference at the network edge, where resource limitations and dynamic network conditions create significant challenges. In this paper, we present PeerSync, a decentralized P2P-based system designed to optimize image distribution in edge environments. PeerSync employs a popularity- and network-aware download engine that dynamically adapts to content popularity and real-time network conditions. PeerSync further integrates automated tracker election for rapid peer discovery and dynamic cache management for efficient storage utilization. We implement PeerSync with 8000+ lines of Rust code and test its performance extensively on both large-scale Docker-based emulations and physical edge devices. Experimental results show that PeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$, and 1.28$\\times$ compared to the Baseline solution, Dragonfly, and Kraken, respectively, while significantly reducing cross-network traffic by 90.72% under congested and varying network conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient container image distribution is crucial for enabling machine learning inference at the network edge, where resource limitations and dynamic network conditions create significant challenges. In this paper, we present PeerSync, a decentralized P2P-based system designed to optimize image distribution in edge environments. PeerSync employs a popularity- and network-aware download engine that dynamically adapts to content popularity and real-time network conditions. PeerSync further integrates automated tracker election for rapid peer discovery and dynamic cache management for efficient storage utilization. We implement PeerSync with 8000+ lines of Rust code and test its performance extensively on both large-scale Docker-based emulations and physical edge devices. Experimental results show that PeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$, and 1.28$\\times$ compared to the Baseline solution, Dragonfly, and Kraken, respectively, while significantly reducing cross-network traffic by 90.72% under congested and varying network conditions."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-27T03:45:07Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Yinuo Deng"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Dongjing Wang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Wenzhuo Qian"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Schahram Dustdar"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng"
            },
            {
                "id": "http://arxiv.org/abs/2512.16843v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16843v1",
                "title": "LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMCache: Layer-Wise Caching Strategies for Accelerated Reuse in Transformer Inference"
                },
                "updated": "2025-12-18T18:18:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    18,
                    18,
                    57,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16843v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16843v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based language models have achieved remarkable performance across a wide range of tasks, yet their high inference latency poses a significant challenge for real-timeand large-scale deployment. While existing caching mechanisms,such as token-level key-value caches, offer speedups in autore-gressive decoding, they are limited in scope and applicability. In this paper, we present LLMCache, a novel layer-wise caching framework that accelerates transformer inference by reusing intermediate activations based on semantic similarity of input sequences. Unlike prior work, LLMCache is model-agnostic,operates across both encoder and decoder architectures, and supports caching at arbitrary transformer layers. We introduce a lightweight fingerprinting mechanism for matching seman-tically similar inputs and propose adaptive eviction strategies to manage cache staleness. Experiments on BERT and GPT-2 across SQuAD, WikiText-103, and OpenBookQA show up to 3.1 X speedup in inference time with <0.5% accuracy degradation. Our results highlight LLMCache as a practical and general-purpose solution for optimizing transformer inference in real-world applications"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T18:18:57Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    18,
                    18,
                    57,
                    3,
                    352,
                    0
                ],
                "arxiv_comment": "Accepted and presented at 13th IEEE International Conference on Intelligent Systems and Embedded Design (ISED-2025)",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Harsh Vardhan Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Harsh Vardhan Bansal"
                },
                "author": "Harsh Vardhan Bansal"
            },
            {
                "id": "http://arxiv.org/abs/2512.16822v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16822v1",
                "title": "MEPIC: Memory Efficient Position Independent Caching for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEPIC: Memory Efficient Position Independent Caching for LLM Serving"
                },
                "updated": "2025-12-18T18:04:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    18,
                    4,
                    1,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16822v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16822v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.\n  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern LLM applications such as deep-research assistants, coding agents, and Retrieval-Augmented Generation (RAG) systems, repeatedly process long prompt histories containing shared document or code chunks, creating significant pressure on the Key Value (KV) cache, which must operate within limited memory while sustaining high throughput and low latency. Prefix caching partially alleviates some of these costs by reusing KV cache for previously processed tokens, but limited by strict prefix matching. Position-independent caching (PIC) enables chunk-level reuse at arbitrary positions, but requires selective recomputation and positional-encoding (PE) adjustments. However, because these operations vary across queries, KV for the same chunk diverges across requests. Moreover, without page alignment, chunk KV layouts diverge in memory, preventing page sharing. These issues result in only modest HBM savings even when many requests reuse the same content.\n  We present MEPIC, a memory-efficient PIC system that enables chunk KV reuse across positions, requests, and batches. MEPIC aligns chunk KV to paged storage, shifts recomputation from token- to block-level so only the first block is request-specific, removes positional encodings via Rotary Position Embedding (RoPE) fusion in the attention kernel, and makes remaining blocks fully shareable. These techniques eliminate most duplicate chunk KV in HBM, reducing usage by up to 2x over state-of-the-art PIC at comparable latency and accuracy, and up to 5x for long prompts, without any model changes."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T18:04:01Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    18,
                    4,
                    1,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Zahra Yousefijamarani"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Rongzhi Gu"
                    },
                    {
                        "name": "Bai Xiaolong"
                    },
                    {
                        "name": "Shan Yizhou"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Wang Lan"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan"
            },
            {
                "id": "http://arxiv.org/abs/2512.16650v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16650v1",
                "title": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix Probing: Lightweight Harmful Content Detection for Large Language Models"
                },
                "updated": "2025-12-18T15:22:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    15,
                    22,
                    14,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16650v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models often face a three-way trade-off among detection accuracy, inference latency, and deployment cost when used in real-world safety-sensitive applications. This paper introduces Prefix Probing, a black-box harmful content detection method that compares the conditional log-probabilities of \"agreement/execution\" versus \"refusal/safety\" opening prefixes and leverages prefix caching to reduce detection overhead to near first-token latency. During inference, the method requires only a single log-probability computation over the probe prefixes to produce a harmfulness score and apply a threshold, without invoking any additional models or multi-stage inference. To further enhance the discriminative power of the prefixes, we design an efficient prefix construction algorithm that automatically discovers highly informative prefixes, substantially improving detection performance. Extensive experiments demonstrate that Prefix Probing achieves detection effectiveness comparable to mainstream external safety models while incurring only minimal computational cost and requiring no extra model deployment, highlighting its strong practicality and efficiency."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T15:22:14Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    15,
                    22,
                    14,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jirui Yang"
                    },
                    {
                        "name": "Hengqi Guo"
                    },
                    {
                        "name": "Zhihui Lu"
                    },
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Yuansen Zhang"
                    },
                    {
                        "name": "Shijing Hu"
                    },
                    {
                        "name": "Qiang Duan"
                    },
                    {
                        "name": "Yinggui Wang"
                    },
                    {
                        "name": "Tao Wei"
                    }
                ],
                "author_detail": {
                    "name": "Tao Wei"
                },
                "author": "Tao Wei"
            },
            {
                "id": "http://arxiv.org/abs/2512.16615v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16615v1",
                "title": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trainable Log-linear Sparse Attention for Efficient Diffusion Transformers"
                },
                "updated": "2025-12-18T14:53:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    14,
                    53,
                    12,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16615v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) set the state of the art in visual generation, yet their quadratic self-attention cost fundamentally limits scaling to long token sequences. Recent Top-K sparse attention approaches reduce the computation of DiTs by compressing tokens into block-wise representation and selecting a small set of relevant key blocks, but still suffer from (i) quadratic selection cost on compressed tokens and (ii) increasing K required to maintain model quality as sequences grow. We identify that their inefficiency is due to the single-level design, as a single coarse level is insufficient to represent the global structure. In this paper, we introduce Log-linear Sparse Attention (LLSA), a trainable sparse attention mechanism for extremely long token sequences that reduces both selection and attention costs from quadratic to log-linear complexity by utilizing a hierarchical structure. LLSA performs hierarchical Top-K selection, progressively adopting sparse Top-K selection with the indices found at the previous level, and introduces a Hierarchical KV Enrichment mechanism that preserves global context while using fewer tokens of different granularity during attention computation. To support efficient training, we develop a high-performance GPU implementation that uses only sparse indices for both the forward and backward passes, eliminating the need for dense attention masks. We evaluate LLSA on high-resolution pixel-space image generation without using patchification and VAE encoding. LLSA accelerates attention inference by 28.27x and DiT training by 6.09x on 256x256 pixel token sequences, while maintaining generation quality. The results demonstrate that LLSA offers a promising direction for training long-sequence DiTs efficiently. Code is available at: https://github.com/SingleZombie/LLSA"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T14:53:12Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    14,
                    53,
                    12,
                    3,
                    352,
                    0
                ],
                "arxiv_comment": "Code is available at: https://github.com/SingleZombie/LLSA",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Zeqi Xiao"
                    },
                    {
                        "name": "Tianyi Wei"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xingang Pan"
                    }
                ],
                "author_detail": {
                    "name": "Xingang Pan"
                },
                "author": "Xingang Pan"
            },
            {
                "id": "http://arxiv.org/abs/2512.04677v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.04677v3",
                "title": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Live Avatar: Streaming Real-time Audio-Driven Avatar Generation with Infinite Length"
                },
                "updated": "2025-12-18T13:02:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    13,
                    2,
                    34,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.04677v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.04677v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing diffusion-based video generation methods are fundamentally constrained by sequential computation and long-horizon inconsistency, limiting their practical adoption in real-time, streaming audio-driven avatar synthesis. We present Live Avatar, an algorithm-system co-designed framework that enables efficient, high-fidelity, and infinite-length avatar generation using a 14-billion-parameter diffusion model. Our approach introduces Timestep-forcing Pipeline Parallelism (TPP), a distributed inference paradigm that pipelines denoising steps across multiple GPUs, effectively breaking the autoregressive bottleneck and ensuring stable, low-latency real-time streaming. To further enhance temporal consistency and mitigate identity drift and color artifacts, we propose the Rolling Sink Frame Mechanism (RSFM), which maintains sequence fidelity by dynamically recalibrating appearance using a cached reference image. Additionally, we leverage Self-Forcing Distribution Matching Distillation to facilitate causal, streamable adaptation of large-scale models without sacrificing visual quality. Live Avatar demonstrates state-of-the-art performance, reaching 20 FPS end-to-end generation on 5 H800 GPUs, and, to the best of our knowledge, is the first to achieve practical, real-time, high-fidelity avatar generation at this scale. Our work establishes a new paradigm for deploying advanced diffusion models in industrial long-form video synthesis applications."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-04T11:11:24Z",
                "published_parsed": [
                    2025,
                    12,
                    4,
                    11,
                    11,
                    24,
                    3,
                    338,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yubo Huang"
                    },
                    {
                        "name": "Hailong Guo"
                    },
                    {
                        "name": "Fangtai Wu"
                    },
                    {
                        "name": "Shifeng Zhang"
                    },
                    {
                        "name": "Shijie Huang"
                    },
                    {
                        "name": "Qijun Gan"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Sirui Zhao"
                    },
                    {
                        "name": "Enhong Chen"
                    },
                    {
                        "name": "Jiaming Liu"
                    },
                    {
                        "name": "Steven Hoi"
                    }
                ],
                "author_detail": {
                    "name": "Steven Hoi"
                },
                "author": "Steven Hoi"
            },
            {
                "id": "http://arxiv.org/abs/2512.16473v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16473v1",
                "title": "Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient CPU-GPU Collaborative Inference for MoE-based LLMs on Memory-Limited Systems"
                },
                "updated": "2025-12-18T12:45:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    12,
                    45,
                    52,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16473v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive results across various tasks, yet their high computational demands pose deployment challenges, especially on consumer-grade hardware. Mixture of Experts (MoE) models provide an efficient solution through selective activation of parameter subsets, which reduces computation requirements. Despite this efficiency, state-of-the-art MoE models still require substantial memory beyond typical consumer GPU capacities. Traditional offloading methods that transfer model weights between CPU and GPU introduce latency, limiting inference performance. This paper presents a novel CPU-GPU collaborative inference framework that incorporates an expert caching mechanism on the GPU to reduce data transfer requirements and enable faster inference through cache hits. Computations are offloaded to CPU for efficient cache miss handling, which benefits from CPU multithreading optimizations. The evaluations of our framework demonstrate performance improvements and highlight the potential of CPU-GPU collaboration to maximize hardware utilization for single-request inference scenarios on consumer-grade systems. The implementation of our framework is available at https://github.com/elsa-lab/MoE-CPU-GPU-Collaborative-Inference."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T12:45:52Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    12,
                    45,
                    52,
                    3,
                    352,
                    0
                ],
                "arxiv_comment": "7 pages, 6 figures, to be published in ASP-DAC 2026",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "En-Ming Huang"
                    },
                    {
                        "name": "Li-Shang Lin"
                    },
                    {
                        "name": "Chun-Yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Yi Lee"
                },
                "author": "Chun-Yi Lee"
            },
            {
                "id": "http://arxiv.org/abs/2512.16148v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16148v1",
                "title": "FlexKV: Flexible Index Offloading for Memory-Disaggregated Key-Value Store",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexKV: Flexible Index Offloading for Memory-Disaggregated Key-Value Store"
                },
                "updated": "2025-12-18T04:03:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    4,
                    3,
                    1,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16148v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Disaggregated memory (DM) is a promising data center architecture that decouples CPU and memory into independent resource pools to improve resource utilization. Building on DM, memory-disaggregated key-value (KV) stores are adopted to efficiently manage remote data. Unfortunately, existing approaches suffer from poor performance due to two critical issues: 1) the overdependence on one-sided atomic operations in index processing, and 2) the constrained efficiency in compute-side caches. To address these issues, we propose FlexKV, a memory-disaggregated KV store with index proxying. Our key idea is to dynamically offload the index to compute nodes, leveraging their powerful CPUs to accelerate index processing and maintain high-performance compute-side caches. Three challenges have to be addressed to enable efficient index proxying on DM, i.e., the load imbalance across compute nodes, the limited memory of compute nodes, and the expensive cache coherence overhead. FlexKV proposes: 1) a rank-aware hotness detection algorithm to continuously balance index load across compute nodes, 2) a two-level CN memory optimization scheme to efficiently utilize compute node memory, and 3) an RPC-aggregated cache management mechanism to reduce cache coherence overhead. The experimental results show that FlexKV improves throughput by up to 2.94$\\times$ and reduces latency by up to 85.2%, compared with the state-of-the-art memory-disaggregated KV stores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated memory (DM) is a promising data center architecture that decouples CPU and memory into independent resource pools to improve resource utilization. Building on DM, memory-disaggregated key-value (KV) stores are adopted to efficiently manage remote data. Unfortunately, existing approaches suffer from poor performance due to two critical issues: 1) the overdependence on one-sided atomic operations in index processing, and 2) the constrained efficiency in compute-side caches. To address these issues, we propose FlexKV, a memory-disaggregated KV store with index proxying. Our key idea is to dynamically offload the index to compute nodes, leveraging their powerful CPUs to accelerate index processing and maintain high-performance compute-side caches. Three challenges have to be addressed to enable efficient index proxying on DM, i.e., the load imbalance across compute nodes, the limited memory of compute nodes, and the expensive cache coherence overhead. FlexKV proposes: 1) a rank-aware hotness detection algorithm to continuously balance index load across compute nodes, 2) a two-level CN memory optimization scheme to efficiently utilize compute node memory, and 3) an RPC-aggregated cache management mechanism to reduce cache coherence overhead. The experimental results show that FlexKV improves throughput by up to 2.94$\\times$ and reduces latency by up to 85.2%, compared with the state-of-the-art memory-disaggregated KV stores."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T04:03:01Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    4,
                    3,
                    1,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Zhisheng Hu"
                    },
                    {
                        "name": "Jiacheng Shen"
                    },
                    {
                        "name": "Ming-Chang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Chang Yang"
                },
                "author": "Ming-Chang Yang"
            },
            {
                "id": "http://arxiv.org/abs/2512.12977v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12977v2",
                "title": "VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLCache: Computing 2% Vision Tokens and Reusing 98% for Vision-Language Inference"
                },
                "updated": "2025-12-18T02:59:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    2,
                    59,
                    27,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12977v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12977v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. We develop an experimental implementation of the proposed VLCache pipeline based on SGLang, enabling significantly faster inference in practical deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents VLCache, a cache reuse framework that exploits both Key-Value (KV) cache and encoder cache from prior multimodal inputs to eliminate costly recomputation when the same multimodal inputs recur. Unlike previous heuristic approaches, we formally identify the cumulative reuse error effect and demonstrate how to minimize the non-prefix cache reuse error effectively. We further analyze the varying importance of model layers and propose a dynamic, layer-aware recomputation strategy to balance accuracy and efficiency. Experimental results show that VLCache achieves an accuracy on par with full recomputation, while requiring only 2-5% of the tokens to compute, yielding 1.2x-16x TTFT speedups. We develop an experimental implementation of the proposed VLCache pipeline based on SGLang, enabling significantly faster inference in practical deployments."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T04:45:47Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    4,
                    45,
                    47,
                    0,
                    349,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Shengling Qin"
                    },
                    {
                        "name": "Hao Yu"
                    },
                    {
                        "name": "Chenxin Wu"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Yizhong Cao"
                    },
                    {
                        "name": "Zhengyang Zhuge"
                    },
                    {
                        "name": "Yuxin Zhou"
                    },
                    {
                        "name": "Wentao Yao"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zhengheng Wang"
                    },
                    {
                        "name": "Shuai Bai"
                    },
                    {
                        "name": "Jianwei Zhang"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin"
            },
            {
                "id": "http://arxiv.org/abs/2512.16056v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16056v1",
                "title": "MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MultiPath Transfer Engine: Breaking GPU and Host-Memory Bandwidth Bottlenecks in LLM Services"
                },
                "updated": "2025-12-18T00:45:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    18,
                    0,
                    45,
                    0,
                    3,
                    352,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16056v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The limited bandwidth of PCIe has emerged as the critical bottleneck for large language model (LLM) performance, such as prefix cache fetching and model switching. Although intra-server multipath data transfer between GPU and host memory is theoretically possible, heterogeneous protocols such as PCIe and NVLink currently limit the bandwidth between host memory and GPUs to that of a single PICe link. This limitation resuals in underutilized intra-server bandwidth. To address this issue, we propose Multipath Memory Access (MMA), a scheme that, to the best of our knowledge, is the first to enalbe efficient multipath data transfer between GPU and host memory. MMA supports seamless deployment via dynamic library injection, enabling LLM applications to benefit from MMA without requiring any code modification. In our testbed, MMA significantly improves the data transfer bandwidth between the GPU and memory, achieving a peak bandwidth of 245 GB/s-representing a 4.62x speedup compared to the natice single-path bandwidth. End-to-end evaluations demonstrate that MMA reduces the time-to-first-token (TTFT) for LLM serving by 1.14x to 2.38x and decreases model-switching latency in vLLM's sleep mode by 1.12x to 2.48x."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T00:45:00Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    0,
                    45,
                    0,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Lingfeng Tang"
                    },
                    {
                        "name": "Daoping Zhang"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Peihao Huang"
                    },
                    {
                        "name": "Feng Jin"
                    },
                    {
                        "name": "Chengguang Xu"
                    },
                    {
                        "name": "Yuxin Chen"
                    },
                    {
                        "name": "Feiqiang Sun"
                    },
                    {
                        "name": "Guo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guo Chen"
                },
                "author": "Guo Chen"
            },
            {
                "id": "http://arxiv.org/abs/2504.16112v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.16112v2",
                "title": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HPU: High-Bandwidth Processing Unit for Scalable, Cost-effective LLM Inference via GPU Co-processing"
                },
                "updated": "2025-12-17T21:40:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    21,
                    40,
                    17,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.16112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.16112v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The attention layer, a core component of Transformer-based LLMs, brings out inefficiencies in current GPU systems due to its low operational intensity and the substantial memory requirements of KV caches. We propose a High-bandwidth Processing Unit (HPU), a memoryintensive co-processor that enhances GPU resource utilization during large-batched LLM inference. By offloading memory-bound operations, the HPU allows the GPU to focus on compute-intensive tasks, increasing overall efficiency. Also, the HPU, as an add-on card, scales out to accommodate surging memory demands driven by large batch sizes and extended sequence lengths. In this paper, we show the HPU prototype implemented with PCIe-based FPGA cards mounted on a GPU system. Our novel GPU-HPU heterogeneous system demonstrates up to 4.1x performance gains and 4.6x energy efficiency improvements over a GPUonly system, providing scalability without increasing the number of GPUs."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-18T03:31:08Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    3,
                    31,
                    8,
                    4,
                    108,
                    0
                ],
                "arxiv_comment": "6 pages",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Myunghyun Rhee"
                    },
                    {
                        "name": "Joonseop Sim"
                    },
                    {
                        "name": "Taeyoung Ahn"
                    },
                    {
                        "name": "Seungyong Lee"
                    },
                    {
                        "name": "Daegun Yoon"
                    },
                    {
                        "name": "Euiseok Kim"
                    },
                    {
                        "name": "Kyoung Park"
                    },
                    {
                        "name": "Youngpyo Joo"
                    },
                    {
                        "name": "Hoshik Kim"
                    }
                ],
                "author_detail": {
                    "name": "Hoshik Kim"
                },
                "author": "Hoshik Kim"
            },
            {
                "id": "http://arxiv.org/abs/2512.15713v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15713v1",
                "title": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models"
                },
                "updated": "2025-12-17T18:59:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    18,
                    59,
                    55,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15713v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T18:59:55Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    18,
                    59,
                    55,
                    2,
                    351,
                    0
                ],
                "arxiv_comment": "11 pages, 5 figures, conference or other essential info",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Lunbin Zeng"
                    },
                    {
                        "name": "Jingfeng Yao"
                    },
                    {
                        "name": "Bencheng Liao"
                    },
                    {
                        "name": "Hongyuan Tao"
                    },
                    {
                        "name": "Wenyu Liu"
                    },
                    {
                        "name": "Xinggang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggang Wang"
                },
                "author": "Xinggang Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.15705v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15705v1",
                "title": "Dynamic Rebatching for Efficient Early-Exit Inference with DREX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Rebatching for Efficient Early-Exit Inference with DREX"
                },
                "updated": "2025-12-17T18:55:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    18,
                    55,
                    45,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15705v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model's layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T18:55:45Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    18,
                    55,
                    45,
                    2,
                    351,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Xuting Liu"
                    },
                    {
                        "name": "Daniel Alexander"
                    },
                    {
                        "name": "Siva Kesava Reddy Kakarla"
                    },
                    {
                        "name": "Behnaz Arzani"
                    },
                    {
                        "name": "Vincent Liu"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Liu"
                },
                "author": "Vincent Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.15834v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15834v1",
                "title": "Optimizing Agentic Language Model Inference via Speculative Tool Calls",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Agentic Language Model Inference via Speculative Tool Calls"
                },
                "updated": "2025-12-17T18:22:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    18,
                    22,
                    44,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15834v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new \"tool cache\" API endpoint to enable LM providers to easily adopt these optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) are becoming increasingly dependent on external tools. LM-based agentic frameworks frequently interact with their environment via such tools to search files, run code, call APIs, etc. Further, modern reasoning-based LMs use tools such as web search and Python code execution to enhance their reasoning capabilities. While tools greatly improve the capabilities of LMs, they also introduce performance bottlenecks during the inference process. In this paper, we introduce novel systems optimizations to address such performance bottlenecks by speculating tool calls and forcing sequences to remain resident in the inference engine to minimize overheads. Our optimizations lead to throughput improvements of several hundred tokens per second when hosting inference for LM agents. We provide a theoretical analysis of our algorithms to provide insights into speculation configurations that will yield the best performance. Further, we recommend a new \"tool cache\" API endpoint to enable LM providers to easily adopt these optimizations."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T18:22:44Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    18,
                    22,
                    44,
                    2,
                    351,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Daniel Nichols"
                    },
                    {
                        "name": "Prajwal Singhania"
                    },
                    {
                        "name": "Charles Jekel"
                    },
                    {
                        "name": "Abhinav Bhatele"
                    },
                    {
                        "name": "Harshitha Menon"
                    }
                ],
                "author_detail": {
                    "name": "Harshitha Menon"
                },
                "author": "Harshitha Menon"
            },
            {
                "id": "http://arxiv.org/abs/2512.15595v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15595v1",
                "title": "Optimizing Bloom Filters for Modern GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Bloom Filters for Modern GPU Architectures"
                },
                "updated": "2025-12-17T17:01:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    17,
                    1,
                    55,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15595v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.\n  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\\times$ ($15.4\\times$) for bulk filter lookup (construction), respectively, achieving above $92\\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bloom filters are a fundamental data structure for approximate membership queries, with applications ranging from data analytics to databases and genomics. Several variants have been proposed to accommodate parallel architectures. GPUs, with massive thread-level parallelism and high-bandwidth memory, are a natural fit for accelerating these Bloom filter variants potentially to billions of operations per second. Although CPU-optimized implementations have been well studied, GPU designs remain underexplored. We close this gap by exploring the design space on GPUs along three dimensions: vectorization, thread cooperation, and compute latency.\n  Our evaluation shows that the combination of these optimization points strongly affects throughput, with the largest gains achieved when the filter fits within the GPU's cache domain. We examine how the hardware responds to different parameter configurations and relate these observations to measured performance trends. Crucially, our optimized design overcomes the conventional trade-off between speed and precision, delivering the throughput typically restricted to high-error variants while maintaining the superior accuracy of high-precision configurations. At iso error rate, the proposed method outperforms the state-of-the-art by $11.35\\times$ ($15.4\\times$) for bulk filter lookup (construction), respectively, achieving above $92\\%$ of the practical speed-of-light across a wide range of configurations on a B200 GPU. We propose a modular CUDA/C++ implementation, which will be openly available soon."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T17:01:55Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    17,
                    1,
                    55,
                    2,
                    351,
                    0
                ],
                "arxiv_comment": "13 pages, 12 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Daniel Jünger"
                    },
                    {
                        "name": "Kevin Kristensen"
                    },
                    {
                        "name": "Yunsong Wang"
                    },
                    {
                        "name": "Xiangyao Yu"
                    },
                    {
                        "name": "Bertil Schmidt"
                    }
                ],
                "author_detail": {
                    "name": "Bertil Schmidt"
                },
                "author": "Bertil Schmidt"
            },
            {
                "id": "http://arxiv.org/abs/2512.15576v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15576v1",
                "title": "Radial electric field and density fluctuations measured by Doppler reflectometry during the post-pellet enhanced confinement phase in W7-X",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radial electric field and density fluctuations measured by Doppler reflectometry during the post-pellet enhanced confinement phase in W7-X"
                },
                "updated": "2025-12-17T16:34:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    16,
                    34,
                    7,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15576v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1088/1741-4326/abddee",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Radial profiles of density fluctuations and radial electric field, $E_r$, have been measured using Doppler reflectometry during the post-pellet enhanced confinement phase achieved, under different heating power levels and magnetic configurations, along the 2018 W7-X experimental campaign. A pronounced $E_r$-well is measured with local values as high as -40 kV/m in the radial range $ρ\\sim 0.7-0.8$ during the post-pellet enhanced confinement phase. The maximum $E_r$ intensity scales with both plasma density and Electron Cyclotron Heating (ECH) power level following a similar trend as the plasma energy content. A good agreement is found when the experimental $E_r$ profiles are compared to simulations carried out using the neoclassical codes DKES and KNOSOS. The density fluctuation level decreases from the plasma edge toward the plasma core and the drop is more pronounced in the post-pellet enhanced confinement phase than in reference gas fuelled plasmas. Besides, in the post-pellet phase, the density fluctuation level is lower in the high iota magnetic configuration than in the standard one. In order to discriminate whether this difference is related to the differences in the plasma profiles or in the stability properties of the two configurations, gyrokinetic simulations have been carried out using the codes \\texttt{stella} and EUTERPE. The simulation results point to the plasma profile evolution after the pellet injection and the stabilization effect of the radial electric field profile as the dominant players in the stabilization of the plasma turbulence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radial profiles of density fluctuations and radial electric field, $E_r$, have been measured using Doppler reflectometry during the post-pellet enhanced confinement phase achieved, under different heating power levels and magnetic configurations, along the 2018 W7-X experimental campaign. A pronounced $E_r$-well is measured with local values as high as -40 kV/m in the radial range $ρ\\sim 0.7-0.8$ during the post-pellet enhanced confinement phase. The maximum $E_r$ intensity scales with both plasma density and Electron Cyclotron Heating (ECH) power level following a similar trend as the plasma energy content. A good agreement is found when the experimental $E_r$ profiles are compared to simulations carried out using the neoclassical codes DKES and KNOSOS. The density fluctuation level decreases from the plasma edge toward the plasma core and the drop is more pronounced in the post-pellet enhanced confinement phase than in reference gas fuelled plasmas. Besides, in the post-pellet phase, the density fluctuation level is lower in the high iota magnetic configuration than in the standard one. In order to discriminate whether this difference is related to the differences in the plasma profiles or in the stability properties of the two configurations, gyrokinetic simulations have been carried out using the codes \\texttt{stella} and EUTERPE. The simulation results point to the plasma profile evolution after the pellet injection and the stabilization effect of the radial electric field profile as the dominant players in the stabilization of the plasma turbulence."
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T16:34:07Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    16,
                    34,
                    7,
                    2,
                    351,
                    0
                ],
                "arxiv_comment": "10 figure",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph"
                },
                "arxiv_journal_ref": "Nucl. Fusion 61 (2021) 046008",
                "authors": [
                    {
                        "name": "T. Estrada"
                    },
                    {
                        "name": "D. Carralero"
                    },
                    {
                        "name": "T. Windisch"
                    },
                    {
                        "name": "E. Sánchez"
                    },
                    {
                        "name": "J. M. García-Regaña"
                    },
                    {
                        "name": "J. Martínez-Fernández"
                    },
                    {
                        "name": "A. de la Peña"
                    },
                    {
                        "name": "J. L. Velasco"
                    },
                    {
                        "name": "J. A. Alonso"
                    },
                    {
                        "name": "M. Beurskens"
                    },
                    {
                        "name": "S. Bozhenkov"
                    },
                    {
                        "name": "H. Damm"
                    },
                    {
                        "name": "G. Fuchert"
                    },
                    {
                        "name": "R. Kleiber"
                    },
                    {
                        "name": "N. Pablant"
                    },
                    {
                        "name": "E. Pasch"
                    }
                ],
                "author_detail": {
                    "name": "E. Pasch"
                },
                "author": "E. Pasch",
                "arxiv_doi": "10.1088/1741-4326/abddee"
            },
            {
                "id": "http://arxiv.org/abs/2512.15550v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15550v1",
                "title": "CTkvr: KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CTkvr: KV Cache Retrieval for Long-Context LLMs via Centroid then Token Indexing"
                },
                "updated": "2025-12-17T15:56:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    15,
                    56,
                    32,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15550v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are increasingly applied in long-context scenarios such as multi-turn conversations. However, long contexts pose significant challenges for inference efficiency, including high memory overhead from Key-Value (KV) cache and increased latency due to excessive memory accesses. Recent methods for dynamic KV selection struggle with trade-offs: block-level indexing degrades accuracy by retrieving irrelevant KV entries, while token-level indexing incurs high latency from inefficient retrieval mechanisms. In this paper, we propose CTKVR, a novel centroid-then-token KV retrieval scheme that addresses these limitations. CTKVR leverages a key observation: query vectors adjacent in position exhibit high similarity after Rotary Position Embedding (RoPE) and share most of their top-k KV cache entries. Based on this insight, CTKVR employs a two-stage retrieval strategy: lightweight centroids are precomputed during prefilling for centroid-grained indexing, followed by token-level refinement for precise KV retrieval. This approach balances retrieval efficiency and accuracy. To further enhance performance, we implement an optimized system for indexing construction and search using CPU-GPU co-execution. Experimentally, CTKVR achieves superior performance across multiple benchmarks with less than 1% accuracy degradation. Meanwhile, CTKVR delivers 3 times and 4 times throughput speedups on Llama-3-8B and Yi-9B at 96K context length across diverse GPU hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly applied in long-context scenarios such as multi-turn conversations. However, long contexts pose significant challenges for inference efficiency, including high memory overhead from Key-Value (KV) cache and increased latency due to excessive memory accesses. Recent methods for dynamic KV selection struggle with trade-offs: block-level indexing degrades accuracy by retrieving irrelevant KV entries, while token-level indexing incurs high latency from inefficient retrieval mechanisms. In this paper, we propose CTKVR, a novel centroid-then-token KV retrieval scheme that addresses these limitations. CTKVR leverages a key observation: query vectors adjacent in position exhibit high similarity after Rotary Position Embedding (RoPE) and share most of their top-k KV cache entries. Based on this insight, CTKVR employs a two-stage retrieval strategy: lightweight centroids are precomputed during prefilling for centroid-grained indexing, followed by token-level refinement for precise KV retrieval. This approach balances retrieval efficiency and accuracy. To further enhance performance, we implement an optimized system for indexing construction and search using CPU-GPU co-execution. Experimentally, CTKVR achieves superior performance across multiple benchmarks with less than 1% accuracy degradation. Meanwhile, CTKVR delivers 3 times and 4 times throughput speedups on Llama-3-8B and Yi-9B at 96K context length across diverse GPU hardware."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T15:56:32Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    15,
                    56,
                    32,
                    2,
                    351,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Kuan Lu"
                    },
                    {
                        "name": "Shuhang Lin"
                    },
                    {
                        "name": "Sai Wu"
                    },
                    {
                        "name": "Yichen Yao"
                    },
                    {
                        "name": "Junhan Yang"
                    },
                    {
                        "name": "Huan Li"
                    },
                    {
                        "name": "Wei Chu"
                    },
                    {
                        "name": "Xu Yinghui"
                    },
                    {
                        "name": "Yuan Qi"
                    },
                    {
                        "name": "Gang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Gang Chen"
                },
                "author": "Gang Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.15206v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15206v1",
                "title": "Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chorus: Harmonizing Context and Sensing Signals for Data-Free Model Customization in IoT"
                },
                "updated": "2025-12-17T08:56:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    8,
                    56,
                    21,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15206v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15206v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In real-world IoT applications, sensor data is usually collected under diverse and dynamic contextual conditions where factors such as sensor placements or ambient environments can significantly affect data patterns and downstream performance. Traditional domain adaptation or generalization methods often ignore such context information or use simplistic integration strategies, making them ineffective in handling unseen context shifts after deployment. In this paper, we propose Chorus, a context-aware, data-free model customization approach that adapts models to unseen deployment conditions without requiring target-domain data. The key idea is to learn effective context representations that capture their influence on sensor data patterns and to adaptively integrate them based on the degree of context shift. Specifically, Chorus first performs unsupervised cross-modal reconstruction between unlabeled sensor data and language-based context embeddings, while regularizing the context embedding space to learn robust, generalizable context representations. Then, it trains a lightweight gated head on limited labeled samples to dynamically balance sensor and context contributions-favoring context when sensor evidence is ambiguous and vice versa. To further reduce inference latency, Chorus employs a context-caching mechanism that reuses cached context representations and updates only upon detected context shifts. Experiments on IMU, speech, and WiFi sensing tasks under diverse context shifts show that Chorus outperforms state-of-the-art baselines by up to 11.3% in unseen contexts, while maintaining comparable latency on smartphone and edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real-world IoT applications, sensor data is usually collected under diverse and dynamic contextual conditions where factors such as sensor placements or ambient environments can significantly affect data patterns and downstream performance. Traditional domain adaptation or generalization methods often ignore such context information or use simplistic integration strategies, making them ineffective in handling unseen context shifts after deployment. In this paper, we propose Chorus, a context-aware, data-free model customization approach that adapts models to unseen deployment conditions without requiring target-domain data. The key idea is to learn effective context representations that capture their influence on sensor data patterns and to adaptively integrate them based on the degree of context shift. Specifically, Chorus first performs unsupervised cross-modal reconstruction between unlabeled sensor data and language-based context embeddings, while regularizing the context embedding space to learn robust, generalizable context representations. Then, it trains a lightweight gated head on limited labeled samples to dynamically balance sensor and context contributions-favoring context when sensor evidence is ambiguous and vice versa. To further reduce inference latency, Chorus employs a context-caching mechanism that reuses cached context representations and updates only upon detected context shifts. Experiments on IMU, speech, and WiFi sensing tasks under diverse context shifts show that Chorus outperforms state-of-the-art baselines by up to 11.3% in unseen contexts, while maintaining comparable latency on smartphone and edge devices."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T08:56:21Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    8,
                    56,
                    21,
                    2,
                    351,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Liyu Zhang"
                    },
                    {
                        "name": "Yejia Liu"
                    },
                    {
                        "name": "Kwun Ho Liu"
                    },
                    {
                        "name": "Runxi Huang"
                    },
                    {
                        "name": "Xiaomin Ouyang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaomin Ouyang"
                },
                "author": "Xiaomin Ouyang"
            },
            {
                "id": "http://arxiv.org/abs/2510.07318v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.07318v2",
                "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Hippocampus Networks for Efficient Long-Context Modeling"
                },
                "updated": "2025-12-17T07:08:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    7,
                    8,
                    49,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.07318v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.07318v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and GatedDeltaNet to augment open-weight LLMs. We also propose an efficient self-distillation training method where the base model's all parameters are frozen and only the parameters from AHNs are optimized. For inference, our method sets a default large sliding window size of 32k for attention, and AHNs activate only when the sequence length exceeds the 32k window, addressing the quadratic-complexity issue of attention that emerges at that scale. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence modeling faces a fundamental trade-off between the efficiency of compressive fixed-size memory in RNN-like models and the fidelity of lossless growing memory in attention-based Transformers. Inspired by the Multi-Store Model in cognitive science, we introduce a memory framework of artificial neural networks. Our method maintains a sliding window of the Transformer's KV cache as lossless short-term memory, while a learnable module termed Artificial Hippocampus Network (AHN) recurrently compresses out-of-window information into a fixed-size compact long-term memory. To validate this framework, we instantiate AHNs using modern RNN-like architectures, including Mamba2, DeltaNet, and GatedDeltaNet to augment open-weight LLMs. We also propose an efficient self-distillation training method where the base model's all parameters are frozen and only the parameters from AHNs are optimized. For inference, our method sets a default large sliding window size of 32k for attention, and AHNs activate only when the sequence length exceeds the 32k window, addressing the quadratic-complexity issue of attention that emerges at that scale. Extensive experiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate that AHN-augmented models consistently outperform sliding window baselines and achieve performance comparable or even superior to full-attention models, while substantially reducing computational and memory requirements. For instance, augmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5% and memory cache by 74.0%, while improving its average score on LV-Eval (128k sequence length) from 4.41 to 5.88. Code is available at: https://github.com/ByteDance-Seed/AHN."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-08T17:59:55Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    55,
                    2,
                    281,
                    0
                ],
                "arxiv_comment": "Code: https://github.com/ByteDance-Seed/AHN",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yunhao Fang"
                    },
                    {
                        "name": "Weihao Yu"
                    },
                    {
                        "name": "Shu Zhong"
                    },
                    {
                        "name": "Qinghao Ye"
                    },
                    {
                        "name": "Xuehan Xiong"
                    },
                    {
                        "name": "Lai Wei"
                    }
                ],
                "author_detail": {
                    "name": "Lai Wei"
                },
                "author": "Lai Wei"
            },
            {
                "id": "http://arxiv.org/abs/2509.25155v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.25155v2",
                "title": "Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context-Driven Performance Modeling for Causal Inference Operators on Neural Processing Units"
                },
                "updated": "2025-12-17T05:01:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    5,
                    1,
                    59,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.25155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.25155v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The proliferation of large language models has driven demand for long-context inference on resource-constrained edge platforms. However, deploying these models on Neural Processing Units (NPUs) presents significant challenges due to architectural mismatch: the quadratic complexity of standard attention conflicts with NPU memory and compute patterns. This paper presents a comprehensive performance analysis of causal inference operators on a modern NPU, benchmarking quadratic attention against sub-quadratic alternatives including structured state-space models and causal convolutions. Our analysis reveals a spectrum of critical bottlenecks: quadratic attention becomes severely memory-bound with catastrophic cache inefficiency, while sub-quadratic variants span from compute-bound on programmable vector cores to memory-bound by data movement. These findings provide essential insights for co-designing hardware-aware models and optimization strategies to enable efficient long-context inference on edge platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of large language models has driven demand for long-context inference on resource-constrained edge platforms. However, deploying these models on Neural Processing Units (NPUs) presents significant challenges due to architectural mismatch: the quadratic complexity of standard attention conflicts with NPU memory and compute patterns. This paper presents a comprehensive performance analysis of causal inference operators on a modern NPU, benchmarking quadratic attention against sub-quadratic alternatives including structured state-space models and causal convolutions. Our analysis reveals a spectrum of critical bottlenecks: quadratic attention becomes severely memory-bound with catastrophic cache inefficiency, while sub-quadratic variants span from compute-bound on programmable vector cores to memory-bound by data movement. These findings provide essential insights for co-designing hardware-aware models and optimization strategies to enable efficient long-context inference on edge platforms."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-29T17:55:43Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    17,
                    55,
                    43,
                    0,
                    272,
                    0
                ],
                "arxiv_comment": "IEEE HiPC 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Neelesh Gupta"
                    },
                    {
                        "name": "Rakshith Jayanth"
                    },
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna"
            },
            {
                "id": "http://arxiv.org/abs/2512.15016v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.15016v1",
                "title": "Uncovering hidden protein conformations with high bandwidth nanopore measurements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering hidden protein conformations with high bandwidth nanopore measurements"
                },
                "updated": "2025-12-17T02:14:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    2,
                    14,
                    4,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.15016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.15016v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Advanced nanopore measurements allow structural probing of molecules with high spatial and temporal resolution. We report high signal-to-noise, 1-10 MHz bandwidth, translocation measurements of the multi-state folding of heme protein cytochrome c in KCl solution through optimally designed silicon nitride pores of 2.3-3.3 nm diameter and 3.6-3.8 nm effective thickness, and an optimal concentration of a denaturant (Gdm-Cl). The pore diameter is slightly smaller than the protein size, forcing the protein to squeeze through the pore. The sufficiently large pore thickness allows enough time for protein probing at an applied field of approximately 250 kV/cm. Through Bayesian Information Criterion score analysis, current blockades reveal six distinct levels, attributed to specific protein states. We calculate the transition probabilities between the states and the conditional probabilities of the protein leaving the pore from each state. We validate the model by simulating events and comparing them to experimental data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced nanopore measurements allow structural probing of molecules with high spatial and temporal resolution. We report high signal-to-noise, 1-10 MHz bandwidth, translocation measurements of the multi-state folding of heme protein cytochrome c in KCl solution through optimally designed silicon nitride pores of 2.3-3.3 nm diameter and 3.6-3.8 nm effective thickness, and an optimal concentration of a denaturant (Gdm-Cl). The pore diameter is slightly smaller than the protein size, forcing the protein to squeeze through the pore. The sufficiently large pore thickness allows enough time for protein probing at an applied field of approximately 250 kV/cm. Through Bayesian Information Criterion score analysis, current blockades reveal six distinct levels, attributed to specific protein states. We calculate the transition probabilities between the states and the conditional probabilities of the protein leaving the pore from each state. We validate the model by simulating events and comparing them to experimental data."
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T02:14:04Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    2,
                    14,
                    4,
                    2,
                    351,
                    0
                ],
                "arxiv_comment": "19 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "physics.bio-ph"
                },
                "authors": [
                    {
                        "name": "Kyril Kavetsky"
                    },
                    {
                        "name": "Sabine Hong"
                    },
                    {
                        "name": "Chih-Yuan Lin"
                    },
                    {
                        "name": "Roger Yang"
                    },
                    {
                        "name": "Marija Drndic"
                    }
                ],
                "author_detail": {
                    "name": "Marija Drndic"
                },
                "author": "Marija Drndic"
            },
            {
                "id": "http://arxiv.org/abs/2512.14975v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14975v1",
                "title": "High voltage and electrode system for a cryogenic experiment to search for the neutron electric dipole moment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High voltage and electrode system for a cryogenic experiment to search for the neutron electric dipole moment"
                },
                "updated": "2025-12-17T00:04:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    17,
                    0,
                    4,
                    30,
                    2,
                    351,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14975v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14975v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The cryogenic approach to the search for the neutron electric dipole moment--performing the experiment in superfluid liquid helium--holds promise for a substantial increase in sensitivity, potentially enabling a sensitivity level of $10^{-28}$ e-cm. A crucial component in realizing such an experiment is the high voltage and electrode system capable of providing an electric field of 75 kV/cm. This, in turn, requires an electric potential of 635 kV to be applied to the high voltage electrode, while simultaneously satisfying other experimental constraints, such as those on heat load and magnetic noise requirements. This paper describes the outcome of a comprehensive development program addressing these challenges. It outlines the system requirements, discusses new insights into relevant physical phenomena, and details selected technical solutions with their corresponding experimental demonstrations and expected performance. The results collectively demonstrate the successful development of the necessary technology for the high-voltage and electrode system for this approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The cryogenic approach to the search for the neutron electric dipole moment--performing the experiment in superfluid liquid helium--holds promise for a substantial increase in sensitivity, potentially enabling a sensitivity level of $10^{-28}$ e-cm. A crucial component in realizing such an experiment is the high voltage and electrode system capable of providing an electric field of 75 kV/cm. This, in turn, requires an electric potential of 635 kV to be applied to the high voltage electrode, while simultaneously satisfying other experimental constraints, such as those on heat load and magnetic noise requirements. This paper describes the outcome of a comprehensive development program addressing these challenges. It outlines the system requirements, discusses new insights into relevant physical phenomena, and details selected technical solutions with their corresponding experimental demonstrations and expected performance. The results collectively demonstrate the successful development of the necessary technology for the high-voltage and electrode system for this approach."
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-17T00:04:30Z",
                "published_parsed": [
                    2025,
                    12,
                    17,
                    0,
                    4,
                    30,
                    2,
                    351,
                    0
                ],
                "arxiv_comment": "20 pages, 17 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "physics.ins-det"
                },
                "authors": [
                    {
                        "name": "M. A. Blatnik"
                    },
                    {
                        "name": "S. M. Clayton"
                    },
                    {
                        "name": "S. A. Currie"
                    },
                    {
                        "name": "B. W. Filippone"
                    },
                    {
                        "name": "M. Makela"
                    },
                    {
                        "name": "C. M. O'Shaughnessy"
                    },
                    {
                        "name": "N. S. Phan"
                    },
                    {
                        "name": "J. C. Ramsey"
                    },
                    {
                        "name": "G. V. Riley"
                    },
                    {
                        "name": "A. Roberts"
                    },
                    {
                        "name": "T. Sandborn"
                    },
                    {
                        "name": "T. J Schaub"
                    },
                    {
                        "name": "G. M. Seidel"
                    },
                    {
                        "name": "E. Smith"
                    },
                    {
                        "name": "I. L. Smythe"
                    },
                    {
                        "name": "J. Surbrook"
                    },
                    {
                        "name": "W. Wei"
                    },
                    {
                        "name": "W. Yao"
                    },
                    {
                        "name": "T. M. Ito"
                    }
                ],
                "author_detail": {
                    "name": "T. M. Ito"
                },
                "author": "T. M. Ito"
            },
            {
                "id": "http://arxiv.org/abs/2509.24832v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.24832v2",
                "title": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemShareKV: Efficient KVCache Sharing for Semantically Similar Prompts via Token-Level LSH Matching"
                },
                "updated": "2025-12-16T22:36:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    22,
                    36,
                    44,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.24832v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.24832v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models (LLMs) continue to scale, the memory footprint of key-value (KV) caches during inference has become a significant bottleneck. Existing approaches primarily focus on compressing KV caches within a single prompt or reusing shared prefixes or frequently ocurred text segments across prompts. However, such strategies are limited in scenarios where prompts are semantically similar but lexically different, which frequently occurs in tasks such as multi-document summarization and conversational agents. We propose \\textit{SemShareKV}, a KV cache sharing and compression framework that accelerates LLM inference by reusing KVCache in semantically similar prompts. Instead of relying on exact token matches, SemShareKV applies fuzzy token matching using locality-sensitive hashing (LSH) on token embeddings and incorporates Rotary Position Embedding (RoPE) to better preserve positional information. By selectively reusing relevant key-value pairs from a reference prompt's cache, SemShareKV reduces redundant computation while maintaining output quality. Experiments on diverse summarization datasets show up to 6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with negligible quality degradation. These results highlight the potential of semantic-aware cache sharing for efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to scale, the memory footprint of key-value (KV) caches during inference has become a significant bottleneck. Existing approaches primarily focus on compressing KV caches within a single prompt or reusing shared prefixes or frequently ocurred text segments across prompts. However, such strategies are limited in scenarios where prompts are semantically similar but lexically different, which frequently occurs in tasks such as multi-document summarization and conversational agents. We propose \\textit{SemShareKV}, a KV cache sharing and compression framework that accelerates LLM inference by reusing KVCache in semantically similar prompts. Instead of relying on exact token matches, SemShareKV applies fuzzy token matching using locality-sensitive hashing (LSH) on token embeddings and incorporates Rotary Position Embedding (RoPE) to better preserve positional information. By selectively reusing relevant key-value pairs from a reference prompt's cache, SemShareKV reduces redundant computation while maintaining output quality. Experiments on diverse summarization datasets show up to 6.25$\\times$ speedup and 42\\% lower GPU memory usage with 5k tokens input, with negligible quality degradation. These results highlight the potential of semantic-aware cache sharing for efficient LLM inference."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-29T14:16:13Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    16,
                    13,
                    0,
                    272,
                    0
                ],
                "arxiv_comment": "11 figures, 14pages",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Xinye Zhao"
                    },
                    {
                        "name": "Spyridon Mastorakis"
                    }
                ],
                "author_detail": {
                    "name": "Spyridon Mastorakis"
                },
                "author": "Spyridon Mastorakis"
            },
            {
                "id": "http://arxiv.org/abs/2512.14946v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14946v1",
                "title": "EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVICPRESS: Joint KV-Cache Compression and Eviction for Efficient LLM Serving"
                },
                "updated": "2025-12-16T22:21:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    22,
                    21,
                    55,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14946v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reusing KV cache is essential for high efficiency of Large Language Model (LLM) inference systems. With more LLM users, the KV cache footprint can easily exceed GPU memory capacity, so prior work has proposed to either evict KV cache to lower-tier storage devices, or compress KV cache so that more KV cache can be fit in the fast memory. However, prior work misses an important opportunity: jointly optimizing the eviction and compression decisions across all KV caches to minimize average generation latency without hurting quality.\n  We propose EVICPRESS, a KV-cache management system that applies lossy compression and adaptive eviction to KV cache across multiple storage tiers. Specifically, for each KV cache of a context, EVICPRESS considers the effect of compression and eviction of the KV cache on the average generation quality and delay across all contexts as a whole. To achieve this, EVICPRESS proposes a unified utility function that quantifies the effect of quality and delay of the lossy compression or eviction. To this end, EVICPRESS's profiling module periodically updates the utility function scores on all possible eviction-compression configurations for all contexts and places KV caches using a fast heuristic to rearrange KV caches on all storage tiers, with the goal of maximizing the utility function scores on each storage tier. Compared to the baselines that evict KV cache or compress KV cache, EVICPRESS achieves higher KV-cache hit rates on fast devices, i.e., lower delay, while preserving high generation quality by applying conservative compression to contexts that are sensitive to compression errors. Evaluation on 12 datasets and 5 models demonstrates that EVICPRESS achieves up to 2.19x faster time-to-first-token (TTFT) at equivalent generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reusing KV cache is essential for high efficiency of Large Language Model (LLM) inference systems. With more LLM users, the KV cache footprint can easily exceed GPU memory capacity, so prior work has proposed to either evict KV cache to lower-tier storage devices, or compress KV cache so that more KV cache can be fit in the fast memory. However, prior work misses an important opportunity: jointly optimizing the eviction and compression decisions across all KV caches to minimize average generation latency without hurting quality.\n  We propose EVICPRESS, a KV-cache management system that applies lossy compression and adaptive eviction to KV cache across multiple storage tiers. Specifically, for each KV cache of a context, EVICPRESS considers the effect of compression and eviction of the KV cache on the average generation quality and delay across all contexts as a whole. To achieve this, EVICPRESS proposes a unified utility function that quantifies the effect of quality and delay of the lossy compression or eviction. To this end, EVICPRESS's profiling module periodically updates the utility function scores on all possible eviction-compression configurations for all contexts and places KV caches using a fast heuristic to rearrange KV caches on all storage tiers, with the goal of maximizing the utility function scores on each storage tier. Compared to the baselines that evict KV cache or compress KV cache, EVICPRESS achieves higher KV-cache hit rates on fast devices, i.e., lower delay, while preserving high generation quality by applying conservative compression to contexts that are sensitive to compression errors. Evaluation on 12 datasets and 5 models demonstrates that EVICPRESS achieves up to 2.19x faster time-to-first-token (TTFT) at equivalent generation quality."
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T22:21:55Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    22,
                    21,
                    55,
                    1,
                    350,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS"
                },
                "authors": [
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Ganesh Ananthanarayanan"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang"
            },
            {
                "id": "http://arxiv.org/abs/2512.14866v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14866v1",
                "title": "Development of a Custom kV-amplitude, pressure-tolerant Radio-Frequency transmitter",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development of a Custom kV-amplitude, pressure-tolerant Radio-Frequency transmitter"
                },
                "updated": "2025-12-16T19:28:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    19,
                    28,
                    33,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14866v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current experiments seeking first-ever observation of Ultra-High Energy Neutrinos (UHEN) typically utilize radio frequency (RF) receiver antennas deployed in cold, radio-transparent polar ice, to measure the coherent RF signals resulting from neutrino interactions with ice molecules. Accurate calibration of the receiver response, sampling the full range of possible neutrino geometries, is necessary to estimate the energy and incoming direction of the incident neutrino. Herein, we detail the design and performance of a custom radio-frequency calibration transmitter, consisting of a battery-powered, kiloVolt-scale signal generator (`IDL' pulser) driving an antenna (South Pole UNiversity of Kansas antenna, or `SPUNK') capable of operating at pressures of 200 atmospheres. Performance was validated by lowering the transmitter into a borehole at the South Pole to a depth of 1740 m, yielding high Signal-to-Noise ratio signals at a distance of 5 km from the source.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current experiments seeking first-ever observation of Ultra-High Energy Neutrinos (UHEN) typically utilize radio frequency (RF) receiver antennas deployed in cold, radio-transparent polar ice, to measure the coherent RF signals resulting from neutrino interactions with ice molecules. Accurate calibration of the receiver response, sampling the full range of possible neutrino geometries, is necessary to estimate the energy and incoming direction of the incident neutrino. Herein, we detail the design and performance of a custom radio-frequency calibration transmitter, consisting of a battery-powered, kiloVolt-scale signal generator (`IDL' pulser) driving an antenna (South Pole UNiversity of Kansas antenna, or `SPUNK') capable of operating at pressures of 200 atmospheres. Performance was validated by lowering the transmitter into a borehole at the South Pole to a depth of 1740 m, yielding high Signal-to-Noise ratio signals at a distance of 5 km from the source."
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T19:28:33Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    19,
                    28,
                    33,
                    1,
                    350,
                    0
                ],
                "arxiv_comment": "To be submitted to Nucl. Instr. and Methods",
                "arxiv_primary_category": {
                    "term": "astro-ph.IM"
                },
                "authors": [
                    {
                        "name": "Christian Hornhuber"
                    },
                    {
                        "name": "Mohammad Ful Hossain Seikh"
                    },
                    {
                        "name": "Mark Stockham"
                    },
                    {
                        "name": "Scott Voigt"
                    },
                    {
                        "name": "Rob Young"
                    },
                    {
                        "name": "Alisa Nozdrina"
                    },
                    {
                        "name": "Sanyukta Agarwal"
                    },
                    {
                        "name": "Shoukat Ali"
                    },
                    {
                        "name": "Kenny Couberly"
                    },
                    {
                        "name": "Dave Besson"
                    }
                ],
                "author_detail": {
                    "name": "Dave Besson"
                },
                "author": "Dave Besson"
            },
            {
                "id": "http://arxiv.org/abs/2512.14699v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14699v1",
                "title": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives"
                },
                "updated": "2025-12-16T18:59:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    18,
                    59,
                    59,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14699v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14699v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T18:59:59Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    18,
                    59,
                    59,
                    1,
                    350,
                    0
                ],
                "arxiv_comment": "Project Page: https://sihuiji.github.io/MemFlow.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Sihui Ji"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xin Tao"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hengshuang Zhao"
                },
                "author": "Hengshuang Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.14681v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14681v1",
                "title": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Accurate Causal Parallel Decoding using Jacobi Forcing"
                },
                "updated": "2025-12-16T18:45:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    18,
                    45,
                    18,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14681v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-token generation has emerged as a promising paradigm for accelerating transformer-based large model inference. Recent efforts primarily explore diffusion Large Language Models (dLLMs) for parallel decoding to reduce inference latency. To achieve AR-level generation quality, many techniques adapt AR models into dLLMs to enable parallel decoding. However, they suffer from limited speedup compared to AR models due to a pretrain-to-posttrain mismatch. Specifically, the masked data distribution in post-training deviates significantly from the real-world data distribution seen during pretraining, and dLLMs rely on bidirectional attention, which conflicts with the causal prior learned during pretraining and hinders the integration of exact KV cache reuse. To address this, we introduce Jacobi Forcing, a progressive distillation paradigm where models are trained on their own generated parallel decoding trajectories, smoothly shifting AR models into efficient parallel decoders while preserving their pretrained causal inference property. The models trained under this paradigm, Jacobi Forcing Model, achieves 3.8x wall-clock speedup on coding and math benchmarks with minimal loss in performance. Based on Jacobi Forcing Models' trajectory characteristics, we introduce multi-block decoding with rejection recycling, which enables up to 4.5x higher token acceptance count per iteration and nearly 4.0x wall-clock speedup, effectively trading additional compute for lower inference latency. Our code is available at https://github.com/hao-ai-lab/JacobiForcing."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T18:45:18Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    18,
                    45,
                    18,
                    1,
                    350,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Lanxiang Hu"
                    },
                    {
                        "name": "Siqi Kou"
                    },
                    {
                        "name": "Yichao Fu"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Tajana Rosing"
                    },
                    {
                        "name": "Yuxiong He"
                    },
                    {
                        "name": "Zhijie Deng"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2501.06425v6",
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "updated": "2025-12-16T16:24:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    16,
                    24,
                    22,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2501.06425v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2501.06425v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, substantially shrinking the KV cache size at inference time. By factorizing these representations into contextual low-rank components and seamlessly integrating with Rotary Position Embedding (RoPE), TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation on language modeling tasks, we demonstrate that T6 surpasses or matches the performance of standard Transformer baselines including Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and Multi-Head Latent Attention (MLA) across various metrics, including perplexity and a range of established evaluation benchmarks. Notably, TPA's memory efficiency and computational efficiency at decoding stage enables processing longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. Project Page: https://github.com/tensorgi/TPA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically necessitates large key-value (KV) caches, resulting in substantial memory overhead during inference. In this paper, we propose Tensor Product Attention (TPA), a novel attention mechanism that uses tensor decompositions to represent queries, keys, and values compactly, substantially shrinking the KV cache size at inference time. By factorizing these representations into contextual low-rank components and seamlessly integrating with Rotary Position Embedding (RoPE), TPA achieves improved model quality alongside memory efficiency. Based on TPA, we introduce the Tensor ProducT ATTenTion Transformer (T6), a new model architecture for sequence modeling. Through extensive empirical evaluation on language modeling tasks, we demonstrate that T6 surpasses or matches the performance of standard Transformer baselines including Multi-Head Attention (MHA), Multi-Query Attention (MQA), Grouped-Query Attention (GQA), and Multi-Head Latent Attention (MLA) across various metrics, including perplexity and a range of established evaluation benchmarks. Notably, TPA's memory efficiency and computational efficiency at decoding stage enables processing longer sequences under fixed resource constraints, addressing a critical scalability challenge in modern language models. Project Page: https://github.com/tensorgi/TPA."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "arxiv_comment": "Published in NeurIPS 2025 (Spotlight); Project Page: https://github.com/tensorgi/TPA",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao"
            },
            {
                "id": "http://arxiv.org/abs/2512.14488v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14488v1",
                "title": "Hybrid Cognitive IoT with Cooperative Caching and SWIPT-EH: A Hierarchical Reinforcement Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Cognitive IoT with Cooperative Caching and SWIPT-EH: A Hierarchical Reinforcement Learning Framework"
                },
                "updated": "2025-12-16T15:18:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    15,
                    18,
                    50,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14488v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/JIOT.2025.3632391.",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "This paper proposes a hierarchical deep reinforcement learning (DRL) framework based on the soft actor-critic (SAC) algorithm for hybrid underlay-overlay cognitive Internet of Things (CIoT) networks with simultaneous wireless information and power transfer (SWIPT)-energy harvesting (EH) and cooperative caching. Unlike prior hierarchical DRL approaches that focus primarily on spectrum access or power control, our work jointly optimizes EH, hybrid access coordination, power allocation, and caching in a unified framework. The joint optimization problem is formulated as a weighted-sum multi-objective task, designed to maximize throughput and cache hit ratio while simultaneously minimizing transmission delay. In the proposed model, CIoT agents jointly optimize EH and data transmission using a learnable time switching (TS) factor. They also coordinate spectrum access under hybrid overlay-underlay paradigms and make power control and cache placement decisions while considering energy, interference, and storage constraints. Specifically, in this work, cooperative caching is used to enable overlay access, while power control is used for underlay access. A novel three-level hierarchical SAC (H-SAC) agent decomposes the mixed discrete-continuous action space into modular subproblems, improving scalability and convergence over flat DRL methods. The high-level policy adjusts the TS factor, the mid-level policy manages spectrum access coordination and cache sharing, and the low-level policy decides transmit power and caching actions for both the CIoT agent and PU content. Simulation results show that the proposed hierarchical SAC approach significantly outperforms benchmark and greedy strategies. It achieves better performance in terms of average sum rate, delay, cache hit ratio, and energy efficiency, even under channel fading and uncertain conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a hierarchical deep reinforcement learning (DRL) framework based on the soft actor-critic (SAC) algorithm for hybrid underlay-overlay cognitive Internet of Things (CIoT) networks with simultaneous wireless information and power transfer (SWIPT)-energy harvesting (EH) and cooperative caching. Unlike prior hierarchical DRL approaches that focus primarily on spectrum access or power control, our work jointly optimizes EH, hybrid access coordination, power allocation, and caching in a unified framework. The joint optimization problem is formulated as a weighted-sum multi-objective task, designed to maximize throughput and cache hit ratio while simultaneously minimizing transmission delay. In the proposed model, CIoT agents jointly optimize EH and data transmission using a learnable time switching (TS) factor. They also coordinate spectrum access under hybrid overlay-underlay paradigms and make power control and cache placement decisions while considering energy, interference, and storage constraints. Specifically, in this work, cooperative caching is used to enable overlay access, while power control is used for underlay access. A novel three-level hierarchical SAC (H-SAC) agent decomposes the mixed discrete-continuous action space into modular subproblems, improving scalability and convergence over flat DRL methods. The high-level policy adjusts the TS factor, the mid-level policy manages spectrum access coordination and cache sharing, and the low-level policy decides transmit power and caching actions for both the CIoT agent and PU content. Simulation results show that the proposed hierarchical SAC approach significantly outperforms benchmark and greedy strategies. It achieves better performance in terms of average sum rate, delay, cache hit ratio, and energy efficiency, even under channel fading and uncertain conditions."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T15:18:50Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    15,
                    18,
                    50,
                    1,
                    350,
                    0
                ],
                "arxiv_comment": "Published in IEEE Internet of Things Journal (Early Access), 2025. This arXiv version is the authors' accepted manuscript",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "arxiv_journal_ref": "IEEE Internet of Things Journal, Early Access, 2025",
                "authors": [
                    {
                        "name": "Nadia Abdolkhani"
                    },
                    {
                        "name": "Walaa Hamouda"
                    }
                ],
                "author_detail": {
                    "name": "Walaa Hamouda"
                },
                "author": "Walaa Hamouda",
                "arxiv_doi": "10.1109/JIOT.2025.3632391."
            },
            {
                "id": "http://arxiv.org/abs/2504.21230v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.21230v3",
                "title": "Kimina Lean Server: A High-Performance Lean Server for Large-Scale Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimina Lean Server: A High-Performance Lean Server for Large-Scale Verification"
                },
                "updated": "2025-12-16T14:04:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    14,
                    4,
                    14,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.21230v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.21230v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce the Kimina Lean Server, an open-source project designed as a high-performance verifier for reinforcement learning pipelines. Built on top of the Lean REPL (Read-Eval-Print Loop) maintained by the Lean FRO, our server combines server-side parallelism by managing multiple Lean processes in parallel with a Least Recently Used (LRU) caching mechanism that reuses Lean imports across requests. On the client side, a lightweight Python package enables submitting proof batches and receiving Lean feedback, including extracted tactics and tactic states.\n  Together, these features enable a scalable workflow for large-scale verification and data extraction. In our experiments, the Kimina Lean Server outperforms previous Lean interaction tools, achieving a 1.5 to 2 times speedup in verification time. Moreover, its improved efficiency has enabled its use in the large-scale training of state-of-the-art models such as Kimina-Prover.\n  We hope that our open-source project will support the neural theorem proving community and accelerate future progress by enabling efficient large-scale verification and proof data extraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Kimina Lean Server, an open-source project designed as a high-performance verifier for reinforcement learning pipelines. Built on top of the Lean REPL (Read-Eval-Print Loop) maintained by the Lean FRO, our server combines server-side parallelism by managing multiple Lean processes in parallel with a Least Recently Used (LRU) caching mechanism that reuses Lean imports across requests. On the client side, a lightweight Python package enables submitting proof batches and receiving Lean feedback, including extracted tactics and tactic states.\n  Together, these features enable a scalable workflow for large-scale verification and data extraction. In our experiments, the Kimina Lean Server outperforms previous Lean interaction tools, achieving a 1.5 to 2 times speedup in verification time. Moreover, its improved efficiency has enabled its use in the large-scale training of state-of-the-art models such as Kimina-Prover.\n  We hope that our open-source project will support the neural theorem proving community and accelerate future progress by enabling efficient large-scale verification and proof data extraction."
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-29T23:43:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "arxiv_comment": "Accepted to the 5th MATH-AI Workshop at NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.LO"
                },
                "authors": [
                    {
                        "name": "Marco Dos Santos"
                    },
                    {
                        "name": "Hugues de Saxcé"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Mantas Baksys"
                    },
                    {
                        "name": "Mert Unsal"
                    },
                    {
                        "name": "Junqi Liu"
                    },
                    {
                        "name": "Zhengying Liu"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.14151v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14151v1",
                "title": "Adaptive Cache Pollution Control for Large Language Model Inference Workloads Using Temporal CNN-Based Prediction and Priority-Aware Replacement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Cache Pollution Control for Large Language Model Inference Workloads Using Temporal CNN-Based Prediction and Priority-Aware Replacement"
                },
                "updated": "2025-12-16T07:16:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    7,
                    16,
                    10,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14151v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14151v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs), such as GPT and LLaMA, introduce unique memory access characteristics during inference due to frequent token sequence lookups and embedding vector retrievals. These workloads generate highly irregular and bursty access patterns, causing traditional prefetching and replacement policies to mispredict and trigger severe cache pollution, thereby degrading system performance. To address this challenge, this paper proposes an Adaptive Cache Pollution Control (ACPC) mechanism tailored for LLM inference workloads, integrating Temporal Convolutional Network (TCN)-based access prediction with a priority-aware replacement strategy. The TCN module learns temporal dependencies in token access sequences to identify potential high-reuse cache lines, while the replacement policy dynamically adjusts eviction priorities based on predicted reuse likelihood and cache occupancy. The proposed framework is implemented and evaluated on representative transformer-based inference traces, including GPT-style autoregressive decoding and embedding retrieval workloads. Experimental results demonstrate that ACPC reduces cache pollution by 41.7 percent, improves cache hit rate by 8.9 percent, and achieves a 60.0 percent reduction in L2 miss penalty, compared with state-of-the-art machine-learning-based replacement baselines. Additionally, the proposed Temporal CNN-based ACPC framework increases token generation throughput by 15.9 percent and achieves the lowest final loss of 0.21, confirming its superior efficiency and stability under complex LLM inference workloads. These results highlight ACPC's effectiveness in recognizing useful cache lines and mitigating redundant prefetches under dynamic LLM access behaviors. The proposed approach provides a scalable, learning-driven solution for optimizing memory efficiency and latency in large-scale LLM serving and inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT and LLaMA, introduce unique memory access characteristics during inference due to frequent token sequence lookups and embedding vector retrievals. These workloads generate highly irregular and bursty access patterns, causing traditional prefetching and replacement policies to mispredict and trigger severe cache pollution, thereby degrading system performance. To address this challenge, this paper proposes an Adaptive Cache Pollution Control (ACPC) mechanism tailored for LLM inference workloads, integrating Temporal Convolutional Network (TCN)-based access prediction with a priority-aware replacement strategy. The TCN module learns temporal dependencies in token access sequences to identify potential high-reuse cache lines, while the replacement policy dynamically adjusts eviction priorities based on predicted reuse likelihood and cache occupancy. The proposed framework is implemented and evaluated on representative transformer-based inference traces, including GPT-style autoregressive decoding and embedding retrieval workloads. Experimental results demonstrate that ACPC reduces cache pollution by 41.7 percent, improves cache hit rate by 8.9 percent, and achieves a 60.0 percent reduction in L2 miss penalty, compared with state-of-the-art machine-learning-based replacement baselines. Additionally, the proposed Temporal CNN-based ACPC framework increases token generation throughput by 15.9 percent and achieves the lowest final loss of 0.21, confirming its superior efficiency and stability under complex LLM inference workloads. These results highlight ACPC's effectiveness in recognizing useful cache lines and mitigating redundant prefetches under dynamic LLM access behaviors. The proposed approach provides a scalable, learning-driven solution for optimizing memory efficiency and latency in large-scale LLM serving and inference systems."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T07:16:10Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    7,
                    16,
                    10,
                    1,
                    350,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Songze Liu"
                    },
                    {
                        "name": "Hongkun Du"
                    },
                    {
                        "name": "Shaowen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Shaowen Wang"
                },
                "author": "Shaowen Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.14142v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14142v1",
                "title": "Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Astraea: A State-Aware Scheduling Engine for LLM-Powered Agents"
                },
                "updated": "2025-12-16T06:55:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    6,
                    55,
                    10,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14142v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14142v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are increasingly being deployed as intelligent agents. Their multi-stage workflows, which alternate between local computation and calls to external network services like Web APIs, introduce a mismatch in their execution pattern and the scheduling granularity of existing inference systems such as vLLM. Existing systems typically focus on per-segment optimization which prevents them from minimizing the end-to-end latency of the complete agentic workflow, i.e., the global Job Completion Time (JCT) over the entire request lifecycle. To address this limitation, we propose Astraea, a service engine designed to shift the optimization from local segments to the global request lifecycle. Astraea employs a state-aware, hierarchical scheduling algorithm that integrates a request's historical state with future predictions. It dynamically classifies requests by their I/O and compute intensive nature and uses an enhanced HRRN policy to balance efficiency and fairness. Astraea also implements an adaptive KV cache manager that intelligently handles the agent state during I/O waits based on the system memory pressure. Extensive experiments show that Astraea reduces average JCT by up to 25.5\\% compared to baseline methods. Moreover, our approach demonstrates strong robustness and stability under high load across various model scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed as intelligent agents. Their multi-stage workflows, which alternate between local computation and calls to external network services like Web APIs, introduce a mismatch in their execution pattern and the scheduling granularity of existing inference systems such as vLLM. Existing systems typically focus on per-segment optimization which prevents them from minimizing the end-to-end latency of the complete agentic workflow, i.e., the global Job Completion Time (JCT) over the entire request lifecycle. To address this limitation, we propose Astraea, a service engine designed to shift the optimization from local segments to the global request lifecycle. Astraea employs a state-aware, hierarchical scheduling algorithm that integrates a request's historical state with future predictions. It dynamically classifies requests by their I/O and compute intensive nature and uses an enhanced HRRN policy to balance efficiency and fairness. Astraea also implements an adaptive KV cache manager that intelligently handles the agent state during I/O waits based on the system memory pressure. Extensive experiments show that Astraea reduces average JCT by up to 25.5\\% compared to baseline methods. Moreover, our approach demonstrates strong robustness and stability under high load across various model scales."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T06:55:10Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    6,
                    55,
                    10,
                    1,
                    350,
                    0
                ],
                "arxiv_comment": "12 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Hongqiu Ni"
                    },
                    {
                        "name": "Jiabao Zhang"
                    },
                    {
                        "name": "Guopeng Li"
                    },
                    {
                        "name": "Zilong Wang"
                    },
                    {
                        "name": "Ruiqi Wu"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Haisheng Tan"
                    }
                ],
                "author_detail": {
                    "name": "Haisheng Tan"
                },
                "author": "Haisheng Tan"
            },
            {
                "id": "http://arxiv.org/abs/2511.22333v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.22333v2",
                "title": "PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PAT: Accelerating LLM Decoding via Prefix-Aware Attention with Resource Efficient Multi-Tile Kernel"
                },
                "updated": "2025-12-16T05:44:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    5,
                    44,
                    34,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.22333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.22333v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: one-query-per-CTA execution repeatedly loads shared prefix KV cache, while one-size-fits-all tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.\n  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 53.5% on average and TPOT by 17.0-93.1% under the same configurations against state-of-the-art attention kernels. PAT's source code is publicly available at https://github.com/flashserve/PAT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM serving is increasingly dominated by decode attention, which is a memory-bound operation due to massive KV cache loading from global memory. Meanwhile, real-world workloads exhibit substantial, hierarchical shared prefixes across requests (e.g., system prompts, tools/templates, RAG). Existing attention implementations fail to fully exploit prefix sharing: one-query-per-CTA execution repeatedly loads shared prefix KV cache, while one-size-fits-all tiling leaves on-chip resources idle and exacerbates bubbles for uneven KV lengths. These choices amplify memory bandwidth pressure and stall memory-bound decode attention.\n  This paper introduces PAT, a prefix-aware attention kernel implementation for LLM decoding that organizes execution with a pack-forward-merge paradigm. PAT packs queries by shared prefix to reduce repeated memory accesses, runs a customized multi-tile kernel to achieve high resource efficiency. It further applies practical multi-stream forwarding and KV splitting to reduce resource bubbles. The final merge performs online softmax with negligible overhead. We implement PAT as an off-the-shelf plugin for vLLM. Evaluation on both real-world and synthetic workloads shows that PAT reduces attention latency by 53.5% on average and TPOT by 17.0-93.1% under the same configurations against state-of-the-art attention kernels. PAT's source code is publicly available at https://github.com/flashserve/PAT."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-27T11:10:30Z",
                "published_parsed": [
                    2025,
                    11,
                    27,
                    11,
                    10,
                    30,
                    3,
                    331,
                    0
                ],
                "arxiv_comment": "Accepted by ASPLOS'26, code available at https://github.com/flashserve/PAT",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Jinjun Yi"
                    },
                    {
                        "name": "Zhixin Zhao"
                    },
                    {
                        "name": "Yitao Hu"
                    },
                    {
                        "name": "Ke Yan"
                    },
                    {
                        "name": "Weiwei Sun"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Laiping Zhao"
                    },
                    {
                        "name": "Yuhao Zhang"
                    },
                    {
                        "name": "Wenxin Li"
                    },
                    {
                        "name": "Keqiu Li"
                    }
                ],
                "author_detail": {
                    "name": "Keqiu Li"
                },
                "author": "Keqiu Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.14096v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14096v1",
                "title": "OUSAC: Optimized Guidance Scheduling with Adaptive Caching for DiT Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OUSAC: Optimized Guidance Scheduling with Adaptive Caching for DiT Acceleration"
                },
                "updated": "2025-12-16T05:11:54Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    5,
                    11,
                    54,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14096v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14096v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have emerged as the dominant paradigm for high-quality image generation, yet their computational expense remains substantial due to iterative denoising. Classifier-Free Guidance (CFG) significantly enhances generation quality and controllability but doubles the computation by requiring both conditional and unconditional forward passes at every timestep. We present OUSAC (Optimized gUidance Scheduling with Adaptive Caching), a framework that accelerates diffusion transformers (DiT) through systematic optimization. Our key insight is that variable guidance scales enable sparse computation: adjusting scales at certain timesteps can compensate for skipping CFG at others, enabling both fewer total sampling steps and fewer CFG steps while maintaining quality. However, variable guidance patterns introduce denoising deviations that undermine standard caching methods, which assume constant CFG scales across steps. Moreover, different transformer blocks are affected at different levels under dynamic conditions. This paper develops a two-stage approach leveraging these insights. Stage-1 employs evolutionary algorithms to jointly optimize which timesteps to skip and what guidance scale to use, eliminating up to 82% of unconditional passes. Stage-2 introduces adaptive rank allocation that tailors calibration efforts per transformer block, maintaining caching effectiveness under variable guidance. Experiments demonstrate that OUSAC significantly outperforms state-of-the-art acceleration methods, achieving 53% computational savings with 15% quality improvement on DiT-XL/2 (ImageNet 512x512), 60% savings with 16.1% improvement on PixArt-alpha (MSCOCO), and 5x speedup on FLUX while improving CLIP Score over the 50-step baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as the dominant paradigm for high-quality image generation, yet their computational expense remains substantial due to iterative denoising. Classifier-Free Guidance (CFG) significantly enhances generation quality and controllability but doubles the computation by requiring both conditional and unconditional forward passes at every timestep. We present OUSAC (Optimized gUidance Scheduling with Adaptive Caching), a framework that accelerates diffusion transformers (DiT) through systematic optimization. Our key insight is that variable guidance scales enable sparse computation: adjusting scales at certain timesteps can compensate for skipping CFG at others, enabling both fewer total sampling steps and fewer CFG steps while maintaining quality. However, variable guidance patterns introduce denoising deviations that undermine standard caching methods, which assume constant CFG scales across steps. Moreover, different transformer blocks are affected at different levels under dynamic conditions. This paper develops a two-stage approach leveraging these insights. Stage-1 employs evolutionary algorithms to jointly optimize which timesteps to skip and what guidance scale to use, eliminating up to 82% of unconditional passes. Stage-2 introduces adaptive rank allocation that tailors calibration efforts per transformer block, maintaining caching effectiveness under variable guidance. Experiments demonstrate that OUSAC significantly outperforms state-of-the-art acceleration methods, achieving 53% computational savings with 15% quality improvement on DiT-XL/2 (ImageNet 512x512), 60% savings with 16.1% improvement on PixArt-alpha (MSCOCO), and 5x speedup on FLUX while improving CLIP Score over the 50-step baseline."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T05:11:54Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    5,
                    11,
                    54,
                    1,
                    350,
                    0
                ],
                "arxiv_comment": "29 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Ruitong Sun"
                    },
                    {
                        "name": "Tianze Yang"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Jin Sun"
                    }
                ],
                "author_detail": {
                    "name": "Jin Sun"
                },
                "author": "Jin Sun"
            },
            {
                "id": "http://arxiv.org/abs/2505.13109v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.13109v3",
                "title": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreeKV: Boosting KV Cache Retrieval for Efficient LLM Inference"
                },
                "updated": "2025-12-16T04:58:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    4,
                    58,
                    49,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.13109v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.13109v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\\times$ speedup compared to SOTA KV retrieval methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been widely deployed with rapidly expanding context windows to support increasingly demanding applications. However, long contexts pose significant deployment challenges, primarily due to the KV cache whose size grows proportionally with context length. While KV cache compression methods are proposed to address this issue, KV dropping methods incur considerable accuracy loss, and KV retrieval methods suffer from significant efficiency bottlenecks. We propose FreeKV, an algorithm-system co-optimization framework to enhance KV retrieval efficiency while preserving accuracy. On the algorithm side, FreeKV introduces speculative retrieval to shift the KV selection and recall processes out of the critical path, combined with fine-grained correction to ensure accuracy. On the system side, FreeKV employs hybrid KV layouts across CPU and GPU memory to eliminate fragmented data transfers, and leverages double-buffered streamed recall to further improve efficiency. Experiments demonstrate that FreeKV achieves near-lossless accuracy across various scenarios and models, delivering up to 13$\\times$ speedup compared to SOTA KV retrieval methods."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-19T13:36:45Z",
                "published_parsed": [
                    2025,
                    5,
                    19,
                    13,
                    36,
                    45,
                    0,
                    139,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Zhenyu Ning"
                    },
                    {
                        "name": "Jing Lin"
                    },
                    {
                        "name": "Yiwu Yao"
                    },
                    {
                        "name": "Danning Ke"
                    },
                    {
                        "name": "Minyi Guo"
                    },
                    {
                        "name": "Jieru Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jieru Zhao"
                },
                "author": "Jieru Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.14080v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14080v1",
                "title": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations"
                },
                "updated": "2025-12-16T04:39:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    4,
                    39,
                    10,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14080v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel \"token rounding\" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel \"token rounding\" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T04:39:10Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    4,
                    39,
                    10,
                    1,
                    350,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Wentao Guo"
                    },
                    {
                        "name": "Mayank Mishra"
                    },
                    {
                        "name": "Xinle Cheng"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Tri Dao"
                    }
                ],
                "author_detail": {
                    "name": "Tri Dao"
                },
                "author": "Tri Dao"
            },
            {
                "id": "http://arxiv.org/abs/2512.14067v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14067v1",
                "title": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient-DLM: From Autoregressive to Diffusion Language Models, and Beyond in Speed"
                },
                "updated": "2025-12-16T04:12:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    4,
                    12,
                    17,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14067v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models (dLMs) have emerged as a promising paradigm that enables parallel, non-autoregressive generation, but their learning efficiency lags behind that of autoregressive (AR) language models when trained from scratch. To this end, we study AR-to-dLM conversion to transform pretrained AR models into efficient dLMs that excel in speed while preserving AR models' task accuracy. We achieve this by identifying limitations in the attention patterns and objectives of existing AR-to-dLM methods and then proposing principles and methodologies for more effective AR-to-dLM conversion. Specifically, we first systematically compare different attention patterns and find that maintaining pretrained AR weight distributions is critical for effective AR-to-dLM conversion. As such, we introduce a continuous pretraining scheme with a block-wise attention pattern, which remains causal across blocks while enabling bidirectional modeling within each block. We find that this approach can better preserve pretrained AR models' weight distributions than fully bidirectional modeling, in addition to its known benefit of enabling KV caching, and leads to a win-win in accuracy and efficiency. Second, to mitigate the training-test gap in mask token distributions (uniform vs. highly left-to-right), we propose a position-dependent token masking strategy that assigns higher masking probabilities to later tokens during training to better mimic test-time behavior. Leveraging this framework, we conduct extensive studies of dLMs' attention patterns, training dynamics, and other design choices, providing actionable insights into scalable AR-to-dLM conversion. These studies lead to the Efficient-DLM family, which outperforms state-of-the-art AR models and dLMs, e.g., our Efficient-DLM 8B achieves +5.4%/+2.7% higher accuracy with 4.5x/2.7x higher throughput compared to Dream 7B and Qwen3 4B, respectively."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T04:12:17Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    4,
                    12,
                    17,
                    1,
                    350,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Lexington Whalen"
                    },
                    {
                        "name": "Zhifan Ye"
                    },
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Maksim Khadkevich"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Yingyan Celine Lin"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov"
            },
            {
                "id": "http://arxiv.org/abs/2512.14029v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.14029v1",
                "title": "Cooperative Caching Towards Efficient Spectrum Utilization in Cognitive-IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cooperative Caching Towards Efficient Spectrum Utilization in Cognitive-IoT Networks"
                },
                "updated": "2025-12-16T02:49:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    16,
                    2,
                    49,
                    43,
                    1,
                    350,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.14029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.14029v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/ICC52391.2025.11160803",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "In cognitive Internet of Things (CIoT) networks, efficient spectrum sharing is essential to address increasing wireless demands. This paper presents a novel deep reinforcement learning (DRL)-based approach for joint cooperative caching and spectrum access coordination in CIoT networks, enabling the CIoT agents to collaborate with primary users (PUs) by caching PU content and serving their requests, fostering mutual benefits. The proposed DRL framework jointly optimizes caching policy and spectrum access under challenging conditions. Unlike traditional cognitive radio (CR) methods, where CIoT agents vacate the spectrum for PUs, or relaying techniques, which merely support spectrum sharing, caching brings data closer to the edge, reducing latency by minimizing retrieval distance. Simulations demonstrate that our approach outperforms others in lowering latency, increasing CIoT and PU cache hit rates, and enhancing network throughput. This approach redefines spectrum sharing, offering a fresh perspective on CIoT network design and illustrating the potential of DRL-guided caching to highlight the benefits of collaboration over dynamic spectrum access scenarios, elevating CIoT performance under constrained resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cognitive Internet of Things (CIoT) networks, efficient spectrum sharing is essential to address increasing wireless demands. This paper presents a novel deep reinforcement learning (DRL)-based approach for joint cooperative caching and spectrum access coordination in CIoT networks, enabling the CIoT agents to collaborate with primary users (PUs) by caching PU content and serving their requests, fostering mutual benefits. The proposed DRL framework jointly optimizes caching policy and spectrum access under challenging conditions. Unlike traditional cognitive radio (CR) methods, where CIoT agents vacate the spectrum for PUs, or relaying techniques, which merely support spectrum sharing, caching brings data closer to the edge, reducing latency by minimizing retrieval distance. Simulations demonstrate that our approach outperforms others in lowering latency, increasing CIoT and PU cache hit rates, and enhancing network throughput. This approach redefines spectrum sharing, offering a fresh perspective on CIoT network design and illustrating the potential of DRL-guided caching to highlight the benefits of collaboration over dynamic spectrum access scenarios, elevating CIoT performance under constrained resources."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-16T02:49:43Z",
                "published_parsed": [
                    2025,
                    12,
                    16,
                    2,
                    49,
                    43,
                    1,
                    350,
                    0
                ],
                "arxiv_comment": "Published in Proc. IEEE ICC 2025. This is the authors' accepted manuscript version",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "arxiv_journal_ref": "Proc. IEEE ICC, 2025, pp. 1310-1315",
                "authors": [
                    {
                        "name": "Nadia Abdolkhani"
                    },
                    {
                        "name": "Walaa Hamouda"
                    }
                ],
                "author_detail": {
                    "name": "Walaa Hamouda"
                },
                "author": "Walaa Hamouda",
                "arxiv_doi": "10.1109/ICC52391.2025.11160803"
            },
            {
                "id": "http://arxiv.org/abs/2512.13615v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.13615v1",
                "title": "Hyper-Minrank: A Unified Hypergraph Characterization of Multi-Sender Index Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hyper-Minrank: A Unified Hypergraph Characterization of Multi-Sender Index Coding"
                },
                "updated": "2025-12-15T18:08:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    18,
                    8,
                    9,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.13615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.13615v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This work introduces a hypergraph formulation that generalizes the classical paradigm of Bar-Yossef et al. to the multi-sender index coding (MSIC) setting. Central to the model is a 4-regular side-information hypergraph G, a new adjacency representation A_G = [A_1 ... A_N], and a simple fitting criterion for sub-hypergraph validity, in the presence of specially designed hyperedges that capture both side information and cross-sender signal cancellation. This formulation establishes a tight achievability-converse equivalence for the general N-sender, K-receiver problem: every valid fitting induces a valid linear multi-sender index code, every linear code induces a valid fitting, and the optimal scalar linear broadcast length equals the hyper-minrank l**lin(G) = hyperminrank(G) = min*{A fits G} sum_{n=1}^N rank(A_n). Beyond this exact characterization, the approach yields hypergraph analogues of Haemers-type bounds on the broadcast length, including a clique-cover upper bound and a lower bound via the clique number of a carefully defined complement hypergraph. Algorithmically, we provide an exact procedure to compute hyperminrank(G), and show that in certain regimes its complexity is asymptotically better than approximate LT-CMAR solutions. The framework captures well-known settings such as embedded index coding, and applies directly to multi-sender cache-aided communications, coded computation, distributed storage, and edge/satellite systems, where hyperminrank can serve as a unified design target.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work introduces a hypergraph formulation that generalizes the classical paradigm of Bar-Yossef et al. to the multi-sender index coding (MSIC) setting. Central to the model is a 4-regular side-information hypergraph G, a new adjacency representation A_G = [A_1 ... A_N], and a simple fitting criterion for sub-hypergraph validity, in the presence of specially designed hyperedges that capture both side information and cross-sender signal cancellation. This formulation establishes a tight achievability-converse equivalence for the general N-sender, K-receiver problem: every valid fitting induces a valid linear multi-sender index code, every linear code induces a valid fitting, and the optimal scalar linear broadcast length equals the hyper-minrank l**lin(G) = hyperminrank(G) = min*{A fits G} sum_{n=1}^N rank(A_n). Beyond this exact characterization, the approach yields hypergraph analogues of Haemers-type bounds on the broadcast length, including a clique-cover upper bound and a lower bound via the clique number of a carefully defined complement hypergraph. Algorithmically, we provide an exact procedure to compute hyperminrank(G), and show that in certain regimes its complexity is asymptotically better than approximate LT-CMAR solutions. The framework captures well-known settings such as embedded index coding, and applies directly to multi-sender cache-aided communications, coded computation, distributed storage, and edge/satellite systems, where hyperminrank can serve as a unified design target."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T18:08:09Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    18,
                    8,
                    9,
                    0,
                    349,
                    0
                ],
                "arxiv_comment": "46 pages, 9 figures",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Ali Khalesi"
                    },
                    {
                        "name": "Petros Elia"
                    }
                ],
                "author_detail": {
                    "name": "Petros Elia"
                },
                "author": "Petros Elia"
            },
            {
                "id": "http://arxiv.org/abs/2512.13586v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.13586v1",
                "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding"
                },
                "updated": "2025-12-15T17:41:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    17,
                    41,
                    19,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.13586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.13586v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\\times$ average speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\\times$ average speedup."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T17:41:19Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    17,
                    41,
                    19,
                    0,
                    349,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jia-Nan Li"
                    },
                    {
                        "name": "Jian Guan"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Chongxuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Chongxuan Li"
                },
                "author": "Chongxuan Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.13109v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.13109v1",
                "title": "Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing"
                },
                "updated": "2025-12-15T09:04:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    9,
                    4,
                    6,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.13109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.13109v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\\% in KV-Retrieval tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\\% in KV-Retrieval tasks."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T09:04:06Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    9,
                    4,
                    6,
                    0,
                    349,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zewen Qiang"
                    },
                    {
                        "name": "Sendong Zhao"
                    },
                    {
                        "name": "Haochun Wang"
                    },
                    {
                        "name": "Bing Qin"
                    },
                    {
                        "name": "Ting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ting Liu"
                },
                "author": "Ting Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.07155v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.07155v3",
                "title": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHIMERA: Adaptive Cache Injection and Semantic Anchor Prompting for Zero-shot Image Morphing with Morphing-oriented Metrics"
                },
                "updated": "2025-12-15T08:33:55Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    8,
                    33,
                    55,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.07155v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.07155v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models exhibit remarkable generative ability, yet achieving smooth and semantically consistent image morphing remains a challenge. Existing approaches often yield abrupt transitions or over-saturated appearances due to the lack of adaptive structural and semantic alignments. We propose CHIMERA, a zero-shot diffusion-based framework that formulates morphing as a cached inversion-guided denoising process. To handle large semantic and appearance disparities, we propose Adaptive Cache Injection and Semantic Anchor Prompting. Adaptive Cache Injection (ACI) caches down, mid, and up blocks features from both inputs during DDIM inversion and re-injects them adaptively during denoising, enabling spatial and semantic alignment in depth- and time-adaptive manners and enabling natural feature fusion and smooth transitions. Semantic Anchor Prompting (SAP) leverages a vision-language model to generate a shared anchor prompt that serves as a semantic anchor, bridging dissimilar inputs and guiding the denoising process toward coherent results. Finally, we introduce the Global-Local Consistency Score (GLCS), a morphing-oriented metric that simultaneously evaluates the global harmonization of the two inputs and the smoothness of the local morphing transition. Extensive experiments and user studies show that CHIMERA achieves smoother and more semantically aligned transitions than existing methods, establishing a new state of the art in image morphing. The code and project page will be publicly released."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-08T04:39:12Z",
                "published_parsed": [
                    2025,
                    12,
                    8,
                    4,
                    39,
                    12,
                    0,
                    342,
                    0
                ],
                "arxiv_comment": "Please visit our project page at https://cmlab-korea.github.io/CHIMERA/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Dahyeon Kye"
                    },
                    {
                        "name": "Jeahun Sung"
                    },
                    {
                        "name": "Mingyu Jeon"
                    },
                    {
                        "name": "Jihyong Oh"
                    }
                ],
                "author_detail": {
                    "name": "Jihyong Oh"
                },
                "author": "Jihyong Oh"
            },
            {
                "id": "http://arxiv.org/abs/2512.13019v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.13019v1",
                "title": "SneakPeek: Future-Guided Instructional Streaming Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SneakPeek: Future-Guided Instructional Streaming Video Generation"
                },
                "updated": "2025-12-15T06:32:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    6,
                    32,
                    57,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.13019v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.13019v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instructional video generation is an emerging task that aims to synthesize coherent demonstrations of procedural activities from textual descriptions. Such capability has broad implications for content creation, education, and human-AI interaction, yet existing video diffusion models struggle to maintain temporal consistency and controllability across long sequences of multiple action steps. We introduce a pipeline for future-driven streaming instructional video generation, dubbed SneakPeek, a diffusion-based autoregressive framework designed to generate precise, stepwise instructional videos conditioned on an initial image and structured textual prompts. Our approach introduces three key innovations to enhance consistency and controllability: (1) predictive causal adaptation, where a causal model learns to perform next-frame prediction and anticipate future keyframes; (2) future-guided self-forcing with a dual-region KV caching scheme to address the exposure bias issue at inference time; (3) multi-prompt conditioning, which provides fine-grained and procedural control over multi-step instructions. Together, these components mitigate temporal drift, preserve motion consistency, and enable interactive video generation where future prompt updates dynamically influence ongoing streaming video generation. Experimental results demonstrate that our method produces temporally coherent and semantically faithful instructional videos that accurately follow complex, multi-step task descriptions."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T06:32:57Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    6,
                    32,
                    57,
                    0,
                    349,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Cheeun Hong"
                    },
                    {
                        "name": "German Barquero"
                    },
                    {
                        "name": "Fadime Sener"
                    },
                    {
                        "name": "Markos Georgopoulos"
                    },
                    {
                        "name": "Edgar Schönfeld"
                    },
                    {
                        "name": "Stefan Popov"
                    },
                    {
                        "name": "Yuming Du"
                    },
                    {
                        "name": "Oscar Mañas"
                    },
                    {
                        "name": "Albert Pumarola"
                    }
                ],
                "author_detail": {
                    "name": "Albert Pumarola"
                },
                "author": "Albert Pumarola"
            },
            {
                "id": "http://arxiv.org/abs/2512.12990v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12990v1",
                "title": "SliceMoE: Bit-Sliced Expert Caching under Miss-Rate Constraints for Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SliceMoE: Bit-Sliced Expert Caching under Miss-Rate Constraints for Efficient MoE Inference"
                },
                "updated": "2025-12-15T05:33:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    5,
                    33,
                    7,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12990v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12990v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "MoE models offer efficient scaling through conditional computation, but their large parameter size and expensive expert offloading make on-device deployment challenging. Existing acceleration techniques such as prefetching or expert clustering often increase energy usage or reduce expert diversity. We present SliceMoE, an energy-efficient MoE inference framework for miss-rate-constrained deployment. SliceMoE introduces Dynamic Bit-Sliced Caching (DBSC), which caches experts at slice-level granularity and assigns precision on demand to expand effective expert capacity. To support mixed-precision experts without memory duplication, we propose Calibration-Free Asymmetric Matryoshka Quantization (AMAT), a truncation-based scheme that maintains compatibility between low-bit and high-bit slices. We further introduce Predictive Cache Warmup (PCW) to reduce early-decode cold misses by reshaping cache contents during prefill. Evaluated on DeepSeek-V2-Lite and Qwen1.5-MoE-A2.7B, SliceMoE reduces decode-stage energy consumption by up to 2.37x and 2.85x, respectively, and improves decode latency by up to 1.81x and 1.64x, while preserving near-high-bit accuracy. These results demonstrate that slice-level caching enables an efficient on-device MoE deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE models offer efficient scaling through conditional computation, but their large parameter size and expensive expert offloading make on-device deployment challenging. Existing acceleration techniques such as prefetching or expert clustering often increase energy usage or reduce expert diversity. We present SliceMoE, an energy-efficient MoE inference framework for miss-rate-constrained deployment. SliceMoE introduces Dynamic Bit-Sliced Caching (DBSC), which caches experts at slice-level granularity and assigns precision on demand to expand effective expert capacity. To support mixed-precision experts without memory duplication, we propose Calibration-Free Asymmetric Matryoshka Quantization (AMAT), a truncation-based scheme that maintains compatibility between low-bit and high-bit slices. We further introduce Predictive Cache Warmup (PCW) to reduce early-decode cold misses by reshaping cache contents during prefill. Evaluated on DeepSeek-V2-Lite and Qwen1.5-MoE-A2.7B, SliceMoE reduces decode-stage energy consumption by up to 2.37x and 2.85x, respectively, and improves decode latency by up to 1.81x and 1.64x, while preserving near-high-bit accuracy. These results demonstrate that slice-level caching enables an efficient on-device MoE deployment."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T05:33:07Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    5,
                    33,
                    7,
                    0,
                    349,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Yuseon Choi"
                    },
                    {
                        "name": "Sangjin Kim"
                    },
                    {
                        "name": "Jungjun Oh"
                    },
                    {
                        "name": "Gwangtae Park"
                    },
                    {
                        "name": "Byeongcheol Kim"
                    },
                    {
                        "name": "Hoi-Jun Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Hoi-Jun Yoo"
                },
                "author": "Hoi-Jun Yoo"
            },
            {
                "id": "http://arxiv.org/abs/2512.11203v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11203v2",
                "title": "AutoRefiner: Improving Autoregressive Video Diffusion Models via Reflective Refinement Over the Stochastic Sampling Path",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoRefiner: Improving Autoregressive Video Diffusion Models via Reflective Refinement Over the Stochastic Sampling Path"
                },
                "updated": "2025-12-15T05:13:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    5,
                    13,
                    40,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11203v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11203v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Autoregressive video diffusion models (AR-VDMs) show strong promise as scalable alternatives to bidirectional VDMs, enabling real-time and interactive applications. Yet there remains room for improvement in their sample fidelity. A promising solution is inference-time alignment, which optimizes the noise space to improve sample fidelity without updating model parameters. Yet, optimization- or search-based methods are computationally impractical for AR-VDMs. Recent text-to-image (T2I) works address this via feedforward noise refiners that modulate sampled noises in a single forward pass. Can such noise refiners be extended to AR-VDMs? We identify the failure of naively extending T2I noise refiners to AR-VDMs and propose AutoRefiner-a noise refiner tailored for AR-VDMs, with two key designs: pathwise noise refinement and a reflective KV-cache. Experiments demonstrate that AutoRefiner serves as an efficient plug-in for AR-VDMs, effectively enhancing sample fidelity by refining noise along stochastic denoising paths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive video diffusion models (AR-VDMs) show strong promise as scalable alternatives to bidirectional VDMs, enabling real-time and interactive applications. Yet there remains room for improvement in their sample fidelity. A promising solution is inference-time alignment, which optimizes the noise space to improve sample fidelity without updating model parameters. Yet, optimization- or search-based methods are computationally impractical for AR-VDMs. Recent text-to-image (T2I) works address this via feedforward noise refiners that modulate sampled noises in a single forward pass. Can such noise refiners be extended to AR-VDMs? We identify the failure of naively extending T2I noise refiners to AR-VDMs and propose AutoRefiner-a noise refiner tailored for AR-VDMs, with two key designs: pathwise noise refinement and a reflective KV-cache. Experiments demonstrate that AutoRefiner serves as an efficient plug-in for AR-VDMs, effectively enhancing sample fidelity by refining noise along stochastic denoising paths."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T01:28:22Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    1,
                    28,
                    22,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Zhengyang Yu"
                    },
                    {
                        "name": "Akio Hayakawa"
                    },
                    {
                        "name": "Masato Ishii"
                    },
                    {
                        "name": "Qingtao Yu"
                    },
                    {
                        "name": "Takashi Shibuya"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji"
            },
            {
                "id": "http://arxiv.org/abs/2512.12963v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12963v1",
                "title": "SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAdapter: Content-Style Disentanglement for Diffusion Style Transfer"
                },
                "updated": "2025-12-15T04:02:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    4,
                    2,
                    14,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12963v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12963v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as the leading approach for style transfer, yet they struggle with photo-realistic transfers, often producing painting-like results or missing detailed stylistic elements. Current methods inadequately address unwanted influence from original content styles and style reference content features. We introduce SCAdapter, a novel technique leveraging CLIP image space to effectively separate and integrate content and style features. Our key innovation systematically extracts pure content from content images and style elements from style references, ensuring authentic transfers. This approach is enhanced through three components: Controllable Style Adaptive Instance Normalization (CSAdaIN) for precise multi-style blending, KVS Injection for targeted style integration, and a style transfer consistency objective maintaining process coherence. Comprehensive experiments demonstrate SCAdapter significantly outperforms state-of-the-art methods in both conventional and diffusion-based baselines. By eliminating DDIM inversion and inference-stage optimization, our method achieves at least $2\\times$ faster inference than other diffusion-based approaches, making it both more effective and efficient for practical applications."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T04:02:14Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    4,
                    2,
                    14,
                    0,
                    349,
                    0
                ],
                "arxiv_comment": "Accepted to WACV 2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Luan Thanh Trinh"
                    },
                    {
                        "name": "Kenji Doi"
                    },
                    {
                        "name": "Atsuki Osanai"
                    }
                ],
                "author_detail": {
                    "name": "Atsuki Osanai"
                },
                "author": "Atsuki Osanai"
            },
            {
                "id": "http://arxiv.org/abs/2512.11306v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11306v2",
                "title": "RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RollMux: Phase-Level Multiplexing for Disaggregated RL Post-Training"
                },
                "updated": "2025-12-15T02:21:28Z",
                "updated_parsed": [
                    2025,
                    12,
                    15,
                    2,
                    21,
                    28,
                    0,
                    349,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11306v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable \"warm-star\" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rollout-training disaggregation is emerging as the standard architecture for Reinforcement Learning (RL) post-training, where memory-bound rollout and compute-bound training are physically disaggregated onto purpose-built clusters to maximize hardware efficiency. However, the strict synchronization required by on-policy algorithms introduces severe dependency bubbles, forcing one cluster to idle while the dependent phase is running on the other. We present RollMux, a cluster scheduling framework that reclaims these bubbles through cross-cluster orchestration. RollMux is built on the insight that the structural idleness of one job can be effectively utilized by the active phase of another. To realize this, we introduce the co-execution group abstraction, which partitions the cluster into isolated locality domains. This abstraction enables a two-tier scheduling architecture: an inter-group scheduler that optimizes job placement using conservative stochastic planning, and an intra-group scheduler that orchestrates a provably optimal round-robin schedule. The group abstraction also imposes a residency constraint, ensuring that massive model states remain cached in host memory to enable \"warm-star\" context switching. We evaluate RollMux on a production-scale testbed with 328 H20 and 328 H800 GPUs. RollMux improves cost efficiency by 1.84x over standard disaggregation and 1.38x over state-of-the-art co-located baselines, all while achieving 100% SLO attainment."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T06:03:56Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    6,
                    3,
                    56,
                    4,
                    346,
                    0
                ],
                "arxiv_comment": "17 pages, 15 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Tianyuan Wu"
                    },
                    {
                        "name": "Lunxi Cao"
                    },
                    {
                        "name": "Yining Wei"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Yuheng Zhao"
                    },
                    {
                        "name": "Dakai An"
                    },
                    {
                        "name": "Shaopan Xiong"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    },
                    {
                        "name": "Ju Huang"
                    },
                    {
                        "name": "Siran Yang"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang"
            },
            {
                "id": "http://arxiv.org/abs/2508.17756v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.17756v2",
                "title": "SuperGen: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGen: An Efficient Ultra-high-resolution Video Generation System with Sketching and Tiling"
                },
                "updated": "2025-12-14T17:45:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    14,
                    17,
                    45,
                    53,
                    6,
                    348,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.17756v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.17756v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have recently achieved remarkable success in generative tasks (e.g., image and video generation), and the demand for high-quality content (e.g., 2K/4K videos) is rapidly increasing across various domains. However, generating ultra-high-resolution videos on existing standard-resolution (e.g., 720p) platforms remains challenging due to the excessive re-training requirements and prohibitively high computational and memory costs. To this end, we introduce SUPERGEN, an efficient tile-based framework for ultra-high-resolution video generation. SUPERGEN features a novel training-free algorithmic innovation with tiling to successfully support a wide range of resolutions without additional training efforts while significantly reducing both memory footprint and computational complexity. Moreover, SUPERGEN incorporates a tile-tailored, adaptive, region-aware caching strategy that accelerates video generation by exploiting redundancy across denoising steps and spatial regions. SUPERGEN also integrates cache-guided, communication-minimized tile parallelism for enhanced throughput and minimized latency. Evaluations show that SUPERGEN maximizes performance gains while achieving high output quality across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently achieved remarkable success in generative tasks (e.g., image and video generation), and the demand for high-quality content (e.g., 2K/4K videos) is rapidly increasing across various domains. However, generating ultra-high-resolution videos on existing standard-resolution (e.g., 720p) platforms remains challenging due to the excessive re-training requirements and prohibitively high computational and memory costs. To this end, we introduce SUPERGEN, an efficient tile-based framework for ultra-high-resolution video generation. SUPERGEN features a novel training-free algorithmic innovation with tiling to successfully support a wide range of resolutions without additional training efforts while significantly reducing both memory footprint and computational complexity. Moreover, SUPERGEN incorporates a tile-tailored, adaptive, region-aware caching strategy that accelerates video generation by exploiting redundancy across denoising steps and spatial regions. SUPERGEN also integrates cache-guided, communication-minimized tile parallelism for enhanced throughput and minimized latency. Evaluations show that SUPERGEN maximizes performance gains while achieving high output quality across various benchmarks."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-25T07:49:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Fanjiang Ye"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yi Mu"
                    },
                    {
                        "name": "Jucheng Shen"
                    },
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Kaijian Wang"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Triston Cao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "T. S. Eugene Ng"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang"
            },
            {
                "id": "http://arxiv.org/abs/2511.12876v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.12876v2",
                "title": "Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think, Speak, Decide: Language-Augmented Multi-Agent Reinforcement Learning for Economic Decision-Making"
                },
                "updated": "2025-12-14T15:38:21Z",
                "updated_parsed": [
                    2025,
                    12,
                    14,
                    15,
                    38,
                    21,
                    6,
                    348,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.12876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.12876v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Economic decision-making depends not only on structured signals such as prices and taxes, but also on unstructured language, including peer dialogue and media narratives. While multi-agent reinforcement learning (MARL) has shown promise in optimizing economic decisions, it struggles with the semantic ambiguity and contextual richness of language. We propose LAMP (Language-Augmented Multi-Agent Policy), a framework that integrates language into economic decision-making and narrows the gap to real-world settings. LAMP follows a Think-Speak-Decide pipeline: (1) Think interprets numerical observations to extract short-term shocks and long-term trends, caching high-value reasoning trajectories; (2) Speak crafts and exchanges strategic messages based on reasoning, updating beliefs by parsing peer communications; and (3) Decide fuses numerical data, reasoning, and reflections into a MARL policy to optimize language-augmented decision-making. Experiments in economic simulation show that LAMP outperforms both MARL and LLM-only baselines in cumulative return (+63.5%, +34.0%), robustness (+18.8%, +59.4%), and interpretability. These results demonstrate the potential of language-augmented policies to deliver more effective and robust economic strategies."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-17T02:09:18Z",
                "published_parsed": [
                    2025,
                    11,
                    17,
                    2,
                    9,
                    18,
                    0,
                    321,
                    0
                ],
                "arxiv_comment": "Extended version of a submission to AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Heyang Ma"
                    },
                    {
                        "name": "Qirui Mi"
                    },
                    {
                        "name": "Qipeng Yang"
                    },
                    {
                        "name": "Zijun Fan"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Haifeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Haifeng Zhang"
                },
                "author": "Haifeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2511.06029v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06029v3",
                "title": "Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving"
                },
                "updated": "2025-12-14T14:18:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    14,
                    14,
                    18,
                    27,
                    6,
                    348,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06029v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06029v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative reasoning with large language models (LLMs) often involves long decoding sequences, leading to substantial memory and latency overheads from accumulating key-value (KV) caches. While existing KV compression methods primarily focus on reducing prefill memory from long input sequences, they fall short in addressing the dynamic and layer-sensitive nature of long-form generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV cache management framework that introduces adaptivity along both the spatial and temporal dimensions of decoding. Along the spatial dimension, Lethe performs layerwise sparsity-aware allocation, assigning token pruning budgets to each transformer layer based on estimated attention redundancy. Along the temporal dimension, Lethe conducts multi-round token pruning during generation, driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends traditional recency-based heuristics by also considering token relevance derived from evolving attention patterns, enabling informed decisions about which tokens to retain or evict. Empirical results demonstrate that Lethe achieves a favorable balance between efficiency and generation quality across diverse models and tasks, increases throughput by up to 2.56x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative reasoning with large language models (LLMs) often involves long decoding sequences, leading to substantial memory and latency overheads from accumulating key-value (KV) caches. While existing KV compression methods primarily focus on reducing prefill memory from long input sequences, they fall short in addressing the dynamic and layer-sensitive nature of long-form generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV cache management framework that introduces adaptivity along both the spatial and temporal dimensions of decoding. Along the spatial dimension, Lethe performs layerwise sparsity-aware allocation, assigning token pruning budgets to each transformer layer based on estimated attention redundancy. Along the temporal dimension, Lethe conducts multi-round token pruning during generation, driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends traditional recency-based heuristics by also considering token relevance derived from evolving attention patterns, enabling informed decisions about which tokens to retain or evict. Empirical results demonstrate that Lethe achieves a favorable balance between efficiency and generation quality across diverse models and tasks, increases throughput by up to 2.56x."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-08T14:52:43Z",
                "published_parsed": [
                    2025,
                    11,
                    8,
                    14,
                    52,
                    43,
                    5,
                    312,
                    0
                ],
                "arxiv_comment": "aaai26 camera-ready version, 10 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Hui Zeng"
                    },
                    {
                        "name": "Daming Zhao"
                    },
                    {
                        "name": "Pengfei Yang"
                    },
                    {
                        "name": "WenXuan Hou"
                    },
                    {
                        "name": "Tianyang Zheng"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Weiye Ji"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai"
            },
            {
                "id": "http://arxiv.org/abs/2501.16055v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2501.16055v2",
                "title": "Random Reshuffling for Stochastic Gradient Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Reshuffling for Stochastic Gradient Langevin Dynamics"
                },
                "updated": "2025-12-14T11:12:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    14,
                    11,
                    12,
                    56,
                    6,
                    348,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2501.16055v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2501.16055v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We examine the use of different randomisation policies for stochastic gradient algorithms used in sampling, based on first-order (or overdamped) Langevin dynamics, the most popular of which is known as Stochastic Gradient Langevin Dynamics. Conventionally, this algorithm is combined with a specific stochastic gradient strategy, called Robbins-Monro. In this work, we study an alternative strategy, Random Reshuffling, and show convincingly that it leads to improved performance via: a) a proof of reduced bias in the Wasserstein metric for strongly convex, gradient Lipschitz potentials; b) an analytical demonstration of reduced bias for a Gaussian model problem; and c) an empirical demonstration of reduced bias in numerical experiments for some logistic regression problems. This is especially important since Random Reshuffling is typically more efficient due to memory access and cache reasons. Such acceleration for the Random Reshuffling policy is familiar from the optimisation literature on stochastic gradient descent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the use of different randomisation policies for stochastic gradient algorithms used in sampling, based on first-order (or overdamped) Langevin dynamics, the most popular of which is known as Stochastic Gradient Langevin Dynamics. Conventionally, this algorithm is combined with a specific stochastic gradient strategy, called Robbins-Monro. In this work, we study an alternative strategy, Random Reshuffling, and show convincingly that it leads to improved performance via: a) a proof of reduced bias in the Wasserstein metric for strongly convex, gradient Lipschitz potentials; b) an analytical demonstration of reduced bias for a Gaussian model problem; and c) an empirical demonstration of reduced bias in numerical experiments for some logistic regression problems. This is especially important since Random Reshuffling is typically more efficient due to memory access and cache reasons. Such acceleration for the Random Reshuffling policy is familiar from the optimisation literature on stochastic gradient descent."
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-01-27T13:53:12Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "arxiv_comment": "23 pages, 11 figures",
                "arxiv_primary_category": {
                    "term": "math.NA"
                },
                "authors": [
                    {
                        "name": "Luke Shaw"
                    },
                    {
                        "name": "Peter A. Whalley"
                    }
                ],
                "author_detail": {
                    "name": "Peter A. Whalley"
                },
                "author": "Peter A. Whalley"
            },
            {
                "id": "http://arxiv.org/abs/2512.12604v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12604v1",
                "title": "No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Cache Left Idle: Accelerating diffusion model via Extreme-slimming Caching"
                },
                "updated": "2025-12-14T09:02:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    14,
                    9,
                    2,
                    18,
                    6,
                    348,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12604v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models achieve remarkable generative quality, but computational overhead scales with step count, model depth, and sequence length. Feature caching is effective since adjacent timesteps yield highly similar features. However, an inherent trade-off remains: aggressive timestep reuse offers large speedups but can easily cross the critical line, hurting fidelity, while block- or token-level reuse is safer but yields limited computational savings. We present X-Slim (eXtreme-Slimming Caching), a training-free, cache-based accelerator that, to our knowledge, is the first unified framework to exploit cacheable redundancy across timesteps, structure (blocks), and space (tokens). Rather than simply mixing levels, X-Slim introduces a dual-threshold controller that turns caching into a push-then-polish process: it first pushes reuse at the timestep level up to an early-warning line, then switches to lightweight block- and token-level refresh to polish the remaining redundancy, and triggers full inference once the critical line is crossed to reset accumulated error. At each level, context-aware indicators decide when and where to cache. Across diverse tasks, X-Slim advances the speed-quality frontier. On FLUX.1-dev and HunyuanVideo, it reduces latency by up to 4.97x and 3.52x with minimal perceptual loss. On DiT-XL/2, it reaches 3.13x acceleration and improves FID by 2.42 over prior methods."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-14T09:02:18Z",
                "published_parsed": [
                    2025,
                    12,
                    14,
                    9,
                    2,
                    18,
                    6,
                    348,
                    0
                ],
                "arxiv_comment": "Project page: https://thu-accdiff.github.io/xslim-page/ Code: https://github.com/THU-AccDiff/xslim",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tingyan Wen"
                    },
                    {
                        "name": "Haoyu Li"
                    },
                    {
                        "name": "Yihuang Chen"
                    },
                    {
                        "name": "Xing Zhou"
                    },
                    {
                        "name": "Lifei Zhu"
                    },
                    {
                        "name": "Xueqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xueqian Wang"
                },
                "author": "Xueqian Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.12595v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12595v1",
                "title": "Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Enhanced Large Language Models for High-Resolution Image Synthesis and Multimodal Data Interpretation"
                },
                "updated": "2025-12-14T08:28:50Z",
                "updated_parsed": [
                    2025,
                    12,
                    14,
                    8,
                    28,
                    50,
                    6,
                    348,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12595v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12595v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research introduces a transformative framework for integrating Vision-Enhanced Large Language Models (LLMs) with advanced transformer-based architectures to tackle challenges in high-resolution image synthesis and multimodal data interpretation. The proposed model incorporates a rectified flow mechanism that connects noise and data with linear paths, enabling efficient and high-quality generation. A bidirectional tokenization strategy is employed to seamlessly merge inputs from text, image, and video modalities, fostering a unified understanding across diverse data types. By embedding spatial-temporal features and leveraging a hybrid text-image sequence modeling approach, the framework achieves unparalleled fidelity in synthesized images and coherent multimodal representations. The architecture is optimized with a noise-aware learning algorithm, addressing discrepancies in noisy data distributions and improving generative performance under varying input conditions. Rigorous evaluations on benchmark datasets demonstrate a 25% increase in image resolution clarity and a 20% reduction in computational requirements compared to diffusion-based methods. Furthermore, the model exhibits robust scalability and adaptability, showcasing its potential in applications like autonomous systems, creative content generation, and advanced video analysis. This work underscores the role of vision-centric LLMs in redefining capabilities in computer vision and multimodal artificial intelligence."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-14T08:28:50Z",
                "published_parsed": [
                    2025,
                    12,
                    14,
                    8,
                    28,
                    50,
                    6,
                    348,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Karthikeya KV"
                    }
                ],
                "author_detail": {
                    "name": "Karthikeya KV"
                },
                "author": "Karthikeya KV"
            },
            {
                "id": "http://arxiv.org/abs/2505.18231v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.18231v2",
                "title": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache"
                },
                "updated": "2025-12-14T08:17:35Z",
                "updated_parsed": [
                    2025,
                    12,
                    14,
                    8,
                    17,
                    35,
                    6,
                    348,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.18231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.18231v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gain over full-precision baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\\times$ throughput gain over full-precision baselines."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-23T12:40:07Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    12,
                    40,
                    7,
                    4,
                    143,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Donghyun Son"
                    },
                    {
                        "name": "Euntae Choi"
                    },
                    {
                        "name": "Sungjoo Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Sungjoo Yoo"
                },
                "author": "Sungjoo Yoo"
            },
            {
                "id": "http://arxiv.org/abs/2512.12498v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12498v1",
                "title": "Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Cache-Based Few-Shot Classification via Patch-Driven Relational Gated Graph Attention"
                },
                "updated": "2025-12-13T23:53:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    13,
                    23,
                    53,
                    0,
                    5,
                    347,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12498v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12498v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the \"platinum minutes\" and the broader \"golden hour\" window in time-critical UAV-driven search-and-rescue and combat casualty care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot image classification remains difficult under limited supervision and visual domain shift. Recent cache-based adaptation approaches (e.g., Tip-Adapter) address this challenge to some extent by learning lightweight residual adapters over frozen features, yet they still inherit CLIP's tendency to encode global, general-purpose representations that are not optimally discriminative to adapt the generalist to the specialist's domain in low-data regimes. We address this limitation with a novel patch-driven relational refinement that learns cache adapter weights from intra-image patch dependencies rather than treating an image embedding as a monolithic vector. Specifically, we introduce a relational gated graph attention network that constructs a patch graph and performs edge-aware attention to emphasize informative inter-patch interactions, producing context-enriched patch embeddings. A learnable multi-aggregation pooling then composes these into compact, task-discriminative representations that better align cache keys with the target few-shot classes. Crucially, the proposed graph refinement is used only during training to distil relational structure into the cache, incurring no additional inference cost beyond standard cache lookup. Final predictions are obtained by a residual fusion of cache similarity scores with CLIP zero-shot logits. Extensive evaluations on 11 benchmarks show consistent gains over state-of-the-art CLIP adapter and cache-based baselines while preserving zero-shot efficiency. We further validate battlefield relevance by introducing an Injured vs. Uninjured Soldier dataset for casualty recognition. It is motivated by the operational need to support triage decisions within the \"platinum minutes\" and the broader \"golden hour\" window in time-critical UAV-driven search-and-rescue and combat casualty care."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-13T23:53:00Z",
                "published_parsed": [
                    2025,
                    12,
                    13,
                    23,
                    53,
                    0,
                    5,
                    347,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Tasweer Ahmad"
                    },
                    {
                        "name": "Arindam Sikdar"
                    },
                    {
                        "name": "Sandip Pradhan"
                    },
                    {
                        "name": "Ardhendu Behera"
                    }
                ],
                "author_detail": {
                    "name": "Ardhendu Behera"
                },
                "author": "Ardhendu Behera"
            },
            {
                "id": "http://arxiv.org/abs/2512.12243v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12243v1",
                "title": "CAR-CHASE: Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAR-CHASE: Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement"
                },
                "updated": "2025-12-13T08:42:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    13,
                    8,
                    42,
                    18,
                    5,
                    347,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12243v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-Agent Path Finding (MAPF) for car-like robots, addressed by algorithms such as Conflict-Based Search with Continuous Time (CL-CBS), faces significant computational challenges due to expensive kinematic heuristic calculations. Traditional heuristic caching assumes that the heuristic function depends only on the state, which is incorrect in CBS where constraints from conflict resolution make the search space context-dependent. We propose \\textbf{CAR-CHASE} (Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement), a novel approach that combines \\textbf{conflict-aware heuristic caching} -- which caches heuristic values based on both state and relevant constraint context -- with an \\textbf{adaptive hybrid heuristic} that intelligently switches between fast approximate and exact computations. Our key innovations are (1) a compact \\emph{conflict fingerprint} that efficiently encodes which constraints affect a state's heuristic, (2) a relevance filter using spatial, temporal, and geometric criteria, and (3) an adaptive switching strategy with theoretical quality bounds. Experimental evaluation on 480 benchmark instances with varying agent counts (10 to 30) and obstacle densities (0\\% and 50\\%) demonstrates a geometric mean speedup of 2.46$\\times$ over the baseline CL-CBS implementation while maintaining solution optimality. The optimizations improve success rate from 77.9\\% to 84.8\\% (+6.9 percentage points), reduce total runtime by 70.1\\%, and enable solving 33 additional instances that previously timed out. Performance gains scale with problem complexity, reaching up to 4.06$\\times$ speedup for challenging 30-agent obstacle scenarios. Our techniques are general and applicable to other CBS variants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Path Finding (MAPF) for car-like robots, addressed by algorithms such as Conflict-Based Search with Continuous Time (CL-CBS), faces significant computational challenges due to expensive kinematic heuristic calculations. Traditional heuristic caching assumes that the heuristic function depends only on the state, which is incorrect in CBS where constraints from conflict resolution make the search space context-dependent. We propose \\textbf{CAR-CHASE} (Car-Like Robot Conflict-Aware Heuristic Adaptive Search Enhancement), a novel approach that combines \\textbf{conflict-aware heuristic caching} -- which caches heuristic values based on both state and relevant constraint context -- with an \\textbf{adaptive hybrid heuristic} that intelligently switches between fast approximate and exact computations. Our key innovations are (1) a compact \\emph{conflict fingerprint} that efficiently encodes which constraints affect a state's heuristic, (2) a relevance filter using spatial, temporal, and geometric criteria, and (3) an adaptive switching strategy with theoretical quality bounds. Experimental evaluation on 480 benchmark instances with varying agent counts (10 to 30) and obstacle densities (0\\% and 50\\%) demonstrates a geometric mean speedup of 2.46$\\times$ over the baseline CL-CBS implementation while maintaining solution optimality. The optimizations improve success rate from 77.9\\% to 84.8\\% (+6.9 percentage points), reduce total runtime by 70.1\\%, and enable solving 33 additional instances that previously timed out. Performance gains scale with problem complexity, reaching up to 4.06$\\times$ speedup for challenging 30-agent obstacle scenarios. Our techniques are general and applicable to other CBS variants."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-13T08:42:18Z",
                "published_parsed": [
                    2025,
                    12,
                    13,
                    8,
                    42,
                    18,
                    5,
                    347,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "HT To"
                    },
                    {
                        "name": "S Nguyen"
                    },
                    {
                        "name": "NH Pham"
                    }
                ],
                "author_detail": {
                    "name": "NH Pham"
                },
                "author": "NH Pham"
            },
            {
                "id": "http://arxiv.org/abs/2510.05476v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.05476v2",
                "title": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided Inter-Node Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided Inter-Node Communications"
                },
                "updated": "2025-12-13T06:18:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    13,
                    6,
                    18,
                    18,
                    5,
                    347,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.05476v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.05476v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3712285.3759816",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Message Passing Interface (MPI) is a foundational programming model for high-performance computing. MPI libraries traditionally employ network interconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP and RoCE) with complex software stacks for cross-node communication. We present cMPI, the first work to optimize MPI point-to-point communication (both one-sided and two-sided) using CXL memory sharing on a real CXL platform, transforming cross-node communication into memory transactions and data copies within CXL memory, bypassing traditional network protocols. We analyze performance across various interconnects and find that CXL memory sharing achieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in small- and medium-scale clusters. We address challenges of CXL memory sharing for MPI communication, including data object management over the dax representation [50], cache coherence, and atomic operations. Overall, cMPI outperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x and 72x in latency and bandwidth, respectively, for small messages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Message Passing Interface (MPI) is a foundational programming model for high-performance computing. MPI libraries traditionally employ network interconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP and RoCE) with complex software stacks for cross-node communication. We present cMPI, the first work to optimize MPI point-to-point communication (both one-sided and two-sided) using CXL memory sharing on a real CXL platform, transforming cross-node communication into memory transactions and data copies within CXL memory, bypassing traditional network protocols. We analyze performance across various interconnects and find that CXL memory sharing achieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in small- and medium-scale clusters. We address challenges of CXL memory sharing for MPI communication, including data object management over the dax representation [50], cache coherence, and atomic operations. Overall, cMPI outperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x and 72x in latency and bandwidth, respectively, for small messages."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-07T00:32:45Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    0,
                    32,
                    45,
                    1,
                    280,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Bin Ma"
                    },
                    {
                        "name": "Jongryool Kim"
                    },
                    {
                        "name": "Byungil Koh"
                    },
                    {
                        "name": "Hoshik Kim"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_doi": "10.1145/3712285.3759816"
            },
            {
                "id": "http://arxiv.org/abs/2512.12091v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12091v1",
                "title": "GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphPerf-RT: A Graph-Driven Performance Model for Hardware-Aware Scheduling of OpenMP Codes"
                },
                "updated": "2025-12-12T23:46:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    23,
                    46,
                    5,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12091v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Performance prediction for OpenMP workloads on heterogeneous embedded SoCs is challenging due to complex interactions between task DAG structure, control-flow irregularity, cache\n  and branch behavior, and thermal dynamics; classical heuristics struggle under workload irregularity, tabular regressors discard structural information, and model-free RL risks\n  overheating resource-constrained devices. We introduce GraphPerf-RT, the first surrogate that unifies task DAG topology, CFG-derived code semantics, and runtime context (per-core\n  DVFS, thermal state, utilization) in a heterogeneous graph representation with typed edges encoding precedence, placement, and contention. Multi-task evidential heads predict\n  makespan, energy, cache and branch misses, and utilization with calibrated uncertainty (Normal-Inverse-Gamma), enabling risk-aware scheduling that filters low-confidence rollouts.\n  We validate GraphPerf-RT on three embedded ARM platforms (Jetson TX2, Jetson Orin NX, RUBIK Pi), achieving R^2 > 0.95 with well-calibrated uncertainty (ECE < 0.05). To\n  demonstrate end-to-end scheduling utility, we integrate the surrogate with four RL methods on Jetson TX2: single-agent model-free (SAMFRL), single-agent model-based (SAMBRL),\n  multi-agent model-free (MAMFRL-D3QN), and multi-agent model-based (MAMBRL-D3QN). Experiments across 5 seeds (200 episodes each) show that MAMBRL-D3QN with GraphPerf-RT as the\n  world model achieves 66% makespan reduction (0.97 +/- 0.35s) and 82% energy reduction (0.006 +/- 0.005J) compared to model-free baselines, demonstrating that accurate,\n  uncertainty-aware surrogates enable effective model-based planning on thermally constrained embedded systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance prediction for OpenMP workloads on heterogeneous embedded SoCs is challenging due to complex interactions between task DAG structure, control-flow irregularity, cache\n  and branch behavior, and thermal dynamics; classical heuristics struggle under workload irregularity, tabular regressors discard structural information, and model-free RL risks\n  overheating resource-constrained devices. We introduce GraphPerf-RT, the first surrogate that unifies task DAG topology, CFG-derived code semantics, and runtime context (per-core\n  DVFS, thermal state, utilization) in a heterogeneous graph representation with typed edges encoding precedence, placement, and contention. Multi-task evidential heads predict\n  makespan, energy, cache and branch misses, and utilization with calibrated uncertainty (Normal-Inverse-Gamma), enabling risk-aware scheduling that filters low-confidence rollouts.\n  We validate GraphPerf-RT on three embedded ARM platforms (Jetson TX2, Jetson Orin NX, RUBIK Pi), achieving R^2 > 0.95 with well-calibrated uncertainty (ECE < 0.05). To\n  demonstrate end-to-end scheduling utility, we integrate the surrogate with four RL methods on Jetson TX2: single-agent model-free (SAMFRL), single-agent model-based (SAMBRL),\n  multi-agent model-free (MAMFRL-D3QN), and multi-agent model-based (MAMBRL-D3QN). Experiments across 5 seeds (200 episodes each) show that MAMBRL-D3QN with GraphPerf-RT as the\n  world model achieves 66% makespan reduction (0.97 +/- 0.35s) and 82% energy reduction (0.006 +/- 0.005J) compared to model-free baselines, demonstrating that accurate,\n  uncertainty-aware surrogates enable effective model-based planning on thermally constrained embedded systems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T23:46:05Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    23,
                    46,
                    5,
                    4,
                    346,
                    0
                ],
                "arxiv_comment": "36 pages, 1 figure, 7 tables",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Mohammad Pivezhandi"
                    },
                    {
                        "name": "Mahdi Banisharif"
                    },
                    {
                        "name": "Saeed Bakhshan"
                    },
                    {
                        "name": "Abusayeed Saifullah"
                    },
                    {
                        "name": "Ali Jannesari"
                    }
                ],
                "author_detail": {
                    "name": "Ali Jannesari"
                },
                "author": "Ali Jannesari"
            },
            {
                "id": "http://arxiv.org/abs/2506.13059v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.13059v2",
                "title": "Multipole Attention for Efficient Long Context Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multipole Attention for Efficient Long Context Reasoning"
                },
                "updated": "2025-12-12T19:51:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    19,
                    51,
                    26,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.13059v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.13059v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on complex problem-solving tasks. While these models have attained high accuracy by leveraging additional computation at test time, they need to generate long chain-of-thought reasoning in order to think before answering, which requires generating thousands of tokens. While sparse attention methods can help reduce the KV cache pressure induced by this long autoregressive reasoning, these methods can introduce errors which disrupt the reasoning process. Additionally, prior methods often pre-process the input to make it easier to identify the important prompt tokens when computing attention during generation, and this pre-processing is challenging to perform online for newly generated reasoning tokens. Our work addresses these challenges by introducing Multipole Attention, which accelerates autoregressive reasoning by only computing exact attention for the most important tokens, while maintaining approximate representations for the remaining tokens. Our method first performs clustering to group together semantically similar key vectors, and then uses the cluster centroids both to identify important key vectors and to approximate the remaining key vectors in order to retain high accuracy. We design a fast cluster update process to quickly re-cluster the input and previously generated tokens, thereby allowing for accelerating attention to the previous output tokens. We evaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our approach can maintain accuracy on complex reasoning tasks even with aggressive attention sparsity settings. We also provide kernel implementations to demonstrate the practical efficiency gains from our method, achieving up to 4.5$\\times$ speedup for attention in long-context reasoning applications. Our code is available at https://github.com/SqueezeAILab/MultipoleAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have shown promising accuracy improvements on complex problem-solving tasks. While these models have attained high accuracy by leveraging additional computation at test time, they need to generate long chain-of-thought reasoning in order to think before answering, which requires generating thousands of tokens. While sparse attention methods can help reduce the KV cache pressure induced by this long autoregressive reasoning, these methods can introduce errors which disrupt the reasoning process. Additionally, prior methods often pre-process the input to make it easier to identify the important prompt tokens when computing attention during generation, and this pre-processing is challenging to perform online for newly generated reasoning tokens. Our work addresses these challenges by introducing Multipole Attention, which accelerates autoregressive reasoning by only computing exact attention for the most important tokens, while maintaining approximate representations for the remaining tokens. Our method first performs clustering to group together semantically similar key vectors, and then uses the cluster centroids both to identify important key vectors and to approximate the remaining key vectors in order to retain high accuracy. We design a fast cluster update process to quickly re-cluster the input and previously generated tokens, thereby allowing for accelerating attention to the previous output tokens. We evaluate our method using emerging LRMs such as Qwen-8B, demonstrating that our approach can maintain accuracy on complex reasoning tasks even with aggressive attention sparsity settings. We also provide kernel implementations to demonstrate the practical efficiency gains from our method, achieving up to 4.5$\\times$ speedup for attention in long-context reasoning applications. Our code is available at https://github.com/SqueezeAILab/MultipoleAttention."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-16T03:00:40Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    3,
                    0,
                    40,
                    0,
                    167,
                    0
                ],
                "arxiv_comment": "15 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "arxiv_journal_ref": "NeurIPS 2025",
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sebastian Zhao"
                    },
                    {
                        "name": "Luca Manolache"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Yakun Sophia Shao"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami"
            },
            {
                "id": "http://arxiv.org/abs/2512.12008v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.12008v1",
                "title": "Hold Onto That Thought: Assessing KV Cache Compression On Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hold Onto That Thought: Assessing KV Cache Compression On Reasoning"
                },
                "updated": "2025-12-12T19:50:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    19,
                    50,
                    34,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.12008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.12008v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable performance on long-context tasks, but are often bottlenecked by memory constraints. Namely, the KV cache, which is used to significantly speed up attention computations, grows linearly with context length. A suite of compression algorithms has been introduced to alleviate cache growth by evicting unimportant tokens. However, several popular strategies are targeted towards the prefill phase, i.e., processing long prompt context, and their performance is rarely assessed on reasoning tasks requiring long decoding. In particular, short but complex prompts, such as those in benchmarks like GSM8K and MATH500, often benefit from multi-step reasoning and self-reflection, resulting in thinking sequences thousands of tokens long. In this work, we benchmark the performance of several popular compression strategies on long-reasoning tasks. For the non-reasoning Llama-3.1-8B-Instruct, we determine that no singular strategy fits all, and that performance is heavily influenced by dataset type. However, we discover that H2O and our decoding-enabled variant of SnapKV are dominant strategies for reasoning models, indicating the utility of heavy-hitter tracking for reasoning traces. We also find that eviction strategies at low budgets can produce longer reasoning traces, revealing a tradeoff between cache size and inference costs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T19:50:34Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    19,
                    50,
                    34,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Aadi Palnitkar"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Hyunwoo Jae"
                    },
                    {
                        "name": "Kyle Rui Sang"
                    },
                    {
                        "name": "Dixi Yao"
                    },
                    {
                        "name": "Shayan Shabihi"
                    },
                    {
                        "name": "Fuheng Zhao"
                    },
                    {
                        "name": "Tian Li"
                    },
                    {
                        "name": "Ce Zhang"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Kunpeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kunpeng Zhang"
                },
                "author": "Kunpeng Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.11769v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11769v1",
                "title": "BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLURR: A Boosted Low-Resource Inference for Vision-Language-Action Models"
                },
                "updated": "2025-12-12T18:30:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    18,
                    30,
                    45,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11769v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11769v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language-action (VLA) models enable impressive zero shot manipulation, but their inference stacks are often too heavy for responsive web demos or high frequency robot control on commodity GPUs. We present BLURR, a lightweight inference wrapper that can be plugged into existing VLA controllers without retraining or changing model checkpoints. Instantiated on the pi-zero VLA controller, BLURR keeps the original observation interfaces and accelerates control by combining an instruction prefix key value cache, mixed precision execution, and a single step rollout schedule that reduces per step computation. In our SimplerEnv based evaluation, BLURR maintains task success rates comparable to the original controller while significantly lowering effective FLOPs and wall clock latency. We also build an interactive web demo that allows users to switch between controllers and toggle inference options in real time while watching manipulation episodes. This highlights BLURR as a practical approach for deploying modern VLA policies under tight compute budgets."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T18:30:45Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    18,
                    30,
                    45,
                    4,
                    346,
                    0
                ],
                "arxiv_comment": "10 pages, 3 figures. Code and integration scripts will be released at this http URL: https://github.com/JijiKing-Sam/BLURR-A-Boosted-Low-Resource-Inference-for-Vision-Language-Action-Model",
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Xiaoyu Ma"
                    },
                    {
                        "name": "Zhengqing Yuan"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Kaiwen Shi"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Yanfang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Yanfang Ye"
                },
                "author": "Yanfang Ye"
            },
            {
                "id": "http://arxiv.org/abs/2512.11550v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11550v1",
                "title": "PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PD-Swap: Prefill-Decode Logic Swapping for End-to-End LLM Inference on Edge FPGAs via Dynamic Partial Reconfiguration"
                },
                "updated": "2025-12-12T13:35:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    13,
                    35,
                    9,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11550v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aggressively quantized large language models (LLMs), such as BitNet-style 1.58-bit Transformers with ternary weights, make it feasible to deploy generative AI on low-power edge FPGAs. However, as prompts grow to tens of thousands of tokens, edge hardware performance drops sharply with sequence length due to quadratic prefill cost and rapidly increasing KV-cache bandwidth demands, making inference latency of longer context length a first-order system concern. Recent studies on LLMs expose a fundamental prefill-decode asymmetry: prefill is compute-bound and dominated by dense matrix-matrix operations, whereas decoding is memory-bandwidth-bound and dominated by KV-cache traffic. A static accelerator must provision resources and a single dataflow for both regimes, leading to duplicated attention logic, underutilized fabric, and tight LUT/URAM limits that cap model size and usable context. We propose a prefill--decode disaggregated LLM accelerator, PD-Swap, that uses Dynamic Partial Reconfiguration (DPR) to time-multiplex the attention module on edge FPGAs. The core table-lookup ternary matrix multiplication and weight-buffering engines remain static, while the attention subsystem is a reconfigurable partition with two phase-specialized architectures: a compute-heavy, token-parallel prefill engine and a bandwidth-optimized, KV-cache-centric decoding engine. A roofline-inspired model and design space exploration jointly optimize reconfigurable-region size, parallelism under reconfiguration and routability constraints, and reconfiguration latency is hidden by computation latency. PD-Swap achieves up to 27~tokens/s decoding throughput, outperforming prior state-of-the-art works by 1.3x--2.1x (larger gains at longer context lengths), without extra area cost."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T13:35:09Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    13,
                    35,
                    9,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Zhiheng Chen"
                    },
                    {
                        "name": "Ye Qiao"
                    },
                    {
                        "name": "Sitao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Sitao Huang"
                },
                "author": "Sitao Huang"
            },
            {
                "id": "http://arxiv.org/abs/2511.00473v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.00473v2",
                "title": "Drinfeld associators and Kashiwara-Vergne associators in higher genera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drinfeld associators and Kashiwara-Vergne associators in higher genera"
                },
                "updated": "2025-12-12T11:32:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    11,
                    32,
                    39,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.00473v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.00473v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "For $g\\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by Alekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in relation to the formality problem of the Goldman-Turaev Lie bialgebra on an oriented surface with a framing, is directly constructed from a genus $g$ analogue of a Drinfeld associator formulated by Gonzalez, which we call a Gonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus 0. The framing is automatically determined from the choice of a Gonzalez-Drinfeld associator, and in the case of genus 1, we show that only one particular framing is realised by our construction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For $g\\geq 0$, a genus $g$ Kashiwara-Vergne associator, introduced by Alekseev-Kawazumi-Kuno-Naef as a solution to the generalised KV equations in relation to the formality problem of the Goldman-Turaev Lie bialgebra on an oriented surface with a framing, is directly constructed from a genus $g$ analogue of a Drinfeld associator formulated by Gonzalez, which we call a Gonzalez-Drinfeld associator. The proof is based on Massuyeau's work in genus 0. The framing is automatically determined from the choice of a Gonzalez-Drinfeld associator, and in the case of genus 1, we show that only one particular framing is realised by our construction."
                },
                "tags": [
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-01T09:53:47Z",
                "published_parsed": [
                    2025,
                    11,
                    1,
                    9,
                    53,
                    47,
                    5,
                    305,
                    0
                ],
                "arxiv_comment": "40 pages. Minor corrections",
                "arxiv_primary_category": {
                    "term": "math.QA"
                },
                "authors": [
                    {
                        "name": "Toyo Taniguchi"
                    }
                ],
                "author_detail": {
                    "name": "Toyo Taniguchi"
                },
                "author": "Toyo Taniguchi"
            },
            {
                "id": "http://arxiv.org/abs/2512.11458v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11458v1",
                "title": "Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Skeleton-based Zero-Shot Action Recognition with Training-Free Test-Time Adaptation"
                },
                "updated": "2025-12-12T10:53:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    10,
                    53,
                    51,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11458v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Skeleton-Cache, the first training-free test-time adaptation framework for skeleton-based zero-shot action recognition (SZAR), aimed at improving model generalization to unseen actions during inference. Skeleton-Cache reformulates inference as a lightweight retrieval process over a non-parametric cache that stores structured skeleton representations, combining both global and fine-grained local descriptors. To guide the fusion of descriptor-wise predictions, we leverage the semantic reasoning capabilities of large language models (LLMs) to assign class-specific importance weights. By integrating these structured descriptors with LLM-guided semantic priors, Skeleton-Cache dynamically adapts to unseen actions without any additional training or access to training data. Extensive experiments on NTU RGB+D 60/120 and PKU-MMD II demonstrate that Skeleton-Cache consistently boosts the performance of various SZAR backbones under both zero-shot and generalized zero-shot settings. The code is publicly available at https://github.com/Alchemist0754/Skeleton-Cache."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T10:53:51Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    10,
                    53,
                    51,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Jingmin Zhu"
                    },
                    {
                        "name": "Anqi Zhu"
                    },
                    {
                        "name": "Hossein Rahmani"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Mohammed Bennamoun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    }
                ],
                "author_detail": {
                    "name": "Qiuhong Ke"
                },
                "author": "Qiuhong Ke"
            },
            {
                "id": "http://arxiv.org/abs/2512.11431v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11431v1",
                "title": "Proving DNSSEC Correctness: A Formal Approach to Secure Domain Name Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proving DNSSEC Correctness: A Formal Approach to Secure Domain Name Resolution"
                },
                "updated": "2025-12-12T10:12:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    10,
                    12,
                    6,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11431v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The Domain Name System Security Extensions (DNSSEC) are critical for preventing DNS spoofing, yet its specifications contain ambiguities and vulnerabilities that elude traditional \"break-and-fix\" approaches. A holistic, foundational security analysis of the protocol has thus remained an open problem. This paper introduces DNSSECVerif, the first framework for comprehensive, automated formal security analysis of the DNSSEC protocol suite. Built on the SAPIC+ symbolic verifier, our high-fidelity model captures protocol-level interactions, including cryptographic operations and stateful caching with fine-grained concurrency control. Using DNSSECVerif, we formally prove four of DNSSEC's core security guarantees and uncover critical ambiguities in the standards--notably, the insecure coexistence of NSEC and NSEC3. Our model also automatically rediscovers three classes of known attacks, demonstrating fundamental weaknesses in the protocol design. To bridge the model-to-reality gap, we validate our findings through targeted testing of mainstream DNS software and a large-scale measurement study of over 2.2 million open resolvers, confirming the real-world impact of these flaws. Our work provides crucial, evidence-based recommendations for hardening DNSSEC specifications and implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Domain Name System Security Extensions (DNSSEC) are critical for preventing DNS spoofing, yet its specifications contain ambiguities and vulnerabilities that elude traditional \"break-and-fix\" approaches. A holistic, foundational security analysis of the protocol has thus remained an open problem. This paper introduces DNSSECVerif, the first framework for comprehensive, automated formal security analysis of the DNSSEC protocol suite. Built on the SAPIC+ symbolic verifier, our high-fidelity model captures protocol-level interactions, including cryptographic operations and stateful caching with fine-grained concurrency control. Using DNSSECVerif, we formally prove four of DNSSEC's core security guarantees and uncover critical ambiguities in the standards--notably, the insecure coexistence of NSEC and NSEC3. Our model also automatically rediscovers three classes of known attacks, demonstrating fundamental weaknesses in the protocol design. To bridge the model-to-reality gap, we validate our findings through targeted testing of mainstream DNS software and a large-scale measurement study of over 2.2 million open resolvers, confirming the real-world impact of these flaws. Our work provides crucial, evidence-based recommendations for hardening DNSSEC specifications and implementations."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T10:12:06Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    10,
                    12,
                    6,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Qifan Zhang"
                    },
                    {
                        "name": "Zilin Shen"
                    },
                    {
                        "name": "Imtiaz Karim"
                    },
                    {
                        "name": "Elisa Bertino"
                    },
                    {
                        "name": "Zhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhou Li"
                },
                "author": "Zhou Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.11423v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11423v1",
                "title": "JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JoyAvatar: Real-time and Infinite Audio-Driven Avatar Generation with Autoregressive Diffusion"
                },
                "updated": "2025-12-12T10:06:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    10,
                    6,
                    1,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11423v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11423v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing DiT-based audio-driven avatar generation methods have achieved considerable progress, yet their broader application is constrained by limitations such as high computational overhead and the inability to synthesize long-duration videos. Autoregressive methods address this problem by applying block-wise autoregressive diffusion methods. However, these methods suffer from the problem of error accumulation and quality degradation. To address this, we propose JoyAvatar, an audio-driven autoregressive model capable of real-time inference and infinite-length video generation with the following contributions: (1) Progressive Step Bootstrapping (PSB), which allocates more denoising steps to initial frames to stabilize generation and reduce error accumulation; (2) Motion Condition Injection (MCI), enhancing temporal coherence by injecting noise-corrupted previous frames as motion condition; and (3) Unbounded RoPE via Cache-Resetting (URCR), enabling infinite-length generation through dynamic positional encoding. Our 1.3B-parameter causal model achieves 16 FPS on a single GPU and achieves competitive results in visual quality, temporal consistency, and lip synchronization."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T10:06:01Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    10,
                    6,
                    1,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Chaochao Li"
                    },
                    {
                        "name": "Ruikui Wang"
                    },
                    {
                        "name": "Liangbo Zhou"
                    },
                    {
                        "name": "Jinheng Feng"
                    },
                    {
                        "name": "Huaishao Luo"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Youzheng Wu"
                    },
                    {
                        "name": "Xiaodong He"
                    }
                ],
                "author_detail": {
                    "name": "Xiaodong He"
                },
                "author": "Xiaodong He"
            },
            {
                "id": "http://arxiv.org/abs/2512.11274v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11274v1",
                "title": "FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FilmWeaver: Weaving Consistent Multi-Shot Videos with Cache-Guided Autoregressive Diffusion"
                },
                "updated": "2025-12-12T04:34:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    4,
                    34,
                    53,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11274v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current video generation models perform well at single-shot synthesis but struggle with multi-shot videos, facing critical challenges in maintaining character and background consistency across shots and flexibly generating videos of arbitrary length and shot count. To address these limitations, we introduce \\textbf{FilmWeaver}, a novel framework designed to generate consistent, multi-shot videos of arbitrary length. First, it employs an autoregressive diffusion paradigm to achieve arbitrary-length video generation. To address the challenge of consistency, our key insight is to decouple the problem into inter-shot consistency and intra-shot coherence. We achieve this through a dual-level cache mechanism: a shot memory caches keyframes from preceding shots to maintain character and scene identity, while a temporal memory retains a history of frames from the current shot to ensure smooth, continuous motion. The proposed framework allows for flexible, multi-round user interaction to create multi-shot videos. Furthermore, due to this decoupled design, our method demonstrates high versatility by supporting downstream tasks such as multi-concept injection and video extension. To facilitate the training of our consistency-aware method, we also developed a comprehensive pipeline to construct a high-quality multi-shot video dataset. Extensive experimental results demonstrate that our method surpasses existing approaches on metrics for both consistency and aesthetic quality, opening up new possibilities for creating more consistent, controllable, and narrative-driven video content. Project Page: https://filmweaver.github.io",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video generation models perform well at single-shot synthesis but struggle with multi-shot videos, facing critical challenges in maintaining character and background consistency across shots and flexibly generating videos of arbitrary length and shot count. To address these limitations, we introduce \\textbf{FilmWeaver}, a novel framework designed to generate consistent, multi-shot videos of arbitrary length. First, it employs an autoregressive diffusion paradigm to achieve arbitrary-length video generation. To address the challenge of consistency, our key insight is to decouple the problem into inter-shot consistency and intra-shot coherence. We achieve this through a dual-level cache mechanism: a shot memory caches keyframes from preceding shots to maintain character and scene identity, while a temporal memory retains a history of frames from the current shot to ensure smooth, continuous motion. The proposed framework allows for flexible, multi-round user interaction to create multi-shot videos. Furthermore, due to this decoupled design, our method demonstrates high versatility by supporting downstream tasks such as multi-concept injection and video extension. To facilitate the training of our consistency-aware method, we also developed a comprehensive pipeline to construct a high-quality multi-shot video dataset. Extensive experimental results demonstrate that our method surpasses existing approaches on metrics for both consistency and aesthetic quality, opening up new possibilities for creating more consistent, controllable, and narrative-driven video content. Project Page: https://filmweaver.github.io"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T04:34:53Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    4,
                    34,
                    53,
                    4,
                    346,
                    0
                ],
                "arxiv_comment": "AAAI-2026",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xiangyang Luo"
                    },
                    {
                        "name": "Qingyu Li"
                    },
                    {
                        "name": "Xiaokun Liu"
                    },
                    {
                        "name": "Wenyu Qin"
                    },
                    {
                        "name": "Miao Yang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Shao-Lun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Shao-Lun Huang"
                },
                "author": "Shao-Lun Huang"
            },
            {
                "id": "http://arxiv.org/abs/2512.11264v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11264v1",
                "title": "Electrical Stability of Cr2O3/\\b{eta}-Ga2O3 and NiOx/\\b{eta}-Ga2O3 Heterojunction Diodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electrical Stability of Cr2O3/\\b{eta}-Ga2O3 and NiOx/\\b{eta}-Ga2O3 Heterojunction Diodes"
                },
                "updated": "2025-12-12T03:57:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    3,
                    57,
                    52,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11264v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This work reports the electrical characteristics comparison study between Cr2O3 and NiOx based heterojunction diodes (HJD) on halide vapor phase epitaxy (HVPE) grown \\b{eta}-Ga2O3 epitaxial layers. Both as-fabricated Cr2O3 and NiOx HJDs exhibited forward current density in a range of 130-150 A/cm^2 at 5 V with rectifying ratios >10^10 and a reverse leakage current density at 10^-8 A/cm^2 at -5 V. The differential specific on-resistance of Cr2O3 and NiOx HJDs was 12.01 mΩ*cm^2 and 12.05 mΩ*cm^2, respectively. Breakdown voltages of Cr2O3 HJDs ranged from 1.4-1.9 kV and 1.5-2.3 kV for NiOx HJDs. Theoretical band alignment between Cr2O3 and \\b{eta}-Ga2O3 was calculated from first principles. The ambient exposed NiOx/HVPE \\b{eta}-Ga2O3 HJDs forward current density degraded after 10 days while that of Cr2O3/HVPE \\b{eta}-Ga2O3 HJDs remained nearly unchanged after the same amount of time. It was later confirmed that the ambient exposed sputtered NiOx sheet resistance (Rsh) degradation gave rise to the reduction of the forward current density of the NiOx based HJDs, and water (H2O) was qualitatively determined to be the agent attributed to the forward conduction degradation by measuring the Rsh of NiOx-on-sapphire reference wafer after exposing it to different environments. The Cr2O3/HVPE \\b{eta}-Ga2O3 HJD also exhibited enhanced thermal stability compared to the NiOx/\\b{eta}-Ga2O3 heterostructures at elevated temperatures. Interfacial nickel gallate (Ga2NiO4) phase formation expected from phase diagrams can explain the reduced thermal stability of NiOx/\\b{eta}-Ga2O3 HJDs. This study indicates that Cr2O3 is a stable p-type oxide for the realization of robust multi-kV \\b{eta}-Ga2O3 HJDs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work reports the electrical characteristics comparison study between Cr2O3 and NiOx based heterojunction diodes (HJD) on halide vapor phase epitaxy (HVPE) grown \\b{eta}-Ga2O3 epitaxial layers. Both as-fabricated Cr2O3 and NiOx HJDs exhibited forward current density in a range of 130-150 A/cm^2 at 5 V with rectifying ratios >10^10 and a reverse leakage current density at 10^-8 A/cm^2 at -5 V. The differential specific on-resistance of Cr2O3 and NiOx HJDs was 12.01 mΩ*cm^2 and 12.05 mΩ*cm^2, respectively. Breakdown voltages of Cr2O3 HJDs ranged from 1.4-1.9 kV and 1.5-2.3 kV for NiOx HJDs. Theoretical band alignment between Cr2O3 and \\b{eta}-Ga2O3 was calculated from first principles. The ambient exposed NiOx/HVPE \\b{eta}-Ga2O3 HJDs forward current density degraded after 10 days while that of Cr2O3/HVPE \\b{eta}-Ga2O3 HJDs remained nearly unchanged after the same amount of time. It was later confirmed that the ambient exposed sputtered NiOx sheet resistance (Rsh) degradation gave rise to the reduction of the forward current density of the NiOx based HJDs, and water (H2O) was qualitatively determined to be the agent attributed to the forward conduction degradation by measuring the Rsh of NiOx-on-sapphire reference wafer after exposing it to different environments. The Cr2O3/HVPE \\b{eta}-Ga2O3 HJD also exhibited enhanced thermal stability compared to the NiOx/\\b{eta}-Ga2O3 heterostructures at elevated temperatures. Interfacial nickel gallate (Ga2NiO4) phase formation expected from phase diagrams can explain the reduced thermal stability of NiOx/\\b{eta}-Ga2O3 HJDs. This study indicates that Cr2O3 is a stable p-type oxide for the realization of robust multi-kV \\b{eta}-Ga2O3 HJDs."
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T03:57:52Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    3,
                    57,
                    52,
                    4,
                    346,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph"
                },
                "authors": [
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Haochen Wang"
                    },
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Chinmoy Nath Saha"
                    },
                    {
                        "name": "Chris G. Van de Walle"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy"
            },
            {
                "id": "http://arxiv.org/abs/2512.11229v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11229v1",
                "title": "REST: Diffusion-based Real-time End-to-end Streaming Talking Head Generation via ID-Context Caching and Asynchronous Streaming Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REST: Diffusion-based Real-time End-to-end Streaming Talking Head Generation via ID-Context Caching and Asynchronous Streaming Distillation"
                },
                "updated": "2025-12-12T02:28:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    2,
                    28,
                    52,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11229v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11229v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion models have significantly advanced the field of talking head generation. However, the slow inference speeds and non-autoregressive paradigms severely constrain the application of diffusion-based THG models. In this study, we propose REST, the first diffusion-based, real-time, end-to-end streaming audio-driven talking head generation framework. To support real-time end-to-end generation, a compact video latent space is first learned through high spatiotemporal VAE compression. Additionally, to enable autoregressive streaming within the compact video latent space, we introduce an ID-Context Cache mechanism, which integrates ID-Sink and Context-Cache principles to key-value caching for maintaining temporal consistency and identity coherence during long-time streaming generation. Furthermore, an Asynchronous Streaming Distillation (ASD) training strategy is proposed to mitigate error accumulation in autoregressive generation and enhance temporal consistency, which leverages a non-streaming teacher with an asynchronous noise schedule to supervise the training of the streaming student model. REST bridges the gap between autoregressive and diffusion-based approaches, demonstrating substantial value for applications requiring real-time talking head generation. Experimental results demonstrate that REST outperforms state-of-the-art methods in both generation speed and overall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have significantly advanced the field of talking head generation. However, the slow inference speeds and non-autoregressive paradigms severely constrain the application of diffusion-based THG models. In this study, we propose REST, the first diffusion-based, real-time, end-to-end streaming audio-driven talking head generation framework. To support real-time end-to-end generation, a compact video latent space is first learned through high spatiotemporal VAE compression. Additionally, to enable autoregressive streaming within the compact video latent space, we introduce an ID-Context Cache mechanism, which integrates ID-Sink and Context-Cache principles to key-value caching for maintaining temporal consistency and identity coherence during long-time streaming generation. Furthermore, an Asynchronous Streaming Distillation (ASD) training strategy is proposed to mitigate error accumulation in autoregressive generation and enhance temporal consistency, which leverages a non-streaming teacher with an asynchronous noise schedule to supervise the training of the streaming student model. REST bridges the gap between autoregressive and diffusion-based approaches, demonstrating substantial value for applications requiring real-time talking head generation. Experimental results demonstrate that REST outperforms state-of-the-art methods in both generation speed and overall performance."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T02:28:52Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    2,
                    28,
                    52,
                    4,
                    346,
                    0
                ],
                "arxiv_comment": "10pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yuzhe Weng"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Jun Du"
                    },
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Xiaoyan Wu"
                    },
                    {
                        "name": "Shan He"
                    },
                    {
                        "name": "Bing Yin"
                    },
                    {
                        "name": "Cong Liu"
                    },
                    {
                        "name": "Qingfeng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qingfeng Liu"
                },
                "author": "Qingfeng Liu"
            },
            {
                "id": "http://arxiv.org/abs/2504.21228v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2504.21228v2",
                "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt Injection Attacks"
                },
                "updated": "2025-12-12T02:25:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    2,
                    25,
                    34,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2504.21228v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2504.21228v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) are susceptible to indirect prompt injection attacks, in which the model inadvertently responds to task messages injected within the prompt context. This vulnerability stems from LLMs' inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune, a defense method that identifies and prunes task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to interpret the input prompt context purely as data rather than as cues for instruction following. To identify these neurons, we introduce a neural attribution mechanism guided by a preferential attribution loss, which enables effective attribution with only a few samples while preserving response quality after pruning. We further enhance the efficacy of neural attribution by leveraging an observed triggering effect inherent in the model's response generation behavior. Notably, our approach does not impose additional formatting on the prompt or introduce extra test-time LLM calls. Experiments show that CachePrune can significantly reduce attack success rates while maintaining clean response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are susceptible to indirect prompt injection attacks, in which the model inadvertently responds to task messages injected within the prompt context. This vulnerability stems from LLMs' inability to distinguish between data and instructions within a prompt. In this paper, we propose CachePrune, a defense method that identifies and prunes task-triggering neurons from the KV cache of the input prompt context. By pruning such neurons, we encourage the LLM to interpret the input prompt context purely as data rather than as cues for instruction following. To identify these neurons, we introduce a neural attribution mechanism guided by a preferential attribution loss, which enables effective attribution with only a few samples while preserving response quality after pruning. We further enhance the efficacy of neural attribution by leveraging an observed triggering effect inherent in the model's response generation behavior. Notably, our approach does not impose additional formatting on the prompt or introduce extra test-time LLM calls. Experiments show that CachePrune can significantly reduce attack success rates while maintaining clean response quality."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-04-29T23:42:21Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Subrata Mitra"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley"
            },
            {
                "id": "http://arxiv.org/abs/2512.11221v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.11221v1",
                "title": "Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference"
                },
                "updated": "2025-12-12T02:02:02Z",
                "updated_parsed": [
                    2025,
                    12,
                    12,
                    2,
                    2,
                    2,
                    4,
                    346,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.11221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.11221v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-12T02:02:02Z",
                "published_parsed": [
                    2025,
                    12,
                    12,
                    2,
                    2,
                    2,
                    4,
                    346,
                    0
                ],
                "arxiv_comment": "6 pages, 3 tables , 1 figure",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Adilet Metinov"
                    },
                    {
                        "name": "Gulida M. Kudakeeva"
                    },
                    {
                        "name": "Bolotbek uulu Nursultan"
                    },
                    {
                        "name": "Gulnara D. Kabaeva"
                    }
                ],
                "author_detail": {
                    "name": "Gulnara D. Kabaeva"
                },
                "author": "Gulnara D. Kabaeva"
            },
            {
                "id": "http://arxiv.org/abs/2511.11907v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.11907v2",
                "title": "KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVSwap: Disk-aware KV Cache Offloading for Long-Context On-device Inference"
                },
                "updated": "2025-12-11T23:35:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    11,
                    23,
                    35,
                    19,
                    3,
                    345,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.11907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.11907v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \\emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size. Existing KV-cache offloading schemes are designed to transfer cache data from GPU memory to CPU memory; however, they are not suitable for embedded and mobile systems, where the CPU and GPU (or NPU) typically share a unified memory and the non-volatile secondary storage (disk) offers limited I/O bandwidth. We present KVSwap, a software framework tailored for local devices that achieves high memory efficiency while effectively leveraging disk storage. KVSwap stores the full cache on disk, uses highly compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining generation quality over existing KV cache offloading schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models (LMs) underpin emerging mobile and embedded AI applications like meeting and video summarization and document analysis, which often require processing multiple long-context inputs. Running an LM locally on-device improves privacy, enables offline use, and reduces cost, but long-context inference quickly hits a \\emph{memory capacity wall} as the key-value (KV) cache grows linearly with context length and batch size. Existing KV-cache offloading schemes are designed to transfer cache data from GPU memory to CPU memory; however, they are not suitable for embedded and mobile systems, where the CPU and GPU (or NPU) typically share a unified memory and the non-volatile secondary storage (disk) offers limited I/O bandwidth. We present KVSwap, a software framework tailored for local devices that achieves high memory efficiency while effectively leveraging disk storage. KVSwap stores the full cache on disk, uses highly compact in-memory metadata to predict which entries to preload, overlaps computation with hardware-aware disk access, and orchestrates read patterns to match storage device characteristics. Our evaluation shows that across representative LMs and storage types, KVSwap delivers higher throughput under tight memory budgets while maintaining generation quality over existing KV cache offloading schemes."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-14T22:37:57Z",
                "published_parsed": [
                    2025,
                    11,
                    14,
                    22,
                    37,
                    57,
                    4,
                    318,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Huawei Zhang"
                    },
                    {
                        "name": "Chunwei Xia"
                    },
                    {
                        "name": "Zheng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Wang"
                },
                "author": "Zheng Wang"
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2512.20618v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20618v1",
                "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongVideoAgent: Multi-Agent Reasoning with Long Videos"
                },
                "updated": "2025-12-23T18:59:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    59,
                    49,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20618v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:59:49Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    59,
                    49,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Runtao Liu"
                    },
                    {
                        "name": "Ziyi Liu"
                    },
                    {
                        "name": "Jiaqi Tang"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Renjie Pi"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qifeng Chen"
                },
                "author": "Qifeng Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.20617v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20617v1",
                "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpatialTree: How Spatial Abilities Branch Out in MLLMs"
                },
                "updated": "2025-12-23T18:59:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    59,
                    46,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20617v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:59:46Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    59,
                    46,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "webpage: https://spatialtree.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yuxi Xiao"
                    },
                    {
                        "name": "Longfei Li"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Xinhang Liu"
                    },
                    {
                        "name": "Sida Peng"
                    },
                    {
                        "name": "Yunchao Wei"
                    },
                    {
                        "name": "Xiaowei Zhou"
                    },
                    {
                        "name": "Bingyi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Bingyi Kang"
                },
                "author": "Bingyi Kang"
            },
            {
                "id": "http://arxiv.org/abs/2512.20612v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20612v1",
                "title": "Making Large Language Models Efficient Dense Retrievers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Large Language Models Efficient Dense Retrievers"
                },
                "updated": "2025-12-23T18:58:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    58,
                    25,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20612v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:58:25Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    58,
                    25,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Yibin Lei"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Andrew Yates"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Yates"
                },
                "author": "Andrew Yates"
            },
            {
                "id": "http://arxiv.org/abs/2512.17038v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17038v2",
                "title": "Do Generalized-Gamma Scale Mixtures of Normals Fit Large Image Datasets?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Generalized-Gamma Scale Mixtures of Normals Fit Large Image Datasets?"
                },
                "updated": "2025-12-23T18:43:04Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    43,
                    4,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17038v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17038v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A scale mixture of normals is a distribution formed by mixing a collection of normal distributions with fixed mean but different variances. A generalized gamma scale mixture draws the variances from a generalized gamma distribution. Generalized gamma scale mixtures of normals have been proposed as an attractive class of parametric priors for Bayesian inference in inverse imaging problems. Generalized gamma scale mixtures have two shape parameters, one that controls the behavior of the distribution about its mode, and the other that controls its tail decay. In this paper, we provide the first demonstration that the prior model is realistic for multiple large imaging data sets. We draw data from remote sensing, medical imaging, and image classification applications. We study the realism of the prior when applied to Fourier and wavelet (Haar and Gabor) transformations of the images, as well as to the coefficients produced by convolving the images against the filters used in the first layer of AlexNet, a popular convolutional neural network trained for image classification. We discuss data augmentation procedures that improve the fit of the model, procedures for identifying approximately exchangeable coefficients, and characterize the parameter regions that best describe the observed data sets. These regions are significantly broader than the region of primary focus in computational work. We show that this prior family provides a substantially better fit to each data set than any of the standard priors it contains. These include Gaussian, Laplace, $\\ell_p$, and Student's $t$ priors. Finally, we identify cases where the prior is unrealistic and highlight characteristic features of images that suggest the model will fit poorly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A scale mixture of normals is a distribution formed by mixing a collection of normal distributions with fixed mean but different variances. A generalized gamma scale mixture draws the variances from a generalized gamma distribution. Generalized gamma scale mixtures of normals have been proposed as an attractive class of parametric priors for Bayesian inference in inverse imaging problems. Generalized gamma scale mixtures have two shape parameters, one that controls the behavior of the distribution about its mode, and the other that controls its tail decay. In this paper, we provide the first demonstration that the prior model is realistic for multiple large imaging data sets. We draw data from remote sensing, medical imaging, and image classification applications. We study the realism of the prior when applied to Fourier and wavelet (Haar and Gabor) transformations of the images, as well as to the coefficients produced by convolving the images against the filters used in the first layer of AlexNet, a popular convolutional neural network trained for image classification. We discuss data augmentation procedures that improve the fit of the model, procedures for identifying approximately exchangeable coefficients, and characterize the parameter regions that best describe the observed data sets. These regions are significantly broader than the region of primary focus in computational work. We show that this prior family provides a substantially better fit to each data set than any of the standard priors it contains. These include Gaussian, Laplace, $\\ell_p$, and Student's $t$ priors. Finally, we identify cases where the prior is unrealistic and highlight characteristic features of images that suggest the model will fit poorly."
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T20:01:44Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    20,
                    1,
                    44,
                    3,
                    352,
                    0
                ],
                "arxiv_comment": "25 pages main text, 21 figures, 7 tables, 6 pages appendix",
                "arxiv_primary_category": {
                    "term": "stat.AP"
                },
                "authors": [
                    {
                        "name": "Brandon Marks"
                    },
                    {
                        "name": "Yash Dave"
                    },
                    {
                        "name": "Zixun Wang"
                    },
                    {
                        "name": "Hannah Chung"
                    },
                    {
                        "name": "Riya Patwa"
                    },
                    {
                        "name": "Simon Cha"
                    },
                    {
                        "name": "Michael Murphy"
                    },
                    {
                        "name": "Alexander Strang"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Strang"
                },
                "author": "Alexander Strang"
            },
            {
                "id": "http://arxiv.org/abs/2512.20591v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20591v1",
                "title": "LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing"
                },
                "updated": "2025-12-23T18:38:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    38,
                    25,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20591v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value < 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value < 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact's spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:38:25Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    38,
                    25,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Changyi Lin"
                    },
                    {
                        "name": "Boda Huo"
                    },
                    {
                        "name": "Mingyang Yu"
                    },
                    {
                        "name": "Emily Ruppel"
                    },
                    {
                        "name": "Bingqing Chen"
                    },
                    {
                        "name": "Jonathan Francis"
                    },
                    {
                        "name": "Ding Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Ding Zhao"
                },
                "author": "Ding Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.20586v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20586v1",
                "title": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent"
                },
                "updated": "2025-12-23T18:32:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    32,
                    17,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20586v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:32:17Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    32,
                    17,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Humza Nusrat"
                    },
                    {
                        "name": "Luke Francisco"
                    },
                    {
                        "name": "Bing Luo"
                    },
                    {
                        "name": "Hassan Bagher-Ebadian"
                    },
                    {
                        "name": "Joshua Kim"
                    },
                    {
                        "name": "Karen Chin-Snyder"
                    },
                    {
                        "name": "Salim Siddiqui"
                    },
                    {
                        "name": "Mira Shah"
                    },
                    {
                        "name": "Eric Mellon"
                    },
                    {
                        "name": "Mohammad Ghassemi"
                    },
                    {
                        "name": "Anthony Doemer"
                    },
                    {
                        "name": "Benjamin Movsas"
                    },
                    {
                        "name": "Kundan Thind"
                    }
                ],
                "author_detail": {
                    "name": "Kundan Thind"
                },
                "author": "Kundan Thind"
            },
            {
                "id": "http://arxiv.org/abs/2512.20581v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20581v1",
                "title": "MERGE-RNA: a physics-based model to predict RNA secondary structure ensembles with chemical probing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERGE-RNA: a physics-based model to predict RNA secondary structure ensembles with chemical probing"
                },
                "updated": "2025-12-23T18:26:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    26,
                    57,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20581v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The function of RNA molecules is deeply related to their secondary structure, which determines which nucleobases are accessible for pairing. Most RNA molecules however function through dynamic and heterogeneous structural ensembles. Chemical probing methods (e.g., DMS probing) rely on selective chemical modification of accessible RNA nucleotides to infer base-pairing status, yet the resulting nucleotide-resolution data represent ensemble averages over dynamic RNA conformations. We present MERGE-RNA, a unified, physics-based framework that explicitly models the full experimental pipeline, from the thermodynamics of probe binding to the mutational profiling readout. By integrating measurements across probe concentrations and replicates, our model learns a small set of transferable and interpretable parameters together with minimal sequence-specific soft constraints. This enables the prediction of secondary structure ensembles that best explain the data and the detection of suboptmal structures involved in dynamic processes. We validate MERGE-RNA on diverse RNAs, showing that it achieves strong structural accuracy while preserving essential conformational heterogeneity. In a designed RNA for which we report new DMS data, MERGE-RNA detects transient intermediate states associated with strand displacement, dynamics that remain invisible to traditional methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The function of RNA molecules is deeply related to their secondary structure, which determines which nucleobases are accessible for pairing. Most RNA molecules however function through dynamic and heterogeneous structural ensembles. Chemical probing methods (e.g., DMS probing) rely on selective chemical modification of accessible RNA nucleotides to infer base-pairing status, yet the resulting nucleotide-resolution data represent ensemble averages over dynamic RNA conformations. We present MERGE-RNA, a unified, physics-based framework that explicitly models the full experimental pipeline, from the thermodynamics of probe binding to the mutational profiling readout. By integrating measurements across probe concentrations and replicates, our model learns a small set of transferable and interpretable parameters together with minimal sequence-specific soft constraints. This enables the prediction of secondary structure ensembles that best explain the data and the detection of suboptmal structures involved in dynamic processes. We validate MERGE-RNA on diverse RNAs, showing that it achieves strong structural accuracy while preserving essential conformational heterogeneity. In a designed RNA for which we report new DMS data, MERGE-RNA detects transient intermediate states associated with strand displacement, dynamics that remain invisible to traditional methods."
                },
                "tags": [
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:26:57Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    26,
                    57,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.BM"
                },
                "authors": [
                    {
                        "name": "Giuseppe Sacco"
                    },
                    {
                        "name": "Jianhui Li"
                    },
                    {
                        "name": "Redmond P. Smyth"
                    },
                    {
                        "name": "Guido Sanguinetti"
                    },
                    {
                        "name": "Giovanni Bussi"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Bussi"
                },
                "author": "Giovanni Bussi"
            },
            {
                "id": "http://arxiv.org/abs/2512.20579v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20579v1",
                "title": "Spin-induced quadrupole moment based test for eccentric binaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spin-induced quadrupole moment based test for eccentric binaries"
                },
                "updated": "2025-12-23T18:26:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    26,
                    37,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20579v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20579v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The spin-induced quadrupole moment-based test of black hole nature is routinely used to probe the true nature of detected binary signals, assuming a circular orbit. We extend the applicability of the method to binaries in eccentric orbits. Considering simulated signals of varying masses, spins, and signal strengths, we demonstrate how the systematic errors resulting from neglecting orbital eccentricity compare with the statistical errors, using a semi-analytic Fisher matrix-based formalism that accounts for both current and future detectors. Further, we quantify the systematic errors by developing a Bayesian inference framework for the current detector network. The inspiral-only aligned spin gravitational wave waveform model for eccentric binaries, TaylorF2Ecc, is employed. For the current detector network, neglecting an initial eccentricity of $e_0^{\\rm inj}=0.1$ defined at $20\\,\\mathrm {Hz} $ can lead to a serious bias in binary parameter inference. Notably, a nearly equal-mass, moderately spinning binary black hole in an eccentric orbit can be identified as a non-black hole binary with extreme spins and asymmetric masses. We demonstrate the criticality of biased estimates that may arise when neglecting the orbital eccentricity while performing tests of black hole nature and discuss prospects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spin-induced quadrupole moment-based test of black hole nature is routinely used to probe the true nature of detected binary signals, assuming a circular orbit. We extend the applicability of the method to binaries in eccentric orbits. Considering simulated signals of varying masses, spins, and signal strengths, we demonstrate how the systematic errors resulting from neglecting orbital eccentricity compare with the statistical errors, using a semi-analytic Fisher matrix-based formalism that accounts for both current and future detectors. Further, we quantify the systematic errors by developing a Bayesian inference framework for the current detector network. The inspiral-only aligned spin gravitational wave waveform model for eccentric binaries, TaylorF2Ecc, is employed. For the current detector network, neglecting an initial eccentricity of $e_0^{\\rm inj}=0.1$ defined at $20\\,\\mathrm {Hz} $ can lead to a serious bias in binary parameter inference. Notably, a nearly equal-mass, moderately spinning binary black hole in an eccentric orbit can be identified as a non-black hole binary with extreme spins and asymmetric masses. We demonstrate the criticality of biased estimates that may arise when neglecting the orbital eccentricity while performing tests of black hole nature and discuss prospects."
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:26:37Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    26,
                    37,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "10 pages, 5 figures",
                "arxiv_primary_category": {
                    "term": "gr-qc"
                },
                "authors": [
                    {
                        "name": "N. V. Krishnendu"
                    }
                ],
                "author_detail": {
                    "name": "N. V. Krishnendu"
                },
                "author": "N. V. Krishnendu"
            },
            {
                "id": "http://arxiv.org/abs/2512.20578v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20578v1",
                "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits"
                },
                "updated": "2025-12-23T18:21:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    21,
                    32,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20578v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:21:32Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    21,
                    32,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Amirhosein Ghasemabadi"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu"
            },
            {
                "id": "http://arxiv.org/abs/2512.20573v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20573v1",
                "title": "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs"
                },
                "updated": "2025-12-23T18:16:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    16,
                    58,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20573v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It \"fails fast\" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and \"wins big\" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\\times$ speedup over vanilla decoding, 1.7$\\times$ over the best naive dLLM drafter, and 1.4$\\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It \"fails fast\" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and \"wins big\" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\\times$ speedup over vanilla decoding, 1.7$\\times$ over the best naive dLLM drafter, and 1.4$\\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:16:58Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    16,
                    58,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali"
            },
            {
                "id": "http://arxiv.org/abs/2512.20569v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20569v1",
                "title": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection"
                },
                "updated": "2025-12-23T18:12:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    12,
                    22,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20569v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \\citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \\citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:12:22Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    12,
                    22,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yanhong Li"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Shawn Tan"
                    },
                    {
                        "name": "Mayank Mishra"
                    },
                    {
                        "name": "Rameswar Panda"
                    },
                    {
                        "name": "Jiawei Zhou"
                    },
                    {
                        "name": "Yoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Yoon Kim"
                },
                "author": "Yoon Kim"
            },
            {
                "id": "http://arxiv.org/abs/2507.18885v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.18885v4",
                "title": "A Minimalist Proof Language for Neural Theorem Proving over Isabelle/HOL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Proof Language for Neural Theorem Proving over Isabelle/HOL"
                },
                "updated": "2025-12-23T17:57:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    57,
                    57,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.18885v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.18885v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Neural Theorem Proving (NTP) employs LLMs to automate formal proofs in proof assistants. While LLMs have achieved relatively remarkable success in informal reasoning tasks using natural languages, the transition to mechanized formal theorem proving presents persistent challenges. Mechanized proof languages often contain many syntactic constructs and diverse, specialized proof tactics, which facilitate expert use but have no direct counterpart in informal mathematical proofs. These prover-specific idioms represent an additional burden for LLM-based NTPs that might be otherwise successful in generating informal proofs. Seeking to bridge this gap between formal proof construction and informal reasoning, in order to better facilitate NTP, this work approaches these challenges from a language design perspective. We look at common reasoning patterns in informal proofs and in existing mechanized proofs, and design Minilang -- a minimalist proof language that captures these reasoning patterns. In contrast to proof languages (informal and formal) that often feature a large collection of operations with unclear semantic boundaries, Minilang is deliberately kept minimalist -- its core design comprises only 10 operations, each with clear semantic distinctions. We further develop a rule-based translator from Isabelle's language (Isar) to Minilang, translating ~340K existing proofs with an ~85% success rate. Using this translated corpus, we finetune two LLMs to compare machine learning performance on Minilang versus the original Isar. Experiments show Minilang benefits the two LLMs by improving the pass@1 success rate on the PISA benchmark by up to 20/29 percentage points in comparison to the Isar-based LLMs w/wo Sledgehammer. The pass@1 rate reaches 69.1%, exceeding the prior work Baldur's pass@64 (65.7%); the pass@8 rate reaches 79.2%, exceeding the SOTA on PISA (71.0%) achieved by Magnushammer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Theorem Proving (NTP) employs LLMs to automate formal proofs in proof assistants. While LLMs have achieved relatively remarkable success in informal reasoning tasks using natural languages, the transition to mechanized formal theorem proving presents persistent challenges. Mechanized proof languages often contain many syntactic constructs and diverse, specialized proof tactics, which facilitate expert use but have no direct counterpart in informal mathematical proofs. These prover-specific idioms represent an additional burden for LLM-based NTPs that might be otherwise successful in generating informal proofs. Seeking to bridge this gap between formal proof construction and informal reasoning, in order to better facilitate NTP, this work approaches these challenges from a language design perspective. We look at common reasoning patterns in informal proofs and in existing mechanized proofs, and design Minilang -- a minimalist proof language that captures these reasoning patterns. In contrast to proof languages (informal and formal) that often feature a large collection of operations with unclear semantic boundaries, Minilang is deliberately kept minimalist -- its core design comprises only 10 operations, each with clear semantic distinctions. We further develop a rule-based translator from Isabelle's language (Isar) to Minilang, translating ~340K existing proofs with an ~85% success rate. Using this translated corpus, we finetune two LLMs to compare machine learning performance on Minilang versus the original Isar. Experiments show Minilang benefits the two LLMs by improving the pass@1 success rate on the PISA benchmark by up to 20/29 percentage points in comparison to the Isar-based LLMs w/wo Sledgehammer. The pass@1 rate reaches 69.1%, exceeding the prior work Baldur's pass@64 (65.7%); the pass@8 rate reaches 79.2%, exceeding the SOTA on PISA (71.0%) achieved by Magnushammer."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-25T02:04:56Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    2,
                    4,
                    56,
                    4,
                    206,
                    0
                ],
                "arxiv_comment": "Accepted in OOPSLA'26",
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Qiyuan Xu"
                    },
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Peixin Wang"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Conrad Watt"
                    }
                ],
                "author_detail": {
                    "name": "Conrad Watt"
                },
                "author": "Conrad Watt"
            },
            {
                "id": "http://arxiv.org/abs/2512.17787v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17787v2",
                "title": "Selected topics on: 1) proposal of interpreting the Crab supernova with a GRB 2) progress in identifying the seven GRBs episodes 3) the role of Sagittarius A in identifying the dark matter component (the X fermion)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selected topics on: 1) proposal of interpreting the Crab supernova with a GRB 2) progress in identifying the seven GRBs episodes 3) the role of Sagittarius A in identifying the dark matter component (the X fermion)"
                },
                "updated": "2025-12-23T17:54:11Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    54,
                    11,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17787v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17787v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As the fiftieth anniversary of our common effort in the field of relativistic astrophysics is approaching, we offer a new look to some of our acquired knowledge in a more complete view, which evidence previous unnoticed connections. They are gaining due prominence in reaching a more complete picture evidencing the main results.\n  We outline the history of GRB observations along with a summary of the contributions made by our group to develop the BdHN interpreting model. We show the seven Episodes characterizing the most powerful BdHNe I occurred to date: GRB 190114C and GRB 220101A. New inferences for the explanation of the highest energy radiation in the TeV are presented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the fiftieth anniversary of our common effort in the field of relativistic astrophysics is approaching, we offer a new look to some of our acquired knowledge in a more complete view, which evidence previous unnoticed connections. They are gaining due prominence in reaching a more complete picture evidencing the main results.\n  We outline the history of GRB observations along with a summary of the contributions made by our group to develop the BdHN interpreting model. We show the seven Episodes characterizing the most powerful BdHNe I occurred to date: GRB 190114C and GRB 220101A. New inferences for the explanation of the highest energy radiation in the TeV are presented."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T16:58:14Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    16,
                    58,
                    14,
                    4,
                    353,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "authors": [
                    {
                        "name": "R. Ruffini"
                    },
                    {
                        "name": "C. Sigismondi"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "H. Quevedo"
                    },
                    {
                        "name": "S. Zhang"
                    },
                    {
                        "name": "Y. Aimuratov"
                    },
                    {
                        "name": "P. Chardonnet"
                    },
                    {
                        "name": "C. L. Fryer"
                    },
                    {
                        "name": "T. Mirtorabi"
                    },
                    {
                        "name": "R. Moradi"
                    },
                    {
                        "name": "M. Prakapenia"
                    },
                    {
                        "name": "F. Rastegarnia"
                    },
                    {
                        "name": "S. -S. Xue"
                    }
                ],
                "author_detail": {
                    "name": "S. -S. Xue"
                },
                "author": "S. -S. Xue"
            },
            {
                "id": "http://arxiv.org/abs/2512.20552v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20552v1",
                "title": "Information-theoretic signatures of causality in Bayesian networks and hypergraphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information-theoretic signatures of causality in Bayesian networks and hypergraphs"
                },
                "updated": "2025-12-23T17:46:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    46,
                    53,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20552v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20552v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Analyzing causality in multivariate systems involves establishing how information is generated, distributed and combined, and thus requires tools that capture interactions beyond pairwise relations. Higher-order information theory provides such tools. In particular, Partial Information Decomposition (PID) allows the decomposition of the information that a set of sources provides about a target into redundant, unique, and synergistic components. Yet the mathematical connection between such higher-order information-theoretic measures and causal structure remains undeveloped. Here we establish the first theoretical correspondence between PID components and causal structure in both Bayesian networks and hypergraphs. We first show that in Bayesian networks unique information precisely characterizes direct causal neighbors, while synergy identifies collider relationships. This establishes a localist causal discovery paradigm in which the structure surrounding each variable can be recovered from its immediate informational footprint, eliminating the need for global search over graph space. Extending these results to higher-order systems, we prove that PID signatures in Bayesian hypergraphs differentiate parents, children, co-heads, and co-tails, revealing a higher-order collider effect unique to multi-tail hyperedges. We also present procedures by which our results can be used to characterize systematically the causal structure of Bayesian networks and hypergraphs. Our results position PID as a rigorous, model-agnostic foundation for inferring both pairwise and higher-order causal structure, and introduce a fundamentally local information-theoretic viewpoint on causal discovery.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing causality in multivariate systems involves establishing how information is generated, distributed and combined, and thus requires tools that capture interactions beyond pairwise relations. Higher-order information theory provides such tools. In particular, Partial Information Decomposition (PID) allows the decomposition of the information that a set of sources provides about a target into redundant, unique, and synergistic components. Yet the mathematical connection between such higher-order information-theoretic measures and causal structure remains undeveloped. Here we establish the first theoretical correspondence between PID components and causal structure in both Bayesian networks and hypergraphs. We first show that in Bayesian networks unique information precisely characterizes direct causal neighbors, while synergy identifies collider relationships. This establishes a localist causal discovery paradigm in which the structure surrounding each variable can be recovered from its immediate informational footprint, eliminating the need for global search over graph space. Extending these results to higher-order systems, we prove that PID signatures in Bayesian hypergraphs differentiate parents, children, co-heads, and co-tails, revealing a higher-order collider effect unique to multi-tail hyperedges. We also present procedures by which our results can be used to characterize systematically the causal structure of Bayesian networks and hypergraphs. Our results position PID as a rigorous, model-agnostic foundation for inferring both pairwise and higher-order causal structure, and introduce a fundamentally local information-theoretic viewpoint on causal discovery."
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T17:46:53Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    46,
                    53,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "20 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.IT"
                },
                "authors": [
                    {
                        "name": "Sung En Chiang"
                    },
                    {
                        "name": "Zhaolu Liu"
                    },
                    {
                        "name": "Robert L. Peach"
                    },
                    {
                        "name": "Mauricio Barahona"
                    }
                ],
                "author_detail": {
                    "name": "Mauricio Barahona"
                },
                "author": "Mauricio Barahona"
            },
            {
                "id": "http://arxiv.org/abs/2512.20550v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20550v1",
                "title": "LLM-Based Authoring of Agent-Based Narratives through Scene Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Authoring of Agent-Based Narratives through Scene Descriptions"
                },
                "updated": "2025-12-23T17:46:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    46,
                    15,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20550v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents a system for procedurally generating agent-based narratives using large language models (LLMs). Users could drag and drop multiple agents and objects into a scene, with each entity automatically assigned semantic metadata describing its identity, role, and potential interactions. The scene structure is then serialized into a natural language prompt and sent to an LLM, which returns a structured string describing a sequence of actions and interactions among agents and objects. The returned string encodes who performed which actions, when, and how. A custom parser interprets this string and triggers coordinated agent behaviors, animations, and interaction modules. The system supports agent-based scenes, dynamic object manipulation, and diverse interaction types. Designed for ease of use and rapid iteration, the system enables the generation of virtual agent activity suitable for prototyping agent narratives. The performance of the developed system was evaluated using four popular lightweight LLMs. Each model's process and response time were measured under multiple complexity scenarios. The collected data were analyzed to compare consistency across the examined scenarios and to highlight the relative efficiency and suitability of each model for procedural agent-based narratives generation. The results demonstrate that LLMs can reliably translate high-level scene descriptions into executable agent-based behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a system for procedurally generating agent-based narratives using large language models (LLMs). Users could drag and drop multiple agents and objects into a scene, with each entity automatically assigned semantic metadata describing its identity, role, and potential interactions. The scene structure is then serialized into a natural language prompt and sent to an LLM, which returns a structured string describing a sequence of actions and interactions among agents and objects. The returned string encodes who performed which actions, when, and how. A custom parser interprets this string and triggers coordinated agent behaviors, animations, and interaction modules. The system supports agent-based scenes, dynamic object manipulation, and diverse interaction types. Designed for ease of use and rapid iteration, the system enables the generation of virtual agent activity suitable for prototyping agent narratives. The performance of the developed system was evaluated using four popular lightweight LLMs. Each model's process and response time were measured under multiple complexity scenarios. The collected data were analyzed to compare consistency across the examined scenarios and to highlight the relative efficiency and suitability of each model for procedural agent-based narratives generation. The results demonstrate that LLMs can reliably translate high-level scene descriptions into executable agent-based behaviors."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T17:46:15Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    46,
                    15,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Vinayak Regmi"
                    },
                    {
                        "name": "Christos Mousas"
                    }
                ],
                "author_detail": {
                    "name": "Christos Mousas"
                },
                "author": "Christos Mousas"
            },
            {
                "id": "http://arxiv.org/abs/2512.13507v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.13507v3",
                "title": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Seedance 1.5 pro: A Native Audio-Visual Joint Generation Foundation Model"
                },
                "updated": "2025-12-23T17:38:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    38,
                    46,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.13507v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.13507v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent strides in video generation have paved the way for unified audio-visual generation. In this work, we present Seedance 1.5 pro, a foundational model engineered specifically for native, joint audio-video generation. Leveraging a dual-branch Diffusion Transformer architecture, the model integrates a cross-modal joint module with a specialized multi-stage data pipeline, achieving exceptional audio-visual synchronization and superior generation quality. To ensure practical utility, we implement meticulous post-training optimizations, including Supervised Fine-Tuning (SFT) on high-quality datasets and Reinforcement Learning from Human Feedback (RLHF) with multi-dimensional reward models. Furthermore, we introduce an acceleration framework that boosts inference speed by over 10X. Seedance 1.5 pro distinguishes itself through precise multilingual and dialect lip-syncing, dynamic cinematic camera control, and enhanced narrative coherence, positioning it as a robust engine for professional-grade content creation. Seedance 1.5 pro is now accessible on Volcano Engine at https://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?type=GenVideo."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-15T16:36:52Z",
                "published_parsed": [
                    2025,
                    12,
                    15,
                    16,
                    36,
                    52,
                    0,
                    349,
                    0
                ],
                "arxiv_comment": "Seedance 1.5 pro Technical Report",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Team Seedance"
                    },
                    {
                        "name": "Heyi Chen"
                    },
                    {
                        "name": "Siyan Chen"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Ying Chen"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Feng Cheng"
                    },
                    {
                        "name": "Tianheng Cheng"
                    },
                    {
                        "name": "Xinqi Cheng"
                    },
                    {
                        "name": "Xuyan Chi"
                    },
                    {
                        "name": "Jian Cong"
                    },
                    {
                        "name": "Jing Cui"
                    },
                    {
                        "name": "Qinpeng Cui"
                    },
                    {
                        "name": "Qide Dong"
                    },
                    {
                        "name": "Junliang Fan"
                    },
                    {
                        "name": "Jing Fang"
                    },
                    {
                        "name": "Zetao Fang"
                    },
                    {
                        "name": "Chengjian Feng"
                    },
                    {
                        "name": "Han Feng"
                    },
                    {
                        "name": "Mingyuan Gao"
                    },
                    {
                        "name": "Yu Gao"
                    },
                    {
                        "name": "Dong Guo"
                    },
                    {
                        "name": "Qiushan Guo"
                    },
                    {
                        "name": "Boyang Hao"
                    },
                    {
                        "name": "Qingkai Hao"
                    },
                    {
                        "name": "Bibo He"
                    },
                    {
                        "name": "Qian He"
                    },
                    {
                        "name": "Tuyen Hoang"
                    },
                    {
                        "name": "Ruoqing Hu"
                    },
                    {
                        "name": "Xi Hu"
                    },
                    {
                        "name": "Weilin Huang"
                    },
                    {
                        "name": "Zhaoyang Huang"
                    },
                    {
                        "name": "Zhongyi Huang"
                    },
                    {
                        "name": "Donglei Ji"
                    },
                    {
                        "name": "Siqi Jiang"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Yunpu Jiang"
                    },
                    {
                        "name": "Zhuo Jiang"
                    },
                    {
                        "name": "Ashley Kim"
                    },
                    {
                        "name": "Jianan Kong"
                    },
                    {
                        "name": "Zhichao Lai"
                    },
                    {
                        "name": "Shanshan Lao"
                    },
                    {
                        "name": "Yichong Leng"
                    },
                    {
                        "name": "Ai Li"
                    },
                    {
                        "name": "Feiya Li"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Huixia Li"
                    },
                    {
                        "name": "JiaShi Li"
                    },
                    {
                        "name": "Liang Li"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Shanshan Li"
                    },
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Xiaojie Li"
                    },
                    {
                        "name": "Xiaoyang Li"
                    },
                    {
                        "name": "Xingxing Li"
                    },
                    {
                        "name": "Yameng Li"
                    },
                    {
                        "name": "Yifu Li"
                    },
                    {
                        "name": "Yiying Li"
                    },
                    {
                        "name": "Chao Liang"
                    },
                    {
                        "name": "Han Liang"
                    },
                    {
                        "name": "Jianzhong Liang"
                    },
                    {
                        "name": "Ying Liang"
                    },
                    {
                        "name": "Zhiqiang Liang"
                    },
                    {
                        "name": "Wang Liao"
                    },
                    {
                        "name": "Yalin Liao"
                    },
                    {
                        "name": "Heng Lin"
                    },
                    {
                        "name": "Kengyu Lin"
                    },
                    {
                        "name": "Shanchuan Lin"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Zhijie Lin"
                    },
                    {
                        "name": "Feng Ling"
                    },
                    {
                        "name": "Fangfang Liu"
                    },
                    {
                        "name": "Gaohong Liu"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Jihao Liu"
                    },
                    {
                        "name": "Shouda Liu"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Sichao Liu"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Yibo Liu"
                    },
                    {
                        "name": "Zikun Liu"
                    },
                    {
                        "name": "Zuxi Liu"
                    },
                    {
                        "name": "Junlin Lyu"
                    },
                    {
                        "name": "Lecheng Lyu"
                    },
                    {
                        "name": "Qian Lyu"
                    },
                    {
                        "name": "Han Mu"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Jingzhe Ning"
                    },
                    {
                        "name": "Xitong Pan"
                    },
                    {
                        "name": "Yanghua Peng"
                    },
                    {
                        "name": "Lianke Qin"
                    },
                    {
                        "name": "Xueqiong Qu"
                    },
                    {
                        "name": "Yuxi Ren"
                    },
                    {
                        "name": "Kai Shen"
                    },
                    {
                        "name": "Guang Shi"
                    },
                    {
                        "name": "Lei Shi"
                    },
                    {
                        "name": "Yan Song"
                    },
                    {
                        "name": "Yinglong Song"
                    },
                    {
                        "name": "Fan Sun"
                    },
                    {
                        "name": "Li Sun"
                    },
                    {
                        "name": "Renfei Sun"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Zeyu Sun"
                    },
                    {
                        "name": "Wenjing Tang"
                    },
                    {
                        "name": "Yaxue Tang"
                    },
                    {
                        "name": "Zirui Tao"
                    },
                    {
                        "name": "Feng Wang"
                    },
                    {
                        "name": "Furui Wang"
                    },
                    {
                        "name": "Jinran Wang"
                    },
                    {
                        "name": "Junkai Wang"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Kexin Wang"
                    },
                    {
                        "name": "Qingyi Wang"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Sen Wang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Tingru Wang"
                    },
                    {
                        "name": "Weichen Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yanhui Wang"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Yuping Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Ziyu Wang"
                    },
                    {
                        "name": "Guoqiang Wei"
                    },
                    {
                        "name": "Wanru Wei"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Guohong Wu"
                    },
                    {
                        "name": "Hanjie Wu"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Ruolan Wu"
                    },
                    {
                        "name": "Xinglong Wu"
                    },
                    {
                        "name": "Yonghui Wu"
                    },
                    {
                        "name": "Ruiqi Xia"
                    },
                    {
                        "name": "Liang Xiang"
                    },
                    {
                        "name": "Fei Xiao"
                    },
                    {
                        "name": "XueFeng Xiao"
                    },
                    {
                        "name": "Pan Xie"
                    },
                    {
                        "name": "Shuangyi Xie"
                    },
                    {
                        "name": "Shuang Xu"
                    },
                    {
                        "name": "Jinlan Xue"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Bangbang Yang"
                    },
                    {
                        "name": "Ceyuan Yang"
                    },
                    {
                        "name": "Jiaqi Yang"
                    },
                    {
                        "name": "Runkai Yang"
                    },
                    {
                        "name": "Tao Yang"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Yihang Yang"
                    },
                    {
                        "name": "ZhiXian Yang"
                    },
                    {
                        "name": "Ziyan Yang"
                    },
                    {
                        "name": "Songting Yao"
                    },
                    {
                        "name": "Yifan Yao"
                    },
                    {
                        "name": "Zilyu Ye"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Jian Yu"
                    },
                    {
                        "name": "Chujie Yuan"
                    },
                    {
                        "name": "Linxiao Yuan"
                    },
                    {
                        "name": "Sichun Zeng"
                    },
                    {
                        "name": "Weihong Zeng"
                    },
                    {
                        "name": "Xuejiao Zeng"
                    },
                    {
                        "name": "Yan Zeng"
                    },
                    {
                        "name": "Chuntao Zhang"
                    },
                    {
                        "name": "Heng Zhang"
                    },
                    {
                        "name": "Jingjie Zhang"
                    },
                    {
                        "name": "Kuo Zhang"
                    },
                    {
                        "name": "Liang Zhang"
                    },
                    {
                        "name": "Liying Zhang"
                    },
                    {
                        "name": "Manlin Zhang"
                    },
                    {
                        "name": "Ting Zhang"
                    },
                    {
                        "name": "Weida Zhang"
                    },
                    {
                        "name": "Xiaohe Zhang"
                    },
                    {
                        "name": "Xinyan Zhang"
                    },
                    {
                        "name": "Yan Zhang"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Zixiang Zhang"
                    },
                    {
                        "name": "Fengxuan Zhao"
                    },
                    {
                        "name": "Huating Zhao"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Hao Zheng"
                    },
                    {
                        "name": "Jianbin Zheng"
                    },
                    {
                        "name": "Xiaozheng Zheng"
                    },
                    {
                        "name": "Yangyang Zheng"
                    },
                    {
                        "name": "Yijie Zheng"
                    },
                    {
                        "name": "Jiexin Zhou"
                    },
                    {
                        "name": "Jiahui Zhu"
                    },
                    {
                        "name": "Kuan Zhu"
                    },
                    {
                        "name": "Shenhan Zhu"
                    },
                    {
                        "name": "Wenjia Zhu"
                    },
                    {
                        "name": "Benhui Zou"
                    },
                    {
                        "name": "Feilong Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Feilong Zuo"
                },
                "author": "Feilong Zuo"
            },
            {
                "id": "http://arxiv.org/abs/2512.20536v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20536v1",
                "title": "Nonlocal decoding of positional and correlational information during development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal decoding of positional and correlational information during development"
                },
                "updated": "2025-12-23T17:26:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    26,
                    6,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20536v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In many developmental systems, cells differentiate into a tissue by reading out morphogen concentration fields, a process fundamentally limited by noise. How much can the precision of this process be improved by nonlocal information, e.g., via cell-cell communication? Using a Bayes-optimal framework, we show that positional inference depends crucially on morphogen spatial correlations and on the ``structural prior'' that encodes the geometry of the cellular lattice performing the readout. We derive upper bounds on positional information gain due to nonlocal readout and identify signal processing algorithms that approximate optimal positional inference, as well as simple chemical reaction schemes which implement such algorithms. Our theory suggests that correlational information can be exploited to significantly enhance developmental precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In many developmental systems, cells differentiate into a tissue by reading out morphogen concentration fields, a process fundamentally limited by noise. How much can the precision of this process be improved by nonlocal information, e.g., via cell-cell communication? Using a Bayes-optimal framework, we show that positional inference depends crucially on morphogen spatial correlations and on the ``structural prior'' that encodes the geometry of the cellular lattice performing the readout. We derive upper bounds on positional information gain due to nonlocal readout and identify signal processing algorithms that approximate optimal positional inference, as well as simple chemical reaction schemes which implement such algorithms. Our theory suggests that correlational information can be exploited to significantly enhance developmental precision."
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T17:26:06Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    26,
                    6,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph"
                },
                "authors": [
                    {
                        "name": "Alex Chen Yi Zhang"
                    },
                    {
                        "name": "Pablo Mateu Hoyos"
                    },
                    {
                        "name": "David Brückner"
                    },
                    {
                        "name": "Gašper Tkačik"
                    }
                ],
                "author_detail": {
                    "name": "Gašper Tkačik"
                },
                "author": "Gašper Tkačik"
            },
            {
                "id": "http://arxiv.org/abs/2512.20535v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20535v1",
                "title": "ARBITER: AI-Driven Filtering for Role-Based Access Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARBITER: AI-Driven Filtering for Role-Based Access Control"
                },
                "updated": "2025-12-23T17:25:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    25,
                    51,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20535v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Role-Based Access Control (RBAC) struggles to adapt to dynamic enterprise environments with documents that contain information that cannot be disclosed to specific user groups. As these documents are used by LLM-driven systems (e.g., in RAG) the problem is exacerbated as LLMs can leak sensitive data due to prompt truncation, classification errors, or loss of system context. We introduce \\our, a system designed to provide RBAC in RAG systems. \\our implements layered input/output validation, role-aware retrieval, and post-generation fact-checking. Unlike traditional RBAC approaches that rely on fine-tuned classifiers, \\our uses LLMs operating in few-shot settings with prompt-based steering for rapid deployment and role updates. We evaluate the approach on 389 queries using a synthetic dataset. Experimental results show 85\\% accuracy and 89\\% F1-score in query filtering, close to traditional RBAC solutions. Results suggest that practical RBAC deployment on RAG systems is approaching the maturity level needed for dynamic enterprise environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-Based Access Control (RBAC) struggles to adapt to dynamic enterprise environments with documents that contain information that cannot be disclosed to specific user groups. As these documents are used by LLM-driven systems (e.g., in RAG) the problem is exacerbated as LLMs can leak sensitive data due to prompt truncation, classification errors, or loss of system context. We introduce \\our, a system designed to provide RBAC in RAG systems. \\our implements layered input/output validation, role-aware retrieval, and post-generation fact-checking. Unlike traditional RBAC approaches that rely on fine-tuned classifiers, \\our uses LLMs operating in few-shot settings with prompt-based steering for rapid deployment and role updates. We evaluate the approach on 389 queries using a synthetic dataset. Experimental results show 85\\% accuracy and 89\\% F1-score in query filtering, close to traditional RBAC solutions. Results suggest that practical RBAC deployment on RAG systems is approaching the maturity level needed for dynamic enterprise environments."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T17:25:51Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    25,
                    51,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Michele Lorenzo"
                    },
                    {
                        "name": "Idilio Drago"
                    },
                    {
                        "name": "Dario Salvadori"
                    },
                    {
                        "name": "Fabio Romolo Vayr"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Romolo Vayr"
                },
                "author": "Fabio Romolo Vayr"
            },
            {
                "id": "http://arxiv.org/abs/2512.20533v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20533v1",
                "title": "Over-the-Air Goal-Oriented Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over-the-Air Goal-Oriented Communications"
                },
                "updated": "2025-12-23T17:24:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    24,
                    39,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20533v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Goal-oriented communications offer an attractive alternative to the Shannon-based communication paradigm, where the data is never reconstructed at the Receiver (RX) side. Rather, focusing on the case of edge inference, the Transmitter (TX) and the RX cooperate to exchange features of the input data that will be used to predict an unseen attribute of them, leveraging information from collected data sets. This chapter demonstrates that the wireless channel can be used to perform computations over the data, when equipped with programmable metasurfaces. The end-to-end system of the TX, RX, and MS-based channel is treated as a single deep neural network which is trained through backpropagation to perform inference on unseen data. Using Stacked Intelligent Metasurfaces (SIM), it is shown that this Metasurfaces-Integrated Neural Network (MINN) can achieve performance comparable to fully digital neural networks under various system parameters and data sets. By offloading computations onto the channel itself, important benefits may be achieved in terms of energy consumption, arising from reduced computations at the transceivers and smaller transmission power required for successful inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Goal-oriented communications offer an attractive alternative to the Shannon-based communication paradigm, where the data is never reconstructed at the Receiver (RX) side. Rather, focusing on the case of edge inference, the Transmitter (TX) and the RX cooperate to exchange features of the input data that will be used to predict an unseen attribute of them, leveraging information from collected data sets. This chapter demonstrates that the wireless channel can be used to perform computations over the data, when equipped with programmable metasurfaces. The end-to-end system of the TX, RX, and MS-based channel is treated as a single deep neural network which is trained through backpropagation to perform inference on unseen data. Using Stacked Intelligent Metasurfaces (SIM), it is shown that this Metasurfaces-Integrated Neural Network (MINN) can achieve performance comparable to fully digital neural networks under various system parameters and data sets. By offloading computations onto the channel itself, important benefits may be achieved in terms of energy consumption, arising from reduced computations at the transceivers and smaller transmission power required for successful inference."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T17:24:39Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    24,
                    39,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "35 pages, 9 figures. Book chapter",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Kyriakos Stylianopoulos"
                    },
                    {
                        "name": "Paolo Di Lorenzo"
                    },
                    {
                        "name": "George C. Alexandropoulos"
                    }
                ],
                "author_detail": {
                    "name": "George C. Alexandropoulos"
                },
                "author": "George C. Alexandropoulos"
            },
            {
                "id": "http://arxiv.org/abs/2512.20523v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20523v1",
                "title": "ScoreMatchingRiesz: Auto-DML with Infinitesimal Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ScoreMatchingRiesz: Auto-DML with Infinitesimal Classification"
                },
                "updated": "2025-12-23T17:14:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    14,
                    14,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20523v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This study proposes Riesz representer estimation methods based on score matching. The Riesz representer is a key component in debiased machine learning for constructing $\\sqrt{n}$-consistent and efficient estimators in causal inference and structural parameter estimation. To estimate the Riesz representer, direct approaches have garnered attention, such as Riesz regression and the covariate balancing propensity score. These approaches can also be interpreted as variants of direct density ratio estimation (DRE) in several applications such as average treatment effect estimation. In DRE, it is well known that flexible models can easily overfit the observed data due to the estimand and the form of the loss function. To address this issue, recent work has proposed modeling the density ratio as a product of multiple intermediate density ratios and estimating it using score-matching techniques, which are often used in the diffusion model literature. We extend score-matching-based DRE methods to Riesz representer estimation. Our proposed method not only mitigates overfitting but also provides insights for causal inference by bridging marginal effects and average policy effects through time score functions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes Riesz representer estimation methods based on score matching. The Riesz representer is a key component in debiased machine learning for constructing $\\sqrt{n}$-consistent and efficient estimators in causal inference and structural parameter estimation. To estimate the Riesz representer, direct approaches have garnered attention, such as Riesz regression and the covariate balancing propensity score. These approaches can also be interpreted as variants of direct density ratio estimation (DRE) in several applications such as average treatment effect estimation. In DRE, it is well known that flexible models can easily overfit the observed data due to the estimand and the form of the loss function. To address this issue, recent work has proposed modeling the density ratio as a product of multiple intermediate density ratios and estimating it using score-matching techniques, which are often used in the diffusion model literature. We extend score-matching-based DRE methods to Riesz representer estimation. Our proposed method not only mitigates overfitting but also provides insights for causal inference by bridging marginal effects and average policy effects through time score functions."
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T17:14:14Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    14,
                    14,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM"
                },
                "authors": [
                    {
                        "name": "Masahiro Kato"
                    }
                ],
                "author_detail": {
                    "name": "Masahiro Kato"
                },
                "author": "Masahiro Kato"
            },
            {
                "id": "http://arxiv.org/abs/2512.20520v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20520v1",
                "title": "Benchmarking LLMs for Predictive Applications in the Intensive Care Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs for Predictive Applications in the Intensive Care Units"
                },
                "updated": "2025-12-23T17:08:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    8,
                    31,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20520v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T17:08:31Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    8,
                    31,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Chehak Malhotra"
                    },
                    {
                        "name": "Mehak Gopal"
                    },
                    {
                        "name": "Akshaya Devadiga"
                    },
                    {
                        "name": "Pradeep Singh"
                    },
                    {
                        "name": "Ridam Pal"
                    },
                    {
                        "name": "Ritwik Kashyap"
                    },
                    {
                        "name": "Tavpritesh Sethi"
                    }
                ],
                "author_detail": {
                    "name": "Tavpritesh Sethi"
                },
                "author": "Tavpritesh Sethi"
            },
            {
                "id": "http://arxiv.org/abs/2509.14423v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.14423v2",
                "title": "200,000+ Deep Learning-inferred Periods of Stellar Variability from the All-Sky Automated Survey for Supernovae",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "200,000+ Deep Learning-inferred Periods of Stellar Variability from the All-Sky Automated Survey for Supernovae"
                },
                "updated": "2025-12-23T16:44:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    44,
                    9,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.14423v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.14423v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.3847/1538-4365/ae1ba7",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Stars exhibit a range of variability periods that depend on their mass, age, and evolutionary stage. For space-based photometric data, convolutional neural networks (CNNs) have demonstrated success in recovering and measuring periodic variability from photometric missions like Kepler and TESS. All-sky ground-based surveys can have similar if not longer baselines than space-based missions; however, these datasets are more challenging to work with due to irregular sampling, more complex systematics, and larger data gaps. In this work, we demonstrate that CNNs can be used to derive variability periods from ground-based surveys. From the All-Sky Automated Survey for Supernovae, we recover 208,260 variability periods between 1 and 30 days, approximately 60% of which are new detections. We recover periods for active RSCVn, anomalous sub-subgiants, and cool dwarfs that are consistent with previously measured rotation periods, while periods for stars above the Kraft break are generally spurious. We also identify periodic signals in tens of thousands of giant stars that correspond to frequencies of stellar oscillations rather than rotation. Our results highlight that CNNs can be used on sparsely sampled ground-based photometry to recover periodicity. We conclude that the findings of our work are very promising for the potential recovery of hundreds of thousands of stellar rotation periods in data from the Vera C. Rubin Observatory's Legacy Survey of Space and Time and the Nancy Grace Roman Space Telescopes Galactic Bulge Time Domain Survey.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stars exhibit a range of variability periods that depend on their mass, age, and evolutionary stage. For space-based photometric data, convolutional neural networks (CNNs) have demonstrated success in recovering and measuring periodic variability from photometric missions like Kepler and TESS. All-sky ground-based surveys can have similar if not longer baselines than space-based missions; however, these datasets are more challenging to work with due to irregular sampling, more complex systematics, and larger data gaps. In this work, we demonstrate that CNNs can be used to derive variability periods from ground-based surveys. From the All-Sky Automated Survey for Supernovae, we recover 208,260 variability periods between 1 and 30 days, approximately 60% of which are new detections. We recover periods for active RSCVn, anomalous sub-subgiants, and cool dwarfs that are consistent with previously measured rotation periods, while periods for stars above the Kraft break are generally spurious. We also identify periodic signals in tens of thousands of giant stars that correspond to frequencies of stellar oscillations rather than rotation. Our results highlight that CNNs can be used on sparsely sampled ground-based photometry to recover periodicity. We conclude that the findings of our work are very promising for the potential recovery of hundreds of thousands of stellar rotation periods in data from the Vera C. Rubin Observatory's Legacy Survey of Space and Time and the Nancy Grace Roman Space Telescopes Galactic Bulge Time Domain Survey."
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-17T20:52:56Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    20,
                    52,
                    56,
                    2,
                    260,
                    0
                ],
                "arxiv_comment": "21 pages, 19 figures, 5 tables. HTML version can be accessed at https://iopscience.iop.org/article/10.3847/1538-4365/ae1ba7",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR"
                },
                "arxiv_journal_ref": "The Astrophysical Journal Supplement Series 282 (2026) 10",
                "authors": [
                    {
                        "name": "Meir E. Schochet"
                    },
                    {
                        "name": "Penelope Planet"
                    },
                    {
                        "name": "Zachary R. Claytor"
                    },
                    {
                        "name": "Jamie Tayar"
                    },
                    {
                        "name": "Adina D. Feinstein"
                    }
                ],
                "author_detail": {
                    "name": "Adina D. Feinstein"
                },
                "author": "Adina D. Feinstein",
                "arxiv_doi": "10.3847/1538-4365/ae1ba7"
            },
            {
                "id": "http://arxiv.org/abs/2512.20492v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20492v1",
                "title": "End-to-end Optimization of Single-Shot Quantum Machine Learning for Bayesian Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end Optimization of Single-Shot Quantum Machine Learning for Bayesian Inference"
                },
                "updated": "2025-12-23T16:35:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    35,
                    32,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20492v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce an end-to-end optimization strategy for quantum machine learning that directly targets performance under finite measurement resources, where learning objectives are defined directly at the level of task performance. The method is applied on a Bayesian quantum metrology task since it provides a natural testbed with known fundamental limits and scaling with system size. The sampling-aware hybrid algorithm achieves a single-shot risk within 1 dB of the -20 dB Bayesian limit using 32 qubits. We extend the Bayesian framework from parameter estimation to global function inference, where the task is to infer a target function of the sensor input drawn from an arbitrary prior, and we demonstrate a clear computational-sensing advantage for direct functional inference over indirect reconstruction. We relate the corresponding Bayesian risk to the Capacity metric and argue that the Resolvable Expressive Capacity provides a natural measure of the space of functions accessible in a single shot. The resulting eigentask analysis identifies noise-robust feature combinations that yield compact estimators with improved accuracy and reduced optimization cost in resource-limited or real-time on-device settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an end-to-end optimization strategy for quantum machine learning that directly targets performance under finite measurement resources, where learning objectives are defined directly at the level of task performance. The method is applied on a Bayesian quantum metrology task since it provides a natural testbed with known fundamental limits and scaling with system size. The sampling-aware hybrid algorithm achieves a single-shot risk within 1 dB of the -20 dB Bayesian limit using 32 qubits. We extend the Bayesian framework from parameter estimation to global function inference, where the task is to infer a target function of the sensor input drawn from an arbitrary prior, and we demonstrate a clear computational-sensing advantage for direct functional inference over indirect reconstruction. We relate the corresponding Bayesian risk to the Capacity metric and argue that the Resolvable Expressive Capacity provides a natural measure of the space of functions accessible in a single shot. The resulting eigentask analysis identifies noise-robust feature combinations that yield compact estimators with improved accuracy and reduced optimization cost in resource-limited or real-time on-device settings."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T16:35:32Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    35,
                    32,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "21 pages, 8 figures, comments are welcome",
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Theodoros Ilias"
                    },
                    {
                        "name": "Fangjun Hu"
                    },
                    {
                        "name": "Marti Vives"
                    },
                    {
                        "name": "Hakan E. Türeci"
                    }
                ],
                "author_detail": {
                    "name": "Hakan E. Türeci"
                },
                "author": "Hakan E. Türeci"
            },
            {
                "id": "http://arxiv.org/abs/2512.20491v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20491v1",
                "title": "Step-DeepResearch Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-DeepResearch Technical Report"
                },
                "updated": "2025-12-23T16:32:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    32,
                    27,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20491v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T16:32:27Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    32,
                    27,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Haikuo Du"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Lin Lin"
                    },
                    {
                        "name": "Mingrui Chen"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Tianchi Yue"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Wenjin Deng"
                    },
                    {
                        "name": "Xiaojian Yuan"
                    },
                    {
                        "name": "Xiaoyun Zhang"
                    },
                    {
                        "name": "Xiangyu Liu"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Yanming Xu"
                    },
                    {
                        "name": "Yicheng Cao"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Yongyao Wang"
                    },
                    {
                        "name": "Yubo Shu"
                    },
                    {
                        "name": "Yurong Zhang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhichao Chang"
                    },
                    {
                        "name": "Binyan Li"
                    },
                    {
                        "name": "Dan Ma"
                    },
                    {
                        "name": "Furong Jia"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Jing Bai"
                    },
                    {
                        "name": "Junlan Liu"
                    },
                    {
                        "name": "Manjiao Liu"
                    },
                    {
                        "name": "Na Wang"
                    },
                    {
                        "name": "Qiuping Wu"
                    },
                    {
                        "name": "Qinxin Du"
                    },
                    {
                        "name": "Shiwei Li"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Yifeng Gong"
                    },
                    {
                        "name": "Yonglin Chen"
                    },
                    {
                        "name": "Yuling Zhao"
                    },
                    {
                        "name": "Yuxuan Lin"
                    },
                    {
                        "name": "Ziqi Ren"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Aihu Zhang"
                    },
                    {
                        "name": "Brian Li"
                    },
                    {
                        "name": "Buyun Ma"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Li Xie"
                    },
                    {
                        "name": "Mingliang Li"
                    },
                    {
                        "name": "Pan Li"
                    },
                    {
                        "name": "Shidong Yang"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xiaojia Liu"
                    },
                    {
                        "name": "Yuchu Luo"
                    },
                    {
                        "name": "Yuan Song"
                    },
                    {
                        "name": "YuanHao Ding"
                    },
                    {
                        "name": "Yuanwei Liang"
                    },
                    {
                        "name": "Zexi Li"
                    },
                    {
                        "name": "Zhaoning Zhang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Jiansheng Chen"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yibo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yibo Zhu"
                },
                "author": "Yibo Zhu"
            },
            {
                "id": "http://arxiv.org/abs/2401.09986v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2401.09986v3",
                "title": "Improving Local Training in Federated Learning via Temperature Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Local Training in Federated Learning via Temperature Scaling"
                },
                "updated": "2025-12-23T16:22:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    22,
                    9,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2401.09986v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2401.09986v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Federated learning is inherently hampered by data heterogeneity: non-i.i.d. training data over local clients. We propose a novel model training approach for federated learning, FLex&Chill, which exploits the Logit Chilling method. Through extensive evaluations, we demonstrate that, in the presence of non-i.i.d. data characteristics inherent in federated learning systems, this approach can expedite model convergence and improve inference accuracy. Quantitatively, from our experiments, we observe up to 6X improvement in the global federated learning model convergence time, and up to 3.37% improvement in inference accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning is inherently hampered by data heterogeneity: non-i.i.d. training data over local clients. We propose a novel model training approach for federated learning, FLex&Chill, which exploits the Logit Chilling method. Through extensive evaluations, we demonstrate that, in the presence of non-i.i.d. data characteristics inherent in federated learning systems, this approach can expedite model convergence and improve inference accuracy. Quantitatively, from our experiments, we observe up to 6X improvement in the global federated learning model convergence time, and up to 3.37% improvement in inference accuracy."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-01-18T14:02:23Z",
                "published_parsed": [
                    2024,
                    1,
                    18,
                    14,
                    2,
                    23,
                    3,
                    18,
                    0
                ],
                "arxiv_comment": "56 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Kichang Lee"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Songkuk Kim"
                    },
                    {
                        "name": "JeongGil Ko"
                    }
                ],
                "author_detail": {
                    "name": "JeongGil Ko"
                },
                "author": "JeongGil Ko"
            },
            {
                "id": "http://arxiv.org/abs/2512.20482v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20482v1",
                "title": "SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization"
                },
                "updated": "2025-12-23T16:18:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    18,
                    39,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20482v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T16:18:39Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    18,
                    39,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Revanth Gangi Reddy"
                    },
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "JaeHyeok Doo"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Daniel Lee"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Yingbo Zhou"
                    },
                    {
                        "name": "Semih Yavuz"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty"
            },
            {
                "id": "http://arxiv.org/abs/2512.20481v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20481v1",
                "title": "Coherence in the brain unfolds across separable temporal regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence in the brain unfolds across separable temporal regimes"
                },
                "updated": "2025-12-23T16:16:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    16,
                    42,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20481v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Coherence in language requires the brain to satisfy two competing temporal demands: gradual accumulation of meaning across extended context and rapid reconfiguration of representations at event boundaries. Despite their centrality to language and thought, how these processes are implemented in the human brain during naturalistic listening remains unclear. Here, we tested whether these two processes can be captured by annotation-free drift and shift signals and whether their neural expression dissociates across large-scale cortical systems. These signals were derived from a large language model (LLM) and formalized contextual drift and event shifts directly from the narrative input. To enable high-precision voxelwise encoding models with stable parameter estimates, we densely sampled one healthy adult across more than 7 hours of listening to thirteen crime stories while collecting ultra high-field (7T) BOLD data. We then modeled the feature-informed hemodynamic response using a regularized encoding framework validated on independent stories. Drift predictions were prevalent in default-mode network hubs, whereas shift predictions were evident bilaterally in the primary auditory cortex and language association cortex. Furthermore, activity in default-mode and parietal networks was best explained by a signal capturing how meaning accumulates and gradually fades over the course of the narrative. Together, these findings show that coherence during language comprehension is implemented through dissociable neural regimes of slow contextual integration and rapid event-driven reconfiguration, offering a mechanistic entry point for understanding disturbances of language coherence in psychiatric disorders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence in language requires the brain to satisfy two competing temporal demands: gradual accumulation of meaning across extended context and rapid reconfiguration of representations at event boundaries. Despite their centrality to language and thought, how these processes are implemented in the human brain during naturalistic listening remains unclear. Here, we tested whether these two processes can be captured by annotation-free drift and shift signals and whether their neural expression dissociates across large-scale cortical systems. These signals were derived from a large language model (LLM) and formalized contextual drift and event shifts directly from the narrative input. To enable high-precision voxelwise encoding models with stable parameter estimates, we densely sampled one healthy adult across more than 7 hours of listening to thirteen crime stories while collecting ultra high-field (7T) BOLD data. We then modeled the feature-informed hemodynamic response using a regularized encoding framework validated on independent stories. Drift predictions were prevalent in default-mode network hubs, whereas shift predictions were evident bilaterally in the primary auditory cortex and language association cortex. Furthermore, activity in default-mode and parietal networks was best explained by a signal capturing how meaning accumulates and gradually fades over the course of the narrative. Together, these findings show that coherence during language comprehension is implemented through dissociable neural regimes of slow contextual integration and rapid event-driven reconfiguration, offering a mechanistic entry point for understanding disturbances of language coherence in psychiatric disorders."
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T16:16:42Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    16,
                    42,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC"
                },
                "authors": [
                    {
                        "name": "Davide Stauba"
                    },
                    {
                        "name": "Finn Rabe"
                    },
                    {
                        "name": "Akhil Misra"
                    },
                    {
                        "name": "Yves Pauli"
                    },
                    {
                        "name": "Roya Hüppi"
                    },
                    {
                        "name": "Nils Lang"
                    },
                    {
                        "name": "Lars Michels"
                    },
                    {
                        "name": "Victoria Edkins"
                    },
                    {
                        "name": "Sascha Frühholz"
                    },
                    {
                        "name": "Iris Sommer"
                    },
                    {
                        "name": "Wolfram Hinzen"
                    },
                    {
                        "name": "Philipp Homan"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Homan"
                },
                "author": "Philipp Homan"
            },
            {
                "id": "http://arxiv.org/abs/2512.18773v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18773v2",
                "title": "Decentralized GNSS at Global Scale via Graph-Aware Diffusion Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized GNSS at Global Scale via Graph-Aware Diffusion Adaptation"
                },
                "updated": "2025-12-23T16:15:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    15,
                    47,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18773v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Network-based Global Navigation Satellite Systems (GNSS) underpin critical infrastructure and autonomous systems, yet typically rely on centralized processing hubs that limit scalability, resilience, and latency. Here we report a global-scale, decentralized GNSS architecture spanning hundreds of ground stations. By modeling the receiver network as a time-varying graph, we employ a deep linear neural network approach to learn topology-aware mixing schedules that optimize information exchange. This enables a gradient tracking diffusion strategy wherein stations execute local inference and exchange succinct messages to achieve two concurrent objectives: centimeter-level self-localization and network-wide consensus on satellite correction products. The consensus products are broadcast to user receivers as corrections, supporting precise point positioning (PPP) and precise point positioning-real-time kinematic (PPP-RTK). Numerical results demonstrate that our method matches the accuracy of centralized baselines while significantly outperforming existing decentralized methods in convergence speed and communication overhead. By reframing decentralized GNSS as a networked signal processing problem, our results pave the way for integrating decentralized optimization, consensus-based inference, and graph-aware learning as effective tools in operational satellite navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network-based Global Navigation Satellite Systems (GNSS) underpin critical infrastructure and autonomous systems, yet typically rely on centralized processing hubs that limit scalability, resilience, and latency. Here we report a global-scale, decentralized GNSS architecture spanning hundreds of ground stations. By modeling the receiver network as a time-varying graph, we employ a deep linear neural network approach to learn topology-aware mixing schedules that optimize information exchange. This enables a gradient tracking diffusion strategy wherein stations execute local inference and exchange succinct messages to achieve two concurrent objectives: centimeter-level self-localization and network-wide consensus on satellite correction products. The consensus products are broadcast to user receivers as corrections, supporting precise point positioning (PPP) and precise point positioning-real-time kinematic (PPP-RTK). Numerical results demonstrate that our method matches the accuracy of centralized baselines while significantly outperforming existing decentralized methods in convergence speed and communication overhead. By reframing decentralized GNSS as a networked signal processing problem, our results pave the way for integrating decentralized optimization, consensus-based inference, and graph-aware learning as effective tools in operational satellite navigation."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-21T15:24:27Z",
                "published_parsed": [
                    2025,
                    12,
                    21,
                    15,
                    24,
                    27,
                    6,
                    355,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Xue Xian Zheng"
                    },
                    {
                        "name": "Xing Liu"
                    },
                    {
                        "name": "Tareq Y. Al-Naffouri"
                    }
                ],
                "author_detail": {
                    "name": "Tareq Y. Al-Naffouri"
                },
                "author": "Tareq Y. Al-Naffouri"
            },
            {
                "id": "http://arxiv.org/abs/2512.20458v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20458v1",
                "title": "Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register"
                },
                "updated": "2025-12-23T15:53:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    53,
                    33,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20458v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs) have enabled agentic search systems that interleave multi-step reasoning with external tool use. However, existing frameworks largely rely on unstructured natural-language reasoning and accumulate raw intermediate traces in the context, which often leads to unstable reasoning trajectories, context overflow, and degraded performance on complex multi-hop queries. In this study, we introduce Laser, a general framework for stabilizing and scaling agentic search. Laser defines a symbolic action protocol that organizes agent behaviors into three spaces: planning, task-solving, and retrospection. Each action is specified with explicit semantics and a deterministic execution format, enabling structured and logical reasoning processes and reliable action parsing. This design makes intermediate decisions interpretable and traceable, enhancing explicit retrospection and fine-grained control over reasoning trajectories. In coordination with parsable actions, Laser further maintains a compact context register that stores only essential states of the reasoning process, allowing the agent to reason over long horizons without uncontrolled context expansion. Experiments on Qwen2.5/3-series models across challenging multi-hop QA datasets show that Laser consistently outperforms existing agentic search baselines under both prompting-only and fine-tuning settings, demonstrating that Laser provides a principled and effective foundation for robust, scalable agentic search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs) have enabled agentic search systems that interleave multi-step reasoning with external tool use. However, existing frameworks largely rely on unstructured natural-language reasoning and accumulate raw intermediate traces in the context, which often leads to unstable reasoning trajectories, context overflow, and degraded performance on complex multi-hop queries. In this study, we introduce Laser, a general framework for stabilizing and scaling agentic search. Laser defines a symbolic action protocol that organizes agent behaviors into three spaces: planning, task-solving, and retrospection. Each action is specified with explicit semantics and a deterministic execution format, enabling structured and logical reasoning processes and reliable action parsing. This design makes intermediate decisions interpretable and traceable, enhancing explicit retrospection and fine-grained control over reasoning trajectories. In coordination with parsable actions, Laser further maintains a compact context register that stores only essential states of the reasoning process, allowing the agent to reason over long horizons without uncontrolled context expansion. Experiments on Qwen2.5/3-series models across challenging multi-hop QA datasets show that Laser consistently outperforms existing agentic search baselines under both prompting-only and fine-tuning settings, demonstrating that Laser provides a principled and effective foundation for robust, scalable agentic search."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T15:53:33Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    53,
                    33,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Shuting Wang"
                    },
                    {
                        "name": "Qiaolin Xia"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Bobsimons"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou"
            },
            {
                "id": "http://arxiv.org/abs/2509.22358v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.22358v2",
                "title": "Stochastic activations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stochastic activations"
                },
                "updated": "2025-12-23T15:51:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    51,
                    7,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.22358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.22358v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce stochastic activations. This novel strategy randomly selects between several non-linear functions in the feed-forward layer of a large language model. In particular, we choose between SILU or RELU depending on a Bernoulli draw. This strategy circumvents the optimization problem associated with RELU, namely, the constant shape for negative inputs that prevents the gradient flow. We leverage this strategy in two ways:\n  (1) We use stochastic activations during pre-training and fine-tune the model with RELU, which is used at inference time to provide sparse latent vectors. This reduces the inference FLOPs and translates into a significant speedup in the CPU. Interestingly, this leads to much better results than training from scratch with the RELU activation function.\n  (2) We evaluate stochastic activations for generation. This strategy performs reasonably well: it is only slightly inferior to the best deterministic non-linearity, namely SILU combined with temperature scaling. This offers an alternative to existing strategies by providing a controlled way to increase the diversity of the generated text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce stochastic activations. This novel strategy randomly selects between several non-linear functions in the feed-forward layer of a large language model. In particular, we choose between SILU or RELU depending on a Bernoulli draw. This strategy circumvents the optimization problem associated with RELU, namely, the constant shape for negative inputs that prevents the gradient flow. We leverage this strategy in two ways:\n  (1) We use stochastic activations during pre-training and fine-tune the model with RELU, which is used at inference time to provide sparse latent vectors. This reduces the inference FLOPs and translates into a significant speedup in the CPU. Interestingly, this leads to much better results than training from scratch with the RELU activation function.\n  (2) We evaluate stochastic activations for generation. This strategy performs reasonably well: it is only slightly inferior to the best deterministic non-linearity, namely SILU combined with temperature scaling. This offers an alternative to existing strategies by providing a controlled way to increase the diversity of the generated text."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-26T13:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    13,
                    53,
                    56,
                    4,
                    269,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Maria Lomeli"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Gergely Szilvasy"
                    },
                    {
                        "name": "Loic Cabannes"
                    },
                    {
                        "name": "Jade Copet"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Gabriel Synnaeve"
                    },
                    {
                        "name": "Pierre-Emmanuel Mazaré"
                    },
                    {
                        "name": "Hervé Jégou"
                    }
                ],
                "author_detail": {
                    "name": "Hervé Jégou"
                },
                "author": "Hervé Jégou"
            },
            {
                "id": "http://arxiv.org/abs/2512.20455v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20455v1",
                "title": "The Indian Pulsar Timing Array Data Release 2: II. Customised Single-Pulsar Noise Analysis and Noise Budget",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Indian Pulsar Timing Array Data Release 2: II. Customised Single-Pulsar Noise Analysis and Noise Budget"
                },
                "updated": "2025-12-23T15:50:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    50,
                    51,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20455v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20455v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present the results of customised single-pulsar noise analysis of 27 millisecond pulsars from the second data release of the Indian Pulsar Timing Array (InPTA-DR2). We model various stochastic noise sources present in the dataset using stationary Gaussian processes and estimate the noise budget of the InPTA-DR2 using Bayesian inference, involving model selection, Fourier harmonics selection, and parameter estimation for each pulsar. We check the efficacy of our noise characterisation by performing the Anderson-Darling test for Gaussianity on the noise-subtracted residuals. We find that all 11 pulsars with time baseline $\\lesssim2.5\\,\\text{yr}$ show Gaussian residuals and do not have evidence for any red noise process in the optimal model, except for PSR J1944$+$0907, which shows presence of DM noise. PSRs J0437$-$4715, J1909$-$3744 and J1939$+$2134 show preference for the most complicated noise model, having achromatic and chromatic red noise processes. Only 4 out of 15 pulsars with time baseline $\\gtrsim2.5\\,\\text{yr}$ show significant non-Gaussianity in noise-subtracted residuals. We suspect that this may require more advanced methods to model noise processes properly. A comparative study of six pulsars with data removed near solar conjunctions showed deviations from the parameter estimates obtained with the original dataset, indicating potential bias in red noise processes due to unmodeled solar-wind effects. The results presented in this work remain broadly consistent with the InPTA-DR1 noise budget, with better constraints obtained on noise processes for several pulsars and support for achromatic red noise in PSR J1012$+$5307 due to the extended time baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the results of customised single-pulsar noise analysis of 27 millisecond pulsars from the second data release of the Indian Pulsar Timing Array (InPTA-DR2). We model various stochastic noise sources present in the dataset using stationary Gaussian processes and estimate the noise budget of the InPTA-DR2 using Bayesian inference, involving model selection, Fourier harmonics selection, and parameter estimation for each pulsar. We check the efficacy of our noise characterisation by performing the Anderson-Darling test for Gaussianity on the noise-subtracted residuals. We find that all 11 pulsars with time baseline $\\lesssim2.5\\,\\text{yr}$ show Gaussian residuals and do not have evidence for any red noise process in the optimal model, except for PSR J1944$+$0907, which shows presence of DM noise. PSRs J0437$-$4715, J1909$-$3744 and J1939$+$2134 show preference for the most complicated noise model, having achromatic and chromatic red noise processes. Only 4 out of 15 pulsars with time baseline $\\gtrsim2.5\\,\\text{yr}$ show significant non-Gaussianity in noise-subtracted residuals. We suspect that this may require more advanced methods to model noise processes properly. A comparative study of six pulsars with data removed near solar conjunctions showed deviations from the parameter estimates obtained with the original dataset, indicating potential bias in red noise processes due to unmodeled solar-wind effects. The results presented in this work remain broadly consistent with the InPTA-DR1 noise budget, with better constraints obtained on noise processes for several pulsars and support for achromatic red noise in PSR J1012$+$5307 due to the extended time baseline."
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T15:50:51Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    50,
                    51,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "21 pages,Submitted for review",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE"
                },
                "authors": [
                    {
                        "name": "K. Nobleson"
                    },
                    {
                        "name": "Churchil Dwivedi"
                    },
                    {
                        "name": "Shantanu Desai"
                    },
                    {
                        "name": "Bhal Chandra Joshi"
                    },
                    {
                        "name": "Himanshu Grover"
                    },
                    {
                        "name": "Debabrata Deb"
                    },
                    {
                        "name": "Vaishnavi Vyasraj"
                    },
                    {
                        "name": "Kunjal Vara"
                    },
                    {
                        "name": "Hemanga Tahbildar"
                    },
                    {
                        "name": "Abhimanyu Susobhanan"
                    },
                    {
                        "name": "Mayuresh Surnis"
                    },
                    {
                        "name": "Aman Srivastava"
                    },
                    {
                        "name": "Shubhit Sardana"
                    },
                    {
                        "name": "Keitaro Takahashi"
                    },
                    {
                        "name": "Amarnath"
                    },
                    {
                        "name": "P. Arumugam"
                    },
                    {
                        "name": "Manjari Bagchi"
                    },
                    {
                        "name": "Neelam Dhanda Batra"
                    },
                    {
                        "name": "Manoneeta Chakraborty"
                    },
                    {
                        "name": "Shaswata Chowdhury"
                    },
                    {
                        "name": "Shebin Jose Jacob"
                    },
                    {
                        "name": "Jibin Jose"
                    },
                    {
                        "name": "Shubham Kala"
                    },
                    {
                        "name": "Ryo Kato"
                    },
                    {
                        "name": "M. A. Krishnakumar"
                    },
                    {
                        "name": "Kuldeep Meena"
                    },
                    {
                        "name": "Avinash Kumar Paladi"
                    },
                    {
                        "name": "Arul Pandian"
                    },
                    {
                        "name": "Kaustubh Rai"
                    },
                    {
                        "name": "Prerna Rana"
                    },
                    {
                        "name": "Manpreet Singh"
                    },
                    {
                        "name": "Jaikhomba Singha"
                    },
                    {
                        "name": "Adya Shukla"
                    },
                    {
                        "name": "Pratik Tarafdar"
                    },
                    {
                        "name": "Prabu Thiagraj"
                    },
                    {
                        "name": "Zenia Zuraiq"
                    }
                ],
                "author_detail": {
                    "name": "Zenia Zuraiq"
                },
                "author": "Zenia Zuraiq"
            },
            {
                "id": "http://arxiv.org/abs/2512.06770v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.06770v3",
                "title": "Foundation Model for Polycrystalline Material Informatics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation Model for Polycrystalline Material Informatics"
                },
                "updated": "2025-12-23T15:45:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    45,
                    1,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.06770v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.06770v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present a three-dimensional foundation model for polycrystalline materials based on a masked autoencoder trained via large-scale self-supervised learning. The model is pretrained on $100{,}000$ voxelized synthetic face-centered cubic (FCC) microstructures whose crystallographic textures systematically span the texture hull using hierarchical simplex sampling. The transferability of the learned latent representations is evaluated on two downstream tasks: homogenized elastic stiffness prediction and nonlinear stress-strain response prediction. For the nonlinear task, the pretrained encoder is coupled with an orientation-aware interaction-based deep material network (ODMN), where latent features are used to infer microstructure-dependent surrogate parameters. The inferred ODMNs are subsequently combined with crystal plasticity to predict stress--strain responses for previously unseen microstructures. In stiffness prediction, the pretrained model achieves validation $R^2$ values exceeding 0.8, compared to below 0.1 for non-pretrained baselines. In nonlinear response prediction, mean stress errors remain below 4\\%. These results demonstrate that self-supervised pretraining yields physically meaningful and transferable microstructural representations, providing a scalable framework for microstructure-property inference in polycrystalline materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a three-dimensional foundation model for polycrystalline materials based on a masked autoencoder trained via large-scale self-supervised learning. The model is pretrained on $100{,}000$ voxelized synthetic face-centered cubic (FCC) microstructures whose crystallographic textures systematically span the texture hull using hierarchical simplex sampling. The transferability of the learned latent representations is evaluated on two downstream tasks: homogenized elastic stiffness prediction and nonlinear stress-strain response prediction. For the nonlinear task, the pretrained encoder is coupled with an orientation-aware interaction-based deep material network (ODMN), where latent features are used to infer microstructure-dependent surrogate parameters. The inferred ODMNs are subsequently combined with crystal plasticity to predict stress--strain responses for previously unseen microstructures. In stiffness prediction, the pretrained model achieves validation $R^2$ values exceeding 0.8, compared to below 0.1 for non-pretrained baselines. In nonlinear response prediction, mean stress errors remain below 4\\%. These results demonstrate that self-supervised pretraining yields physically meaningful and transferable microstructural representations, providing a scalable framework for microstructure-property inference in polycrystalline materials."
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-07T10:13:30Z",
                "published_parsed": [
                    2025,
                    12,
                    7,
                    10,
                    13,
                    30,
                    6,
                    341,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE"
                },
                "authors": [
                    {
                        "name": "Ting-Ju Wei"
                    },
                    {
                        "name": "Chuin-Shan Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chuin-Shan Chen"
                },
                "author": "Chuin-Shan Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.20451v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20451v1",
                "title": "Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Motion Pattern: An Empirical Study of Physical Forces for Human Motion Understanding"
                },
                "updated": "2025-12-23T15:43:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    43,
                    48,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20451v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Human motion understanding has advanced rapidly through vision-based progress in recognition, tracking, and captioning. However, most existing methods overlook physical cues such as joint actuation forces that are fundamental in biomechanics. This gap motivates our study: if and when do physically inferred forces enhance motion understanding? By incorporating forces into established motion understanding pipelines, we systematically evaluate their impact across baseline models on 3 major tasks: gait recognition, action recognition, and fine-grained video captioning. Across 8 benchmarks, incorporating forces yields consistent performance gains; for example, on CASIA-B, Rank-1 gait recognition accuracy improved from 89.52% to 90.39% (+0.87), with larger gain observed under challenging conditions: +2.7% when wearing a coat and +3.0% at the side view. On Gait3D, performance also increases from 46.0% to 47.3% (+1.3). In action recognition, CTR-GCN achieved +2.00% on Penn Action, while high-exertion classes like punching/slapping improved by +6.96%. Even in video captioning, Qwen2.5-VL's ROUGE-L score rose from 0.310 to 0.339 (+0.029), indicating that physics-inferred forces enhance temporal grounding and semantic richness. These results demonstrate that force cues can substantially complement visual and kinematic features under dynamic, occluded, or appearance-varying conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human motion understanding has advanced rapidly through vision-based progress in recognition, tracking, and captioning. However, most existing methods overlook physical cues such as joint actuation forces that are fundamental in biomechanics. This gap motivates our study: if and when do physically inferred forces enhance motion understanding? By incorporating forces into established motion understanding pipelines, we systematically evaluate their impact across baseline models on 3 major tasks: gait recognition, action recognition, and fine-grained video captioning. Across 8 benchmarks, incorporating forces yields consistent performance gains; for example, on CASIA-B, Rank-1 gait recognition accuracy improved from 89.52% to 90.39% (+0.87), with larger gain observed under challenging conditions: +2.7% when wearing a coat and +3.0% at the side view. On Gait3D, performance also increases from 46.0% to 47.3% (+1.3). In action recognition, CTR-GCN achieved +2.00% on Penn Action, while high-exertion classes like punching/slapping improved by +6.96%. Even in video captioning, Qwen2.5-VL's ROUGE-L score rose from 0.310 to 0.339 (+0.029), indicating that physics-inferred forces enhance temporal grounding and semantic richness. These results demonstrate that force cues can substantially complement visual and kinematic features under dynamic, occluded, or appearance-varying conditions."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T15:43:48Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    43,
                    48,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Anh Dao"
                    },
                    {
                        "name": "Manh Tran"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Xiaoming Liu"
                    },
                    {
                        "name": "Zijun Cui"
                    }
                ],
                "author_detail": {
                    "name": "Zijun Cui"
                },
                "author": "Zijun Cui"
            },
            {
                "id": "http://arxiv.org/abs/2506.12460v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.12460v3",
                "title": "Binarization-Aware Adjuster for Discrete Decision Learning with an Application to Edge Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Binarization-Aware Adjuster for Discrete Decision Learning with an Application to Edge Detection"
                },
                "updated": "2025-12-23T15:42:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    42,
                    0,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.12460v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.12460v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Discrete decision tasks in machine learning exhibit a fundamental misalignment between training and inference: models are optimized with continuous-valued outputs but evaluated using discrete predictions. This misalignment arises from the discontinuity of discretization operations, which prevents decision behavior from being directly incorporated into gradient-based optimization. To address this issue, we propose a theoretically grounded framework termed the Binarization-Aware Adjuster (BAA), which embeds binarization characteristics into continuous optimization. The framework is built upon the Distance Weight Function (DWF), which modulates loss contributions according to prediction correctness and proximity to the decision threshold, thereby aligning optimization emphasis with decision-critical regions while remaining compatible with standard learning pipelines. We apply the proposed BAA framework to the edge detection (ED) task, a representative binary decision problem. Experimental results on representative models and datasets show that incorporating BAA into optimization leads to consistent performance improvements, supporting its effectiveness. Overall, this work establishes a principled approach for aligning continuous optimization with discrete decision behavior, with its effectiveness demonstrated in a concrete application setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete decision tasks in machine learning exhibit a fundamental misalignment between training and inference: models are optimized with continuous-valued outputs but evaluated using discrete predictions. This misalignment arises from the discontinuity of discretization operations, which prevents decision behavior from being directly incorporated into gradient-based optimization. To address this issue, we propose a theoretically grounded framework termed the Binarization-Aware Adjuster (BAA), which embeds binarization characteristics into continuous optimization. The framework is built upon the Distance Weight Function (DWF), which modulates loss contributions according to prediction correctness and proximity to the decision threshold, thereby aligning optimization emphasis with decision-critical regions while remaining compatible with standard learning pipelines. We apply the proposed BAA framework to the edge detection (ED) task, a representative binary decision problem. Experimental results on representative models and datasets show that incorporating BAA into optimization leads to consistent performance improvements, supporting its effectiveness. Overall, this work establishes a principled approach for aligning continuous optimization with discrete decision behavior, with its effectiveness demonstrated in a concrete application setting."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-14T11:56:44Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    11,
                    56,
                    44,
                    5,
                    165,
                    0
                ],
                "arxiv_comment": "28 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Hao Shu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Shu"
                },
                "author": "Hao Shu"
            },
            {
                "id": "http://arxiv.org/abs/2512.20448v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20448v1",
                "title": "Enriching Earth Observation labeled data with Quantum Conditioned Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enriching Earth Observation labeled data with Quantum Conditioned Diffusion Models"
                },
                "updated": "2025-12-23T15:40:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    40,
                    31,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20448v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid adoption of diffusion models (DMs) in the Earth Observation (EO) domain has unlocked new generative capabilities aimed at producing new samples, whose statistical properties closely match real imagery, for tasks such as synthesizing missing data, augmenting scarce labeled datasets, and improving image reconstruction. This is particularly relevant in EO, where labeled data are often costly to obtain and limited in availability. However, classical DMs still face significant computational limitations, requiring hundreds to thousands of inference steps, as well as difficulties in capturing the intricate spatial and spectral correlations characteristic of EO data. Recent research in Quantum Machine Learning (QML), including initial attempts of Quantum Generative Models, offers a fundamentally different approach to overcome these challenges. Motivated by these considerations, we introduce the Quanvolutional Conditioned U-Net (QCU-Net), a hybrid quantum--classical architecture that applies quantum operations within a conditioned diffusion framework using a novel quanvolutional feature-extraction approach, for generating synthetic labeled EO imagery. Extensive experiments on the EuroSAT RGB dataset demonstrate that our QCU-Net achieves superior results. Notably, it reduces the Fréchet Inception Distance by 64%, lowers the Kernel Inception Distance by 76%, and yields higher semantic accuracy. Ablation studies further reveal that strategically positioning quantum layers and employing entangling variational circuits enhance model performance and convergence. This work represents the first successful adaptation of class-conditioned quantum diffusion modeling in the EO domain, paving the way for quantum-enhanced remote sensing imagery synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid adoption of diffusion models (DMs) in the Earth Observation (EO) domain has unlocked new generative capabilities aimed at producing new samples, whose statistical properties closely match real imagery, for tasks such as synthesizing missing data, augmenting scarce labeled datasets, and improving image reconstruction. This is particularly relevant in EO, where labeled data are often costly to obtain and limited in availability. However, classical DMs still face significant computational limitations, requiring hundreds to thousands of inference steps, as well as difficulties in capturing the intricate spatial and spectral correlations characteristic of EO data. Recent research in Quantum Machine Learning (QML), including initial attempts of Quantum Generative Models, offers a fundamentally different approach to overcome these challenges. Motivated by these considerations, we introduce the Quanvolutional Conditioned U-Net (QCU-Net), a hybrid quantum--classical architecture that applies quantum operations within a conditioned diffusion framework using a novel quanvolutional feature-extraction approach, for generating synthetic labeled EO imagery. Extensive experiments on the EuroSAT RGB dataset demonstrate that our QCU-Net achieves superior results. Notably, it reduces the Fréchet Inception Distance by 64%, lowers the Kernel Inception Distance by 76%, and yields higher semantic accuracy. Ablation studies further reveal that strategically positioning quantum layers and employing entangling variational circuits enhance model performance and convergence. This work represents the first successful adaptation of class-conditioned quantum diffusion modeling in the EO domain, paving the way for quantum-enhanced remote sensing imagery synthesis."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T15:40:31Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    40,
                    31,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Francesco Mauro"
                    },
                    {
                        "name": "Francesca De Falco"
                    },
                    {
                        "name": "Lorenzo Papa"
                    },
                    {
                        "name": "Andrea Ceschini"
                    },
                    {
                        "name": "Alessandro Sebastianelli"
                    },
                    {
                        "name": "Paolo Gamba"
                    },
                    {
                        "name": "Massimo Panella"
                    },
                    {
                        "name": "Silvia Ullo"
                    }
                ],
                "author_detail": {
                    "name": "Silvia Ullo"
                },
                "author": "Silvia Ullo"
            },
            {
                "id": "http://arxiv.org/abs/2310.05805v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2310.05805v3",
                "title": "Boosted Control Functions: Distribution generalization and invariance in confounded models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosted Control Functions: Distribution generalization and invariance in confounded models"
                },
                "updated": "2025-12-23T15:25:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    25,
                    13,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2310.05805v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2310.05805v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Modern machine learning methods and the availability of large-scale data have significantly advanced our ability to predict target quantities from large sets of covariates. However, these methods often struggle under distributional shifts, particularly in the presence of hidden confounding. While the impact of hidden confounding is well-studied in causal effect estimation, e.g., instrumental variables, its implications for prediction tasks under shifting distributions remain underexplored. This work addresses this gap by introducing a strong notion of invariance that, unlike existing weaker notions, allows for distribution generalization even in the presence of nonlinear, non-identifiable structural functions. Central to this framework is the Boosted Control Function (BCF), a novel, identifiable target of inference that satisfies the proposed strong invariance notion and is provably worst-case optimal under distributional shifts. The theoretical foundation of our work lies in Simultaneous Equation Models for Distribution Generalization (SIMDGs), which bridge machine learning with econometrics by describing data-generating processes under distributional shifts. To put these insights into practice, we propose the ControlTwicing algorithm to estimate the BCF using nonparametric machine-learning techniques and study its generalization performance on synthetic and real-world datasets compared to robust and empirical risk minimization approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern machine learning methods and the availability of large-scale data have significantly advanced our ability to predict target quantities from large sets of covariates. However, these methods often struggle under distributional shifts, particularly in the presence of hidden confounding. While the impact of hidden confounding is well-studied in causal effect estimation, e.g., instrumental variables, its implications for prediction tasks under shifting distributions remain underexplored. This work addresses this gap by introducing a strong notion of invariance that, unlike existing weaker notions, allows for distribution generalization even in the presence of nonlinear, non-identifiable structural functions. Central to this framework is the Boosted Control Function (BCF), a novel, identifiable target of inference that satisfies the proposed strong invariance notion and is provably worst-case optimal under distributional shifts. The theoretical foundation of our work lies in Simultaneous Equation Models for Distribution Generalization (SIMDGs), which bridge machine learning with econometrics by describing data-generating processes under distributional shifts. To put these insights into practice, we propose the ControlTwicing algorithm to estimate the BCF using nonparametric machine-learning techniques and study its generalization performance on synthetic and real-world datasets compared to robust and empirical risk minimization approaches."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-10-09T15:43:46Z",
                "published_parsed": [
                    2023,
                    10,
                    9,
                    15,
                    43,
                    46,
                    0,
                    282,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "Nicola Gnecco"
                    },
                    {
                        "name": "Jonas Peters"
                    },
                    {
                        "name": "Sebastian Engelke"
                    },
                    {
                        "name": "Niklas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Pfister"
                },
                "author": "Niklas Pfister"
            },
            {
                "id": "http://arxiv.org/abs/2511.21878v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21878v2",
                "title": "Advancing Automated In-Isolation Validation in Repository-Level Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Automated In-Isolation Validation in Repository-Level Code Translation"
                },
                "updated": "2025-12-23T15:17:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    17,
                    24,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21878v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T19:53:46Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    19,
                    53,
                    46,
                    2,
                    330,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Kaiyao Ke"
                    },
                    {
                        "name": "Ali Reza Ibrahimzada"
                    },
                    {
                        "name": "Rangeet Pan"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    }
                ],
                "author_detail": {
                    "name": "Reyhaneh Jabbarvand"
                },
                "author": "Reyhaneh Jabbarvand"
            },
            {
                "id": "http://arxiv.org/abs/2512.20429v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20429v1",
                "title": "Exploring the nature of gravity with quantum information methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the nature of gravity with quantum information methods"
                },
                "updated": "2025-12-23T15:16:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    16,
                    16,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20429v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20429v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The aim of this article is to provide an introduction to the use of quantum information methods for investigating the interface between quantum theory and gravity. To this end, we discuss the basic principles of two current research streams that use this approach. The first one explores a phenomenon known as gravitationally induced entanglement, which aims to infer whether the gravitational field responsible for the interaction between two massive bodies must be quantized or not. The second stream investigates causal structures, thereby providing indirect evidence that spacetime may exhibit non-classical behavior. Before presenting these topics, we briefly review some fundamental concepts and experiments from quantum information theory, such as the Mach-Zehnder interferometer, the Stern-Gerlach experiment, Bell inequalities and entanglement, and the language of quantum circuits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The aim of this article is to provide an introduction to the use of quantum information methods for investigating the interface between quantum theory and gravity. To this end, we discuss the basic principles of two current research streams that use this approach. The first one explores a phenomenon known as gravitationally induced entanglement, which aims to infer whether the gravitational field responsible for the interaction between two massive bodies must be quantized or not. The second stream investigates causal structures, thereby providing indirect evidence that spacetime may exhibit non-classical behavior. Before presenting these topics, we briefly review some fundamental concepts and experiments from quantum information theory, such as the Mach-Zehnder interferometer, the Stern-Gerlach experiment, Bell inequalities and entanglement, and the language of quantum circuits."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T15:16:16Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    16,
                    16,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "This manuscript is based on an article that was published in Brazilian portuguese language in the journal Revista Brasileira de Ensino de Física, vol. 47, suppl. 3, e20250371 (2025), https://doi.org/10.1590/1806-9126-RBEF-2025-0371",
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Bruna Sahdo"
                    },
                    {
                        "name": "Natália Salomé Móller"
                    }
                ],
                "author_detail": {
                    "name": "Natália Salomé Móller"
                },
                "author": "Natália Salomé Móller"
            },
            {
                "id": "http://arxiv.org/abs/2512.20425v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20425v1",
                "title": "Arbitrary laser frequency modulation algorithm based on iterative on-the-fly deconvolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arbitrary laser frequency modulation algorithm based on iterative on-the-fly deconvolution"
                },
                "updated": "2025-12-23T15:10:20Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    10,
                    20,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20425v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1364/AO.577439",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "I present a general laser modulation control algorithm. I implement the LIDAR Frequency Modulated Continuous Wave (FMCW) scheme as a special case of study. My proposal applies to any arbitrary modulation pattern and is based on an iterative algorithm that infers the laser transfer function in order to perform on-the-fly deconvolution. I present an experimental proof-of-principle using an external-cavity diode laser, the accuracy of which I analyse by comparing the obtained frequency response with a targeted modulation pattern. In addition to the FMCW scheme, I am also testing square wave modulations, which are more demanding in terms of bandwidth.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I present a general laser modulation control algorithm. I implement the LIDAR Frequency Modulated Continuous Wave (FMCW) scheme as a special case of study. My proposal applies to any arbitrary modulation pattern and is based on an iterative algorithm that infers the laser transfer function in order to perform on-the-fly deconvolution. I present an experimental proof-of-principle using an external-cavity diode laser, the accuracy of which I analyse by comparing the obtained frequency response with a targeted modulation pattern. In addition to the FMCW scheme, I am also testing square wave modulations, which are more demanding in terms of bandwidth."
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T15:10:20Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    10,
                    20,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics"
                },
                "arxiv_journal_ref": "Appl. Opt. 65, 341-348 (2026)",
                "authors": [
                    {
                        "name": "Thierry Chanelière"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Chanelière"
                },
                "author": "Thierry Chanelière",
                "arxiv_doi": "10.1364/AO.577439"
            },
            {
                "id": "http://arxiv.org/abs/2509.18708v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.18708v2",
                "title": "Optimization-centric cutting feedback for semiparametric models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization-centric cutting feedback for semiparametric models"
                },
                "updated": "2025-12-23T15:10:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    10,
                    17,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.18708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.18708v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Complex statistical models are often built by combining multiple submodels, called modules. Here, we consider modular inference where the modules contain both parametric and nonparametric components. In such cases, standard Bayesian inference can be highly sensitive to misspecification in any module, and common priors for the nonparametric components may compromise inference for the parametric components, and vice versa. We propose a novel ``optimization-centric'' approach to cutting feedback for semiparametric modular inference, which can address misspecification and prior-data conflicts. Proposed cut posteriors are defined via a variational optimization problem like other generalized posteriors, but regularization is based on Rényi divergence, instead of Kullback-Leibler divergence (KLD). We show empirically that defining the cut posterior using Rényi divergence delivers more robust inference than KLD, and Rényi divergence reduces the tendency of uncertainty underestimation when the variational approximations impose strong parametric or independence assumptions. Novel posterior concentration results that accommodate the Rényi divergence and allow for semiparametric components are derived, extending existing results for cut posteriors that only apply to KLD and parametric models. These new methods are demonstrated in a benchmark example and two real examples: Gaussian process adjustments for confounding in causal inference and misspecified copula models with nonparametric marginals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complex statistical models are often built by combining multiple submodels, called modules. Here, we consider modular inference where the modules contain both parametric and nonparametric components. In such cases, standard Bayesian inference can be highly sensitive to misspecification in any module, and common priors for the nonparametric components may compromise inference for the parametric components, and vice versa. We propose a novel ``optimization-centric'' approach to cutting feedback for semiparametric modular inference, which can address misspecification and prior-data conflicts. Proposed cut posteriors are defined via a variational optimization problem like other generalized posteriors, but regularization is based on Rényi divergence, instead of Kullback-Leibler divergence (KLD). We show empirically that defining the cut posterior using Rényi divergence delivers more robust inference than KLD, and Rényi divergence reduces the tendency of uncertainty underestimation when the variational approximations impose strong parametric or independence assumptions. Novel posterior concentration results that accommodate the Rényi divergence and allow for semiparametric components are derived, extending existing results for cut posteriors that only apply to KLD and parametric models. These new methods are demonstrated in a benchmark example and two real examples: Gaussian process adjustments for confounding in causal inference and misspecified copula models with nonparametric marginals."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-23T06:46:16Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    46,
                    16,
                    1,
                    266,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Linda S. L. Tan"
                    },
                    {
                        "name": "David J. Nott"
                    },
                    {
                        "name": "David T. Frazier"
                    }
                ],
                "author_detail": {
                    "name": "David T. Frazier"
                },
                "author": "David T. Frazier"
            },
            {
                "id": "http://arxiv.org/abs/2512.16531v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16531v3",
                "title": "Scaling Laws for Energy Efficiency of Local LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Energy Efficiency of Local LLMs"
                },
                "updated": "2025-12-23T15:02:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    2,
                    39,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16531v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16531v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T13:40:33Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    13,
                    40,
                    33,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Ander Alvarez"
                    },
                    {
                        "name": "Alessandro Genuardi"
                    },
                    {
                        "name": "Nilotpal Sinha"
                    },
                    {
                        "name": "Antonio Tiene"
                    },
                    {
                        "name": "Mikail Okyay"
                    },
                    {
                        "name": "Bakbergen Ryskulov"
                    },
                    {
                        "name": "David Montero"
                    },
                    {
                        "name": "Samuel Mugel"
                    },
                    {
                        "name": "Román Orús"
                    }
                ],
                "author_detail": {
                    "name": "Román Orús"
                },
                "author": "Román Orús"
            },
            {
                "id": "http://arxiv.org/abs/2512.20415v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20415v1",
                "title": "Resolution and Robustness Bounds for Reconstructive Spectrometers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resolution and Robustness Bounds for Reconstructive Spectrometers"
                },
                "updated": "2025-12-23T14:58:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    58,
                    39,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20415v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Reconstructive spectrometers are a promising emerging class of devices that combine complex light scattering with inference to enable compact, high-resolution spectrometry. Thus far, the physical determinants of these devices' performance remain under-explored. We show that under a broad range of conditions, the noise-induced error for spectral reconstruction is governed by the Fisher information. We then use random matrix theory to derive a closed-form relation linking the variance bound to a set of key physical parameters: the spectral correlation length, the mean transmittance, and the number of frequency and measurement channels. The analysis reveals certain fundamental trade-offs between these physical parameters, and establishes the conditions for a spectrometer to achieve ``super-resolution'' below the limit set by the spectral correlation length. Our theory is confirmed using numerical validations with a random matrix model as well as full-wave simulations. These results establish a physically-grounded framework for designing and analyzing performant and noise-robust reconstructive spectrometers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructive spectrometers are a promising emerging class of devices that combine complex light scattering with inference to enable compact, high-resolution spectrometry. Thus far, the physical determinants of these devices' performance remain under-explored. We show that under a broad range of conditions, the noise-induced error for spectral reconstruction is governed by the Fisher information. We then use random matrix theory to derive a closed-form relation linking the variance bound to a set of key physical parameters: the spectral correlation length, the mean transmittance, and the number of frequency and measurement channels. The analysis reveals certain fundamental trade-offs between these physical parameters, and establishes the conditions for a spectrometer to achieve ``super-resolution'' below the limit set by the spectral correlation length. Our theory is confirmed using numerical validations with a random matrix model as well as full-wave simulations. These results establish a physically-grounded framework for designing and analyzing performant and noise-robust reconstructive spectrometers."
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T14:58:39Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    58,
                    39,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "13 pages, 6 figures. Includes Supplementary Materials",
                "arxiv_primary_category": {
                    "term": "physics.optics"
                },
                "authors": [
                    {
                        "name": "Changyan Zhu"
                    },
                    {
                        "name": "Hsuan Lo"
                    },
                    {
                        "name": "Jianbo Yu"
                    },
                    {
                        "name": "Qijie Wang"
                    },
                    {
                        "name": "Y. D. Chong"
                    }
                ],
                "author_detail": {
                    "name": "Y. D. Chong"
                },
                "author": "Y. D. Chong"
            },
            {
                "id": "http://arxiv.org/abs/2512.20405v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20405v1",
                "title": "ChatGPT: Excellent Paper! Accept It. Editor: Imposter Found! Review Rejected",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT: Excellent Paper! Accept It. Editor: Imposter Found! Review Rejected"
                },
                "updated": "2025-12-23T14:54:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    54,
                    45,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20405v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) like ChatGPT are now widely used in writing and reviewing scientific papers. While this trend accelerates publication growth and reduces human workload, it also introduces serious risks. Papers written or reviewed by LLMs may lack real novelty, contain fabricated or biased results, or mislead downstream research that others depend on. Such issues can damage reputations, waste resources, and even endanger lives when flawed studies influence medical or safety-critical systems. This research explores both the offensive and defensive sides of this growing threat. On the attack side, we demonstrate how an author can inject hidden prompts inside a PDF that secretly guide or \"jailbreak\" LLM reviewers into giving overly positive feedback and biased acceptance. On the defense side, we propose an \"inject-and-detect\" strategy for editors, where invisible trigger prompts are embedded into papers; if a review repeats or reacts to these triggers, it reveals that the review was generated by an LLM, not a human. This method turns prompt injections from vulnerability into a verification tool. We outline our design, expected model behaviors, and ethical safeguards for deployment. The goal is to expose how fragile today's peer-review process becomes under LLM influence and how editorial awareness can help restore trust in scientific evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT are now widely used in writing and reviewing scientific papers. While this trend accelerates publication growth and reduces human workload, it also introduces serious risks. Papers written or reviewed by LLMs may lack real novelty, contain fabricated or biased results, or mislead downstream research that others depend on. Such issues can damage reputations, waste resources, and even endanger lives when flawed studies influence medical or safety-critical systems. This research explores both the offensive and defensive sides of this growing threat. On the attack side, we demonstrate how an author can inject hidden prompts inside a PDF that secretly guide or \"jailbreak\" LLM reviewers into giving overly positive feedback and biased acceptance. On the defense side, we propose an \"inject-and-detect\" strategy for editors, where invisible trigger prompts are embedded into papers; if a review repeats or reacts to these triggers, it reveals that the review was generated by an LLM, not a human. This method turns prompt injections from vulnerability into a verification tool. We outline our design, expected model behaviors, and ethical safeguards for deployment. The goal is to expose how fragile today's peer-review process becomes under LLM influence and how editorial awareness can help restore trust in scientific evaluation."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T14:54:45Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    54,
                    45,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Kanchon Gharami"
                    },
                    {
                        "name": "Sanjiv Kumar Sarkar"
                    },
                    {
                        "name": "Yongxin Liu"
                    },
                    {
                        "name": "Shafika Showkat Moni"
                    }
                ],
                "author_detail": {
                    "name": "Shafika Showkat Moni"
                },
                "author": "Shafika Showkat Moni"
            },
            {
                "id": "http://arxiv.org/abs/2512.20403v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20403v1",
                "title": "BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples"
                },
                "updated": "2025-12-23T14:46:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    46,
                    43,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20403v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Distilling knowledge from large proprietary models (e.g., GPT-4) to tiny deployable models (less than 1B parameters) faces a critical capacity-budget trap: the 1000x capacity gap between teachers and students prevents effective direct transfer, while API costs prohibit extensive data collection. We introduce BRIDGE (Budget-Aware Reasoning via Intermediate Distillation), a two-phase framework that resolves these constraints through strategic intermediation and budget asymmetry. In Phase 1, a mid-sized Teacher Assistant (TA; e.g., about 7B) learns from the black-box teacher on a strictly limited subset of data (e.g., 3-5%), selected via a zero-API-cost pipeline that balances entropic difficulty and semantic diversity using only local TA inference. In Phase 2, we exploit this asymmetry-teacher queries are expensive, whereas TA inference is free to amplify supervision: the refined TA generates synthetic rationales for the full dataset to train the tiny student. Crucially, we apply an instruction-tuning curriculum to establish behavioral alignment in the tiny student before transferring reasoning. Our theoretical analysis shows that BRIDGE yields tighter generalization bounds than direct distillation when data is abundant. Experiments across medical, legal, and financial benchmarks demonstrate consistent improvements: BRIDGE delivers student performance gains of 28-41%, closing the capability gap with proprietary teachers by 12-16% while using 10x fewer teacher queries. Notably, BRIDGE defies the conventional cost-performance frontier, surpassing direct distillation baselines that use 100% of the budget while consuming only 5% of the resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling knowledge from large proprietary models (e.g., GPT-4) to tiny deployable models (less than 1B parameters) faces a critical capacity-budget trap: the 1000x capacity gap between teachers and students prevents effective direct transfer, while API costs prohibit extensive data collection. We introduce BRIDGE (Budget-Aware Reasoning via Intermediate Distillation), a two-phase framework that resolves these constraints through strategic intermediation and budget asymmetry. In Phase 1, a mid-sized Teacher Assistant (TA; e.g., about 7B) learns from the black-box teacher on a strictly limited subset of data (e.g., 3-5%), selected via a zero-API-cost pipeline that balances entropic difficulty and semantic diversity using only local TA inference. In Phase 2, we exploit this asymmetry-teacher queries are expensive, whereas TA inference is free to amplify supervision: the refined TA generates synthetic rationales for the full dataset to train the tiny student. Crucially, we apply an instruction-tuning curriculum to establish behavioral alignment in the tiny student before transferring reasoning. Our theoretical analysis shows that BRIDGE yields tighter generalization bounds than direct distillation when data is abundant. Experiments across medical, legal, and financial benchmarks demonstrate consistent improvements: BRIDGE delivers student performance gains of 28-41%, closing the capability gap with proprietary teachers by 12-16% while using 10x fewer teacher queries. Notably, BRIDGE defies the conventional cost-performance frontier, surpassing direct distillation baselines that use 100% of the budget while consuming only 5% of the resources."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T14:46:43Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    46,
                    43,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xuan-An Le"
                    },
                    {
                        "name": "Minh-Nam Tran"
                    },
                    {
                        "name": "Son Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Son Nguyen"
                },
                "author": "Son Nguyen"
            },
            {
                "id": "http://arxiv.org/abs/2512.20396v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20396v1",
                "title": "Symmaries: Automatic Inference of Formal Security Summaries for Java Programs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmaries: Automatic Inference of Formal Security Summaries for Java Programs"
                },
                "updated": "2025-12-23T14:33:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    33,
                    31,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20396v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20396v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We introduce a scalable, modular, and sound approach for automatically constructing formal security specifications for Java bytecode programs in the form of method summaries. A summary provides an abstract representation of a method's security behavior, consisting of the conditions under which the method can be securely invoked, together with specifications of information flows and aliasing updates. Such summaries can be consumed by static code analysis tools and also help developers understand the behavior of code segments, such as libraries, in order to evaluate their security implications when reused in applications. Our approach is implemented in a tool called Symmaries, which automates the generation of security summaries. We applied Symmaries to Java API libraries to extract their security specifications and to large real-world applications to evaluate its scalability. Our results show that the tool successfully scales to analyze applications with hundreds of thousands of lines of code, and that Symmaries achieves a promising precision depending on the heap model used. We prove the soundness of our approach in terms of guaranteeing termination-insensitive non-interference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a scalable, modular, and sound approach for automatically constructing formal security specifications for Java bytecode programs in the form of method summaries. A summary provides an abstract representation of a method's security behavior, consisting of the conditions under which the method can be securely invoked, together with specifications of information flows and aliasing updates. Such summaries can be consumed by static code analysis tools and also help developers understand the behavior of code segments, such as libraries, in order to evaluate their security implications when reused in applications. Our approach is implemented in a tool called Symmaries, which automates the generation of security summaries. We applied Symmaries to Java API libraries to extract their security specifications and to large real-world applications to evaluate its scalability. Our results show that the tool successfully scales to analyze applications with hundreds of thousands of lines of code, and that Symmaries achieves a promising precision depending on the heap model used. We prove the soundness of our approach in terms of guaranteeing termination-insensitive non-interference."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.FL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T14:33:31Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    33,
                    31,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Narges Khakpour"
                    },
                    {
                        "name": "Nicolas Berthier"
                    }
                ],
                "author_detail": {
                    "name": "Nicolas Berthier"
                },
                "author": "Nicolas Berthier"
            },
            {
                "id": "http://arxiv.org/abs/2512.20385v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20385v1",
                "title": "Generalized method of L-moment estimation for stationary and nonstationary extreme value models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalized method of L-moment estimation for stationary and nonstationary extreme value models"
                },
                "updated": "2025-12-23T14:19:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    19,
                    58,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20385v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Precisely estimating out-of-sample upper quantiles is very important in risk assessment and in engineering practice for structural design to prevent a greater disaster. For this purpose, the generalized extreme value (GEV) distribution has been broadly used. To estimate the parameters of GEV distribution, the maximum likelihood estimation (MLE) and L-moment estimation (LME) methods have been primarily employed. For a better estimation using the MLE, several studies considered the generalized MLE (penalized likelihood or Bayesian) methods to cooperate with a penalty function or prior information for parameters. However, a generalized LME method for the same purpose has not been developed yet in the literature. We thus propose the generalized method of L-moment estimation (GLME) to cooperate with a penalty function or prior information. The proposed estimation is based on the generalized L-moment distance and a multivariate normal likelihood approximation. Because the L-moment estimator is more efficient and robust for small samples than the MLE, we reasonably expect the advantages of LME to continue to hold for GLME. The proposed method is applied to the stationary and nonstationary GEV models with two novel (data-adaptive) penalty functions to correct the bias of LME. A simulation study indicates that the biases of LME are considerably corrected by the GLME with slight increases in the standard error. Applications to US flood damage data and maximum rainfall at Phliu Agromet in Thailand illustrate the usefulness of the proposed method. This study may promote further work on penalized or Bayesian inferences based on L-moments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precisely estimating out-of-sample upper quantiles is very important in risk assessment and in engineering practice for structural design to prevent a greater disaster. For this purpose, the generalized extreme value (GEV) distribution has been broadly used. To estimate the parameters of GEV distribution, the maximum likelihood estimation (MLE) and L-moment estimation (LME) methods have been primarily employed. For a better estimation using the MLE, several studies considered the generalized MLE (penalized likelihood or Bayesian) methods to cooperate with a penalty function or prior information for parameters. However, a generalized LME method for the same purpose has not been developed yet in the literature. We thus propose the generalized method of L-moment estimation (GLME) to cooperate with a penalty function or prior information. The proposed estimation is based on the generalized L-moment distance and a multivariate normal likelihood approximation. Because the L-moment estimator is more efficient and robust for small samples than the MLE, we reasonably expect the advantages of LME to continue to hold for GLME. The proposed method is applied to the stationary and nonstationary GEV models with two novel (data-adaptive) penalty functions to correct the bias of LME. A simulation study indicates that the biases of LME are considerably corrected by the GLME with slight increases in the standard error. Applications to US flood damage data and maximum rainfall at Phliu Agromet in Thailand illustrate the usefulness of the proposed method. This study may promote further work on penalized or Bayesian inferences based on L-moments."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T14:19:58Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    19,
                    58,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Yonggwan Shin"
                    },
                    {
                        "name": "Yire Shin"
                    },
                    {
                        "name": "Jihong Park"
                    },
                    {
                        "name": "Jeong-Soo Park"
                    }
                ],
                "author_detail": {
                    "name": "Jeong-Soo Park"
                },
                "author": "Jeong-Soo Park"
            },
            {
                "id": "http://arxiv.org/abs/2512.20368v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20368v1",
                "title": "Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Avoiding the Price of Adaptivity: Inference in Linear Contextual Bandits via Stability"
                },
                "updated": "2025-12-23T13:53:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    53,
                    53,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20368v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Statistical inference in contextual bandits is complicated by the adaptive, non-i.i.d. nature of the data. A growing body of work has shown that classical least-squares inference may fail under adaptive sampling, and that constructing valid confidence intervals for linear functionals of the model parameter typically requires paying an unavoidable inflation of order $\\sqrt{d \\log T}$. This phenomenon -- often referred to as the price of adaptivity -- highlights the inherent difficulty of reliable inference under general contextual bandit policies.\n  A key structural property that circumvents this limitation is the \\emph{stability} condition of Lai and Wei, which requires the empirical feature covariance to concentrate around a deterministic limit. When stability holds, the ordinary least-squares estimator satisfies a central limit theorem, and classical Wald-type confidence intervals -- designed for i.i.d. data -- become asymptotically valid even under adaptation, \\emph{without} incurring the $\\sqrt{d \\log T}$ price of adaptivity.\n  In this paper, we propose and analyze a penalized EXP4 algorithm for linear contextual bandits. Our first main result shows that this procedure satisfies the Lai--Wei stability condition and therefore admits valid Wald-type confidence intervals for linear functionals. Our second result establishes that the same algorithm achieves regret guarantees that are minimax optimal up to logarithmic factors, demonstrating that stability and statistical efficiency can coexist within a single contextual bandit method. Finally, we complement our theory with simulations illustrating the empirical normality of the resulting estimators and the sharpness of the corresponding confidence intervals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference in contextual bandits is complicated by the adaptive, non-i.i.d. nature of the data. A growing body of work has shown that classical least-squares inference may fail under adaptive sampling, and that constructing valid confidence intervals for linear functionals of the model parameter typically requires paying an unavoidable inflation of order $\\sqrt{d \\log T}$. This phenomenon -- often referred to as the price of adaptivity -- highlights the inherent difficulty of reliable inference under general contextual bandit policies.\n  A key structural property that circumvents this limitation is the \\emph{stability} condition of Lai and Wei, which requires the empirical feature covariance to concentrate around a deterministic limit. When stability holds, the ordinary least-squares estimator satisfies a central limit theorem, and classical Wald-type confidence intervals -- designed for i.i.d. data -- become asymptotically valid even under adaptation, \\emph{without} incurring the $\\sqrt{d \\log T}$ price of adaptivity.\n  In this paper, we propose and analyze a penalized EXP4 algorithm for linear contextual bandits. Our first main result shows that this procedure satisfies the Lai--Wei stability condition and therefore admits valid Wald-type confidence intervals for linear functionals. Our second result establishes that the same algorithm achieves regret guarantees that are minimax optimal up to logarithmic factors, demonstrating that stability and statistical efficiency can coexist within a single contextual bandit method. Finally, we complement our theory with simulations illustrating the empirical normality of the resulting estimators and the sharpness of the corresponding confidence intervals."
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T13:53:53Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    53,
                    53,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML"
                },
                "authors": [
                    {
                        "name": "Samya Praharaj"
                    },
                    {
                        "name": "Koulik Khamaru"
                    }
                ],
                "author_detail": {
                    "name": "Koulik Khamaru"
                },
                "author": "Koulik Khamaru"
            },
            {
                "id": "http://arxiv.org/abs/2512.20362v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20362v1",
                "title": "CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation"
                },
                "updated": "2025-12-23T13:44:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    44,
                    41,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20362v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.\n  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.\n  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.\n  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.\n  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T13:44:41Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    44,
                    41,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "37 pages, 42 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "V. Kovalev"
                    },
                    {
                        "name": "A. Kuvshinov"
                    },
                    {
                        "name": "A. Buzovkin"
                    },
                    {
                        "name": "D. Pokidov"
                    },
                    {
                        "name": "D. Timonin"
                    }
                ],
                "author_detail": {
                    "name": "D. Timonin"
                },
                "author": "D. Timonin"
            },
            {
                "id": "http://arxiv.org/abs/2512.20353v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20353v1",
                "title": "Allocating Students to Schools: Theory, Methods, and Empirical Insights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Allocating Students to Schools: Theory, Methods, and Empirical Insights"
                },
                "updated": "2025-12-23T13:33:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    33,
                    53,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20353v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20353v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1016/bs.hesmat.2025.10.004",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "This chapter surveys the application of matching theory to school choice, motivated by the shift from neighborhood assignment systems to choice-based models. Since educational choice is not mediated by price, the design of allocation mechanisms is critical. The chapter first reviews theoretical contributions, exploring the fundamental trade-offs between efficiency, stability, and strategy-proofness, and covers design challenges such as tie-breaking, cardinal welfare, and affirmative action. It then transitions to the empirical landscape, focusing on the central challenge of inferring student preferences from application data, especially under strategic mechanisms. We review various estimation approaches and discuss key insights on parental preferences, market design trade-offs, and the effectiveness of school choice policies?",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This chapter surveys the application of matching theory to school choice, motivated by the shift from neighborhood assignment systems to choice-based models. Since educational choice is not mediated by price, the design of allocation mechanisms is critical. The chapter first reviews theoretical contributions, exploring the fundamental trade-offs between efficiency, stability, and strategy-proofness, and covers design challenges such as tie-breaking, cardinal welfare, and affirmative action. It then transitions to the empirical landscape, focusing on the central challenge of inferring student preferences from application data, especially under strategic mechanisms. We review various estimation approaches and discuss key insights on parental preferences, market design trade-offs, and the effectiveness of school choice policies?"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T13:33:53Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    33,
                    53,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN"
                },
                "arxiv_journal_ref": "in Handbook of the Economics of Matching, vol. 2, 2025",
                "authors": [
                    {
                        "name": "Yeon-Koo Che"
                    },
                    {
                        "name": "Julien Grenet"
                    },
                    {
                        "name": "Yinghua He"
                    }
                ],
                "author_detail": {
                    "name": "Yinghua He"
                },
                "author": "Yinghua He",
                "arxiv_doi": "10.1016/bs.hesmat.2025.10.004"
            },
            {
                "id": "http://arxiv.org/abs/2512.20352v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20352v1",
                "title": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation"
                },
                "updated": "2025-12-23T13:32:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    32,
                    43,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20352v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($κ$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($κ= 0.907$, cosine=95.3%), followed by GPT-4o ($κ= 0.853$, cosine=92.6%) and Claude ($κ= 0.842$, cosine=92.1%). All three models achieve a high agreement ($κ> 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($κ$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($κ= 0.907$, cosine=95.3%), followed by GPT-4o ($κ= 0.853$, cosine=92.6%) and Claude ($κ= 0.842$, cosine=92.1%). All three models achieve a high agreement ($κ> 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T13:32:43Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    32,
                    43,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "11 pages, 1 figure, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Nilesh Jain"
                    },
                    {
                        "name": "Seyi Adeyinka"
                    },
                    {
                        "name": "Leor Roseman"
                    },
                    {
                        "name": "Aza Allsop"
                    }
                ],
                "author_detail": {
                    "name": "Aza Allsop"
                },
                "author": "Aza Allsop"
            },
            {
                "id": "http://arxiv.org/abs/2507.20993v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.20993v2",
                "title": "Learning Treatment Policies From Multimodal Electronic Health Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Treatment Policies From Multimodal Electronic Health Records"
                },
                "updated": "2025-12-23T13:19:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    19,
                    34,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.20993v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.20993v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We study how to learn effective treatment policies from multimodal electronic health records (EHRs) that consist of tabular data and clinical text. These policies can help physicians make better treatment decisions and allocate healthcare resources more efficiently. Causal policy learning methods prioritize patients with the largest expected treatment benefit. Yet, existing estimators assume tabular covariates that satisfy strong causal assumptions, which are typically violated in the multimodal setting. As a result, predictive models of baseline risk are commonly used in practice to guide such decisions, as they extend naturally to multimodal data. However, such risk-based policies are not designed to identify which patients benefit most from treatment. We propose an extension of causal policy learning that uses expert-provided annotations during training to supervise treatment effect estimation, while using only multimodal representations as input during inference. We show that the proposed method achieves strong empirical performance across synthetic, semi-synthetic, and real-world EHR datasets, thereby offering practical insights into applying causal machine learning to realistic clinical data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study how to learn effective treatment policies from multimodal electronic health records (EHRs) that consist of tabular data and clinical text. These policies can help physicians make better treatment decisions and allocate healthcare resources more efficiently. Causal policy learning methods prioritize patients with the largest expected treatment benefit. Yet, existing estimators assume tabular covariates that satisfy strong causal assumptions, which are typically violated in the multimodal setting. As a result, predictive models of baseline risk are commonly used in practice to guide such decisions, as they extend naturally to multimodal data. However, such risk-based policies are not designed to identify which patients benefit most from treatment. We propose an extension of causal policy learning that uses expert-provided annotations during training to supervise treatment effect estimation, while using only multimodal representations as input during inference. We show that the proposed method achieves strong empirical performance across synthetic, semi-synthetic, and real-world EHR datasets, thereby offering practical insights into applying causal machine learning to realistic clinical data."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-28T16:52:31Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    52,
                    31,
                    0,
                    209,
                    0
                ],
                "arxiv_comment": "Preprint. Under review",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Henri Arno"
                    },
                    {
                        "name": "Thomas Demeester"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Demeester"
                },
                "author": "Thomas Demeester"
            },
            {
                "id": "http://arxiv.org/abs/2512.20333v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20333v1",
                "title": "SynCraft: Guiding Large Language Models to Predict Edit Sequences for Molecular Synthesizability Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynCraft: Guiding Large Language Models to Predict Edit Sequences for Molecular Synthesizability Optimization"
                },
                "updated": "2025-12-23T13:07:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    7,
                    22,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20333v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative artificial intelligence has revolutionized the exploration of chemical space, yet a critical bottleneck remains that a substantial fraction of generated molecules is synthetically inaccessible. Current solutions, such as post-hoc filtering or projection-based methods, often compromise structural novelty or disrupt key pharmacophores by forcing molecules into pre-defined synthetic templates. Herein, we introduce SynCraft, a reasoning-based framework that reframes synthesizability optimization not as a sequence translation task, but as a precise structural editing problem. Leveraging the emergent reasoning capabilities of Large Language Models, SynCraft navigates the \"synthesis cliff\" where minimal structural modifications yield significant gains in synthetic feasibility. By predicting executable sequences of atom-level edits rather than generating SMILES strings directly, SynCraft circumvents the syntactic fragility of LLMs while harnessing their chemical intuition. Extensive benchmarks demonstrate that SynCraft outperforms state-of-the-art baselines in generating synthesizable analogs with high structural fidelity. Furthermore, through interaction-aware prompting, SynCraft successfully replicates expert medicinal chemistry intuition in editing PLK1 inhibitors and rescuing high-scoring but previously discarded RIPK1 candidates in previous molecular generation literatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence has revolutionized the exploration of chemical space, yet a critical bottleneck remains that a substantial fraction of generated molecules is synthetically inaccessible. Current solutions, such as post-hoc filtering or projection-based methods, often compromise structural novelty or disrupt key pharmacophores by forcing molecules into pre-defined synthetic templates. Herein, we introduce SynCraft, a reasoning-based framework that reframes synthesizability optimization not as a sequence translation task, but as a precise structural editing problem. Leveraging the emergent reasoning capabilities of Large Language Models, SynCraft navigates the \"synthesis cliff\" where minimal structural modifications yield significant gains in synthetic feasibility. By predicting executable sequences of atom-level edits rather than generating SMILES strings directly, SynCraft circumvents the syntactic fragility of LLMs while harnessing their chemical intuition. Extensive benchmarks demonstrate that SynCraft outperforms state-of-the-art baselines in generating synthesizable analogs with high structural fidelity. Furthermore, through interaction-aware prompting, SynCraft successfully replicates expert medicinal chemistry intuition in editing PLK1 inhibitors and rescuing high-scoring but previously discarded RIPK1 candidates in previous molecular generation literatures."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T13:07:22Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    7,
                    22,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "28 pages, 4 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Junren Li"
                    },
                    {
                        "name": "Luhua Lai"
                    }
                ],
                "author_detail": {
                    "name": "Luhua Lai"
                },
                "author": "Luhua Lai"
            },
            {
                "id": "http://arxiv.org/abs/2512.20328v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20328v1",
                "title": "Toward Explaining Large Language Models in Software Engineering Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Explaining Large Language Models in Software Engineering Tasks"
                },
                "updated": "2025-12-23T12:56:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    56,
                    18,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20328v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:56:18Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    56,
                    18,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Antonio Vitale"
                    },
                    {
                        "name": "Khai-Nguyen Nguyen"
                    },
                    {
                        "name": "Denys Poshyvanyk"
                    },
                    {
                        "name": "Rocco Oliveto"
                    },
                    {
                        "name": "Simone Scalabrino"
                    },
                    {
                        "name": "Antonio Mastropaolo"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Mastropaolo"
                },
                "author": "Antonio Mastropaolo"
            },
            {
                "id": "http://arxiv.org/abs/2512.20324v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20324v1",
                "title": "Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles"
                },
                "updated": "2025-12-23T12:48:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    48,
                    5,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20324v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:48:05Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    48,
                    5,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Nurul Labib Sayeedi"
                    },
                    {
                        "name": "Md. Faiyaz Abdullah Sayeedi"
                    },
                    {
                        "name": "Khushnur Binte Jahangir"
                    },
                    {
                        "name": "Swakkhar Shatabda"
                    },
                    {
                        "name": "Sarah Masud Preum"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Masud Preum"
                },
                "author": "Sarah Masud Preum"
            },
            {
                "id": "http://arxiv.org/abs/2512.20323v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20323v1",
                "title": "Differentially Private Feature Release for Wireless Sensing: Adaptive Privacy Budget Allocation on CSI Spectrograms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Feature Release for Wireless Sensing: Adaptive Privacy Budget Allocation on CSI Spectrograms"
                },
                "updated": "2025-12-23T12:45:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    45,
                    49,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20323v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Wi-Fi/RF-based human sensing has achieved remarkable progress with deep learning, yet practical deployments increasingly require feature sharing for cloud analytics, collaborative training, or benchmark evaluation. Releasing intermediate representations such as CSI spectrograms can inadvertently expose sensitive information, including user identity, location, and membership, motivating formal privacy guarantees. In this paper, we study differentially private (DP) feature release for wireless sensing and propose an adaptive privacy budget allocation mechanism tailored to the highly non-uniform structure of CSI time-frequency representations. Our pipeline converts CSI to bounded spectrogram features, applies sensitivity control via clipping, estimates task-relevant importance over the time-frequency plane, and allocates a global privacy budget across spectrogram blocks before injecting calibrated Gaussian noise. Experiments on multi-user activity sensing (WiMANS), multi-person 3D pose estimation (Person-in-WiFi 3D), and respiration monitoring (Resp-CSI) show that adaptive allocation consistently improves the privacy-utility frontier over uniform perturbation under the same privacy budget. Our method yields higher accuracy and lower error while substantially reducing empirical leakage in identity and membership inference attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wi-Fi/RF-based human sensing has achieved remarkable progress with deep learning, yet practical deployments increasingly require feature sharing for cloud analytics, collaborative training, or benchmark evaluation. Releasing intermediate representations such as CSI spectrograms can inadvertently expose sensitive information, including user identity, location, and membership, motivating formal privacy guarantees. In this paper, we study differentially private (DP) feature release for wireless sensing and propose an adaptive privacy budget allocation mechanism tailored to the highly non-uniform structure of CSI time-frequency representations. Our pipeline converts CSI to bounded spectrogram features, applies sensitivity control via clipping, estimates task-relevant importance over the time-frequency plane, and allocates a global privacy budget across spectrogram blocks before injecting calibrated Gaussian noise. Experiments on multi-user activity sensing (WiMANS), multi-person 3D pose estimation (Person-in-WiFi 3D), and respiration monitoring (Resp-CSI) show that adaptive allocation consistently improves the privacy-utility frontier over uniform perturbation under the same privacy budget. Our method yields higher accuracy and lower error while substantially reducing empirical leakage in identity and membership inference attacks."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:45:49Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    45,
                    49,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "19pages,4figures",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Ipek Sena Yilmaz"
                    },
                    {
                        "name": "Onur G. Tuncer"
                    },
                    {
                        "name": "Zeynep E. Aksoy"
                    },
                    {
                        "name": "Zeynep Yağmur Baydemir"
                    }
                ],
                "author_detail": {
                    "name": "Zeynep Yağmur Baydemir"
                },
                "author": "Zeynep Yağmur Baydemir"
            },
            {
                "id": "http://arxiv.org/abs/2512.20319v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20319v1",
                "title": "Deep Learning Classification of EEG Responses to Multi-Dimensional Transcranial Electrical Stimulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Classification of EEG Responses to Multi-Dimensional Transcranial Electrical Stimulation"
                },
                "updated": "2025-12-23T12:40:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    40,
                    51,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20319v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A major shortcoming of medical practice is the lack of an objective measure of conscious level. Impairment of consciousness is common, e.g. following brain injury and seizures, which can also interfere with sensory processing and volitional responses. This is also an important pitfall in neurophysiological methods that infer awareness via command following, e.g. using functional MRI or electroencephalography (EEG).\n  Transcranial electrical stimulation (TES) can be employed to non-invasively stimulate the brain, bypassing sensory inputs, and has already showed promising results in providing reliable indicators of brain state. However, current non-invasive solutions have been limited to magnetic stimulation, which is not easily translatable to clinical settings. Our long-term vision is to develop an objective measure of brain state that can be used at the bedside, without requiring patients to understand commands or initiate motor responses.\n  In this study, we demonstrated the feasibility of a framework using Deep Learning algorithms to classify EEG brain responses evoked by a defined multi-dimensional pattern of TES. We collected EEG-TES data from 11 participants and found that delivering transcranial direct current stimulation (tDCS) to posterior cortical areas targeting the angular gyrus elicited an exceptionally reliable brain response. For this paradigm, our best Convolutional Neural Network model reached a 92% classification F1-score on Holdout data from participants never seen during training, significantly surpassing human-level performance at 60-70% accuracy.\n  These findings establish a framework for robust consciousness measurement for clinical use. In this spirit, we documented and open-sourced our datasets and codebase in full, to be used freely by the neuroscience and AI research communities, who may replicate our results with free tools like GitHub, Kaggle, and Colab.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A major shortcoming of medical practice is the lack of an objective measure of conscious level. Impairment of consciousness is common, e.g. following brain injury and seizures, which can also interfere with sensory processing and volitional responses. This is also an important pitfall in neurophysiological methods that infer awareness via command following, e.g. using functional MRI or electroencephalography (EEG).\n  Transcranial electrical stimulation (TES) can be employed to non-invasively stimulate the brain, bypassing sensory inputs, and has already showed promising results in providing reliable indicators of brain state. However, current non-invasive solutions have been limited to magnetic stimulation, which is not easily translatable to clinical settings. Our long-term vision is to develop an objective measure of brain state that can be used at the bedside, without requiring patients to understand commands or initiate motor responses.\n  In this study, we demonstrated the feasibility of a framework using Deep Learning algorithms to classify EEG brain responses evoked by a defined multi-dimensional pattern of TES. We collected EEG-TES data from 11 participants and found that delivering transcranial direct current stimulation (tDCS) to posterior cortical areas targeting the angular gyrus elicited an exceptionally reliable brain response. For this paradigm, our best Convolutional Neural Network model reached a 92% classification F1-score on Holdout data from participants never seen during training, significantly surpassing human-level performance at 60-70% accuracy.\n  These findings establish a framework for robust consciousness measurement for clinical use. In this spirit, we documented and open-sourced our datasets and codebase in full, to be used freely by the neuroscience and AI research communities, who may replicate our results with free tools like GitHub, Kaggle, and Colab."
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:40:51Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    40,
                    51,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "For open-sourced datasets and source code, see: https://github.com/alexispomares/DL-EEG-TES",
                "arxiv_primary_category": {
                    "term": "q-bio.NC"
                },
                "authors": [
                    {
                        "name": "Alexis Pomares Pastor"
                    },
                    {
                        "name": "Ines Ribeiro Violante"
                    },
                    {
                        "name": "Gregory Scott"
                    }
                ],
                "author_detail": {
                    "name": "Gregory Scott"
                },
                "author": "Gregory Scott"
            },
            {
                "id": "http://arxiv.org/abs/2412.15076v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.15076v3",
                "title": "Digital N-of-1 Trials and their Application in Experimental Physiology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital N-of-1 Trials and their Application in Experimental Physiology"
                },
                "updated": "2025-12-23T12:36:08Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    36,
                    8,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.15076v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.15076v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Traditionally, studies in experimental physiology have been conducted in small groups of human participants, animal models or cell lines. Identifying optimal study designs that achieve sufficient power for drawing proper statistical inferences to detect group level effects with small sample sizes has been challenging. Moreover, average effects derived from traditional group-level inference do not necessarily apply to individual participants. Here, we introduce N-of-1 trials as an innovative study design that can be used to draw valid statistical inference about the effects of interventions on individual participants and can be aggregated across multiple study participants to provide population-level inferences more efficiently than standard group randomized trials. N-of-1 trials have been used since the late 1980s, but without large-scale adoption and with few applications in experimental physiology research settings. In this manuscript, we introduce the key components and design features of N-of-1 trials, describe statistical analysis and interpretations of the results, and describe some available digital tools to facilitate their use using examples from experimental physiology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditionally, studies in experimental physiology have been conducted in small groups of human participants, animal models or cell lines. Identifying optimal study designs that achieve sufficient power for drawing proper statistical inferences to detect group level effects with small sample sizes has been challenging. Moreover, average effects derived from traditional group-level inference do not necessarily apply to individual participants. Here, we introduce N-of-1 trials as an innovative study design that can be used to draw valid statistical inference about the effects of interventions on individual participants and can be aggregated across multiple study participants to provide population-level inferences more efficiently than standard group randomized trials. N-of-1 trials have been used since the late 1980s, but without large-scale adoption and with few applications in experimental physiology research settings. In this manuscript, we introduce the key components and design features of N-of-1 trials, describe statistical analysis and interpretations of the results, and describe some available digital tools to facilitate their use using examples from experimental physiology."
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-19T17:26:02Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    26,
                    2,
                    3,
                    354,
                    0
                ],
                "arxiv_comment": "Accepted in Experimental Physiology",
                "arxiv_primary_category": {
                    "term": "stat.AP"
                },
                "authors": [
                    {
                        "name": "Stefan Konigorski"
                    },
                    {
                        "name": "Mathias Ried-Larsen"
                    },
                    {
                        "name": "Christopher H Schmid"
                    }
                ],
                "author_detail": {
                    "name": "Christopher H Schmid"
                },
                "author": "Christopher H Schmid"
            },
            {
                "id": "http://arxiv.org/abs/2512.20312v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20312v1",
                "title": "TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning"
                },
                "updated": "2025-12-23T12:30:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    30,
                    37,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20312v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce \\textbf{TableGPT-R1}, a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce \\textbf{TableGPT-R1}, a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:30:37Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    30,
                    37,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Saisai Yang"
                    },
                    {
                        "name": "Qingyi Huang"
                    },
                    {
                        "name": "Jing Yuan"
                    },
                    {
                        "name": "Liangyu Zha"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Yuhang Yang"
                    },
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Yucheng Wei"
                    },
                    {
                        "name": "Liyao Li"
                    },
                    {
                        "name": "Wentao Ye"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Junlin Zhou"
                    },
                    {
                        "name": "Haobo Wang"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Junbo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Zhao"
                },
                "author": "Junbo Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2509.01564v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.01564v2",
                "title": "Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief"
                },
                "updated": "2025-12-23T12:29:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    29,
                    17,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.01564v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.01564v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language tasks, but often exhibit overconfidence and generate plausible yet incorrect answers. This overconfidence, especially in models undergone Reinforcement Learning from Human Feedback (RLHF), poses significant challenges for reliable uncertainty estimation and safe deployment. In this paper, we propose EAGLE (Expectation of AGgregated internaL bEief), a novel self-evaluation-based calibration method that leverages the internal hidden states of LLMs to derive more accurate confidence scores. Instead of relying on the model's final output, our approach extracts internal beliefs from multiple intermediate layers during self-evaluation. By aggregating these layer-wise beliefs and calculating the expectation over the resulting confidence score distribution, EAGLE produces a refined confidence score that more faithfully reflects the model's internal certainty. Extensive experiments on diverse datasets and LLMs demonstrate that EAGLE significantly improves calibration performance over existing baselines. We also provide an in-depth analysis of EAGLE, including a layer-wise examination of uncertainty patterns, a study of the impact of self-evaluation prompts, and an analysis of the effect of self-evaluation score range.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language tasks, but often exhibit overconfidence and generate plausible yet incorrect answers. This overconfidence, especially in models undergone Reinforcement Learning from Human Feedback (RLHF), poses significant challenges for reliable uncertainty estimation and safe deployment. In this paper, we propose EAGLE (Expectation of AGgregated internaL bEief), a novel self-evaluation-based calibration method that leverages the internal hidden states of LLMs to derive more accurate confidence scores. Instead of relying on the model's final output, our approach extracts internal beliefs from multiple intermediate layers during self-evaluation. By aggregating these layer-wise beliefs and calculating the expectation over the resulting confidence score distribution, EAGLE produces a refined confidence score that more faithfully reflects the model's internal certainty. Extensive experiments on diverse datasets and LLMs demonstrate that EAGLE significantly improves calibration performance over existing baselines. We also provide an in-depth analysis of EAGLE, including a layer-wise examination of uncertainty patterns, a study of the impact of self-evaluation prompts, and an analysis of the effect of self-evaluation score range."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-01T15:50:10Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    50,
                    10,
                    0,
                    244,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zeguan Xiao"
                    },
                    {
                        "name": "Diyang Dou"
                    },
                    {
                        "name": "Boya Xiong"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Guanhua Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guanhua Chen"
                },
                "author": "Guanhua Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.20299v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20299v1",
                "title": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System"
                },
                "updated": "2025-12-23T12:08:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    8,
                    0,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20299v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:08:00Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    8,
                    0,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Zhongyu Xia"
                    },
                    {
                        "name": "Wenhao Chen"
                    },
                    {
                        "name": "Yongtao Wang"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Hsuan Yang"
                },
                "author": "Ming-Hsuan Yang"
            },
            {
                "id": "http://arxiv.org/abs/2512.20298v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20298v1",
                "title": "Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives"
                },
                "updated": "2025-12-23T12:05:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    5,
                    1,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20298v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term \"narcissism.\" Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term \"narcissism.\" Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:05:01Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    5,
                    1,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Karolina Drożdż"
                    },
                    {
                        "name": "Kacper Dudzic"
                    },
                    {
                        "name": "Anna Sterna"
                    },
                    {
                        "name": "Marcin Moskalewicz"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Moskalewicz"
                },
                "author": "Marcin Moskalewicz"
            },
            {
                "id": "http://arxiv.org/abs/2512.20293v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20293v1",
                "title": "AprielGuard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AprielGuard"
                },
                "updated": "2025-12-23T12:01:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    1,
                    32,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20293v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:01:32Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    1,
                    32,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jaykumar Kasundra"
                    },
                    {
                        "name": "Anjaneya Praharaj"
                    },
                    {
                        "name": "Sourabh Surana"
                    },
                    {
                        "name": "Lakshmi Sirisha Chodisetty"
                    },
                    {
                        "name": "Sourav Sharma"
                    },
                    {
                        "name": "Abhigya Verma"
                    },
                    {
                        "name": "Abhishek Bhardwaj"
                    },
                    {
                        "name": "Debasish Kanhar"
                    },
                    {
                        "name": "Aakash Bhagat"
                    },
                    {
                        "name": "Khalil Slimi"
                    },
                    {
                        "name": "Seganrasan Subramanian"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Ranga Prasad Chenna"
                    },
                    {
                        "name": "Srinivas Sunkara"
                    }
                ],
                "author_detail": {
                    "name": "Srinivas Sunkara"
                },
                "author": "Srinivas Sunkara"
            },
            {
                "id": "http://arxiv.org/abs/2512.20291v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20291v1",
                "title": "Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts with Gradient Conflict-Driven Subspace Topology Pruning for Emergent Modularity"
                },
                "updated": "2025-12-23T12:00:10Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    0,
                    10,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20291v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Mixture-of-Experts (MoE) architectures achieve parameter efficiency through conditional computation, yet contemporary designs suffer from two fundamental limitations: structural parameter isolation that causes catastrophic forgetting, and instruction-overfitting that degrades performance in instruction-free scenarios. We propose CDSP-MoE (Conflict-Driven Subspace Pruning MoE), a framework that addresses these issues through a paradigm shift from isolated expert containers to dynamic expert instantiation within a shared physical subspace. Grounded in the Universal Weight Subspace Hypothesis, CDSP-MoE maintains a super-complete parameter backbone where logical experts are carved out via learnable topology masks. Unlike prior work that uses gradient conflict for token reassignment or optimization surgery, we leverage it as a structural supervisory signal: a Lagged Gradient Game penalizes interfering connections in the shared manifold, enabling the topology to spontaneously prune conflicting pathways and evolve interpretable modular structures. Experimental results demonstrate that CDSP-MoE achieves robust content-driven routing without human-defined task labels, maintaining semantic specialization even under strict blind inference protocols where explicit instructions are absent. Code is available at: https://github.com/konodiodaaaaa1/Conflict-Driven-Subspace-Pruning-Mixture-of-Experts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) architectures achieve parameter efficiency through conditional computation, yet contemporary designs suffer from two fundamental limitations: structural parameter isolation that causes catastrophic forgetting, and instruction-overfitting that degrades performance in instruction-free scenarios. We propose CDSP-MoE (Conflict-Driven Subspace Pruning MoE), a framework that addresses these issues through a paradigm shift from isolated expert containers to dynamic expert instantiation within a shared physical subspace. Grounded in the Universal Weight Subspace Hypothesis, CDSP-MoE maintains a super-complete parameter backbone where logical experts are carved out via learnable topology masks. Unlike prior work that uses gradient conflict for token reassignment or optimization surgery, we leverage it as a structural supervisory signal: a Lagged Gradient Game penalizes interfering connections in the shared manifold, enabling the topology to spontaneously prune conflicting pathways and evolve interpretable modular structures. Experimental results demonstrate that CDSP-MoE achieves robust content-driven routing without human-defined task labels, maintaining semantic specialization even under strict blind inference protocols where explicit instructions are absent. Code is available at: https://github.com/konodiodaaaaa1/Conflict-Driven-Subspace-Pruning-Mixture-of-Experts"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:00:10Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    0,
                    10,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuxing Gan"
                    },
                    {
                        "name": "Ziyu Lei"
                    }
                ],
                "author_detail": {
                    "name": "Ziyu Lei"
                },
                "author": "Ziyu Lei"
            },
            {
                "id": "http://arxiv.org/abs/2308.03970v6",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2308.03970v6",
                "title": "Optimal Clustering with Dependent Costs in Bayesian Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Clustering with Dependent Costs in Bayesian Networks"
                },
                "updated": "2025-12-23T11:51:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    51,
                    31,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2308.03970v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2308.03970v6",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Background: Clustering of nodes in Bayesian Networks (BNs) and related graphical models such as Dynamic BNs (DBNs) has been demonstrated to enhance computational efficiency and improve model learning. It typically involves partitioning the underlying Directed Acyclic Graph (DAG) into cliques or optimising for some cost or criteria.\n  Objectives: We focus on a critical but understudied aspect of optimal clustering involving cost dependency. This is where inference outcomes and hence clustering costs depend on both nodes within a cluster and the mapping of clusters that are connected by at least one arc. Methods: We propose a novel algorithm called Dependent Cluster MAPping (DCMAP) which can, given an arbitrary, positive cost function, iteratively and rapidly find near-optimal, then optimal cluster mappings.\n  Results: DCMAP is shown analytically to be optimal in terms of finding all of the least cost cluster mapping solutions and with no more iterations than an equally informed algorithm. Demonstrated on a complex systems seagrass DBN with $9.91\\times10^9$ and $1.51\\times10^{21}$ possible cluster mappings for 25 and 50 node configurations, it took 856 and 1569 iterations on average to find the first optimal solution, respectively.\n  Conclusions: The effectiveness of DCMAP enables future research in BN learning using optimisation, such as through enhancing computational efficiency or minimising entropy for learning. This is critically important as computation of marginal distributions or updating model parameters is NP-hard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Clustering of nodes in Bayesian Networks (BNs) and related graphical models such as Dynamic BNs (DBNs) has been demonstrated to enhance computational efficiency and improve model learning. It typically involves partitioning the underlying Directed Acyclic Graph (DAG) into cliques or optimising for some cost or criteria.\n  Objectives: We focus on a critical but understudied aspect of optimal clustering involving cost dependency. This is where inference outcomes and hence clustering costs depend on both nodes within a cluster and the mapping of clusters that are connected by at least one arc. Methods: We propose a novel algorithm called Dependent Cluster MAPping (DCMAP) which can, given an arbitrary, positive cost function, iteratively and rapidly find near-optimal, then optimal cluster mappings.\n  Results: DCMAP is shown analytically to be optimal in terms of finding all of the least cost cluster mapping solutions and with no more iterations than an equally informed algorithm. Demonstrated on a complex systems seagrass DBN with $9.91\\times10^9$ and $1.51\\times10^{21}$ possible cluster mappings for 25 and 50 node configurations, it took 856 and 1569 iterations on average to find the first optimal solution, respectively.\n  Conclusions: The effectiveness of DCMAP enables future research in BN learning using optimisation, such as through enhancing computational efficiency or minimising entropy for learning. This is critically important as computation of marginal distributions or updating model parameters is NP-hard."
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2023-08-08T01:01:37Z",
                "published_parsed": [
                    2023,
                    8,
                    8,
                    1,
                    1,
                    37,
                    1,
                    220,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS"
                },
                "authors": [
                    {
                        "name": "Paul Pao-Yen Wu"
                    },
                    {
                        "name": "Fabrizio Ruggeri"
                    },
                    {
                        "name": "Kerrie Mengersen"
                    }
                ],
                "author_detail": {
                    "name": "Kerrie Mengersen"
                },
                "author": "Kerrie Mengersen"
            },
            {
                "id": "http://arxiv.org/abs/2512.20276v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20276v1",
                "title": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge"
                },
                "updated": "2025-12-23T11:29:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    29,
                    3,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20276v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T11:29:03Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    29,
                    3,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yuntao Dai"
                    },
                    {
                        "name": "Hang Gu"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Qianyu Cheng"
                    },
                    {
                        "name": "Yifei Zheng"
                    },
                    {
                        "name": "Zhiyong Qiu"
                    },
                    {
                        "name": "Lei Gong"
                    },
                    {
                        "name": "Wenqi Lou"
                    },
                    {
                        "name": "Xuehai Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xuehai Zhou"
                },
                "author": "Xuehai Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2410.11819v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2410.11819v4",
                "title": "Consistent time reversal and reliable and accurate inference in the presence of memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistent time reversal and reliable and accurate inference in the presence of memory"
                },
                "updated": "2025-12-23T11:27:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    27,
                    59,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2410.11819v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2410.11819v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Thermodynamic inference from coarse observations remains a key challenge. Memory, in particular correlations between consecutively observed mesostates, blur signatures of irreversibility and must be accounted for in defining physical time-reversal, which remains an open problem. We derive an experimentally accessible k-th order estimator for the entropy production rate. Using novel measure-theoretic techniques we prove necessary and sufficient conditions for guaranteed lower bounds on the dissipation even in the strongly non-Markovian setting. The proof reveals that estimators saturated in the order unravel the duration of memory which needs to be considered in defining physically consistent time-reversal. We show that Markovian estimators in absence of a time-scale separation lead to artifacts, which convey no physical meaning. Similarly, estimators not saturated in the order may overestimate the dissipation. The necessity of correctly accounting for memory in thermodynamic inference from strongly non-Markovian observations underscores the still underappreciated challenges and intricacies in defining and understanding irreversibility in presence of memory. Our results will hopefully stimulate experiments systematically considering thermodynamic inference on multiple scales consistently accounting for memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thermodynamic inference from coarse observations remains a key challenge. Memory, in particular correlations between consecutively observed mesostates, blur signatures of irreversibility and must be accounted for in defining physical time-reversal, which remains an open problem. We derive an experimentally accessible k-th order estimator for the entropy production rate. Using novel measure-theoretic techniques we prove necessary and sufficient conditions for guaranteed lower bounds on the dissipation even in the strongly non-Markovian setting. The proof reveals that estimators saturated in the order unravel the duration of memory which needs to be considered in defining physically consistent time-reversal. We show that Markovian estimators in absence of a time-scale separation lead to artifacts, which convey no physical meaning. Similarly, estimators not saturated in the order may overestimate the dissipation. The necessity of correctly accounting for memory in thermodynamic inference from strongly non-Markovian observations underscores the still underappreciated challenges and intricacies in defining and understanding irreversibility in presence of memory. Our results will hopefully stimulate experiments systematically considering thermodynamic inference on multiple scales consistently accounting for memory."
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-10-15T17:46:53Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    46,
                    53,
                    1,
                    289,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech"
                },
                "authors": [
                    {
                        "name": "Tassilo Schwarz"
                    },
                    {
                        "name": "Anatoly B. Kolomeisky"
                    },
                    {
                        "name": "Aljaž Godec"
                    }
                ],
                "author_detail": {
                    "name": "Aljaž Godec"
                },
                "author": "Aljaž Godec"
            },
            {
                "id": "http://arxiv.org/abs/2512.20275v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20275v1",
                "title": "Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks"
                },
                "updated": "2025-12-23T11:27:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    27,
                    17,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20275v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As networks evolve toward 5G Standalone and 6G, operators face orchestration challenges that exceed the limits of static automation and Deep Reinforcement Learning. Although Large Language Model (LLM) agents offer a path toward intent-based networking, they introduce stochastic risks, including topology hallucinations and policy non-compliance. To mitigate this, we propose Graph-Symbolic Policy Enforcement and Control (G-SPEC), a neuro-symbolic framework that constrains probabilistic planning with deterministic verification. The architecture relies on a Governance Triad - a telecom-adapted agent (TSLAM-4B), a Network Knowledge Graph (NKG), and SHACL constraints. We evaluated G-SPEC on a simulated 450-node 5G Core, achieving zero safety violations and a 94.1% remediation success rate, significantly outperforming the 82.4% baseline. Ablation analysis indicates that NKG validation drives the majority of safety gains (68%), followed by SHACL policies (24%). Scalability tests on topologies ranging from 10K to 100K nodes demonstrate that validation latency scales as $O(k^{1.2})$ where $k$ is subgraph size. With a processing overhead of 142ms, G-SPEC is viable for SMO-layer operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As networks evolve toward 5G Standalone and 6G, operators face orchestration challenges that exceed the limits of static automation and Deep Reinforcement Learning. Although Large Language Model (LLM) agents offer a path toward intent-based networking, they introduce stochastic risks, including topology hallucinations and policy non-compliance. To mitigate this, we propose Graph-Symbolic Policy Enforcement and Control (G-SPEC), a neuro-symbolic framework that constrains probabilistic planning with deterministic verification. The architecture relies on a Governance Triad - a telecom-adapted agent (TSLAM-4B), a Network Knowledge Graph (NKG), and SHACL constraints. We evaluated G-SPEC on a simulated 450-node 5G Core, achieving zero safety violations and a 94.1% remediation success rate, significantly outperforming the 82.4% baseline. Ablation analysis indicates that NKG validation drives the majority of safety gains (68%), followed by SHACL policies (24%). Scalability tests on topologies ranging from 10K to 100K nodes demonstrate that validation latency scales as $O(k^{1.2})$ where $k$ is subgraph size. With a processing overhead of 142ms, G-SPEC is viable for SMO-layer operations."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T11:27:17Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    27,
                    17,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "15 pages, 3 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Divya Vijay"
                    },
                    {
                        "name": "Vignesh Ethiraj"
                    }
                ],
                "author_detail": {
                    "name": "Vignesh Ethiraj"
                },
                "author": "Vignesh Ethiraj"
            },
            {
                "id": "http://arxiv.org/abs/2512.20264v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20264v1",
                "title": "The AI Scaling Wall of Diminishing Returns: Of LLMs, Electric Dogs, and General Relativity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI Scaling Wall of Diminishing Returns: Of LLMs, Electric Dogs, and General Relativity"
                },
                "updated": "2025-12-23T11:18:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    18,
                    3,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20264v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLMs are hitting the scaling wall - compute grows 10-100x while accuracy barely moves. This note quantifies the slowdown and argues that the next leap in AI will come not from bigger models, but from smarter, more efficient ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are hitting the scaling wall - compute grows 10-100x while accuracy barely moves. This note quantifies the slowdown and argues that the next leap in AI will come not from bigger models, but from smarter, more efficient ones."
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T11:18:03Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    18,
                    3,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM"
                },
                "authors": [
                    {
                        "name": "Hemant Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Hemant Shukla"
                },
                "author": "Hemant Shukla"
            },
            {
                "id": "http://arxiv.org/abs/2512.19126v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19126v2",
                "title": "AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards"
                },
                "updated": "2025-12-23T11:15:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    15,
                    52,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19126v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T08:07:00Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    8,
                    7,
                    0,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zihan Lin"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Hexiong Yang"
                    },
                    {
                        "name": "Jiajun Chai"
                    },
                    {
                        "name": "Jie Cao"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He"
            },
            {
                "id": "http://arxiv.org/abs/2512.20250v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20250v1",
                "title": "Inference in Latent Force Models Using Optimal State Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference in Latent Force Models Using Optimal State Estimation"
                },
                "updated": "2025-12-23T11:04:56Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    4,
                    56,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20250v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Latent force models, a class of hybrid modeling approaches, integrate physical knowledge of system dynamics with a latent force - an unknown, unmeasurable input modeled as a Gaussian process. In this work, we introduce two optimal state estimation frameworks to reconstruct the latent forces and to estimate the states. In contrast to state-of-the-art approaches, the designed estimators enable the consideration of system-inherent constraints. Finally, the performance of the novel frameworks is investigated in several numerical examples. In particular, we demonstrate the performance of the new framework in a real-world biomedical example - the hypothalamic-pituitary-thyroid axis - using hormone measurements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent force models, a class of hybrid modeling approaches, integrate physical knowledge of system dynamics with a latent force - an unknown, unmeasurable input modeled as a Gaussian process. In this work, we introduce two optimal state estimation frameworks to reconstruct the latent forces and to estimate the states. In contrast to state-of-the-art approaches, the designed estimators enable the consideration of system-inherent constraints. Finally, the performance of the novel frameworks is investigated in several numerical examples. In particular, we demonstrate the performance of the new framework in a real-world biomedical example - the hypothalamic-pituitary-thyroid axis - using hormone measurements."
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T11:04:56Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    4,
                    56,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "8 pages",
                "arxiv_primary_category": {
                    "term": "eess.SY"
                },
                "authors": [
                    {
                        "name": "Tobias M. Wolff"
                    },
                    {
                        "name": "Victor G. Lopez"
                    },
                    {
                        "name": "Matthias A. Müller"
                    },
                    {
                        "name": "Thomas Beckers"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Beckers"
                },
                "author": "Thomas Beckers"
            },
            {
                "id": "http://arxiv.org/abs/2512.20249v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20249v1",
                "title": "Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Multimodal Brain Decoding via Cross-Subject Soft-ROI Fusion"
                },
                "updated": "2025-12-23T11:04:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    4,
                    34,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20249v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20249v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multimodal brain decoding aims to reconstruct semantic information that is consistent with visual stimuli from brain activity signals such as fMRI, and then generate readable natural language descriptions. However, multimodal brain decoding still faces key challenges in cross-subject generalization and interpretability. We propose a BrainROI model and achieve leading-level results in brain-captioning evaluation on the NSD dataset. Under the cross-subject setting, compared with recent state-of-the-art methods and representative baselines, metrics such as BLEU-4 and CIDEr show clear improvements. Firstly, to address the heterogeneity of functional brain topology across subjects, we design a new fMRI encoder. We use multi-atlas soft functional parcellations (soft-ROI) as a shared space. We extend the discrete ROI Concatenation strategy in MINDLLM to a voxel-wise gated fusion mechanism (Voxel-gate). We also ensure consistent ROI mapping through global label alignment, which enhances cross-subject transferability. Secondly, to overcome the limitations of manual and black-box prompting methods in stability and transparency, we introduce an interpretable prompt optimization process. In a small-sample closed loop, we use a locally deployed Qwen model to iteratively generate and select human-readable prompts. This process improves the stability of prompt design and preserves an auditable optimization trajectory. Finally, we impose parameterized decoding constraints during inference to further improve the stability and quality of the generated descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal brain decoding aims to reconstruct semantic information that is consistent with visual stimuli from brain activity signals such as fMRI, and then generate readable natural language descriptions. However, multimodal brain decoding still faces key challenges in cross-subject generalization and interpretability. We propose a BrainROI model and achieve leading-level results in brain-captioning evaluation on the NSD dataset. Under the cross-subject setting, compared with recent state-of-the-art methods and representative baselines, metrics such as BLEU-4 and CIDEr show clear improvements. Firstly, to address the heterogeneity of functional brain topology across subjects, we design a new fMRI encoder. We use multi-atlas soft functional parcellations (soft-ROI) as a shared space. We extend the discrete ROI Concatenation strategy in MINDLLM to a voxel-wise gated fusion mechanism (Voxel-gate). We also ensure consistent ROI mapping through global label alignment, which enhances cross-subject transferability. Secondly, to overcome the limitations of manual and black-box prompting methods in stability and transparency, we introduce an interpretable prompt optimization process. In a small-sample closed loop, we use a locally deployed Qwen model to iteratively generate and select human-readable prompts. This process improves the stability of prompt design and preserves an auditable optimization trajectory. Finally, we impose parameterized decoding constraints during inference to further improve the stability and quality of the generated descriptions."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T11:04:34Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    4,
                    34,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "15 pages, 2 figures, 4 tables. Submitted to ICPR 2026",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xuanyu Hu"
                    }
                ],
                "author_detail": {
                    "name": "Xuanyu Hu"
                },
                "author": "Xuanyu Hu"
            },
            {
                "id": "http://arxiv.org/abs/2512.20237v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20237v1",
                "title": "MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents"
                },
                "updated": "2025-12-23T10:49:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    49,
                    42,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20237v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T10:49:42Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    49,
                    42,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "16 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Xingbo Du"
                    },
                    {
                        "name": "Loka Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Le Song"
                    }
                ],
                "author_detail": {
                    "name": "Le Song"
                },
                "author": "Le Song"
            },
            {
                "id": "http://arxiv.org/abs/2512.18315v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.18315v2",
                "title": "On Efficient Adjustment for Micro Causal Effects in Summary Causal Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Efficient Adjustment for Micro Causal Effects in Summary Causal Graphs"
                },
                "updated": "2025-12-23T10:30:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    30,
                    5,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.18315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.18315v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Observational studies in fields such as epidemiology often rely on covariate adjustment to estimate causal effects. Classical graphical criteria, like the back-door criterion and the generalized adjustment criterion, are powerful tools for identifying valid adjustment sets in directed acyclic graphs (DAGs). However, these criteria are not directly applicable to summary causal graphs (SCGs), which are abstractions of DAGs commonly used in dynamic systems. In SCGs, each node typically represents an entire time series and may involve cycles, making classical criteria inapplicable for identifying causal effects. Recent work established complete conditions for determining whether the micro causal effect of a treatment or an exposure $X_{t-γ}$ on an outcome $Y_t$ is identifiable via covariate adjustment in SCGs, under the assumption of no hidden confounding. However, these identifiability conditions have two main limitations. First, they are complex, relying on cumbersome definitions and requiring the enumeration of multiple paths in the SCG, which can be computationally expensive. Second, when these conditions are satisfied, they only provide two valid adjustment sets, limiting flexibility in practical applications. In this paper, we propose an equivalent but simpler formulation of those identifiability conditions and introduce a new criterion that identifies a broader class of valid adjustment sets in SCGs. Additionally, we characterize the quasi-optimal adjustment set among these, i.e., the one that minimizes the asymptotic variance of the causal effect estimator. Our contributions offer both theoretical advancement and practical tools for more flexible and efficient causal inference in abstracted causal graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observational studies in fields such as epidemiology often rely on covariate adjustment to estimate causal effects. Classical graphical criteria, like the back-door criterion and the generalized adjustment criterion, are powerful tools for identifying valid adjustment sets in directed acyclic graphs (DAGs). However, these criteria are not directly applicable to summary causal graphs (SCGs), which are abstractions of DAGs commonly used in dynamic systems. In SCGs, each node typically represents an entire time series and may involve cycles, making classical criteria inapplicable for identifying causal effects. Recent work established complete conditions for determining whether the micro causal effect of a treatment or an exposure $X_{t-γ}$ on an outcome $Y_t$ is identifiable via covariate adjustment in SCGs, under the assumption of no hidden confounding. However, these identifiability conditions have two main limitations. First, they are complex, relying on cumbersome definitions and requiring the enumeration of multiple paths in the SCG, which can be computationally expensive. Second, when these conditions are satisfied, they only provide two valid adjustment sets, limiting flexibility in practical applications. In this paper, we propose an equivalent but simpler formulation of those identifiability conditions and introduce a new criterion that identifies a broader class of valid adjustment sets in SCGs. Additionally, we characterize the quasi-optimal adjustment set among these, i.e., the one that minimizes the asymptotic variance of the causal effect estimator. Our contributions offer both theoretical advancement and practical tools for more flexible and efficient causal inference in abstracted causal graphs."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-20T11:02:44Z",
                "published_parsed": [
                    2025,
                    12,
                    20,
                    11,
                    2,
                    44,
                    5,
                    354,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Isabela Belciug"
                    },
                    {
                        "name": "Simon Ferreira"
                    },
                    {
                        "name": "Charles K. Assaad"
                    }
                ],
                "author_detail": {
                    "name": "Charles K. Assaad"
                },
                "author": "Charles K. Assaad"
            },
            {
                "id": "http://arxiv.org/abs/2512.20219v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20219v1",
                "title": "Estimation and Inference for Causal Explainability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimation and Inference for Causal Explainability"
                },
                "updated": "2025-12-23T10:18:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    18,
                    1,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20219v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Understanding how much each variable contributes to an outcome is a central question across disciplines. A causal view of explainability is favorable for its ability in uncovering underlying mechanisms and generalizing to new contexts. Based on a family of causal explainability quantities, we develop methods for their estimation and inference. In particular, we construct a one-step correction estimator using semi-parametric efficiency theory, which explicitly leverages the independence structure of variables to reduce the asymptotic variance. For a null hypothesis on the boundary, i.e., zero explainability, we show its equivalence to Fisher's sharp null, which motivates a randomization-based inference procedure. Finally, we illustrate the empirical efficacy of our approach through simulations as well as an immigration experiment dataset, where we investigate how features and their interactions shape public opinion toward admitting immigrants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how much each variable contributes to an outcome is a central question across disciplines. A causal view of explainability is favorable for its ability in uncovering underlying mechanisms and generalizing to new contexts. Based on a family of causal explainability quantities, we develop methods for their estimation and inference. In particular, we construct a one-step correction estimator using semi-parametric efficiency theory, which explicitly leverages the independence structure of variables to reduce the asymptotic variance. For a null hypothesis on the boundary, i.e., zero explainability, we show its equivalence to Fisher's sharp null, which motivates a randomization-based inference procedure. Finally, we illustrate the empirical efficacy of our approach through simulations as well as an immigration experiment dataset, where we investigate how features and their interactions shape public opinion toward admitting immigrants."
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T10:18:01Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    18,
                    1,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "35 pages, 5 figures, 7 tables",
                "arxiv_primary_category": {
                    "term": "stat.ME"
                },
                "authors": [
                    {
                        "name": "Weihan Zhang"
                    },
                    {
                        "name": "Zijun Gao"
                    }
                ],
                "author_detail": {
                    "name": "Zijun Gao"
                },
                "author": "Zijun Gao"
            },
            {
                "id": "http://arxiv.org/abs/2512.20211v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20211v1",
                "title": "Aliasing-Free Neural Audio Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aliasing-Free Neural Audio Synthesis"
                },
                "updated": "2025-12-23T10:04:48Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    4,
                    48,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20211v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Neural vocoders and codecs reconstruct waveforms from acoustic representations, which directly impact the audio quality. Among existing methods, upsampling-based time-domain models are superior in both inference speed and synthesis quality, achieving state-of-the-art performance. Still, despite their success in producing perceptually natural sound, their synthesis fidelity remains limited due to the aliasing artifacts brought by the inadequately designed model architectures. In particular, the unconstrained nonlinear activation generates an infinite number of harmonics that exceed the Nyquist frequency, resulting in ``folded-back'' aliasing artifacts. The widely used upsampling layer, ConvTranspose, copies the mirrored low-frequency parts to fill the empty high-frequency region, resulting in ``mirrored'' aliasing artifacts. Meanwhile, the combination of its inherent periodicity and the mirrored DC bias also brings ``tonal artifact,'' resulting in constant-frequency ringing. This paper aims to solve these issues from a signal processing perspective. Specifically, we apply oversampling and anti-derivative anti-aliasing to the activation function to obtain its anti-aliased form, and replace the problematic ConvTranspose layer with resampling to avoid the ``tonal artifact'' and eliminate aliased components. Based on our proposed anti-aliased modules, we introduce Pupu-Vocoder and Pupu-Codec, and release high-quality pre-trained checkpoints to facilitate audio generation research. We build a test signal benchmark to illustrate the effectiveness of the anti-aliased modules, and conduct experiments on speech, singing voice, music, and audio to validate our proposed models. Experimental results confirm that our lightweight Pupu-Vocoder and Pupu-Codec models can easily outperform existing systems on singing voice, music, and audio, while achieving comparable performance on speech.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural vocoders and codecs reconstruct waveforms from acoustic representations, which directly impact the audio quality. Among existing methods, upsampling-based time-domain models are superior in both inference speed and synthesis quality, achieving state-of-the-art performance. Still, despite their success in producing perceptually natural sound, their synthesis fidelity remains limited due to the aliasing artifacts brought by the inadequately designed model architectures. In particular, the unconstrained nonlinear activation generates an infinite number of harmonics that exceed the Nyquist frequency, resulting in ``folded-back'' aliasing artifacts. The widely used upsampling layer, ConvTranspose, copies the mirrored low-frequency parts to fill the empty high-frequency region, resulting in ``mirrored'' aliasing artifacts. Meanwhile, the combination of its inherent periodicity and the mirrored DC bias also brings ``tonal artifact,'' resulting in constant-frequency ringing. This paper aims to solve these issues from a signal processing perspective. Specifically, we apply oversampling and anti-derivative anti-aliasing to the activation function to obtain its anti-aliased form, and replace the problematic ConvTranspose layer with resampling to avoid the ``tonal artifact'' and eliminate aliased components. Based on our proposed anti-aliased modules, we introduce Pupu-Vocoder and Pupu-Codec, and release high-quality pre-trained checkpoints to facilitate audio generation research. We build a test signal benchmark to illustrate the effectiveness of the anti-aliased modules, and conduct experiments on speech, singing voice, music, and audio to validate our proposed models. Experimental results confirm that our lightweight Pupu-Vocoder and Pupu-Codec models can easily outperform existing systems on singing voice, music, and audio, while achieving comparable performance on speech."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T10:04:48Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    4,
                    48,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "Submitted to TASLP",
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Yicheng Gu"
                    },
                    {
                        "name": "Junan Zhang"
                    },
                    {
                        "name": "Chaoren Wang"
                    },
                    {
                        "name": "Jerry Li"
                    },
                    {
                        "name": "Zhizheng Wu"
                    },
                    {
                        "name": "Lauri Juvela"
                    }
                ],
                "author_detail": {
                    "name": "Lauri Juvela"
                },
                "author": "Lauri Juvela"
            },
            {
                "id": "http://arxiv.org/abs/2512.20210v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20210v1",
                "title": "Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs"
                },
                "updated": "2025-12-23T10:03:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    3,
                    47,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20210v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T10:03:47Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    3,
                    47,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Yinan Ni"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Yuqi Tang"
                    },
                    {
                        "name": "Zhimin Qiu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Tingzhou Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Tingzhou Yuan"
                },
                "author": "Tingzhou Yuan"
            },
            {
                "id": "http://arxiv.org/abs/2508.16580v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.16580v2",
                "title": "Adaptive Command: Real-Time Policy Adjustment via Language Models in StarCraft II",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Command: Real-Time Policy Adjustment via Language Models in StarCraft II"
                },
                "updated": "2025-12-23T09:58:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    58,
                    30,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.16580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.16580v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Adaptive Command, a novel framework integrating large language models (LLMs) with behavior trees for real-time strategic decision-making in StarCraft II. Our system focuses on enhancing human-AI collaboration in complex, dynamic environments through natural language interactions. The framework comprises: (1) an LLM-based strategic advisor, (2) a behavior tree for action execution, and (3) a natural language interface with speech capabilities. User studies demonstrate significant improvements in player decision-making and strategic adaptability, particularly benefiting novice players and those with disabilities. This work contributes to the field of real-time human-AI collaborative decision-making, offering insights applicable beyond RTS games to various complex decision-making scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Adaptive Command, a novel framework integrating large language models (LLMs) with behavior trees for real-time strategic decision-making in StarCraft II. Our system focuses on enhancing human-AI collaboration in complex, dynamic environments through natural language interactions. The framework comprises: (1) an LLM-based strategic advisor, (2) a behavior tree for action execution, and (3) a natural language interface with speech capabilities. User studies demonstrate significant improvements in player decision-making and strategic adaptability, particularly benefiting novice players and those with disabilities. This work contributes to the field of real-time human-AI collaborative decision-making, offering insights applicable beyond RTS games to various complex decision-making scenarios."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-05T03:26:58Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    3,
                    26,
                    58,
                    1,
                    217,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Weiyu Ma"
                    },
                    {
                        "name": "Dongyu Xu"
                    },
                    {
                        "name": "Shu Lin"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.20203v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20203v1",
                "title": "Well Begun is Half Done: Location-Aware and Trace-Guided Iterative Automated Vulnerability Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Well Begun is Half Done: Location-Aware and Trace-Guided Iterative Automated Vulnerability Repair"
                },
                "updated": "2025-12-23T09:54:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    54,
                    22,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20203v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3744916.3773218",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The advances of large language models (LLMs) have paved the way for automated software vulnerability repair approaches, which iteratively refine the patch until it becomes plausible. Nevertheless, existing LLM-based vulnerability repair approaches face notable limitations: 1) they ignore the concern of locations that need to be patched and focus solely on the repair content. 2) they lack quality assessment for generated candidate patches in the iterative process.\n  To tackle the two limitations, we propose \\sysname, an LLM-based approach that provides information about where should be patched first. Furthermore, \\sysname improves the iterative repair strategy by assessing the quality of test-failing patches and selecting the best patch for the next iteration. We introduce two dimensions to assess the quality of patches: whether they introduce new vulnerabilities and the taint statement coverage. We evaluated \\sysname on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability. The experimental results demonstrate that \\sysname exhibits substantial improvements compared with the Neural Machine Translation-based, Program Analysis-based, and LLM-based state-of-the-art vulnerability repair approaches. Specifically, \\sysname is able to generate 27 plausible patches, which is comparable to or even 8 to 22 more plausible patches than the baselines. In terms of correct patch generation, \\sysname repairs 8 to 13 additional vulnerabilities compared with existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advances of large language models (LLMs) have paved the way for automated software vulnerability repair approaches, which iteratively refine the patch until it becomes plausible. Nevertheless, existing LLM-based vulnerability repair approaches face notable limitations: 1) they ignore the concern of locations that need to be patched and focus solely on the repair content. 2) they lack quality assessment for generated candidate patches in the iterative process.\n  To tackle the two limitations, we propose \\sysname, an LLM-based approach that provides information about where should be patched first. Furthermore, \\sysname improves the iterative repair strategy by assessing the quality of test-failing patches and selecting the best patch for the next iteration. We introduce two dimensions to assess the quality of patches: whether they introduce new vulnerabilities and the taint statement coverage. We evaluated \\sysname on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability. The experimental results demonstrate that \\sysname exhibits substantial improvements compared with the Neural Machine Translation-based, Program Analysis-based, and LLM-based state-of-the-art vulnerability repair approaches. Specifically, \\sysname is able to generate 27 plausible patches, which is comparable to or even 8 to 22 more plausible patches than the baselines. In terms of correct patch generation, \\sysname repairs 8 to 13 additional vulnerabilities compared with existing approaches."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:54:22Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    54,
                    22,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "Accepted by ICSE 2026",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Zhenlei Ye"
                    },
                    {
                        "name": "Xiaobing Sun"
                    },
                    {
                        "name": "Sicong Cao"
                    },
                    {
                        "name": "Lili Bo"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "arxiv_doi": "10.1145/3744916.3773218"
            },
            {
                "id": "http://arxiv.org/abs/2512.20198v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20198v1",
                "title": "Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling"
                },
                "updated": "2025-12-23T09:43:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    43,
                    32,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20198v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) rely on self-attention for contextual understanding, demanding high-throughput inference and large-scale token parallelism (LTPP). Existing dynamic sparsity accelerators falter under LTPP scenarios due to stage-isolated optimizations. Revisiting the end-to-end sparsity acceleration flow, we identify an overlooked opportunity: cross-stage coordination can substantially reduce redundant computation and memory access. We propose STAR, a cross-stage compute- and memory-efficient algorithm-hardware co-design tailored for Transformer inference under LTPP. STAR introduces a leading-zero-based sparsity prediction using log-domain add-only operations to minimize prediction overhead. It further employs distributed sorting and a sorted updating FlashAttention mechanism, guided by a coordinated tiling strategy that enables fine-grained stage interaction for improved memory efficiency and latency. These optimizations are supported by a dedicated STAR accelerator architecture, achieving up to 9.2$\\times$ speedup and 71.2$\\times$ energy efficiency over A100, and surpassing SOTA accelerators by up to 16.1$\\times$ energy and 27.1$\\times$ area efficiency gains. Further, we deploy STAR onto a multi-core spatial architecture, optimizing dataflow and execution orchestration for ultra-long sequence processing. Architectural evaluation shows that, compared to the baseline design, Spatial-STAR achieves a 20.1$\\times$ throughput improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on self-attention for contextual understanding, demanding high-throughput inference and large-scale token parallelism (LTPP). Existing dynamic sparsity accelerators falter under LTPP scenarios due to stage-isolated optimizations. Revisiting the end-to-end sparsity acceleration flow, we identify an overlooked opportunity: cross-stage coordination can substantially reduce redundant computation and memory access. We propose STAR, a cross-stage compute- and memory-efficient algorithm-hardware co-design tailored for Transformer inference under LTPP. STAR introduces a leading-zero-based sparsity prediction using log-domain add-only operations to minimize prediction overhead. It further employs distributed sorting and a sorted updating FlashAttention mechanism, guided by a coordinated tiling strategy that enables fine-grained stage interaction for improved memory efficiency and latency. These optimizations are supported by a dedicated STAR accelerator architecture, achieving up to 9.2$\\times$ speedup and 71.2$\\times$ energy efficiency over A100, and surpassing SOTA accelerators by up to 16.1$\\times$ energy and 27.1$\\times$ area efficiency gains. Further, we deploy STAR onto a multi-core spatial architecture, optimizing dataflow and execution orchestration for ultra-long sequence processing. Architectural evaluation shows that, compared to the baseline design, Spatial-STAR achieves a 20.1$\\times$ throughput improvement."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:43:32Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    43,
                    32,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "Accepted for publication in IEEE Transactions on Computers",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Huizheng Wang"
                    },
                    {
                        "name": "Taiquan Wei"
                    },
                    {
                        "name": "Hongbin Wang"
                    },
                    {
                        "name": "Zichuan Wang"
                    },
                    {
                        "name": "Xinru Tang"
                    },
                    {
                        "name": "Zhiheng Yue"
                    },
                    {
                        "name": "Shaojun Wei"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shouyi Yin"
                },
                "author": "Shouyi Yin"
            },
            {
                "id": "http://arxiv.org/abs/2509.17247v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.17247v3",
                "title": "DeepASA: An Object-Oriented Multi-Purpose Network for Auditory Scene Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepASA: An Object-Oriented Multi-Purpose Network for Auditory Scene Analysis"
                },
                "updated": "2025-12-23T09:37:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    37,
                    51,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.17247v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.17247v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose DeepASA, a multi-purpose model for auditory scene analysis that performs multi-input multi-output (MIMO) source separation, dereverberation, sound event detection (SED), audio classification, and direction-of-arrival estimation (DoAE) within a unified framework. DeepASA is designed for complex auditory scenes where multiple, often similar, sound sources overlap in time and move dynamically in space. To achieve robust and consistent inference across tasks, we introduce an object-oriented processing (OOP) strategy. This approach encapsulates diverse auditory features into object-centric representations and refines them through a chain-of-inference (CoI) mechanism. The pipeline comprises a dynamic temporal kernel-based feature extractor, a transformer-based aggregator, and an object separator that yields per-object features. These features feed into multiple task-specific decoders. Our object-centric representations naturally resolve the parameter association ambiguity inherent in traditional track-wise processing. However, early-stage object separation can lead to failure in downstream ASA tasks. To address this, we implement temporal coherence matching (TCM) within the chain-of-inference, enabling multi-task fusion and iterative refinement of object features using estimated auditory parameters. We evaluate DeepASA on representative spatial audio benchmark datasets, including ASA2, MC-FUSS, and STARSS23. Experimental results show that our model achieves state-of-the-art performance across all evaluated tasks, demonstrating its effectiveness in both source separation and auditory parameter estimation under diverse spatial auditory scenes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose DeepASA, a multi-purpose model for auditory scene analysis that performs multi-input multi-output (MIMO) source separation, dereverberation, sound event detection (SED), audio classification, and direction-of-arrival estimation (DoAE) within a unified framework. DeepASA is designed for complex auditory scenes where multiple, often similar, sound sources overlap in time and move dynamically in space. To achieve robust and consistent inference across tasks, we introduce an object-oriented processing (OOP) strategy. This approach encapsulates diverse auditory features into object-centric representations and refines them through a chain-of-inference (CoI) mechanism. The pipeline comprises a dynamic temporal kernel-based feature extractor, a transformer-based aggregator, and an object separator that yields per-object features. These features feed into multiple task-specific decoders. Our object-centric representations naturally resolve the parameter association ambiguity inherent in traditional track-wise processing. However, early-stage object separation can lead to failure in downstream ASA tasks. To address this, we implement temporal coherence matching (TCM) within the chain-of-inference, enabling multi-task fusion and iterative refinement of object features using estimated auditory parameters. We evaluate DeepASA on representative spatial audio benchmark datasets, including ASA2, MC-FUSS, and STARSS23. Experimental results show that our model achieves state-of-the-art performance across all evaluated tasks, demonstrating its effectiveness in both source separation and auditory parameter estimation under diverse spatial auditory scenes."
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-21T21:44:10Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    44,
                    10,
                    6,
                    264,
                    0
                ],
                "arxiv_comment": "21 pages, 13 figures, 11 tables, published in NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "eess.AS"
                },
                "authors": [
                    {
                        "name": "Dongheon Lee"
                    },
                    {
                        "name": "Younghoo Kwon"
                    },
                    {
                        "name": "Jung-Woo Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jung-Woo Choi"
                },
                "author": "Jung-Woo Choi"
            },
            {
                "id": "http://arxiv.org/abs/2510.03289v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.03289v2",
                "title": "Why mask diffusion does not work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why mask diffusion does not work"
                },
                "updated": "2025-12-23T09:36:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    36,
                    38,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.03289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.03289v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The main advantages of diffusion language models over autoregressive (AR) models lie in their ability to support parallel generation and bidirectional attention, enabling a more controllable generation process. In recent years, open-source mask diffusion language models have emerged, most of which are based on a variant known as absorbing diffusion. However, this paper demonstrates why mask diffusion faces inherent difficulties in achieving parallel generation and bidirectional attention. We also propose the most effective training and inference strategies for mask diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The main advantages of diffusion language models over autoregressive (AR) models lie in their ability to support parallel generation and bidirectional attention, enabling a more controllable generation process. In recent years, open-source mask diffusion language models have emerged, most of which are based on a variant known as absorbing diffusion. However, this paper demonstrates why mask diffusion faces inherent difficulties in achieving parallel generation and bidirectional attention. We also propose the most effective training and inference strategies for mask diffusion."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-29T12:07:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    7,
                    9,
                    0,
                    272,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Haocheng Sun"
                    },
                    {
                        "name": "Cynthia Xin Wen"
                    },
                    {
                        "name": "Edward Hong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Edward Hong Wang"
                },
                "author": "Edward Hong Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.20193v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20193v1",
                "title": "Decay of $f(R)$ quintessence into dark matter: mitigating the Hubble tension?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decay of $f(R)$ quintessence into dark matter: mitigating the Hubble tension?"
                },
                "updated": "2025-12-23T09:34:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    34,
                    33,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20193v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose a revised cosmological scenario that extends the $Λ$ Cold Dark Matter ($Λ$CDM) framework by incorporating metric $f(R)$ gravity in the Jordan frame. In this model, the dark energy component arises from a non-minimally coupled scalar field, decomposed into a smooth background (set to unity to recover General Relativity) and a rapidly varying, massive fluctuation that decays into the dark matter sector. In the near-GR limit, this setup provides a phenomenological extension of $Λ$CDM characterized by two additional parameters: the present-day value of the scalar fluctuation and a normalized decay rate. Using a Markov Chain Monte Carlo analysis of low-redshift cosmological data, comprising Type Ia Supernovae, Baryon Acoustic Oscillation (BAO), and Cosmic Chronometer measurements, we find that the proposed model achieves a better overall fit than $Λ$CDM, while the Bayesian evidence remains statistically inconclusive given the inclusion of two extra parameters. The model predicts a moderate increase in the inferred value of $H_0$ and an improved consistency with DESI BAO data when adopting the SH0ES prior. Furthermore, describing dark matter particle creation as a transition phase in the late Universe offers an intriguing physical interpretation, potentially capturing features already present in current data and providing a promising avenue to explore extensions of the standard cosmological model within modified gravity frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a revised cosmological scenario that extends the $Λ$ Cold Dark Matter ($Λ$CDM) framework by incorporating metric $f(R)$ gravity in the Jordan frame. In this model, the dark energy component arises from a non-minimally coupled scalar field, decomposed into a smooth background (set to unity to recover General Relativity) and a rapidly varying, massive fluctuation that decays into the dark matter sector. In the near-GR limit, this setup provides a phenomenological extension of $Λ$CDM characterized by two additional parameters: the present-day value of the scalar fluctuation and a normalized decay rate. Using a Markov Chain Monte Carlo analysis of low-redshift cosmological data, comprising Type Ia Supernovae, Baryon Acoustic Oscillation (BAO), and Cosmic Chronometer measurements, we find that the proposed model achieves a better overall fit than $Λ$CDM, while the Bayesian evidence remains statistically inconclusive given the inclusion of two extra parameters. The model predicts a moderate increase in the inferred value of $H_0$ and an improved consistency with DESI BAO data when adopting the SH0ES prior. Furthermore, describing dark matter particle creation as a transition phase in the late Universe offers an intriguing physical interpretation, potentially capturing features already present in current data and providing a promising avenue to explore extensions of the standard cosmological model within modified gravity frameworks."
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:34:33Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    34,
                    33,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "15 pages, 2 figures",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO"
                },
                "authors": [
                    {
                        "name": "Giovanni Montani"
                    },
                    {
                        "name": "Luis A. Escamilla"
                    },
                    {
                        "name": "Nakia Carlevaro"
                    },
                    {
                        "name": "Eleonora Di Valentino"
                    }
                ],
                "author_detail": {
                    "name": "Eleonora Di Valentino"
                },
                "author": "Eleonora Di Valentino"
            },
            {
                "id": "http://arxiv.org/abs/2511.16193v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16193v3",
                "title": "Fast LLM Post-training via Decoupled and Fastest-of-N Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast LLM Post-training via Decoupled and Fastest-of-N Speculation"
                },
                "updated": "2025-12-23T09:31:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    31,
                    34,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16193v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16193v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. This work, SpecActor, achieves fast rollout with speculative decoding that deploys a fast draft path to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges that hinder speculation efficiency: (1) a Decoupled speculation method that overcomes the computation inefficiency issue when executing speculative decoding with relative large per-worker batch size -- a common configuration in training but unfriendly to speculation, and (2) a Fastest-of-N speculation method that selects and combines different draft methods according to the rollout progress to approximate the optimal draft method even when the best one is unknown a priori. Extensive evaluations on production traces show that SpecActor accelerates mean rollout speed by 2.0--2.4x, with up to 2.7x speedup, over common post-training baselines. The results are consistent across both dense and MoE models and across different RL algorithms. Notably, SpecActor is 1.1--2.6x faster compared to vanilla speculative rollout in different traces. The accelerated rollout achieves 1.4--2.3x faster end-to-end training time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. This work, SpecActor, achieves fast rollout with speculative decoding that deploys a fast draft path to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges that hinder speculation efficiency: (1) a Decoupled speculation method that overcomes the computation inefficiency issue when executing speculative decoding with relative large per-worker batch size -- a common configuration in training but unfriendly to speculation, and (2) a Fastest-of-N speculation method that selects and combines different draft methods according to the rollout progress to approximate the optimal draft method even when the best one is unknown a priori. Extensive evaluations on production traces show that SpecActor accelerates mean rollout speed by 2.0--2.4x, with up to 2.7x speedup, over common post-training baselines. The results are consistent across both dense and MoE models and across different RL algorithms. Notably, SpecActor is 1.1--2.6x faster compared to vanilla speculative rollout in different traces. The accelerated rollout achieves 1.4--2.3x faster end-to-end training time."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T10:00:03Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    10,
                    0,
                    3,
                    3,
                    324,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Rongxin Cheng"
                    },
                    {
                        "name": "Kai Zhou"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Mingcong Han"
                    },
                    {
                        "name": "Mingjing Ai"
                    },
                    {
                        "name": "Yeju Zhou"
                    },
                    {
                        "name": "Baoquan Zhong"
                    },
                    {
                        "name": "Wencong Xiao"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.20188v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20188v1",
                "title": "Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation"
                },
                "updated": "2025-12-23T09:28:20Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    28,
                    20,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20188v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:28:20Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    28,
                    20,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Teqiang Zou"
                    },
                    {
                        "name": "Hongliang Zeng"
                    },
                    {
                        "name": "Yuxuan Nong"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Kehui Liu"
                    },
                    {
                        "name": "Haotian Yang"
                    },
                    {
                        "name": "Xinyang Ling"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Lianyang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lianyang Ma"
                },
                "author": "Lianyang Ma"
            },
            {
                "id": "http://arxiv.org/abs/2512.19316v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19316v2",
                "title": "Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Implicit Heart Coordinates: 3D cardiac shape reconstruction from sparse segmentations"
                },
                "updated": "2025-12-23T09:25:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    25,
                    34,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19316v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19316v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate reconstruction of cardiac anatomy from sparse clinical images remains a major challenge in patient-specific modeling. While neural implicit functions have previously been applied to this task, their application to mapping anatomical consistency across subjects has been limited. In this work, we introduce Neural Implicit Heart Coordinates (NIHCs), a standardized implicit coordinate system, based on universal ventricular coordinates, that provides a common anatomical reference frame for the human heart. Our method predicts NIHCs directly from a limited number of 2D segmentations (sparse acquisition) and subsequently decodes them into dense 3D segmentations and high-resolution meshes at arbitrary output resolution. Trained on a large dataset of 5,000 cardiac meshes, the model achieves high reconstruction accuracy on clinical contours, with mean Euclidean surface errors of 2.51$\\pm$0.33 mm in a diseased cohort (n=4549) and 2.3$\\pm$0.36 mm in a healthy cohort (n=5576). The NIHC representation enables anatomically coherent reconstruction even under severe slice sparsity and segmentation noise, faithfully recovering complex structures such as the valve planes. Compared with traditional pipelines, inference time is reduced from over 60 s to 5-15 s. These results demonstrate that NIHCs constitute a robust and efficient anatomical representation for patient-specific 3D cardiac reconstruction from minimal input data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate reconstruction of cardiac anatomy from sparse clinical images remains a major challenge in patient-specific modeling. While neural implicit functions have previously been applied to this task, their application to mapping anatomical consistency across subjects has been limited. In this work, we introduce Neural Implicit Heart Coordinates (NIHCs), a standardized implicit coordinate system, based on universal ventricular coordinates, that provides a common anatomical reference frame for the human heart. Our method predicts NIHCs directly from a limited number of 2D segmentations (sparse acquisition) and subsequently decodes them into dense 3D segmentations and high-resolution meshes at arbitrary output resolution. Trained on a large dataset of 5,000 cardiac meshes, the model achieves high reconstruction accuracy on clinical contours, with mean Euclidean surface errors of 2.51$\\pm$0.33 mm in a diseased cohort (n=4549) and 2.3$\\pm$0.36 mm in a healthy cohort (n=5576). The NIHC representation enables anatomically coherent reconstruction even under severe slice sparsity and segmentation noise, faithfully recovering complex structures such as the valve planes. Compared with traditional pipelines, inference time is reduced from over 60 s to 5-15 s. These results demonstrate that NIHCs constitute a robust and efficient anatomical representation for patient-specific 3D cardiac reconstruction from minimal input data."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T12:07:05Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    12,
                    7,
                    5,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "42 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Marica Muffoletto"
                    },
                    {
                        "name": "Uxio Hermida"
                    },
                    {
                        "name": "Charlène Mauger"
                    },
                    {
                        "name": "Avan Suinesiaputra"
                    },
                    {
                        "name": "Yiyang Xu"
                    },
                    {
                        "name": "Richard Burns"
                    },
                    {
                        "name": "Lisa Pankewitz"
                    },
                    {
                        "name": "Andrew D McCulloch"
                    },
                    {
                        "name": "Steffen E Petersen"
                    },
                    {
                        "name": "Daniel Rueckert"
                    },
                    {
                        "name": "Alistair A Young"
                    }
                ],
                "author_detail": {
                    "name": "Alistair A Young"
                },
                "author": "Alistair A Young"
            },
            {
                "id": "http://arxiv.org/abs/2512.20184v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20184v1",
                "title": "Reaching Agreement Among Reasoning LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reaching Agreement Among Reasoning LLM Agents"
                },
                "updated": "2025-12-23T09:20:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    20,
                    42,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20184v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.\n  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.\n  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:20:42Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    20,
                    42,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Chaoyi Ruan"
                    },
                    {
                        "name": "Yiliang Wang"
                    },
                    {
                        "name": "Ziji Shi"
                    },
                    {
                        "name": "Jialin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Li"
                },
                "author": "Jialin Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.20182v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20182v1",
                "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaithLens: Detecting and Explaining Faithfulness Hallucination"
                },
                "updated": "2025-12-23T09:20:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    20,
                    32,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20182v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:20:32Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    20,
                    32,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Qingyi Wang"
                    },
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Yuzhuo Bai"
                    },
                    {
                        "name": "Guanqiao Chen"
                    },
                    {
                        "name": "Kangyang Luo"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Fanchao Qi"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun"
            },
            {
                "id": "http://arxiv.org/abs/2508.06244v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.06244v2",
                "title": "Membership Inference Attack with Partial Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Inference Attack with Partial Features"
                },
                "updated": "2025-12-23T09:18:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    18,
                    27,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.06244v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.06244v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Machine learning models are vulnerable to membership inference attack, which can be used to determine whether a given sample appears in the training data. Most existing methods assume the attacker has full access to the features of the target sample. This assumption, however, does not hold in many real-world scenarios where only partial features are available, thereby limiting the applicability of these methods. In this work, we introduce Partial Feature Membership Inference (PFMI), a scenario where the adversary observes only partial features of each sample and aims to infer whether this observed subset was present in the training set. To address this problem, we propose MRAD (Memory-guided Reconstruction and Anomaly Detection), a two-stage attack framework that works in both white-box and black-box settings. In the first stage, MRAD leverages the latent memory of the target model to reconstruct the unknown features of the sample. We observe that when the known features are absent from the training set, the reconstructed sample deviates significantly from the true data distribution. Consequently, in the second stage, we use anomaly detection algorithms to measure the deviation between the reconstructed sample and the training data distribution, thereby determining whether the known features belong to a member of the training set. Empirical results demonstrate that MRAD is effective across various datasets, and maintains compatibility with off-the-shelf anomaly detection techniques. For example, on STL-10, our attack exceeds an AUC of around 0.75 even with 60% of the missing features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models are vulnerable to membership inference attack, which can be used to determine whether a given sample appears in the training data. Most existing methods assume the attacker has full access to the features of the target sample. This assumption, however, does not hold in many real-world scenarios where only partial features are available, thereby limiting the applicability of these methods. In this work, we introduce Partial Feature Membership Inference (PFMI), a scenario where the adversary observes only partial features of each sample and aims to infer whether this observed subset was present in the training set. To address this problem, we propose MRAD (Memory-guided Reconstruction and Anomaly Detection), a two-stage attack framework that works in both white-box and black-box settings. In the first stage, MRAD leverages the latent memory of the target model to reconstruct the unknown features of the sample. We observe that when the known features are absent from the training set, the reconstructed sample deviates significantly from the true data distribution. Consequently, in the second stage, we use anomaly detection algorithms to measure the deviation between the reconstructed sample and the training data distribution, thereby determining whether the known features belong to a member of the training set. Empirical results demonstrate that MRAD is effective across various datasets, and maintains compatibility with off-the-shelf anomaly detection techniques. For example, on STL-10, our attack exceeds an AUC of around 0.75 even with 60% of the missing features."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-08T11:56:13Z",
                "published_parsed": [
                    2025,
                    8,
                    8,
                    11,
                    56,
                    13,
                    4,
                    220,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xurun Wang"
                    },
                    {
                        "name": "Guangrui Liu"
                    },
                    {
                        "name": "Xinjie Li"
                    },
                    {
                        "name": "Haoyu He"
                    },
                    {
                        "name": "Lin Yao"
                    },
                    {
                        "name": "Zhongyun Hua"
                    },
                    {
                        "name": "Weizhe Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Weizhe Zhang"
                },
                "author": "Weizhe Zhang"
            },
            {
                "id": "http://arxiv.org/abs/2512.20179v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20179v1",
                "title": "RESPOND: Risk-Enhanced Structured Pattern for LLM-driven Online Node-level Decision-making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RESPOND: Risk-Enhanced Structured Pattern for LLM-driven Online Node-level Decision-making"
                },
                "updated": "2025-12-23T09:17:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    17,
                    44,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20179v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current LLM-based driving agents that rely on unstructured plain-text memory suffer from low-precision scene retrieval and inefficient reflection. To address this limitation, we present RESPOND, a structured decision-making framework for LLM-driven agents grounded in explicit risk patterns. RESPOND represents each ego-centric scene using a unified 5 by 3 matrix that encodes spatial topology and road constraints, enabling consistent and reliable retrieval of spatial risk configurations. Based on this representation, a hybrid rule and LLM decision pipeline is developed with a two-tier memory mechanism. In high-risk contexts, exact pattern matching enables rapid and safe reuse of verified actions, while in low-risk contexts, sub-pattern matching supports personalized driving style adaptation. In addition, a pattern-aware reflection mechanism abstracts tactical corrections from crash and near-miss frames to update structured memory, achieving one-crash-to-generalize learning. Extensive experiments demonstrate the effectiveness of RESPOND. In highway-env, RESPOND outperforms state-of-the-art LLM-based and reinforcement learning based driving agents while producing substantially fewer collisions. With step-wise human feedback, the agent acquires a Sporty driving style within approximately 20 decision steps through sub-pattern abstraction. For real-world validation, RESPOND is evaluated on 53 high-risk cut-in scenarios extracted from the HighD dataset. For each event, intervention is applied immediately before the cut-in and RESPOND re-decides the driving action. Compared to recorded human behavior, RESPOND reduces subsequent risk in 84.9 percent of scenarios, demonstrating its practical feasibility under real-world driving conditions. These results highlight RESPONDs potential for autonomous driving, personalized driving assistance, and proactive hazard mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLM-based driving agents that rely on unstructured plain-text memory suffer from low-precision scene retrieval and inefficient reflection. To address this limitation, we present RESPOND, a structured decision-making framework for LLM-driven agents grounded in explicit risk patterns. RESPOND represents each ego-centric scene using a unified 5 by 3 matrix that encodes spatial topology and road constraints, enabling consistent and reliable retrieval of spatial risk configurations. Based on this representation, a hybrid rule and LLM decision pipeline is developed with a two-tier memory mechanism. In high-risk contexts, exact pattern matching enables rapid and safe reuse of verified actions, while in low-risk contexts, sub-pattern matching supports personalized driving style adaptation. In addition, a pattern-aware reflection mechanism abstracts tactical corrections from crash and near-miss frames to update structured memory, achieving one-crash-to-generalize learning. Extensive experiments demonstrate the effectiveness of RESPOND. In highway-env, RESPOND outperforms state-of-the-art LLM-based and reinforcement learning based driving agents while producing substantially fewer collisions. With step-wise human feedback, the agent acquires a Sporty driving style within approximately 20 decision steps through sub-pattern abstraction. For real-world validation, RESPOND is evaluated on 53 high-risk cut-in scenarios extracted from the HighD dataset. For each event, intervention is applied immediately before the cut-in and RESPOND re-decides the driving action. Compared to recorded human behavior, RESPOND reduces subsequent risk in 84.9 percent of scenarios, demonstrating its practical feasibility under real-world driving conditions. These results highlight RESPONDs potential for autonomous driving, personalized driving assistance, and proactive hazard mitigation."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:17:44Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    17,
                    44,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "28 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Dan Chen"
                    },
                    {
                        "name": "Heye Huang"
                    },
                    {
                        "name": "Tiantian Chen"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Yongji Li"
                    },
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Sikai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Sikai Chen"
                },
                "author": "Sikai Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.20176v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20176v1",
                "title": "Optimistic TEE-Rollups: A Hybrid Architecture for Scalable and Verifiable Generative AI Inference on Blockchain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimistic TEE-Rollups: A Hybrid Architecture for Scalable and Verifiable Generative AI Inference on Blockchain"
                },
                "updated": "2025-12-23T09:16:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    16,
                    41,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20176v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid integration of Large Language Models (LLMs) into decentralized physical infrastructure networks (DePIN) is currently bottlenecked by the Verifiability Trilemma, which posits that a decentralized inference system cannot simultaneously achieve high computational integrity, low latency, and low cost. Existing cryptographic solutions, such as Zero-Knowledge Machine Learning (ZKML), suffer from superlinear proving overheads (O(k NlogN)) that render them infeasible for billionparameter models. Conversely, optimistic approaches (opML) impose prohibitive dispute windows, preventing real-time interactivity, while recent \"Proof of Quality\" (PoQ) paradigms sacrifice cryptographic integrity for subjective semantic evaluation, leaving networks vulnerable to model downgrade attacks and reward hacking. In this paper, we introduce Optimistic TEE-Rollups (OTR), a hybrid verification protocol that harmonizes these constraints. OTR leverages NVIDIA H100 Confidential Computing Trusted Execution Environments (TEEs) to provide sub-second Provisional Finality, underpinned by an optimistic fraud-proof mechanism and stochastic Zero-Knowledge spot-checks to mitigate hardware side-channel risks. We formally define Proof of Efficient Attribution (PoEA), a consensus mechanism that cryptographically binds execution traces to hardware attestations, thereby guaranteeing model authenticity. Extensive simulations demonstrate that OTR achieves 99% of the throughput of centralized baselines with a marginal cost overhead of $0.07 per query, maintaining Byzantine fault tolerance against rational adversaries even in the presence of transient hardware vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid integration of Large Language Models (LLMs) into decentralized physical infrastructure networks (DePIN) is currently bottlenecked by the Verifiability Trilemma, which posits that a decentralized inference system cannot simultaneously achieve high computational integrity, low latency, and low cost. Existing cryptographic solutions, such as Zero-Knowledge Machine Learning (ZKML), suffer from superlinear proving overheads (O(k NlogN)) that render them infeasible for billionparameter models. Conversely, optimistic approaches (opML) impose prohibitive dispute windows, preventing real-time interactivity, while recent \"Proof of Quality\" (PoQ) paradigms sacrifice cryptographic integrity for subjective semantic evaluation, leaving networks vulnerable to model downgrade attacks and reward hacking. In this paper, we introduce Optimistic TEE-Rollups (OTR), a hybrid verification protocol that harmonizes these constraints. OTR leverages NVIDIA H100 Confidential Computing Trusted Execution Environments (TEEs) to provide sub-second Provisional Finality, underpinned by an optimistic fraud-proof mechanism and stochastic Zero-Knowledge spot-checks to mitigate hardware side-channel risks. We formally define Proof of Efficient Attribution (PoEA), a consensus mechanism that cryptographically binds execution traces to hardware attestations, thereby guaranteeing model authenticity. Extensive simulations demonstrate that OTR achieves 99% of the throughput of centralized baselines with a marginal cost overhead of $0.07 per query, maintaining Byzantine fault tolerance against rational adversaries even in the presence of transient hardware vulnerabilities."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:16:41Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    16,
                    41,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Aaron Chan"
                    },
                    {
                        "name": "Alex Ding"
                    },
                    {
                        "name": "Frank Chen"
                    },
                    {
                        "name": "Alan Wu"
                    },
                    {
                        "name": "Bruce Zhang"
                    },
                    {
                        "name": "Arther Tian"
                    }
                ],
                "author_detail": {
                    "name": "Arther Tian"
                },
                "author": "Arther Tian"
            },
            {
                "id": "http://arxiv.org/abs/2512.20173v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20173v1",
                "title": "Offline Safe Policy Optimization From Heterogeneous Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Safe Policy Optimization From Heterogeneous Feedback"
                },
                "updated": "2025-12-23T09:07:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    7,
                    53,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20173v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \\textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \\textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \\textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \\textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:07:53Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    7,
                    53,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "Accepted at AAMAS 2026 (Extended Abstract)",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Ze Gong"
                    },
                    {
                        "name": "Pradeep Varakantham"
                    },
                    {
                        "name": "Akshat Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Akshat Kumar"
                },
                "author": "Akshat Kumar"
            },
            {
                "id": "http://arxiv.org/abs/2512.20169v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20169v1",
                "title": "Learning to Reason in LLMs by Expectation Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason in LLMs by Expectation Maximization"
                },
                "updated": "2025-12-23T08:56:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    56,
                    49,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20169v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T08:56:49Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    56,
                    49,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "12 pages, 3 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Junghyun Lee"
                    },
                    {
                        "name": "Branislav Kveton"
                    },
                    {
                        "name": "Sunav Choudhary"
                    },
                    {
                        "name": "Subhojyoti Mukherjee"
                    },
                    {
                        "name": "Anup Rao"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Alexa Siu"
                    }
                ],
                "author_detail": {
                    "name": "Alexa Siu"
                },
                "author": "Alexa Siu"
            },
            {
                "id": "http://arxiv.org/abs/2512.20168v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20168v1",
                "title": "Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography"
                },
                "updated": "2025-12-23T08:53:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    53,
                    36,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20168v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "By integrating language understanding with perceptual modalities such as images, multimodal large language models (MLLMs) constitute a critical substrate for modern AI systems, particularly intelligent agents operating in open and interactive environments. However, their increasing accessibility also raises heightened risks of misuse, such as generating harmful or unsafe content. To mitigate these risks, alignment techniques are commonly applied to align model behavior with human values. Despite these efforts, recent studies have shown that jailbreak attacks can circumvent alignment and elicit unsafe outputs. Currently, most existing jailbreak methods are tailored for open-source models and exhibit limited effectiveness against commercial MLLM-integrated systems, which often employ additional filters. These filters can detect and prevent malicious input and output content, significantly reducing jailbreak threats. In this paper, we reveal that the success of these safety filters heavily relies on a critical assumption that malicious content must be explicitly visible in either the input or the output. This assumption, while often valid for traditional LLM-integrated systems, breaks down in MLLM-integrated systems, where attackers can leverage multiple modalities to conceal adversarial intent, leading to a false sense of security in existing MLLM-integrated systems. To challenge this assumption, we propose Odysseus, a novel jailbreak paradigm that introduces dual steganography to covertly embed malicious queries and responses into benign-looking images. Extensive experiments on benchmark datasets demonstrate that our Odysseus successfully jailbreaks several pioneering and realistic MLLM-integrated systems, achieving up to 99% attack success rate. It exposes a fundamental blind spot in existing defenses, and calls for rethinking cross-modal security in MLLM-integrated systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By integrating language understanding with perceptual modalities such as images, multimodal large language models (MLLMs) constitute a critical substrate for modern AI systems, particularly intelligent agents operating in open and interactive environments. However, their increasing accessibility also raises heightened risks of misuse, such as generating harmful or unsafe content. To mitigate these risks, alignment techniques are commonly applied to align model behavior with human values. Despite these efforts, recent studies have shown that jailbreak attacks can circumvent alignment and elicit unsafe outputs. Currently, most existing jailbreak methods are tailored for open-source models and exhibit limited effectiveness against commercial MLLM-integrated systems, which often employ additional filters. These filters can detect and prevent malicious input and output content, significantly reducing jailbreak threats. In this paper, we reveal that the success of these safety filters heavily relies on a critical assumption that malicious content must be explicitly visible in either the input or the output. This assumption, while often valid for traditional LLM-integrated systems, breaks down in MLLM-integrated systems, where attackers can leverage multiple modalities to conceal adversarial intent, leading to a false sense of security in existing MLLM-integrated systems. To challenge this assumption, we propose Odysseus, a novel jailbreak paradigm that introduces dual steganography to covertly embed malicious queries and responses into benign-looking images. Extensive experiments on benchmark datasets demonstrate that our Odysseus successfully jailbreaks several pioneering and realistic MLLM-integrated systems, achieving up to 99% attack success rate. It exposes a fundamental blind spot in existing defenses, and calls for rethinking cross-modal security in MLLM-integrated systems."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T08:53:36Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    53,
                    36,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "This paper is accepted by Network and Distributed System Security Symposium (NDSS) 2026",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Jiameng Cheng"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao"
            },
            {
                "id": "http://arxiv.org/abs/2506.09349v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.09349v4",
                "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations"
                },
                "updated": "2025-12-23T08:50:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    50,
                    59,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.09349v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.09349v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-11T02:57:22Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    57,
                    22,
                    2,
                    162,
                    0
                ],
                "arxiv_comment": "Work in progress",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chao-Hong Tan"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Luyao Cheng"
                    },
                    {
                        "name": "Hai Yu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Xiang Lv"
                    },
                    {
                        "name": "Tianyu Zhao"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yukun Ma"
                    },
                    {
                        "name": "Yafeng Chen"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Xiangang Li"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye"
            },
            {
                "id": "http://arxiv.org/abs/2510.23649v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.23649v3",
                "title": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models"
                },
                "updated": "2025-12-23T08:47:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    47,
                    31,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.23649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.23649v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As the length of input text increases, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. In this work, Low Rank Query and Key attention (LRQK) is introduced, a two-stage framework that jointly decomposes full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then employs these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism where only missing full-precision KV pairs are transferred, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal accuracy loss. Our code is available at https://github.com/tenghuilee/LRQK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of input text increases, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. In this work, Low Rank Query and Key attention (LRQK) is introduced, a two-stage framework that jointly decomposes full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then employs these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism where only missing full-precision KV pairs are transferred, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal accuracy loss. Our code is available at https://github.com/tenghuilee/LRQK."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-25T11:43:27Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "arxiv_comment": "https://neurips.cc/virtual/2025/loc/san-diego/poster/118451",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "arxiv_journal_ref": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
                "authors": [
                    {
                        "name": "Tenghui Li"
                    },
                    {
                        "name": "Guoxu Zhou"
                    },
                    {
                        "name": "Xuyang Zhao"
                    },
                    {
                        "name": "Yuning Qiu"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.20165v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20165v1",
                "title": "Spectral or spatial? Leveraging both for speaker extraction in challenging data conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectral or spatial? Leveraging both for speaker extraction in challenging data conditions"
                },
                "updated": "2025-12-23T08:44:38Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    44,
                    38,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20165v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20165v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents a robust multi-channel speaker extraction algorithm designed to handle inaccuracies in reference information. While existing approaches often rely solely on either spatial or spectral cues to identify the target speaker, our method integrates both sources of information to enhance robustness. A key aspect of our approach is its emphasis on stability, ensuring reliable performance even when one of the features is degraded or misleading. Given a noisy mixture and two potentially unreliable cues, a dedicated network is trained to dynamically balance their contributions-or disregard the less informative one when necessary. We evaluate the system under challenging conditions by simulating inference-time errors using a simple direction of arrival (DOA) estimator and a noisy spectral enrollment process. Experimental results demonstrate that the proposed model successfully extracts the desired speaker even in the presence of substantial reference inaccuracies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a robust multi-channel speaker extraction algorithm designed to handle inaccuracies in reference information. While existing approaches often rely solely on either spatial or spectral cues to identify the target speaker, our method integrates both sources of information to enhance robustness. A key aspect of our approach is its emphasis on stability, ensuring reliable performance even when one of the features is degraded or misleading. Given a noisy mixture and two potentially unreliable cues, a dedicated network is trained to dynamically balance their contributions-or disregard the less informative one when necessary. We evaluate the system under challenging conditions by simulating inference-time errors using a simple direction of arrival (DOA) estimator and a noisy spectral enrollment process. Experimental results demonstrate that the proposed model successfully extracts the desired speaker even in the presence of substantial reference inaccuracies."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T08:44:38Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    44,
                    38,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Aviad Eisenberg"
                    },
                    {
                        "name": "Sharon Gannot"
                    },
                    {
                        "name": "Shlomo E. Chazan"
                    }
                ],
                "author_detail": {
                    "name": "Shlomo E. Chazan"
                },
                "author": "Shlomo E. Chazan"
            },
            {
                "id": "http://arxiv.org/abs/2512.20164v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20164v1",
                "title": "AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications"
                },
                "updated": "2025-12-23T08:42:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    42,
                    9,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20164v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by \"adversarial instructions\" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by \"adversarial instructions\" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T08:42:09Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    42,
                    9,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Honglin Mu"
                    },
                    {
                        "name": "Jinghao Liu"
                    },
                    {
                        "name": "Kaiyang Wan"
                    },
                    {
                        "name": "Rui Xing"
                    },
                    {
                        "name": "Xiuying Chen"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che"
            },
            {
                "id": "http://arxiv.org/abs/2512.20162v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20162v1",
                "title": "Concept Generalization in Humans and Large Language Models: Insights from the Number Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept Generalization in Humans and Large Language Models: Insights from the Number Game"
                },
                "updated": "2025-12-23T08:41:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    41,
                    3,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20162v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We compare human and large language model (LLM) generalization in the number game, a concept inference task. Using a Bayesian model as an analytical framework, we examined the inductive biases and inference strategies of humans and LLMs. The Bayesian model captured human behavior better than LLMs in that humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules. Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize. These contrasts highlight the fundamental differences in how humans and LLMs infer and generalize mathematical concepts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compare human and large language model (LLM) generalization in the number game, a concept inference task. Using a Bayesian model as an analytical framework, we examined the inductive biases and inference strategies of humans and LLMs. The Bayesian model captured human behavior better than LLMs in that humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules. Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize. These contrasts highlight the fundamental differences in how humans and LLMs infer and generalize mathematical concepts."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T08:41:03Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    41,
                    3,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Arghavan Bazigaran"
                    },
                    {
                        "name": "Hansem Sohn"
                    }
                ],
                "author_detail": {
                    "name": "Hansem Sohn"
                },
                "author": "Hansem Sohn"
            },
            {
                "id": "http://arxiv.org/abs/2512.20159v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20159v1",
                "title": "AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration"
                },
                "updated": "2025-12-23T08:39:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    39,
                    22,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20159v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have been increasingly deployed in real-world software engineering, fostering the development of code evaluation metrics to study the quality of LLM-generated code. Conventional rule-based metrics merely score programs based on their surface-level similarities with reference programs instead of analyzing functionality and code quality in depth. To address this limitation, researchers have developed LLM-as-a-judge metrics, prompting LLMs to evaluate and score code, and curated various code evaluation benchmarks to validate their effectiveness. However, these benchmarks suffer from critical limitations, hindering reliable assessments of evaluation capability: Some feature coarse-grained binary labels, which reduce rich code behavior to a single bit of information, obscuring subtle errors. Others propose fine-grained but subjective, vaguely-defined evaluation criteria, introducing unreliability in manually-annotated scores, which is the ground-truth they rely on. Furthermore, they often use uncontrolled data synthesis methods, leading to unbalanced score distributions that poorly represent real-world code generation scenarios.\n  To curate a diverse benchmark with programs of well-balanced distributions across various quality levels and streamline the manual annotation procedure, we propose AXIOM, a novel perturbation-based framework for synthesizing code evaluation benchmarks at scale. It reframes program scores as the refinement effort needed for deployment, consisting of two stages: (1) Rule-guided perturbation, which prompts LLMs to apply sequences of predefined perturbation rules to existing high-quality programs to modify their functionality and code quality, enabling us to precisely control each program's target score to achieve balanced score distributions. (2) Multisource quality calibration, which first selects a subset of...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly deployed in real-world software engineering, fostering the development of code evaluation metrics to study the quality of LLM-generated code. Conventional rule-based metrics merely score programs based on their surface-level similarities with reference programs instead of analyzing functionality and code quality in depth. To address this limitation, researchers have developed LLM-as-a-judge metrics, prompting LLMs to evaluate and score code, and curated various code evaluation benchmarks to validate their effectiveness. However, these benchmarks suffer from critical limitations, hindering reliable assessments of evaluation capability: Some feature coarse-grained binary labels, which reduce rich code behavior to a single bit of information, obscuring subtle errors. Others propose fine-grained but subjective, vaguely-defined evaluation criteria, introducing unreliability in manually-annotated scores, which is the ground-truth they rely on. Furthermore, they often use uncontrolled data synthesis methods, leading to unbalanced score distributions that poorly represent real-world code generation scenarios.\n  To curate a diverse benchmark with programs of well-balanced distributions across various quality levels and streamline the manual annotation procedure, we propose AXIOM, a novel perturbation-based framework for synthesizing code evaluation benchmarks at scale. It reframes program scores as the refinement effort needed for deployment, consisting of two stages: (1) Rule-guided perturbation, which prompts LLMs to apply sequences of predefined perturbation rules to existing high-quality programs to modify their functionality and code quality, enabling us to precisely control each program's target score to achieve balanced score distributions. (2) Multisource quality calibration, which first selects a subset of..."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T08:39:22Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    39,
                    22,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Ruiqi Wang"
                    },
                    {
                        "name": "Xinchen Wang"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Chun Yong Chong"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Qing Liao"
                    }
                ],
                "author_detail": {
                    "name": "Qing Liao"
                },
                "author": "Qing Liao"
            },
            {
                "id": "http://arxiv.org/abs/2512.20156v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20156v1",
                "title": "Fun-Audio-Chat Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fun-Audio-Chat Technical Report"
                },
                "updated": "2025-12-23T08:35:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    35,
                    27,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20156v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T08:35:27Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    35,
                    27,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "21 pages, https://github.com/FunAudioLLM/Fun-Audio-Chat",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Luyao Cheng"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Xiangang Li"
                    },
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Chao-Hong Tan"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Junhao Xu"
                    },
                    {
                        "name": "Jieping Ye"
                    },
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Qiquan Zhang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou"
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2512.20618v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20618v1",
                "title": "LongVideoAgent: Multi-Agent Reasoning with Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongVideoAgent: Multi-Agent Reasoning with Long Videos"
                },
                "updated": "2025-12-23T18:59:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    59,
                    49,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20618v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:59:49Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    59,
                    49,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Runtao Liu"
                    },
                    {
                        "name": "Ziyi Liu"
                    },
                    {
                        "name": "Jiaqi Tang"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Renjie Pi"
                    },
                    {
                        "name": "Jipeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qifeng Chen"
                },
                "author": "Qifeng Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.20617v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20617v1",
                "title": "SpatialTree: How Spatial Abilities Branch Out in MLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpatialTree: How Spatial Abilities Branch Out in MLLMs"
                },
                "updated": "2025-12-23T18:59:46Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    59,
                    46,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20617v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive \"thinking\" is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:59:46Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    59,
                    46,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "webpage: https://spatialtree.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yuxi Xiao"
                    },
                    {
                        "name": "Longfei Li"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Xinhang Liu"
                    },
                    {
                        "name": "Sida Peng"
                    },
                    {
                        "name": "Yunchao Wei"
                    },
                    {
                        "name": "Xiaowei Zhou"
                    },
                    {
                        "name": "Bingyi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Bingyi Kang"
                },
                "author": "Bingyi Kang"
            },
            {
                "id": "http://arxiv.org/abs/2512.20612v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20612v1",
                "title": "Making Large Language Models Efficient Dense Retrievers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Making Large Language Models Efficient Dense Retrievers"
                },
                "updated": "2025-12-23T18:58:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    58,
                    25,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20612v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20612v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that directly fine-tuning large language models (LLMs) for dense retrieval yields strong performance, but their substantial parameter counts make them computationally inefficient. While prior studies have revealed significant layer redundancy in LLMs for generative tasks, it remains unclear whether similar redundancy exists when these models are adapted for retrieval tasks, which require encoding entire sequences into fixed representations rather than generating tokens iteratively. To this end, we conduct a comprehensive analysis of layer redundancy in LLM-based dense retrievers. We find that, in contrast to generative settings, MLP layers are substantially more prunable, while attention layers remain critical for semantic aggregation. Building on this insight, we propose EffiR, a framework for developing efficient retrievers that performs large-scale MLP compression through a coarse-to-fine strategy (coarse-grained depth reduction followed by fine-grained width reduction), combined with retrieval-specific fine-tuning. Across diverse BEIR datasets and LLM backbones, EffiR achieves substantial reductions in model size and inference cost while preserving the performance of full-size models."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:58:25Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    58,
                    25,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Yibin Lei"
                    },
                    {
                        "name": "Shwai He"
                    },
                    {
                        "name": "Ang Li"
                    },
                    {
                        "name": "Andrew Yates"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Yates"
                },
                "author": "Andrew Yates"
            },
            {
                "id": "http://arxiv.org/abs/2512.20610v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20610v1",
                "title": "FedPOD: the deployable units of training for federated learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedPOD: the deployable units of training for federated learning"
                },
                "updated": "2025-12-23T18:57:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    57,
                    53,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20610v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper proposes FedPOD (Proportionally Orchestrated Derivative) for optimizing learning efficiency and communication cost in federated learning among multiple clients. Inspired by FedPIDAvg, we define a round-wise task for FedPOD to enhance training efficiency. FedPIDAvg achieved performance improvement by incorporating the training loss reduction for prediction entropy as weights using differential terms. Furthermore, by modeling data distribution with a Poisson distribution and using a PID controller, it reduced communication costs even in skewed data distribution. However, excluding participants classified as outliers based on the Poisson distribution can limit data utilization. Additionally, PID controller requires the same participants to be maintained throughout the federated learning process as it uses previous rounds' learning information in the current round. In our approach, FedPOD addresses these issues by including participants excluded as outliers, eliminating dependency on previous rounds' learning information, and applying a method for calculating validation loss at each round. In this challenge, FedPOD presents comparable performance to FedPIDAvg in metrics of Dice score, 0.78, 0.71 and 0.72 for WT, ET and TC in average, and projected convergence score, 0.74 in average. Furthermore, the concept of FedPOD draws inspiration from Kubernetes' smallest computing unit, POD, designed to be compatible with Kubernetes auto-scaling. Extending round-wise tasks of FedPOD to POD units allows flexible design by applying scale-out similar to Kubernetes' auto-scaling. This work demonstrated the potentials of FedPOD to enhance federated learning by improving efficiency, flexibility, and performance in metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes FedPOD (Proportionally Orchestrated Derivative) for optimizing learning efficiency and communication cost in federated learning among multiple clients. Inspired by FedPIDAvg, we define a round-wise task for FedPOD to enhance training efficiency. FedPIDAvg achieved performance improvement by incorporating the training loss reduction for prediction entropy as weights using differential terms. Furthermore, by modeling data distribution with a Poisson distribution and using a PID controller, it reduced communication costs even in skewed data distribution. However, excluding participants classified as outliers based on the Poisson distribution can limit data utilization. Additionally, PID controller requires the same participants to be maintained throughout the federated learning process as it uses previous rounds' learning information in the current round. In our approach, FedPOD addresses these issues by including participants excluded as outliers, eliminating dependency on previous rounds' learning information, and applying a method for calculating validation loss at each round. In this challenge, FedPOD presents comparable performance to FedPIDAvg in metrics of Dice score, 0.78, 0.71 and 0.72 for WT, ET and TC in average, and projected convergence score, 0.74 in average. Furthermore, the concept of FedPOD draws inspiration from Kubernetes' smallest computing unit, POD, designed to be compatible with Kubernetes auto-scaling. Extending round-wise tasks of FedPOD to POD units allows flexible design by applying scale-out similar to Kubernetes' auto-scaling. This work demonstrated the potentials of FedPOD to enhance federated learning by improving efficiency, flexibility, and performance in metrics."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:57:53Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    57,
                    53,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "12 pages, 12 figures, MICCAI",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Daewoon Kim"
                    },
                    {
                        "name": "Si Young Yie"
                    },
                    {
                        "name": "Jae Sung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jae Sung Lee"
                },
                "author": "Jae Sung Lee"
            },
            {
                "id": "http://arxiv.org/abs/2512.20586v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20586v1",
                "title": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent"
                },
                "updated": "2025-12-23T18:32:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    32,
                    17,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20586v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p > 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:32:17Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    32,
                    17,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Humza Nusrat"
                    },
                    {
                        "name": "Luke Francisco"
                    },
                    {
                        "name": "Bing Luo"
                    },
                    {
                        "name": "Hassan Bagher-Ebadian"
                    },
                    {
                        "name": "Joshua Kim"
                    },
                    {
                        "name": "Karen Chin-Snyder"
                    },
                    {
                        "name": "Salim Siddiqui"
                    },
                    {
                        "name": "Mira Shah"
                    },
                    {
                        "name": "Eric Mellon"
                    },
                    {
                        "name": "Mohammad Ghassemi"
                    },
                    {
                        "name": "Anthony Doemer"
                    },
                    {
                        "name": "Benjamin Movsas"
                    },
                    {
                        "name": "Kundan Thind"
                    }
                ],
                "author_detail": {
                    "name": "Kundan Thind"
                },
                "author": "Kundan Thind"
            },
            {
                "id": "http://arxiv.org/abs/2512.20578v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20578v1",
                "title": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits"
                },
                "updated": "2025-12-23T18:21:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    21,
                    32,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20578v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20578v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) generate fluent and complex outputs but often fail to recognize their own mistakes and hallucinations. Existing approaches typically rely on external judges, multi-sample consistency, or text-based self-critique, which incur additional compute or correlate weakly with true correctness. We ask: can LLMs predict their own failures by inspecting internal states during inference? We introduce Gnosis, a lightweight self-awareness mechanism that enables frozen LLMs to perform intrinsic self-verification by decoding signals from hidden states and attention patterns. Gnosis passively observes internal traces, compresses them into fixed-budget descriptors, and predicts correctness with negligible inference cost, adding only ~5M parameters and operating independently of sequence length. Across math reasoning, open-domain question answering, and academic knowledge benchmarks, and over frozen backbones ranging from 1.7B to 20B parameters, Gnosis consistently outperforms strong internal baselines and large external judges in both accuracy and calibration. Moreover, it generalizes zero-shot to partial generations, enabling early detection of failing trajectories and compute-aware control. These results show that reliable correctness cues are intrinsic to generation process and can be extracted efficiently without external supervision."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:21:32Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    21,
                    32,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Amirhosein Ghasemabadi"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu"
            },
            {
                "id": "http://arxiv.org/abs/2512.20576v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20576v1",
                "title": "Performative Policy Gradient: Optimality in Performative Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performative Policy Gradient: Optimality in Performative Reinforcement Learning"
                },
                "updated": "2025-12-23T18:20:06Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    20,
                    6,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20576v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20576v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the Performative Policy Gradient algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to performatively optimal policies, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves performative stability but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms standard policy gradient algorithms and the existing performative RL algorithms aiming for stability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the Performative Policy Gradient algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to performatively optimal policies, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves performative stability but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms standard policy gradient algorithms and the existing performative RL algorithms aiming for stability."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:20:06Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    20,
                    6,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Debabrota Basu"
                    },
                    {
                        "name": "Udvas Das"
                    },
                    {
                        "name": "Brahim Driss"
                    },
                    {
                        "name": "Uddalak Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Uddalak Mukherjee"
                },
                "author": "Uddalak Mukherjee"
            },
            {
                "id": "http://arxiv.org/abs/2512.20573v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20573v1",
                "title": "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs"
                },
                "updated": "2025-12-23T18:16:58Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    16,
                    58,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20573v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It \"fails fast\" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and \"wins big\" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\\times$ speedup over vanilla decoding, 1.7$\\times$ over the best naive dLLM drafter, and 1.4$\\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM's speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It \"fails fast\" by spending minimal compute in hard-to-speculate regions to shrink speculation latency and \"wins big\" by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\\times$ speedup over vanilla decoding, 1.7$\\times$ over the best naive dLLM drafter, and 1.4$\\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:16:58Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    16,
                    58,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuofu Chen"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali"
            },
            {
                "id": "http://arxiv.org/abs/2512.20569v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20569v1",
                "title": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling to Hybrid Attention Models via KL-Guided Layer Selection"
                },
                "updated": "2025-12-23T18:12:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    12,
                    22,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20569v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20569v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \\citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \\citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T18:12:22Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    18,
                    12,
                    22,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yanhong Li"
                    },
                    {
                        "name": "Songlin Yang"
                    },
                    {
                        "name": "Shawn Tan"
                    },
                    {
                        "name": "Mayank Mishra"
                    },
                    {
                        "name": "Rameswar Panda"
                    },
                    {
                        "name": "Jiawei Zhou"
                    },
                    {
                        "name": "Yoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Yoon Kim"
                },
                "author": "Yoon Kim"
            },
            {
                "id": "http://arxiv.org/abs/2507.18885v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2507.18885v4",
                "title": "A Minimalist Proof Language for Neural Theorem Proving over Isabelle/HOL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Minimalist Proof Language for Neural Theorem Proving over Isabelle/HOL"
                },
                "updated": "2025-12-23T17:57:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    57,
                    57,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2507.18885v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2507.18885v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Neural Theorem Proving (NTP) employs LLMs to automate formal proofs in proof assistants. While LLMs have achieved relatively remarkable success in informal reasoning tasks using natural languages, the transition to mechanized formal theorem proving presents persistent challenges. Mechanized proof languages often contain many syntactic constructs and diverse, specialized proof tactics, which facilitate expert use but have no direct counterpart in informal mathematical proofs. These prover-specific idioms represent an additional burden for LLM-based NTPs that might be otherwise successful in generating informal proofs. Seeking to bridge this gap between formal proof construction and informal reasoning, in order to better facilitate NTP, this work approaches these challenges from a language design perspective. We look at common reasoning patterns in informal proofs and in existing mechanized proofs, and design Minilang -- a minimalist proof language that captures these reasoning patterns. In contrast to proof languages (informal and formal) that often feature a large collection of operations with unclear semantic boundaries, Minilang is deliberately kept minimalist -- its core design comprises only 10 operations, each with clear semantic distinctions. We further develop a rule-based translator from Isabelle's language (Isar) to Minilang, translating ~340K existing proofs with an ~85% success rate. Using this translated corpus, we finetune two LLMs to compare machine learning performance on Minilang versus the original Isar. Experiments show Minilang benefits the two LLMs by improving the pass@1 success rate on the PISA benchmark by up to 20/29 percentage points in comparison to the Isar-based LLMs w/wo Sledgehammer. The pass@1 rate reaches 69.1%, exceeding the prior work Baldur's pass@64 (65.7%); the pass@8 rate reaches 79.2%, exceeding the SOTA on PISA (71.0%) achieved by Magnushammer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Theorem Proving (NTP) employs LLMs to automate formal proofs in proof assistants. While LLMs have achieved relatively remarkable success in informal reasoning tasks using natural languages, the transition to mechanized formal theorem proving presents persistent challenges. Mechanized proof languages often contain many syntactic constructs and diverse, specialized proof tactics, which facilitate expert use but have no direct counterpart in informal mathematical proofs. These prover-specific idioms represent an additional burden for LLM-based NTPs that might be otherwise successful in generating informal proofs. Seeking to bridge this gap between formal proof construction and informal reasoning, in order to better facilitate NTP, this work approaches these challenges from a language design perspective. We look at common reasoning patterns in informal proofs and in existing mechanized proofs, and design Minilang -- a minimalist proof language that captures these reasoning patterns. In contrast to proof languages (informal and formal) that often feature a large collection of operations with unclear semantic boundaries, Minilang is deliberately kept minimalist -- its core design comprises only 10 operations, each with clear semantic distinctions. We further develop a rule-based translator from Isabelle's language (Isar) to Minilang, translating ~340K existing proofs with an ~85% success rate. Using this translated corpus, we finetune two LLMs to compare machine learning performance on Minilang versus the original Isar. Experiments show Minilang benefits the two LLMs by improving the pass@1 success rate on the PISA benchmark by up to 20/29 percentage points in comparison to the Isar-based LLMs w/wo Sledgehammer. The pass@1 rate reaches 69.1%, exceeding the prior work Baldur's pass@64 (65.7%); the pass@8 rate reaches 79.2%, exceeding the SOTA on PISA (71.0%) achieved by Magnushammer."
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-07-25T02:04:56Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    2,
                    4,
                    56,
                    4,
                    206,
                    0
                ],
                "arxiv_comment": "Accepted in OOPSLA'26",
                "arxiv_primary_category": {
                    "term": "cs.PL"
                },
                "authors": [
                    {
                        "name": "Qiyuan Xu"
                    },
                    {
                        "name": "Renxi Wang"
                    },
                    {
                        "name": "Peixin Wang"
                    },
                    {
                        "name": "Haonan Li"
                    },
                    {
                        "name": "Conrad Watt"
                    }
                ],
                "author_detail": {
                    "name": "Conrad Watt"
                },
                "author": "Conrad Watt"
            },
            {
                "id": "http://arxiv.org/abs/2512.20550v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20550v1",
                "title": "LLM-Based Authoring of Agent-Based Narratives through Scene Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Based Authoring of Agent-Based Narratives through Scene Descriptions"
                },
                "updated": "2025-12-23T17:46:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    46,
                    15,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20550v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents a system for procedurally generating agent-based narratives using large language models (LLMs). Users could drag and drop multiple agents and objects into a scene, with each entity automatically assigned semantic metadata describing its identity, role, and potential interactions. The scene structure is then serialized into a natural language prompt and sent to an LLM, which returns a structured string describing a sequence of actions and interactions among agents and objects. The returned string encodes who performed which actions, when, and how. A custom parser interprets this string and triggers coordinated agent behaviors, animations, and interaction modules. The system supports agent-based scenes, dynamic object manipulation, and diverse interaction types. Designed for ease of use and rapid iteration, the system enables the generation of virtual agent activity suitable for prototyping agent narratives. The performance of the developed system was evaluated using four popular lightweight LLMs. Each model's process and response time were measured under multiple complexity scenarios. The collected data were analyzed to compare consistency across the examined scenarios and to highlight the relative efficiency and suitability of each model for procedural agent-based narratives generation. The results demonstrate that LLMs can reliably translate high-level scene descriptions into executable agent-based behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a system for procedurally generating agent-based narratives using large language models (LLMs). Users could drag and drop multiple agents and objects into a scene, with each entity automatically assigned semantic metadata describing its identity, role, and potential interactions. The scene structure is then serialized into a natural language prompt and sent to an LLM, which returns a structured string describing a sequence of actions and interactions among agents and objects. The returned string encodes who performed which actions, when, and how. A custom parser interprets this string and triggers coordinated agent behaviors, animations, and interaction modules. The system supports agent-based scenes, dynamic object manipulation, and diverse interaction types. Designed for ease of use and rapid iteration, the system enables the generation of virtual agent activity suitable for prototyping agent narratives. The performance of the developed system was evaluated using four popular lightweight LLMs. Each model's process and response time were measured under multiple complexity scenarios. The collected data were analyzed to compare consistency across the examined scenarios and to highlight the relative efficiency and suitability of each model for procedural agent-based narratives generation. The results demonstrate that LLMs can reliably translate high-level scene descriptions into executable agent-based behaviors."
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T17:46:15Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    46,
                    15,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR"
                },
                "authors": [
                    {
                        "name": "Vinayak Regmi"
                    },
                    {
                        "name": "Christos Mousas"
                    }
                ],
                "author_detail": {
                    "name": "Christos Mousas"
                },
                "author": "Christos Mousas"
            },
            {
                "id": "http://arxiv.org/abs/2512.20535v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20535v1",
                "title": "ARBITER: AI-Driven Filtering for Role-Based Access Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARBITER: AI-Driven Filtering for Role-Based Access Control"
                },
                "updated": "2025-12-23T17:25:51Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    25,
                    51,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20535v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Role-Based Access Control (RBAC) struggles to adapt to dynamic enterprise environments with documents that contain information that cannot be disclosed to specific user groups. As these documents are used by LLM-driven systems (e.g., in RAG) the problem is exacerbated as LLMs can leak sensitive data due to prompt truncation, classification errors, or loss of system context. We introduce \\our, a system designed to provide RBAC in RAG systems. \\our implements layered input/output validation, role-aware retrieval, and post-generation fact-checking. Unlike traditional RBAC approaches that rely on fine-tuned classifiers, \\our uses LLMs operating in few-shot settings with prompt-based steering for rapid deployment and role updates. We evaluate the approach on 389 queries using a synthetic dataset. Experimental results show 85\\% accuracy and 89\\% F1-score in query filtering, close to traditional RBAC solutions. Results suggest that practical RBAC deployment on RAG systems is approaching the maturity level needed for dynamic enterprise environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-Based Access Control (RBAC) struggles to adapt to dynamic enterprise environments with documents that contain information that cannot be disclosed to specific user groups. As these documents are used by LLM-driven systems (e.g., in RAG) the problem is exacerbated as LLMs can leak sensitive data due to prompt truncation, classification errors, or loss of system context. We introduce \\our, a system designed to provide RBAC in RAG systems. \\our implements layered input/output validation, role-aware retrieval, and post-generation fact-checking. Unlike traditional RBAC approaches that rely on fine-tuned classifiers, \\our uses LLMs operating in few-shot settings with prompt-based steering for rapid deployment and role updates. We evaluate the approach on 389 queries using a synthetic dataset. Experimental results show 85\\% accuracy and 89\\% F1-score in query filtering, close to traditional RBAC solutions. Results suggest that practical RBAC deployment on RAG systems is approaching the maturity level needed for dynamic enterprise environments."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T17:25:51Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    25,
                    51,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Michele Lorenzo"
                    },
                    {
                        "name": "Idilio Drago"
                    },
                    {
                        "name": "Dario Salvadori"
                    },
                    {
                        "name": "Fabio Romolo Vayr"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Romolo Vayr"
                },
                "author": "Fabio Romolo Vayr"
            },
            {
                "id": "http://arxiv.org/abs/2512.20520v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20520v1",
                "title": "Benchmarking LLMs for Predictive Applications in the Intensive Care Units",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs for Predictive Applications in the Intensive Care Units"
                },
                "updated": "2025-12-23T17:08:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    8,
                    31,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20520v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of LLMs, various tasks across the natural language processing domain have been transformed. However, their application in predictive tasks remains less researched. This study compares large language models, including GatorTron-Base (trained on clinical data), Llama 8B, and Mistral 7B, against models like BioBERT, DocBERT, BioClinicalBERT, Word2Vec, and Doc2Vec, setting benchmarks for predicting Shock in critically ill patients. Timely prediction of shock can enable early interventions, thus improving patient outcomes. Text data from 17,294 ICU stays of patients in the MIMIC III database were scored for length of stay > 24 hours and shock index (SI) > 0.7 to yield 355 and 87 patients with normal and abnormal SI-index, respectively. Both focal and cross-entropy losses were used during finetuning to address class imbalances. Our findings indicate that while GatorTron Base achieved the highest weighted recall of 80.5%, the overall performance metrics were comparable between SLMs and LLMs. This suggests that LLMs are not inherently superior to SLMs in predicting future clinical events despite their strong performance on text-based tasks. To achieve meaningful clinical outcomes, future efforts in training LLMs should prioritize developing models capable of predicting clinical trajectories rather than focusing on simpler tasks such as named entity recognition or phenotyping."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T17:08:31Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    17,
                    8,
                    31,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Chehak Malhotra"
                    },
                    {
                        "name": "Mehak Gopal"
                    },
                    {
                        "name": "Akshaya Devadiga"
                    },
                    {
                        "name": "Pradeep Singh"
                    },
                    {
                        "name": "Ridam Pal"
                    },
                    {
                        "name": "Ritwik Kashyap"
                    },
                    {
                        "name": "Tavpritesh Sethi"
                    }
                ],
                "author_detail": {
                    "name": "Tavpritesh Sethi"
                },
                "author": "Tavpritesh Sethi"
            },
            {
                "id": "http://arxiv.org/abs/2512.20491v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20491v1",
                "title": "Step-DeepResearch Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-DeepResearch Technical Report"
                },
                "updated": "2025-12-23T16:32:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    32,
                    27,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20491v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs shift toward autonomous agents, Deep Research has emerged as a pivotal metric. However, existing academic benchmarks like BrowseComp often fail to meet real-world demands for open-ended research, which requires robust skills in intent recognition, long-horizon decision-making, and cross-source verification. To address this, we introduce Step-DeepResearch, a cost-effective, end-to-end agent. We propose a Data Synthesis Strategy Based on Atomic Capabilities to reinforce planning and report writing, combined with a progressive training path from agentic mid-training to SFT and RL. Enhanced by a Checklist-style Judger, this approach significantly improves robustness. Furthermore, to bridge the evaluation gap in the Chinese domain, we establish ADR-Bench for realistic deep research scenarios. Experimental results show that Step-DeepResearch (32B) scores 61.4% on Scale AI Research Rubrics. On ADR-Bench, it significantly outperforms comparable models and rivals SOTA closed-source models like OpenAI and Gemini DeepResearch. These findings prove that refined training enables medium-sized models to achieve expert-level capabilities at industry-leading cost-efficiency."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T16:32:27Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    32,
                    27,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Haikuo Du"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Lin Lin"
                    },
                    {
                        "name": "Mingrui Chen"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Tianchi Yue"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wei Yuan"
                    },
                    {
                        "name": "Wenjin Deng"
                    },
                    {
                        "name": "Xiaojian Yuan"
                    },
                    {
                        "name": "Xiaoyun Zhang"
                    },
                    {
                        "name": "Xiangyu Liu"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Yanming Xu"
                    },
                    {
                        "name": "Yicheng Cao"
                    },
                    {
                        "name": "Yifei Zhang"
                    },
                    {
                        "name": "Yongyao Wang"
                    },
                    {
                        "name": "Yubo Shu"
                    },
                    {
                        "name": "Yurong Zhang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhichao Chang"
                    },
                    {
                        "name": "Binyan Li"
                    },
                    {
                        "name": "Dan Ma"
                    },
                    {
                        "name": "Furong Jia"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Jing Bai"
                    },
                    {
                        "name": "Junlan Liu"
                    },
                    {
                        "name": "Manjiao Liu"
                    },
                    {
                        "name": "Na Wang"
                    },
                    {
                        "name": "Qiuping Wu"
                    },
                    {
                        "name": "Qinxin Du"
                    },
                    {
                        "name": "Shiwei Li"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Yifeng Gong"
                    },
                    {
                        "name": "Yonglin Chen"
                    },
                    {
                        "name": "Yuling Zhao"
                    },
                    {
                        "name": "Yuxuan Lin"
                    },
                    {
                        "name": "Ziqi Ren"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Aihu Zhang"
                    },
                    {
                        "name": "Brian Li"
                    },
                    {
                        "name": "Buyun Ma"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Li Xie"
                    },
                    {
                        "name": "Mingliang Li"
                    },
                    {
                        "name": "Pan Li"
                    },
                    {
                        "name": "Shidong Yang"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xiaojia Liu"
                    },
                    {
                        "name": "Yuchu Luo"
                    },
                    {
                        "name": "Yuan Song"
                    },
                    {
                        "name": "YuanHao Ding"
                    },
                    {
                        "name": "Yuanwei Liang"
                    },
                    {
                        "name": "Zexi Li"
                    },
                    {
                        "name": "Zhaoning Zhang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Jiansheng Chen"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Yibo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yibo Zhu"
                },
                "author": "Yibo Zhu"
            },
            {
                "id": "http://arxiv.org/abs/2512.20487v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20487v1",
                "title": "Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-temporal Adaptive Red-Green-Blue and Long-Wave Infrared Fusion for You Only Look Once-Based Landmine Detection from Unmanned Aerial Systems"
                },
                "updated": "2025-12-23T16:26:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    26,
                    47,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20487v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20487v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Landmines remain a persistent humanitarian threat, with 110 million actively deployed mines across 60 countries, claiming 26,000 casualties annually. This research evaluates adaptive Red-Green-Blue (RGB) and Long-Wave Infrared (LWIR) fusion for Unmanned Aerial Systems (UAS)-based detection of surface-laid landmines, leveraging the thermal contrast between the ordnance and the surrounding soil to enhance feature extraction. Using You Only Look Once (YOLO) architectures (v8, v10, v11) across 114 test images, generating 35,640 model-condition evaluations, YOLOv11 achieved optimal performance (86.8% mAP), with 10 to 30% thermal fusion at 5 to 10m altitude identified as the optimal detection parameters. A complementary architectural comparison revealed that while RF-DETR achieved the highest accuracy (69.2% mAP), followed by Faster R-CNN (67.6%), YOLOv11 (64.2%), and RetinaNet (50.2%), YOLOv11 trained 17.7 times faster than the transformer-based RF-DETR (41 minutes versus 12 hours), presenting a critical accuracy-efficiency tradeoff for operational deployment. Aggregated multi-temporal training datasets outperformed season-specific approaches by 1.8 to 9.6%, suggesting that models benefit from exposure to diverse thermal conditions. Anti-Tank (AT) mines achieved 61.9% detection accuracy, compared with 19.2% for Anti-Personnel (AP) mines, reflecting both the size differential and thermal-mass differences between these ordnance classes. As this research examined surface-laid mines where thermal contrast is maximized, future research should quantify thermal contrast effects for mines buried at varying depths across heterogeneous soil types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Landmines remain a persistent humanitarian threat, with 110 million actively deployed mines across 60 countries, claiming 26,000 casualties annually. This research evaluates adaptive Red-Green-Blue (RGB) and Long-Wave Infrared (LWIR) fusion for Unmanned Aerial Systems (UAS)-based detection of surface-laid landmines, leveraging the thermal contrast between the ordnance and the surrounding soil to enhance feature extraction. Using You Only Look Once (YOLO) architectures (v8, v10, v11) across 114 test images, generating 35,640 model-condition evaluations, YOLOv11 achieved optimal performance (86.8% mAP), with 10 to 30% thermal fusion at 5 to 10m altitude identified as the optimal detection parameters. A complementary architectural comparison revealed that while RF-DETR achieved the highest accuracy (69.2% mAP), followed by Faster R-CNN (67.6%), YOLOv11 (64.2%), and RetinaNet (50.2%), YOLOv11 trained 17.7 times faster than the transformer-based RF-DETR (41 minutes versus 12 hours), presenting a critical accuracy-efficiency tradeoff for operational deployment. Aggregated multi-temporal training datasets outperformed season-specific approaches by 1.8 to 9.6%, suggesting that models benefit from exposure to diverse thermal conditions. Anti-Tank (AT) mines achieved 61.9% detection accuracy, compared with 19.2% for Anti-Personnel (AP) mines, reflecting both the size differential and thermal-mass differences between these ordnance classes. As this research examined surface-laid mines where thermal contrast is maximized, future research should quantify thermal contrast effects for mines buried at varying depths across heterogeneous soil types."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T16:26:47Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    26,
                    47,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "21 pages with 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "James E. Gallagher"
                    },
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Jana Kosecka"
                    }
                ],
                "author_detail": {
                    "name": "Jana Kosecka"
                },
                "author": "Jana Kosecka"
            },
            {
                "id": "http://arxiv.org/abs/2512.20482v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20482v1",
                "title": "SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SweRank+: Multilingual, Multi-Turn Code Ranking for Software Issue Localization"
                },
                "updated": "2025-12-23T16:18:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    18,
                    39,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20482v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20482v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maintaining large-scale, multilingual codebases hinges on accurately localizing issues, which requires mapping natural-language error descriptions to the relevant functions that need to be modified. However, existing ranking approaches are often Python-centric and perform a single-pass search over the codebase. This work introduces SweRank+, a framework that couples SweRankMulti, a cross-lingual code ranking tool, with SweRankAgent, an agentic search setup, for iterative, multi-turn reasoning over the code repository. SweRankMulti comprises a code embedding retriever and a listwise LLM reranker, and is trained using a carefully curated large-scale issue localization dataset spanning multiple popular programming languages. SweRankAgent adopts an agentic search loop that moves beyond single-shot localization with a memory buffer to reason and accumulate relevant localization candidates over multiple turns. Our experiments on issue localization benchmarks spanning various languages demonstrate new state-of-the-art performance with SweRankMulti, while SweRankAgent further improves localization over single-pass ranking."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T16:18:39Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    18,
                    39,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Revanth Gangi Reddy"
                    },
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Wenting Zhao"
                    },
                    {
                        "name": "JaeHyeok Doo"
                    },
                    {
                        "name": "Tarun Suresh"
                    },
                    {
                        "name": "Daniel Lee"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Yingbo Zhou"
                    },
                    {
                        "name": "Semih Yavuz"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty"
            },
            {
                "id": "http://arxiv.org/abs/2512.20481v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20481v1",
                "title": "Coherence in the brain unfolds across separable temporal regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence in the brain unfolds across separable temporal regimes"
                },
                "updated": "2025-12-23T16:16:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    16,
                    42,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20481v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Coherence in language requires the brain to satisfy two competing temporal demands: gradual accumulation of meaning across extended context and rapid reconfiguration of representations at event boundaries. Despite their centrality to language and thought, how these processes are implemented in the human brain during naturalistic listening remains unclear. Here, we tested whether these two processes can be captured by annotation-free drift and shift signals and whether their neural expression dissociates across large-scale cortical systems. These signals were derived from a large language model (LLM) and formalized contextual drift and event shifts directly from the narrative input. To enable high-precision voxelwise encoding models with stable parameter estimates, we densely sampled one healthy adult across more than 7 hours of listening to thirteen crime stories while collecting ultra high-field (7T) BOLD data. We then modeled the feature-informed hemodynamic response using a regularized encoding framework validated on independent stories. Drift predictions were prevalent in default-mode network hubs, whereas shift predictions were evident bilaterally in the primary auditory cortex and language association cortex. Furthermore, activity in default-mode and parietal networks was best explained by a signal capturing how meaning accumulates and gradually fades over the course of the narrative. Together, these findings show that coherence during language comprehension is implemented through dissociable neural regimes of slow contextual integration and rapid event-driven reconfiguration, offering a mechanistic entry point for understanding disturbances of language coherence in psychiatric disorders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence in language requires the brain to satisfy two competing temporal demands: gradual accumulation of meaning across extended context and rapid reconfiguration of representations at event boundaries. Despite their centrality to language and thought, how these processes are implemented in the human brain during naturalistic listening remains unclear. Here, we tested whether these two processes can be captured by annotation-free drift and shift signals and whether their neural expression dissociates across large-scale cortical systems. These signals were derived from a large language model (LLM) and formalized contextual drift and event shifts directly from the narrative input. To enable high-precision voxelwise encoding models with stable parameter estimates, we densely sampled one healthy adult across more than 7 hours of listening to thirteen crime stories while collecting ultra high-field (7T) BOLD data. We then modeled the feature-informed hemodynamic response using a regularized encoding framework validated on independent stories. Drift predictions were prevalent in default-mode network hubs, whereas shift predictions were evident bilaterally in the primary auditory cortex and language association cortex. Furthermore, activity in default-mode and parietal networks was best explained by a signal capturing how meaning accumulates and gradually fades over the course of the narrative. Together, these findings show that coherence during language comprehension is implemented through dissociable neural regimes of slow contextual integration and rapid event-driven reconfiguration, offering a mechanistic entry point for understanding disturbances of language coherence in psychiatric disorders."
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T16:16:42Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    16,
                    16,
                    42,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC"
                },
                "authors": [
                    {
                        "name": "Davide Stauba"
                    },
                    {
                        "name": "Finn Rabe"
                    },
                    {
                        "name": "Akhil Misra"
                    },
                    {
                        "name": "Yves Pauli"
                    },
                    {
                        "name": "Roya Hüppi"
                    },
                    {
                        "name": "Nils Lang"
                    },
                    {
                        "name": "Lars Michels"
                    },
                    {
                        "name": "Victoria Edkins"
                    },
                    {
                        "name": "Sascha Frühholz"
                    },
                    {
                        "name": "Iris Sommer"
                    },
                    {
                        "name": "Wolfram Hinzen"
                    },
                    {
                        "name": "Philipp Homan"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Homan"
                },
                "author": "Philipp Homan"
            },
            {
                "id": "http://arxiv.org/abs/2512.20458v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20458v1",
                "title": "Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register"
                },
                "updated": "2025-12-23T15:53:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    53,
                    33,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20458v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs) have enabled agentic search systems that interleave multi-step reasoning with external tool use. However, existing frameworks largely rely on unstructured natural-language reasoning and accumulate raw intermediate traces in the context, which often leads to unstable reasoning trajectories, context overflow, and degraded performance on complex multi-hop queries. In this study, we introduce Laser, a general framework for stabilizing and scaling agentic search. Laser defines a symbolic action protocol that organizes agent behaviors into three spaces: planning, task-solving, and retrospection. Each action is specified with explicit semantics and a deterministic execution format, enabling structured and logical reasoning processes and reliable action parsing. This design makes intermediate decisions interpretable and traceable, enhancing explicit retrospection and fine-grained control over reasoning trajectories. In coordination with parsable actions, Laser further maintains a compact context register that stores only essential states of the reasoning process, allowing the agent to reason over long horizons without uncontrolled context expansion. Experiments on Qwen2.5/3-series models across challenging multi-hop QA datasets show that Laser consistently outperforms existing agentic search baselines under both prompting-only and fine-tuning settings, demonstrating that Laser provides a principled and effective foundation for robust, scalable agentic search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs) have enabled agentic search systems that interleave multi-step reasoning with external tool use. However, existing frameworks largely rely on unstructured natural-language reasoning and accumulate raw intermediate traces in the context, which often leads to unstable reasoning trajectories, context overflow, and degraded performance on complex multi-hop queries. In this study, we introduce Laser, a general framework for stabilizing and scaling agentic search. Laser defines a symbolic action protocol that organizes agent behaviors into three spaces: planning, task-solving, and retrospection. Each action is specified with explicit semantics and a deterministic execution format, enabling structured and logical reasoning processes and reliable action parsing. This design makes intermediate decisions interpretable and traceable, enhancing explicit retrospection and fine-grained control over reasoning trajectories. In coordination with parsable actions, Laser further maintains a compact context register that stores only essential states of the reasoning process, allowing the agent to reason over long horizons without uncontrolled context expansion. Experiments on Qwen2.5/3-series models across challenging multi-hop QA datasets show that Laser consistently outperforms existing agentic search baselines under both prompting-only and fine-tuning settings, demonstrating that Laser provides a principled and effective foundation for robust, scalable agentic search."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T15:53:33Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    53,
                    33,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Shuting Wang"
                    },
                    {
                        "name": "Qiaolin Xia"
                    },
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Yu Lu"
                    },
                    {
                        "name": "Bobsimons"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou"
            },
            {
                "id": "http://arxiv.org/abs/2511.06914v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06914v2",
                "title": "A Low-Cost ATmega32-Based Embedded System for Automated Patient Queue and Health Data Management in Private Medical Chambers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Low-Cost ATmega32-Based Embedded System for Automated Patient Queue and Health Data Management in Private Medical Chambers"
                },
                "updated": "2025-12-23T15:33:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    33,
                    0,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06914v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06914v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This paper presents a low-cost, stand-alone embedded system that automates patient queue handling and basic health data acquisition for small private medical chambers. The proposed design separates interaction into two physically distinct modules: a patient's self-service corner for entering basic details and measuring vital signs, and a doctor's corner for reviewing the current patient's information and advancing the queue. A single ATmega32 microcontroller coordinates both modules, interfacing with an LM35 temperature sensor, an XD-58C pulse sensor, matrix keypads for data entry, and dual 16$\\times$2 LCDs for guided interaction and clinician-side display. Unlike IoT-first approaches that require continuous connectivity and higher deployment overhead, the system operates offline and provides deterministic local operation suitable for resource-constrained settings. Experimental validation shows temperature readings within $\\pm 1^{\\circ}$C (LM35 range tested), resting pulse readings within $\\pm 3$~BPM, and button-to-display latency below 1.2~s, demonstrating reliable real-time performance under limited hardware resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a low-cost, stand-alone embedded system that automates patient queue handling and basic health data acquisition for small private medical chambers. The proposed design separates interaction into two physically distinct modules: a patient's self-service corner for entering basic details and measuring vital signs, and a doctor's corner for reviewing the current patient's information and advancing the queue. A single ATmega32 microcontroller coordinates both modules, interfacing with an LM35 temperature sensor, an XD-58C pulse sensor, matrix keypads for data entry, and dual 16$\\times$2 LCDs for guided interaction and clinician-side display. Unlike IoT-first approaches that require continuous connectivity and higher deployment overhead, the system operates offline and provides deterministic local operation suitable for resource-constrained settings. Experimental validation shows temperature readings within $\\pm 1^{\\circ}$C (LM35 range tested), resting pulse readings within $\\pm 3$~BPM, and button-to-display latency below 1.2~s, demonstrating reliable real-time performance under limited hardware resources."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-10T10:09:06Z",
                "published_parsed": [
                    2025,
                    11,
                    10,
                    10,
                    9,
                    6,
                    0,
                    314,
                    0
                ],
                "arxiv_comment": "v2: revised writing and presentation",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Kawshik Kumar Paul"
                    },
                    {
                        "name": "Mahdi Hasnat Siyam"
                    },
                    {
                        "name": "Khandokar Md. Rahat Hossain"
                    }
                ],
                "author_detail": {
                    "name": "Khandokar Md. Rahat Hossain"
                },
                "author": "Khandokar Md. Rahat Hossain"
            },
            {
                "id": "http://arxiv.org/abs/2512.20431v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20431v1",
                "title": "Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skin Lesion Classification Using a Soft Voting Ensemble of Convolutional Neural Networks"
                },
                "updated": "2025-12-23T15:20:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    20,
                    47,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20431v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20431v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1109/ECCE64574.2025.11013422",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Skin cancer can be identified by dermoscopic examination and ocular inspection, but early detection significantly increases survival chances. Artificial intelligence (AI), using annotated skin images and Convolutional Neural Networks (CNNs), improves diagnostic accuracy. This paper presents an early skin cancer classification method using a soft voting ensemble of CNNs. In this investigation, three benchmark datasets, namely HAM10000, ISIC 2016, and ISIC 2019, were used. The process involved rebalancing, image augmentation, and filtering techniques, followed by a hybrid dual encoder for segmentation via transfer learning. Accurate segmentation focused classification models on clinically significant features, reducing background artifacts and improving accuracy. Classification was performed through an ensemble of MobileNetV2, VGG19, and InceptionV3, balancing accuracy and speed for real-world deployment. The method achieved lesion recognition accuracies of 96.32\\%, 90.86\\%, and 93.92\\% for the three datasets. The system performance was evaluated using established skin lesion detection metrics, yielding impressive results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skin cancer can be identified by dermoscopic examination and ocular inspection, but early detection significantly increases survival chances. Artificial intelligence (AI), using annotated skin images and Convolutional Neural Networks (CNNs), improves diagnostic accuracy. This paper presents an early skin cancer classification method using a soft voting ensemble of CNNs. In this investigation, three benchmark datasets, namely HAM10000, ISIC 2016, and ISIC 2019, were used. The process involved rebalancing, image augmentation, and filtering techniques, followed by a hybrid dual encoder for segmentation via transfer learning. Accurate segmentation focused classification models on clinically significant features, reducing background artifacts and improving accuracy. Classification was performed through an ensemble of MobileNetV2, VGG19, and InceptionV3, balancing accuracy and speed for real-world deployment. The method achieved lesion recognition accuracies of 96.32\\%, 90.86\\%, and 93.92\\% for the three datasets. The system performance was evaluated using established skin lesion detection metrics, yielding impressive results."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T15:20:47Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    20,
                    47,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "Authors' version of the paper published in proceedings of ECCE, DOI: https://doi.org/10.1109/ECCE64574.2025.11013422",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Abdullah Al Shafi"
                    },
                    {
                        "name": "Abdul Muntakim"
                    },
                    {
                        "name": "Pintu Chandra Shill"
                    },
                    {
                        "name": "Rowzatul Zannat"
                    },
                    {
                        "name": "Abdullah Al-Amin"
                    }
                ],
                "author_detail": {
                    "name": "Abdullah Al-Amin"
                },
                "author": "Abdullah Al-Amin",
                "arxiv_doi": "10.1109/ECCE64574.2025.11013422"
            },
            {
                "id": "http://arxiv.org/abs/2511.21878v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.21878v2",
                "title": "Advancing Automated In-Isolation Validation in Repository-Level Code Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Automated In-Isolation Validation in Repository-Level Code Translation"
                },
                "updated": "2025-12-23T15:17:24Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    17,
                    24,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.21878v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.21878v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Repository-level code translation aims to migrate entire repositories across programming languages while preserving functionality automatically. Despite advancements in repository-level code translation, validating the translations remains challenging. This paper proposes TRAM, which combines context-aware type resolution with mock-based in-isolation validation to achieve high-quality translations between programming languages. Prior to translation, TRAM retrieves API documentation and contextual code information for each variable type in the source language. It then prompts a large language model (LLM) with retrieved contextual information to resolve type mappings across languages with precise semantic interpretations. Using the automatically constructed type mapping, TRAM employs a custom serialization/deserialization workflow that automatically constructs equivalent mock objects in the target language. This enables each method fragment to be validated in isolation, without the high cost of using agents for translation validation, or the heavy manual effort required by existing approaches that rely on language interoperability. TRAM demonstrates state-of-the-art performance in Java-to-Python translation, underscoring the effectiveness of its integration of RAG-based type resolution with reliable in-isolation validation."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-26T19:53:46Z",
                "published_parsed": [
                    2025,
                    11,
                    26,
                    19,
                    53,
                    46,
                    2,
                    330,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Kaiyao Ke"
                    },
                    {
                        "name": "Ali Reza Ibrahimzada"
                    },
                    {
                        "name": "Rangeet Pan"
                    },
                    {
                        "name": "Saurabh Sinha"
                    },
                    {
                        "name": "Reyhaneh Jabbarvand"
                    }
                ],
                "author_detail": {
                    "name": "Reyhaneh Jabbarvand"
                },
                "author": "Reyhaneh Jabbarvand"
            },
            {
                "id": "http://arxiv.org/abs/2512.16531v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16531v3",
                "title": "Scaling Laws for Energy Efficiency of Local LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Energy Efficiency of Local LLMs"
                },
                "updated": "2025-12-23T15:02:39Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    15,
                    2,
                    39,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16531v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16531v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local large language models and vision-language models on edge devices requires balancing accuracy with constrained computational and energy budgets. Although graphics processors dominate modern artificial-intelligence deployment, most consumer hardware--including laptops, desktops, industrial controllers, and embedded systems--relies on central processing units. Despite this, the computational laws governing central-processing-unit-only inference for local language and vision-language workloads remain largely unexplored. We systematically benchmark large language and vision-language models on two representative central-processing-unit tiers widely used for local inference: a MacBook Pro M2, reflecting mainstream laptop-class deployment, and a Raspberry Pi 5, representing constrained, low-power embedded settings. Using a unified methodology based on continuous sampling of processor and memory usage together with area-under-curve integration, we characterize how computational load scales with input text length for language models and with image resolution for vision-language models. We uncover two empirical scaling laws: (1) computational cost for language-model inference scales approximately linearly with token length; and (2) vision-language models exhibit a preprocessing-driven \"resolution knee\", where compute remains constant above an internal resolution clamp and decreases sharply below it. Beyond these laws, we show that quantum-inspired compression reduces processor and memory usage by up to 71.9% and energy consumption by up to 62%, while preserving or improving semantic accuracy. These results provide a systematic quantification of multimodal central-processing-unit-only scaling for local language and vision-language workloads, and they identify model compression and input-resolution preprocessing as effective, low-cost levers for sustainable edge inference."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T13:40:33Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    13,
                    40,
                    33,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Ander Alvarez"
                    },
                    {
                        "name": "Alessandro Genuardi"
                    },
                    {
                        "name": "Nilotpal Sinha"
                    },
                    {
                        "name": "Antonio Tiene"
                    },
                    {
                        "name": "Mikail Okyay"
                    },
                    {
                        "name": "Bakbergen Ryskulov"
                    },
                    {
                        "name": "David Montero"
                    },
                    {
                        "name": "Samuel Mugel"
                    },
                    {
                        "name": "Román Orús"
                    }
                ],
                "author_detail": {
                    "name": "Román Orús"
                },
                "author": "Román Orús"
            },
            {
                "id": "http://arxiv.org/abs/2512.20407v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20407v1",
                "title": "AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AUDRON: A Deep Learning Framework with Fused Acoustic Signatures for Drone Type Recognition"
                },
                "updated": "2025-12-23T14:55:08Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    55,
                    8,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20407v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Unmanned aerial vehicles (UAVs), commonly known as drones, are increasingly used across diverse domains, including logistics, agriculture, surveillance, and defense. While these systems provide numerous benefits, their misuse raises safety and security concerns, making effective detection mechanisms essential. Acoustic sensing offers a low-cost and non-intrusive alternative to vision or radar-based detection, as drone propellers generate distinctive sound patterns. This study introduces AUDRON (AUdio-based Drone Recognition Network), a hybrid deep learning framework for drone sound detection, employing a combination of Mel-Frequency Cepstral Coefficients (MFCC), Short-Time Fourier Transform (STFT) spectrograms processed with convolutional neural networks (CNNs), recurrent layers for temporal modeling, and autoencoder-based representations. Feature-level fusion integrates complementary information before classification. Experimental evaluation demonstrates that AUDRON effectively differentiates drone acoustic signatures from background noise, achieving high accuracy while maintaining generalizability across varying conditions. AUDRON achieves 98.51 percent and 97.11 percent accuracy in binary and multiclass classification. The results highlight the advantage of combining multiple feature representations with deep learning for reliable acoustic drone detection, suggesting the framework's potential for deployment in security and surveillance applications where visual or radar sensing may be limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned aerial vehicles (UAVs), commonly known as drones, are increasingly used across diverse domains, including logistics, agriculture, surveillance, and defense. While these systems provide numerous benefits, their misuse raises safety and security concerns, making effective detection mechanisms essential. Acoustic sensing offers a low-cost and non-intrusive alternative to vision or radar-based detection, as drone propellers generate distinctive sound patterns. This study introduces AUDRON (AUdio-based Drone Recognition Network), a hybrid deep learning framework for drone sound detection, employing a combination of Mel-Frequency Cepstral Coefficients (MFCC), Short-Time Fourier Transform (STFT) spectrograms processed with convolutional neural networks (CNNs), recurrent layers for temporal modeling, and autoencoder-based representations. Feature-level fusion integrates complementary information before classification. Experimental evaluation demonstrates that AUDRON effectively differentiates drone acoustic signatures from background noise, achieving high accuracy while maintaining generalizability across varying conditions. AUDRON achieves 98.51 percent and 97.11 percent accuracy in binary and multiclass classification. The results highlight the advantage of combining multiple feature representations with deep learning for reliable acoustic drone detection, suggesting the framework's potential for deployment in security and surveillance applications where visual or radar sensing may be limited."
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T14:55:08Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    55,
                    8,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "Presented at the 2025 IEEE 22nd India Council International Conference (INDICON). 6 pages, 3 figures",
                "arxiv_primary_category": {
                    "term": "cs.SD"
                },
                "authors": [
                    {
                        "name": "Rajdeep Chatterjee"
                    },
                    {
                        "name": "Sudip Chakrabarty"
                    },
                    {
                        "name": "Trishaani Acharjee"
                    },
                    {
                        "name": "Deepanjali Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Deepanjali Mishra"
                },
                "author": "Deepanjali Mishra"
            },
            {
                "id": "http://arxiv.org/abs/2512.20405v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20405v1",
                "title": "ChatGPT: Excellent Paper! Accept It. Editor: Imposter Found! Review Rejected",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT: Excellent Paper! Accept It. Editor: Imposter Found! Review Rejected"
                },
                "updated": "2025-12-23T14:54:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    54,
                    45,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20405v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) like ChatGPT are now widely used in writing and reviewing scientific papers. While this trend accelerates publication growth and reduces human workload, it also introduces serious risks. Papers written or reviewed by LLMs may lack real novelty, contain fabricated or biased results, or mislead downstream research that others depend on. Such issues can damage reputations, waste resources, and even endanger lives when flawed studies influence medical or safety-critical systems. This research explores both the offensive and defensive sides of this growing threat. On the attack side, we demonstrate how an author can inject hidden prompts inside a PDF that secretly guide or \"jailbreak\" LLM reviewers into giving overly positive feedback and biased acceptance. On the defense side, we propose an \"inject-and-detect\" strategy for editors, where invisible trigger prompts are embedded into papers; if a review repeats or reacts to these triggers, it reveals that the review was generated by an LLM, not a human. This method turns prompt injections from vulnerability into a verification tool. We outline our design, expected model behaviors, and ethical safeguards for deployment. The goal is to expose how fragile today's peer-review process becomes under LLM influence and how editorial awareness can help restore trust in scientific evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) like ChatGPT are now widely used in writing and reviewing scientific papers. While this trend accelerates publication growth and reduces human workload, it also introduces serious risks. Papers written or reviewed by LLMs may lack real novelty, contain fabricated or biased results, or mislead downstream research that others depend on. Such issues can damage reputations, waste resources, and even endanger lives when flawed studies influence medical or safety-critical systems. This research explores both the offensive and defensive sides of this growing threat. On the attack side, we demonstrate how an author can inject hidden prompts inside a PDF that secretly guide or \"jailbreak\" LLM reviewers into giving overly positive feedback and biased acceptance. On the defense side, we propose an \"inject-and-detect\" strategy for editors, where invisible trigger prompts are embedded into papers; if a review repeats or reacts to these triggers, it reveals that the review was generated by an LLM, not a human. This method turns prompt injections from vulnerability into a verification tool. We outline our design, expected model behaviors, and ethical safeguards for deployment. The goal is to expose how fragile today's peer-review process becomes under LLM influence and how editorial awareness can help restore trust in scientific evaluation."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T14:54:45Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    54,
                    45,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Kanchon Gharami"
                    },
                    {
                        "name": "Sanjiv Kumar Sarkar"
                    },
                    {
                        "name": "Yongxin Liu"
                    },
                    {
                        "name": "Shafika Showkat Moni"
                    }
                ],
                "author_detail": {
                    "name": "Shafika Showkat Moni"
                },
                "author": "Shafika Showkat Moni"
            },
            {
                "id": "http://arxiv.org/abs/2512.20403v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20403v1",
                "title": "BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BRIDGE: Budget-aware Reasoning via Intermediate Distillation with Guided Examples"
                },
                "updated": "2025-12-23T14:46:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    46,
                    43,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20403v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Distilling knowledge from large proprietary models (e.g., GPT-4) to tiny deployable models (less than 1B parameters) faces a critical capacity-budget trap: the 1000x capacity gap between teachers and students prevents effective direct transfer, while API costs prohibit extensive data collection. We introduce BRIDGE (Budget-Aware Reasoning via Intermediate Distillation), a two-phase framework that resolves these constraints through strategic intermediation and budget asymmetry. In Phase 1, a mid-sized Teacher Assistant (TA; e.g., about 7B) learns from the black-box teacher on a strictly limited subset of data (e.g., 3-5%), selected via a zero-API-cost pipeline that balances entropic difficulty and semantic diversity using only local TA inference. In Phase 2, we exploit this asymmetry-teacher queries are expensive, whereas TA inference is free to amplify supervision: the refined TA generates synthetic rationales for the full dataset to train the tiny student. Crucially, we apply an instruction-tuning curriculum to establish behavioral alignment in the tiny student before transferring reasoning. Our theoretical analysis shows that BRIDGE yields tighter generalization bounds than direct distillation when data is abundant. Experiments across medical, legal, and financial benchmarks demonstrate consistent improvements: BRIDGE delivers student performance gains of 28-41%, closing the capability gap with proprietary teachers by 12-16% while using 10x fewer teacher queries. Notably, BRIDGE defies the conventional cost-performance frontier, surpassing direct distillation baselines that use 100% of the budget while consuming only 5% of the resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distilling knowledge from large proprietary models (e.g., GPT-4) to tiny deployable models (less than 1B parameters) faces a critical capacity-budget trap: the 1000x capacity gap between teachers and students prevents effective direct transfer, while API costs prohibit extensive data collection. We introduce BRIDGE (Budget-Aware Reasoning via Intermediate Distillation), a two-phase framework that resolves these constraints through strategic intermediation and budget asymmetry. In Phase 1, a mid-sized Teacher Assistant (TA; e.g., about 7B) learns from the black-box teacher on a strictly limited subset of data (e.g., 3-5%), selected via a zero-API-cost pipeline that balances entropic difficulty and semantic diversity using only local TA inference. In Phase 2, we exploit this asymmetry-teacher queries are expensive, whereas TA inference is free to amplify supervision: the refined TA generates synthetic rationales for the full dataset to train the tiny student. Crucially, we apply an instruction-tuning curriculum to establish behavioral alignment in the tiny student before transferring reasoning. Our theoretical analysis shows that BRIDGE yields tighter generalization bounds than direct distillation when data is abundant. Experiments across medical, legal, and financial benchmarks demonstrate consistent improvements: BRIDGE delivers student performance gains of 28-41%, closing the capability gap with proprietary teachers by 12-16% while using 10x fewer teacher queries. Notably, BRIDGE defies the conventional cost-performance frontier, surpassing direct distillation baselines that use 100% of the budget while consuming only 5% of the resources."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T14:46:43Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    46,
                    43,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Xuan-An Le"
                    },
                    {
                        "name": "Minh-Nam Tran"
                    },
                    {
                        "name": "Son Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Son Nguyen"
                },
                "author": "Son Nguyen"
            },
            {
                "id": "http://arxiv.org/abs/2506.18009v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.18009v2",
                "title": "ISAC Network Planning: Sensing Coverage Analysis and 3-D BS Deployment Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ISAC Network Planning: Sensing Coverage Analysis and 3-D BS Deployment Optimization"
                },
                "updated": "2025-12-23T14:22:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    22,
                    53,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.18009v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.18009v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Integrated sensing and communication (ISAC) networks strive to deliver both high-precision target localization and high-throughput data services across the entire coverage area. In this work, we examine the fundamental trade-off between sensing and communication from the perspective of base station (BS) deployment. Furthermore, we conceive a design that simultaneously maximizes the target localization coverage, while guaranteeing the desired communication performance. In contrast to existing schemes optimized for a single target, an effective network-level approach has to ensure consistent localization accuracy throughout the entire service area. While employing time-of-flight (ToF) based localization, we first analyze the deployment problem from a localization-performance coverage perspective, aiming for minimizing the area Cramer-Rao Lower Bound (A-CRLB) to ensure uniformly high positioning accuracy across the service area. We prove that for a fixed number of BSs, uniformly scaling the service area by a factor κincreases the optimal A-CRLB in proportion to κ^{2β}, where βis the BS-to-target pathloss exponent. Based on this, we derive an approximate scaling law that links the achievable A-CRLB across the area of interest to the dimensionality of the sensing area. We also show that cooperative BSs extend the coverage but yield marginal A-CRLB improvement as the dimensionality of the sensing area grows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated sensing and communication (ISAC) networks strive to deliver both high-precision target localization and high-throughput data services across the entire coverage area. In this work, we examine the fundamental trade-off between sensing and communication from the perspective of base station (BS) deployment. Furthermore, we conceive a design that simultaneously maximizes the target localization coverage, while guaranteeing the desired communication performance. In contrast to existing schemes optimized for a single target, an effective network-level approach has to ensure consistent localization accuracy throughout the entire service area. While employing time-of-flight (ToF) based localization, we first analyze the deployment problem from a localization-performance coverage perspective, aiming for minimizing the area Cramer-Rao Lower Bound (A-CRLB) to ensure uniformly high positioning accuracy across the service area. We prove that for a fixed number of BSs, uniformly scaling the service area by a factor κincreases the optimal A-CRLB in proportion to κ^{2β}, where βis the BS-to-target pathloss exponent. Based on this, we derive an approximate scaling law that links the achievable A-CRLB across the area of interest to the dimensionality of the sensing area. We also show that cooperative BSs extend the coverage but yield marginal A-CRLB improvement as the dimensionality of the sensing area grows."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-22T12:12:10Z",
                "published_parsed": [
                    2025,
                    6,
                    22,
                    12,
                    12,
                    10,
                    6,
                    173,
                    0
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Wireless Communications",
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "arxiv_journal_ref": "IEEE Transactions on Wireless Communications, 2025",
                "authors": [
                    {
                        "name": "Kaitao Meng"
                    },
                    {
                        "name": "Kawon Han"
                    },
                    {
                        "name": "Christos Masouros"
                    },
                    {
                        "name": "Lajos Hanzo"
                    }
                ],
                "author_detail": {
                    "name": "Lajos Hanzo"
                },
                "author": "Lajos Hanzo"
            },
            {
                "id": "http://arxiv.org/abs/2508.08189v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.08189v3",
                "title": "Reinforcement Learning for Large Model: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning for Large Model: A Survey"
                },
                "updated": "2025-12-23T14:03:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    14,
                    3,
                    44,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.08189v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.08189v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances at the intersection of reinforcement learning (RL) and visual intelligence have enabled agents that not only perceive complex visual scenes but also reason, generate, and act within them. This survey offers a critical and up-to-date synthesis of the field. We first formalize visual RL problems and trace the evolution of policy-optimization strategies from RLHF to verifiable reward paradigms, and from Proximal Policy Optimization to Group Relative Policy Optimization. We then organize more than 200 representative works into four thematic pillars: multi-modal large language models, visual generation, unified model frameworks, and vision-language-action models. For each pillar we examine algorithmic design, reward engineering, benchmark progress, and we distill trends such as curriculum-driven training, preference-aligned diffusion, and unified reward modeling. Finally, we review evaluation protocols spanning set-level fidelity, sample-level preference, and state-level stability, and we identify open challenges that include sample efficiency, generalization, and safe deployment. Our goal is to provide researchers and practitioners with a coherent map of the rapidly expanding landscape of visual RL and to highlight promising directions for future inquiry. Resources are available at: https://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-11T17:08:55Z",
                "published_parsed": [
                    2025,
                    8,
                    11,
                    17,
                    8,
                    55,
                    0,
                    223,
                    0
                ],
                "arxiv_comment": "22 pages",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Weijia Wu"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Joya Chen"
                    },
                    {
                        "name": "Kevin Qinghong Lin"
                    },
                    {
                        "name": "Qingwei Meng"
                    },
                    {
                        "name": "Yiming Zhang"
                    },
                    {
                        "name": "Yuke Qiu"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou"
            },
            {
                "id": "http://arxiv.org/abs/2512.20374v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20374v1",
                "title": "CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLIP Based Region-Aware Feature Fusion for Automated BBPS Scoring in Colonoscopy Images"
                },
                "updated": "2025-12-23T13:58:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    58,
                    12,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20374v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20374v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Accurate assessment of bowel cleanliness is essential for effective colonoscopy procedures. The Boston Bowel Preparation Scale (BBPS) offers a standardized scoring system but suffers from subjectivity and inter-observer variability when performed manually. In this paper, to support robust training and evaluation, we construct a high-quality colonoscopy dataset comprising 2,240 images from 517 subjects, annotated with expert-agreed BBPS scores. We propose a novel automated BBPS scoring framework that leverages the CLIP model with adapter-based transfer learning and a dedicated fecal-feature extraction branch. Our method fuses global visual features with stool-related textual priors to improve the accuracy of bowel cleanliness evaluation without requiring explicit segmentation. Extensive experiments on both our dataset and the public NERTHU dataset demonstrate the superiority of our approach over existing baselines, highlighting its potential for clinical deployment in computer-aided colonoscopy analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate assessment of bowel cleanliness is essential for effective colonoscopy procedures. The Boston Bowel Preparation Scale (BBPS) offers a standardized scoring system but suffers from subjectivity and inter-observer variability when performed manually. In this paper, to support robust training and evaluation, we construct a high-quality colonoscopy dataset comprising 2,240 images from 517 subjects, annotated with expert-agreed BBPS scores. We propose a novel automated BBPS scoring framework that leverages the CLIP model with adapter-based transfer learning and a dedicated fecal-feature extraction branch. Our method fuses global visual features with stool-related textual priors to improve the accuracy of bowel cleanliness evaluation without requiring explicit segmentation. Extensive experiments on both our dataset and the public NERTHU dataset demonstrate the superiority of our approach over existing baselines, highlighting its potential for clinical deployment in computer-aided colonoscopy analysis."
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T13:58:12Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    58,
                    12,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "12 pages, 9 figures, BMVC 2025 submission",
                "arxiv_primary_category": {
                    "term": "eess.IV"
                },
                "authors": [
                    {
                        "name": "Yujia Fu"
                    },
                    {
                        "name": "Zhiyu Dong"
                    },
                    {
                        "name": "Tianwen Qian"
                    },
                    {
                        "name": "Chenye Zheng"
                    },
                    {
                        "name": "Danian Ji"
                    },
                    {
                        "name": "Linhai Zhuo"
                    }
                ],
                "author_detail": {
                    "name": "Linhai Zhuo"
                },
                "author": "Linhai Zhuo"
            },
            {
                "id": "http://arxiv.org/abs/2512.20365v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20365v1",
                "title": "Before We Inject: Assessing the Impact of Silica-Based Aerosols on Stratospheric Chemistry via a Kinetic Model Informed by Molecular Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Before We Inject: Assessing the Impact of Silica-Based Aerosols on Stratospheric Chemistry via a Kinetic Model Informed by Molecular Dynamics"
                },
                "updated": "2025-12-23T13:50:25Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    50,
                    25,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20365v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1021/acs.jpca.5c04880",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "Stratospheric aerosol injection (SAI) has been proposed as a geoengineering strategy to mitigate global warming by increasing Earth's albedo. Silica-based materials, such as diamond-doped silica aerogels, have shown promising optical properties, but their impact on stratospheric chemistry, ozone one in particular, remains largely unknown. Here, we present first-principles molecular dynamics (MD) simulations of the heterogeneous reaction between hydrogen chloride ($\\mathrm{HCl}$) and chlorine nitrate ($\\mathrm{ClONO_2}$), two main reservoirs of stratospheric chlorine and nitrogen species, on a dry, hydroxylated $α$-quartz silica interface. Our results reveal a barrierless reaction pathway toward the formation of chlorine gas ($\\mathrm{Cl}_2$), a major contributor to stratospheric ozone loss. We design a heterogeneous kinetic model informed by our MD simulation and available experimental data: despite the barrierless formation of $\\mathrm{Cl_2}$, the higher surface affinities and partial pressures of $\\mathrm{HNO_3}$ and $\\mathrm{HCl}$ compared to those of $\\mathrm{ClONO_2}$ result in a negligible reaction probability, $γ_\\mathrm{ClONO_2}$, upon chlorine nitrate collision with the silica surface. Since $γ_\\mathrm{ClONO_2}$ enters as a proportionality constant in the definition of the heterogeneous reaction rate, our kinetic model indicates that the injection of silica-based aerosols may have only a limited impact on stratospheric ozone depletion driven by $\\mathrm{HCl}$ and $\\mathrm{ClONO_2}$ chemistry. At the same time, our findings also underscore the scarcity of experimental data, the need of better theoretical frameworks for the inclusion of MD results into kinetic models, and the urgency for further experimental validations of silica-based SAI technologies before their deployment in climate intervention strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stratospheric aerosol injection (SAI) has been proposed as a geoengineering strategy to mitigate global warming by increasing Earth's albedo. Silica-based materials, such as diamond-doped silica aerogels, have shown promising optical properties, but their impact on stratospheric chemistry, ozone one in particular, remains largely unknown. Here, we present first-principles molecular dynamics (MD) simulations of the heterogeneous reaction between hydrogen chloride ($\\mathrm{HCl}$) and chlorine nitrate ($\\mathrm{ClONO_2}$), two main reservoirs of stratospheric chlorine and nitrogen species, on a dry, hydroxylated $α$-quartz silica interface. Our results reveal a barrierless reaction pathway toward the formation of chlorine gas ($\\mathrm{Cl}_2$), a major contributor to stratospheric ozone loss. We design a heterogeneous kinetic model informed by our MD simulation and available experimental data: despite the barrierless formation of $\\mathrm{Cl_2}$, the higher surface affinities and partial pressures of $\\mathrm{HNO_3}$ and $\\mathrm{HCl}$ compared to those of $\\mathrm{ClONO_2}$ result in a negligible reaction probability, $γ_\\mathrm{ClONO_2}$, upon chlorine nitrate collision with the silica surface. Since $γ_\\mathrm{ClONO_2}$ enters as a proportionality constant in the definition of the heterogeneous reaction rate, our kinetic model indicates that the injection of silica-based aerosols may have only a limited impact on stratospheric ozone depletion driven by $\\mathrm{HCl}$ and $\\mathrm{ClONO_2}$ chemistry. At the same time, our findings also underscore the scarcity of experimental data, the need of better theoretical frameworks for the inclusion of MD results into kinetic models, and the urgency for further experimental validations of silica-based SAI technologies before their deployment in climate intervention strategies."
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T13:50:25Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    50,
                    25,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "31 pages, 4 figures",
                "arxiv_primary_category": {
                    "term": "physics.chem-ph"
                },
                "arxiv_journal_ref": "The Journal of Physical Chemistry A 2025 129 (47), 10962-10971",
                "authors": [
                    {
                        "name": "Dennis Lima"
                    },
                    {
                        "name": "Saif Al-Kuwari"
                    },
                    {
                        "name": "Ivan Gladich"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Gladich"
                },
                "author": "Ivan Gladich",
                "arxiv_doi": "10.1021/acs.jpca.5c04880"
            },
            {
                "id": "http://arxiv.org/abs/2512.20362v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20362v1",
                "title": "CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CRAFT: Continuous Reasoning and Agentic Feedback Tuning for Multimodal Text-to-Image Generation"
                },
                "updated": "2025-12-23T13:44:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    44,
                    41,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20362v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20362v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.\n  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.\n  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work has shown that inference-time reasoning and reflection can improve text-to-image generation without retraining. However, existing approaches often rely on implicit, holistic critiques or unconstrained prompt rewrites, making their behavior difficult to interpret, control, or stop reliably. In contrast, large language models have benefited from explicit, structured forms of **thinking** based on verification, targeted correction, and early stopping.\n  We introduce CRAFT (Continuous Reasoning and Agentic Feedback Tuning), a training-free, model-agnostic framework that brings this structured reasoning paradigm to multimodal image generation. CRAFT decomposes a prompt into dependency-structured visual questions, veries generated images using a vision-language model, and applies targeted prompt edits through an LLM agent only where constraints fail. The process iterates with an explicit stopping criterion once all constraints are satised, yielding an interpretable and controllable inference-time renement loop.\n  Across multiple model families and challenging benchmarks, CRAFT consistently improves compositional accuracy, text rendering, and preference-based evaluations, with particularly strong gains for lightweight generators. Importantly, these improvements incur only a negligible inference-time overhead, allowing smaller or cheaper models to approach the quality of substantially more expensive systems. Our results suggest that explicitly structured, constraint-driven inference-time reasoning is a key ingredient for improving the reliability of multimodal generative models."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T13:44:41Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    44,
                    41,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "37 pages, 42 figures",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "V. Kovalev"
                    },
                    {
                        "name": "A. Kuvshinov"
                    },
                    {
                        "name": "A. Buzovkin"
                    },
                    {
                        "name": "D. Pokidov"
                    },
                    {
                        "name": "D. Timonin"
                    }
                ],
                "author_detail": {
                    "name": "D. Timonin"
                },
                "author": "D. Timonin"
            },
            {
                "id": "http://arxiv.org/abs/2412.11800v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2412.11800v3",
                "title": "Scalable Temporal Anomaly Causality Discovery in Large Systems: Achieving Computational Efficiency with Binary Anomaly Flag Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Temporal Anomaly Causality Discovery in Large Systems: Achieving Computational Efficiency with Binary Anomaly Flag Data"
                },
                "updated": "2025-12-23T13:43:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    43,
                    57,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2412.11800v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2412.11800v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Extracting anomaly causality facilitates diagnostics once monitoring systems detect system faults. Identifying anomaly causes in large systems involves investigating a broader set of monitoring variables across multiple subsystems. However, learning graphical causal models (GCMs) comes with a significant computational burden that restrains the applicability of most existing methods in real-time and large-scale deployments. In addition, modern monitoring applications for large systems often generate large amounts of binary alarm flags, and the distinct characteristics of binary anomaly data -- the meaning of state transition and data sparsity -- challenge existing causality learning mechanisms. This study proposes an anomaly causal discovery approach (AnomalyCD), addressing the accuracy and computational challenges of generating GCMs from temporal binary flag datasets. The AnomalyCD presents several strategies, such as anomaly data-aware causality testing, sparse data and prior link compression, and edge pruning adjustment approaches. We validate the performance of the approach on two datasets: monitoring sensor data from the readout-box system of the Compact Muon Solenoid experiment at CERN, and a public dataset from an information technology monitoring system. The results on temporal GCMs demonstrate a considerable reduction of computation overhead and a moderate enhancement of accuracy on the binary anomaly datasets Source code: https://github.com/muleina/AnomalyCD .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extracting anomaly causality facilitates diagnostics once monitoring systems detect system faults. Identifying anomaly causes in large systems involves investigating a broader set of monitoring variables across multiple subsystems. However, learning graphical causal models (GCMs) comes with a significant computational burden that restrains the applicability of most existing methods in real-time and large-scale deployments. In addition, modern monitoring applications for large systems often generate large amounts of binary alarm flags, and the distinct characteristics of binary anomaly data -- the meaning of state transition and data sparsity -- challenge existing causality learning mechanisms. This study proposes an anomaly causal discovery approach (AnomalyCD), addressing the accuracy and computational challenges of generating GCMs from temporal binary flag datasets. The AnomalyCD presents several strategies, such as anomaly data-aware causality testing, sparse data and prior link compression, and edge pruning adjustment approaches. We validate the performance of the approach on two datasets: monitoring sensor data from the readout-box system of the Compact Muon Solenoid experiment at CERN, and a public dataset from an information technology monitoring system. The results on temporal GCMs demonstrate a considerable reduction of computation overhead and a moderate enhancement of accuracy on the binary anomaly datasets Source code: https://github.com/muleina/AnomalyCD ."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-12-16T14:11:28Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    11,
                    28,
                    0,
                    351,
                    0
                ],
                "arxiv_comment": "34 pages, 17 figures, 8 tables",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Mulugeta Weldezgina Asres"
                    },
                    {
                        "name": "Christian Walter Omlin"
                    },
                    {
                        "name": "The CMS-HCAL Collaboration"
                    }
                ],
                "author_detail": {
                    "name": "The CMS-HCAL Collaboration"
                },
                "author": "The CMS-HCAL Collaboration"
            },
            {
                "id": "http://arxiv.org/abs/2512.20355v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20355v1",
                "title": "FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAR-AVIO: Fast and Robust Schur-Complement Based Acoustic-Visual-Inertial Fusion Odometry with Sensor Calibration"
                },
                "updated": "2025-12-23T13:36:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    36,
                    47,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20355v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation. While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms. Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots. FAR-AVIO embeds a Schur complement formulation into an Extended Kalman Filter(EKF), enabling joint pose-landmark optimization for accuracy while maintaining constant-time updates by efficiently marginalizing landmark states. On top of this backbone, we introduce Adaptive Weight Adjustment and Reliability Evaluation(AWARE), an online sensor health module that continuously assesses the reliability of visual, inertial and DVL measurements and adaptively regulates their sigma weights, and we develop an efficient online calibration scheme that jointly estimates DVL-IMU extrinsics, without dedicated calibration manoeuvres. Numerical simulations and real-world underwater experiments consistently show that FAR-AVIO outperforms state-of-the-art underwater SLAM baselines in both localization accuracy and computational efficiency, enabling robust operation on low-power embedded platforms. Our implementation has been released as open source software at https://far-vido.gitbook.io/far-vido-docs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underwater environments impose severe challenges to visual-inertial odometry systems, as strong light attenuation, marine snow and turbidity, together with weakly exciting motions, degrade inertial observability and cause frequent tracking failures over long-term operation. While tightly coupled acoustic-visual-inertial fusion, typically implemented through an acoustic Doppler Velocity Log (DVL) integrated with visual-inertial measurements, can provide accurate state estimation, the associated graph-based optimization is often computationally prohibitive for real-time deployment on resource-constrained platforms. Here we present FAR-AVIO, a Schur-Complement based, tightly coupled acoustic-visual-inertial odometry framework tailored for underwater robots. FAR-AVIO embeds a Schur complement formulation into an Extended Kalman Filter(EKF), enabling joint pose-landmark optimization for accuracy while maintaining constant-time updates by efficiently marginalizing landmark states. On top of this backbone, we introduce Adaptive Weight Adjustment and Reliability Evaluation(AWARE), an online sensor health module that continuously assesses the reliability of visual, inertial and DVL measurements and adaptively regulates their sigma weights, and we develop an efficient online calibration scheme that jointly estimates DVL-IMU extrinsics, without dedicated calibration manoeuvres. Numerical simulations and real-world underwater experiments consistently show that FAR-AVIO outperforms state-of-the-art underwater SLAM baselines in both localization accuracy and computational efficiency, enabling robust operation on low-power embedded platforms. Our implementation has been released as open source software at https://far-vido.gitbook.io/far-vido-docs."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T13:36:47Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    36,
                    47,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Hao Wei"
                    },
                    {
                        "name": "Peiji Wang"
                    },
                    {
                        "name": "Qianhao Wang"
                    },
                    {
                        "name": "Tong Qin"
                    },
                    {
                        "name": "Fei Gao"
                    },
                    {
                        "name": "Yulin Si"
                    }
                ],
                "author_detail": {
                    "name": "Yulin Si"
                },
                "author": "Yulin Si"
            },
            {
                "id": "http://arxiv.org/abs/2512.20352v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20352v1",
                "title": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-LLM Thematic Analysis with Dual Reliability Metrics: Combining Cohen's Kappa and Semantic Similarity for Qualitative Research Validation"
                },
                "updated": "2025-12-23T13:32:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    32,
                    43,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20352v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20352v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($κ$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($κ= 0.907$, cosine=95.3%), followed by GPT-4o ($κ= 0.853$, cosine=92.6%) and Claude ($κ= 0.842$, cosine=92.1%). All three models achieve a high agreement ($κ> 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qualitative research faces a critical reliability challenge: traditional inter-rater agreement methods require multiple human coders, are time-intensive, and often yield moderate consistency. We present a multi-perspective validation framework for LLM-based thematic analysis that combines ensemble validation with dual reliability metrics: Cohen's Kappa ($κ$) for inter-rater agreement and cosine similarity for semantic consistency. Our framework enables configurable analysis parameters (1-6 seeds, temperature 0.0-2.0), supports custom prompt structures with variable substitution, and provides consensus theme extraction across any JSON format. As proof-of-concept, we evaluate three leading LLMs (Gemini 2.5 Pro, GPT-4o, Claude 3.5 Sonnet) on a psychedelic art therapy interview transcript, conducting six independent runs per model. Results demonstrate Gemini achieves highest reliability ($κ= 0.907$, cosine=95.3%), followed by GPT-4o ($κ= 0.853$, cosine=92.6%) and Claude ($κ= 0.842$, cosine=92.1%). All three models achieve a high agreement ($κ> 0.80$), validating the multi-run ensemble approach. The framework successfully extracts consensus themes across runs, with Gemini identifying 6 consensus themes (50-83% consistency), GPT-4o identifying 5 themes, and Claude 4 themes. Our open-source implementation provides researchers with transparent reliability metrics, flexible configuration, and structure-agnostic consensus extraction, establishing methodological foundations for reliable AI-assisted qualitative research."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T13:32:43Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    32,
                    43,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "11 pages, 1 figure, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Nilesh Jain"
                    },
                    {
                        "name": "Seyi Adeyinka"
                    },
                    {
                        "name": "Leor Roseman"
                    },
                    {
                        "name": "Aza Allsop"
                    }
                ],
                "author_detail": {
                    "name": "Aza Allsop"
                },
                "author": "Aza Allsop"
            },
            {
                "id": "http://arxiv.org/abs/2512.20344v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20344v1",
                "title": "A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice"
                },
                "updated": "2025-12-23T13:26:13Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    26,
                    13,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20344v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT07117266). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating reliable detection of six clinically critical radiographic findings. Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores, reduced interpretation time by 18.3% (P < 0.001), and was preferred by a majority of experts in 54.3% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T13:26:13Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    26,
                    13,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2507.19493",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yaowei Bai"
                    },
                    {
                        "name": "Ruiheng Zhang"
                    },
                    {
                        "name": "Yu Lei"
                    },
                    {
                        "name": "Xuhua Duan"
                    },
                    {
                        "name": "Jingfeng Yao"
                    },
                    {
                        "name": "Shuguang Ju"
                    },
                    {
                        "name": "Chaoyang Wang"
                    },
                    {
                        "name": "Wei Yao"
                    },
                    {
                        "name": "Yiwan Guo"
                    },
                    {
                        "name": "Guilin Zhang"
                    },
                    {
                        "name": "Chao Wan"
                    },
                    {
                        "name": "Qian Yuan"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Wenjuan Tang"
                    },
                    {
                        "name": "Biqiang Zhu"
                    },
                    {
                        "name": "Xinggang Wang"
                    },
                    {
                        "name": "Tao Sun"
                    },
                    {
                        "name": "Wei Zhou"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Yongchao Xu"
                    },
                    {
                        "name": "Chuansheng Zheng"
                    },
                    {
                        "name": "Huangxuan Zhao"
                    },
                    {
                        "name": "Bo Du"
                    }
                ],
                "author_detail": {
                    "name": "Bo Du"
                },
                "author": "Bo Du"
            },
            {
                "id": "http://arxiv.org/abs/2512.20333v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20333v1",
                "title": "SynCraft: Guiding Large Language Models to Predict Edit Sequences for Molecular Synthesizability Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SynCraft: Guiding Large Language Models to Predict Edit Sequences for Molecular Synthesizability Optimization"
                },
                "updated": "2025-12-23T13:07:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    7,
                    22,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20333v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20333v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Generative artificial intelligence has revolutionized the exploration of chemical space, yet a critical bottleneck remains that a substantial fraction of generated molecules is synthetically inaccessible. Current solutions, such as post-hoc filtering or projection-based methods, often compromise structural novelty or disrupt key pharmacophores by forcing molecules into pre-defined synthetic templates. Herein, we introduce SynCraft, a reasoning-based framework that reframes synthesizability optimization not as a sequence translation task, but as a precise structural editing problem. Leveraging the emergent reasoning capabilities of Large Language Models, SynCraft navigates the \"synthesis cliff\" where minimal structural modifications yield significant gains in synthetic feasibility. By predicting executable sequences of atom-level edits rather than generating SMILES strings directly, SynCraft circumvents the syntactic fragility of LLMs while harnessing their chemical intuition. Extensive benchmarks demonstrate that SynCraft outperforms state-of-the-art baselines in generating synthesizable analogs with high structural fidelity. Furthermore, through interaction-aware prompting, SynCraft successfully replicates expert medicinal chemistry intuition in editing PLK1 inhibitors and rescuing high-scoring but previously discarded RIPK1 candidates in previous molecular generation literatures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence has revolutionized the exploration of chemical space, yet a critical bottleneck remains that a substantial fraction of generated molecules is synthetically inaccessible. Current solutions, such as post-hoc filtering or projection-based methods, often compromise structural novelty or disrupt key pharmacophores by forcing molecules into pre-defined synthetic templates. Herein, we introduce SynCraft, a reasoning-based framework that reframes synthesizability optimization not as a sequence translation task, but as a precise structural editing problem. Leveraging the emergent reasoning capabilities of Large Language Models, SynCraft navigates the \"synthesis cliff\" where minimal structural modifications yield significant gains in synthetic feasibility. By predicting executable sequences of atom-level edits rather than generating SMILES strings directly, SynCraft circumvents the syntactic fragility of LLMs while harnessing their chemical intuition. Extensive benchmarks demonstrate that SynCraft outperforms state-of-the-art baselines in generating synthesizable analogs with high structural fidelity. Furthermore, through interaction-aware prompting, SynCraft successfully replicates expert medicinal chemistry intuition in editing PLK1 inhibitors and rescuing high-scoring but previously discarded RIPK1 candidates in previous molecular generation literatures."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T13:07:22Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    13,
                    7,
                    22,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "28 pages, 4 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Junren Li"
                    },
                    {
                        "name": "Luhua Lai"
                    }
                ],
                "author_detail": {
                    "name": "Luhua Lai"
                },
                "author": "Luhua Lai"
            },
            {
                "id": "http://arxiv.org/abs/2512.20328v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20328v1",
                "title": "Toward Explaining Large Language Models in Software Engineering Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Explaining Large Language Models in Software Engineering Tasks"
                },
                "updated": "2025-12-23T12:56:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    56,
                    18,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20328v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20328v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in Large Language Models (LLMs) has substantially advanced the automation of software engineering (SE) tasks, enabling complex activities such as code generation and code summarization. However, the black-box nature of LLMs remains a major barrier to their adoption in high-stakes and safety-critical domains, where explainability and transparency are vital for trust, accountability, and effective human supervision. Despite increasing interest in explainable AI for software engineering, existing methods lack domain-specific explanations aligned with how practitioners reason about SE artifacts. To address this gap, we introduce FeatureSHAP, the first fully automated, model-agnostic explainability framework tailored to software engineering tasks. Based on Shapley values, FeatureSHAP attributes model outputs to high-level input features through systematic input perturbation and task-specific similarity comparisons, while remaining compatible with both open-source and proprietary LLMs. We evaluate FeatureSHAP on two bi-modal SE tasks: code generation and code summarization. The results show that FeatureSHAP assigns less importance to irrelevant input features and produces explanations with higher fidelity than baseline methods. A practitioner survey involving 37 participants shows that FeatureSHAP helps practitioners better interpret model outputs and make more informed decisions. Collectively, FeatureSHAP represents a meaningful step toward practical explainable AI in software engineering. FeatureSHAP is available at https://github.com/deviserlab/FeatureSHAP."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:56:18Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    56,
                    18,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Antonio Vitale"
                    },
                    {
                        "name": "Khai-Nguyen Nguyen"
                    },
                    {
                        "name": "Denys Poshyvanyk"
                    },
                    {
                        "name": "Rocco Oliveto"
                    },
                    {
                        "name": "Simone Scalabrino"
                    },
                    {
                        "name": "Antonio Mastropaolo"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Mastropaolo"
                },
                "author": "Antonio Mastropaolo"
            },
            {
                "id": "http://arxiv.org/abs/2512.20324v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20324v1",
                "title": "Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can LLMs Solve My Grandma's Riddle? Evaluating Multilingual Large Language Models on Reasoning Traditional Bangla Tricky Riddles"
                },
                "updated": "2025-12-23T12:48:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    48,
                    5,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20324v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show impressive performance on many NLP benchmarks, yet their ability to reason in figurative, culturally grounded, and low-resource settings remains underexplored. We address this gap for Bangla by introducing BanglaRiddleEval, a benchmark of 1,244 traditional Bangla riddles instantiated across four tasks (4,976 riddle-task artifacts in total). Using an LLM-based pipeline, we generate Chain-of-Thought explanations, semantically coherent distractors, and fine-grained ambiguity annotations, and evaluate a diverse suite of open-source and closed-source models under different prompting strategies. Models achieve moderate semantic overlap on generative QA but low correctness, MCQ accuracy peaks at only about 56% versus an 83% human baseline, and ambiguity resolution ranges from roughly 26% to 68%, with high-quality explanations confined to the strongest models. These results show that current LLMs capture some cues needed for Bangla riddle reasoning but remain far from human-level performance, establishing BanglaRiddleEval as a challenging new benchmark for low-resource figurative reasoning. All data, code, and evaluation scripts are available on GitHub: https://github.com/Labib1610/BanglaRiddleEval."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:48:05Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    48,
                    5,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Nurul Labib Sayeedi"
                    },
                    {
                        "name": "Md. Faiyaz Abdullah Sayeedi"
                    },
                    {
                        "name": "Khushnur Binte Jahangir"
                    },
                    {
                        "name": "Swakkhar Shatabda"
                    },
                    {
                        "name": "Sarah Masud Preum"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Masud Preum"
                },
                "author": "Sarah Masud Preum"
            },
            {
                "id": "http://arxiv.org/abs/2512.20323v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20323v1",
                "title": "Differentially Private Feature Release for Wireless Sensing: Adaptive Privacy Budget Allocation on CSI Spectrograms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Differentially Private Feature Release for Wireless Sensing: Adaptive Privacy Budget Allocation on CSI Spectrograms"
                },
                "updated": "2025-12-23T12:45:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    45,
                    49,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20323v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20323v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Wi-Fi/RF-based human sensing has achieved remarkable progress with deep learning, yet practical deployments increasingly require feature sharing for cloud analytics, collaborative training, or benchmark evaluation. Releasing intermediate representations such as CSI spectrograms can inadvertently expose sensitive information, including user identity, location, and membership, motivating formal privacy guarantees. In this paper, we study differentially private (DP) feature release for wireless sensing and propose an adaptive privacy budget allocation mechanism tailored to the highly non-uniform structure of CSI time-frequency representations. Our pipeline converts CSI to bounded spectrogram features, applies sensitivity control via clipping, estimates task-relevant importance over the time-frequency plane, and allocates a global privacy budget across spectrogram blocks before injecting calibrated Gaussian noise. Experiments on multi-user activity sensing (WiMANS), multi-person 3D pose estimation (Person-in-WiFi 3D), and respiration monitoring (Resp-CSI) show that adaptive allocation consistently improves the privacy-utility frontier over uniform perturbation under the same privacy budget. Our method yields higher accuracy and lower error while substantially reducing empirical leakage in identity and membership inference attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wi-Fi/RF-based human sensing has achieved remarkable progress with deep learning, yet practical deployments increasingly require feature sharing for cloud analytics, collaborative training, or benchmark evaluation. Releasing intermediate representations such as CSI spectrograms can inadvertently expose sensitive information, including user identity, location, and membership, motivating formal privacy guarantees. In this paper, we study differentially private (DP) feature release for wireless sensing and propose an adaptive privacy budget allocation mechanism tailored to the highly non-uniform structure of CSI time-frequency representations. Our pipeline converts CSI to bounded spectrogram features, applies sensitivity control via clipping, estimates task-relevant importance over the time-frequency plane, and allocates a global privacy budget across spectrogram blocks before injecting calibrated Gaussian noise. Experiments on multi-user activity sensing (WiMANS), multi-person 3D pose estimation (Person-in-WiFi 3D), and respiration monitoring (Resp-CSI) show that adaptive allocation consistently improves the privacy-utility frontier over uniform perturbation under the same privacy budget. Our method yields higher accuracy and lower error while substantially reducing empirical leakage in identity and membership inference attacks."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:45:49Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    45,
                    49,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "19pages,4figures",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Ipek Sena Yilmaz"
                    },
                    {
                        "name": "Onur G. Tuncer"
                    },
                    {
                        "name": "Zeynep E. Aksoy"
                    },
                    {
                        "name": "Zeynep Yağmur Baydemir"
                    }
                ],
                "author_detail": {
                    "name": "Zeynep Yağmur Baydemir"
                },
                "author": "Zeynep Yağmur Baydemir"
            },
            {
                "id": "http://arxiv.org/abs/2512.20312v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20312v1",
                "title": "TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TableGPT-R1: Advancing Tabular Reasoning Through Reinforcement Learning"
                },
                "updated": "2025-12-23T12:30:37Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    30,
                    37,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20312v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce \\textbf{TableGPT-R1}, a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tabular data serves as the backbone of modern data analysis and scientific research. While Large Language Models (LLMs) fine-tuned via Supervised Fine-Tuning (SFT) have significantly improved natural language interaction with such structured data, they often fall short in handling the complex, multi-step reasoning and robust code execution required for real-world table tasks. Reinforcement Learning (RL) offers a promising avenue to enhance these capabilities, yet its application in the tabular domain faces three critical hurdles: the scarcity of high-quality agentic trajectories with closed-loop code execution and environment feedback on diverse table structures, the extreme heterogeneity of feedback signals ranging from rigid SQL execution to open-ended data interpretation, and the risk of catastrophic forgetting of general knowledge during vertical specialization. To overcome these challenges and unlock advanced reasoning on complex tables, we introduce \\textbf{TableGPT-R1}, a specialized tabular model built on a systematic RL framework. Our approach integrates a comprehensive data engineering pipeline that synthesizes difficulty-stratified agentic trajectories for both supervised alignment and RL rollouts, a task-adaptive reward system that combines rule-based verification with a criteria-injected reward model and incorporates process-level step reward shaping with behavioral regularization, and a multi-stage training framework that progressively stabilizes reasoning before specializing in table-specific tasks. Extensive evaluations demonstrate that TableGPT-R1 achieves state-of-the-art performance on authoritative benchmarks, significantly outperforming baseline models while retaining robust general capabilities. Our model is available at https://huggingface.co/tablegpt/TableGPT-R1."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:30:37Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    30,
                    37,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Saisai Yang"
                    },
                    {
                        "name": "Qingyi Huang"
                    },
                    {
                        "name": "Jing Yuan"
                    },
                    {
                        "name": "Liangyu Zha"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Yuhang Yang"
                    },
                    {
                        "name": "Ning Wang"
                    },
                    {
                        "name": "Yucheng Wei"
                    },
                    {
                        "name": "Liyao Li"
                    },
                    {
                        "name": "Wentao Ye"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Junlin Zhou"
                    },
                    {
                        "name": "Haobo Wang"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Junbo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Junbo Zhao"
                },
                "author": "Junbo Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2509.01564v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.01564v2",
                "title": "Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Uncertainty Estimation in LLMs with Expectation of Aggregated Internal Belief"
                },
                "updated": "2025-12-23T12:29:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    29,
                    17,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.01564v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.01564v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language tasks, but often exhibit overconfidence and generate plausible yet incorrect answers. This overconfidence, especially in models undergone Reinforcement Learning from Human Feedback (RLHF), poses significant challenges for reliable uncertainty estimation and safe deployment. In this paper, we propose EAGLE (Expectation of AGgregated internaL bEief), a novel self-evaluation-based calibration method that leverages the internal hidden states of LLMs to derive more accurate confidence scores. Instead of relying on the model's final output, our approach extracts internal beliefs from multiple intermediate layers during self-evaluation. By aggregating these layer-wise beliefs and calculating the expectation over the resulting confidence score distribution, EAGLE produces a refined confidence score that more faithfully reflects the model's internal certainty. Extensive experiments on diverse datasets and LLMs demonstrate that EAGLE significantly improves calibration performance over existing baselines. We also provide an in-depth analysis of EAGLE, including a layer-wise examination of uncertainty patterns, a study of the impact of self-evaluation prompts, and an analysis of the effect of self-evaluation score range.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language tasks, but often exhibit overconfidence and generate plausible yet incorrect answers. This overconfidence, especially in models undergone Reinforcement Learning from Human Feedback (RLHF), poses significant challenges for reliable uncertainty estimation and safe deployment. In this paper, we propose EAGLE (Expectation of AGgregated internaL bEief), a novel self-evaluation-based calibration method that leverages the internal hidden states of LLMs to derive more accurate confidence scores. Instead of relying on the model's final output, our approach extracts internal beliefs from multiple intermediate layers during self-evaluation. By aggregating these layer-wise beliefs and calculating the expectation over the resulting confidence score distribution, EAGLE produces a refined confidence score that more faithfully reflects the model's internal certainty. Extensive experiments on diverse datasets and LLMs demonstrate that EAGLE significantly improves calibration performance over existing baselines. We also provide an in-depth analysis of EAGLE, including a layer-wise examination of uncertainty patterns, a study of the impact of self-evaluation prompts, and an analysis of the effect of self-evaluation score range."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-01T15:50:10Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    15,
                    50,
                    10,
                    0,
                    244,
                    0
                ],
                "arxiv_comment": "Accepted by AAAI 2026",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zeguan Xiao"
                    },
                    {
                        "name": "Diyang Dou"
                    },
                    {
                        "name": "Boya Xiong"
                    },
                    {
                        "name": "Yun Chen"
                    },
                    {
                        "name": "Guanhua Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guanhua Chen"
                },
                "author": "Guanhua Chen"
            },
            {
                "id": "http://arxiv.org/abs/2509.14844v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2509.14844v2",
                "title": "Non-Intrusive Parametrized-Background Data-Weak Reconstruction of Cardiac Displacement Fields from Sparse MRI-like Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-Intrusive Parametrized-Background Data-Weak Reconstruction of Cardiac Displacement Fields from Sparse MRI-like Observations"
                },
                "updated": "2025-12-23T12:08:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    8,
                    33,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2509.14844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2509.14844v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Personalized cardiac diagnostics require accurate reconstruction of myocardial displacement fields from sparse clinical imaging data, yet current methods often demand intrusive access to computational models. In this work, we apply the non-intrusive Parametrized-Background Data-Weak (PBDW) approach to three-dimensional (3D) cardiac displacement field reconstruction from limited Magnetic Resonance Image (MRI)-like observations. Our implementation requires only solution snapshots -- no governing equations, assembly routines, or solver access -- enabling immediate deployment across commercial and research codes using different constitutive models. Additionally, we introduce two enhancements: an H-size minibatch worst-case Orthogonal Matching Pursuit (wOMP) algorithm that improves Sensor Selection (SS) computational efficiency while maintaining reconstruction accuracy, and memory optimization techniques exploiting block matrix structures in vectorial problems. We demonstrate the effectiveness of the method through validation on a 3D left ventricular model with simulated scar tissue. Starting with noise-free reconstruction, we systematically incorporate Gaussian noise and spatial sparsity mimicking realistic MRI acquisition protocols. Results show exceptional accuracy in noise-free conditions (relative L2 error of order O(1e-5)), robust performance with 10% noise (relative L2 error of order O(1e-2)), and effective reconstruction from sparse measurements (relative L2 error of order O(1e-2)). The online reconstruction achieves four-order-of-magnitude computational speed-up compared to full Finite Element (FE) simulations, with reconstruction times under one tenth of second for sparse scenarios, demonstrating significant potential for integration into clinical cardiac modeling workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized cardiac diagnostics require accurate reconstruction of myocardial displacement fields from sparse clinical imaging data, yet current methods often demand intrusive access to computational models. In this work, we apply the non-intrusive Parametrized-Background Data-Weak (PBDW) approach to three-dimensional (3D) cardiac displacement field reconstruction from limited Magnetic Resonance Image (MRI)-like observations. Our implementation requires only solution snapshots -- no governing equations, assembly routines, or solver access -- enabling immediate deployment across commercial and research codes using different constitutive models. Additionally, we introduce two enhancements: an H-size minibatch worst-case Orthogonal Matching Pursuit (wOMP) algorithm that improves Sensor Selection (SS) computational efficiency while maintaining reconstruction accuracy, and memory optimization techniques exploiting block matrix structures in vectorial problems. We demonstrate the effectiveness of the method through validation on a 3D left ventricular model with simulated scar tissue. Starting with noise-free reconstruction, we systematically incorporate Gaussian noise and spatial sparsity mimicking realistic MRI acquisition protocols. Results show exceptional accuracy in noise-free conditions (relative L2 error of order O(1e-5)), robust performance with 10% noise (relative L2 error of order O(1e-2)), and effective reconstruction from sparse measurements (relative L2 error of order O(1e-2)). The online reconstruction achieves four-order-of-magnitude computational speed-up compared to full Finite Element (FE) simulations, with reconstruction times under one tenth of second for sparse scenarios, demonstrating significant potential for integration into clinical cardiac modeling workflows."
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-09-18T11:10:24Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    11,
                    10,
                    24,
                    3,
                    261,
                    0
                ],
                "arxiv_comment": "42 pages, 12 figures, 6 tables",
                "arxiv_primary_category": {
                    "term": "physics.med-ph"
                },
                "authors": [
                    {
                        "name": "Francesco C. Mantegazza"
                    },
                    {
                        "name": "Federica Caforio"
                    },
                    {
                        "name": "Christoph Augustin"
                    },
                    {
                        "name": "Matthias A. F. Gsell"
                    },
                    {
                        "name": "Gundolf Haase"
                    },
                    {
                        "name": "Elias Karabelas"
                    }
                ],
                "author_detail": {
                    "name": "Elias Karabelas"
                },
                "author": "Elias Karabelas"
            },
            {
                "id": "http://arxiv.org/abs/2512.20299v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20299v1",
                "title": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KnowVal: A Knowledge-Augmented and Value-Guided Autonomous Driving System"
                },
                "updated": "2025-12-23T12:08:00Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    8,
                    0,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20299v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual-language reasoning, driving knowledge, and value alignment are essential for advanced autonomous driving systems. However, existing approaches largely rely on data-driven learning, making it difficult to capture the complex logic underlying decision-making through imitation or limited reinforcement rewards. To address this, we propose KnowVal, a new autonomous driving system that enables visual-language reasoning through the synergistic integration of open-world perception and knowledge retrieval. Specifically, we construct a comprehensive driving knowledge graph that encodes traffic laws, defensive driving principles, and ethical norms, complemented by an efficient LLM-based retrieval mechanism tailored for driving scenarios. Furthermore, we develop a human-preference dataset and train a Value Model to guide interpretable, value-aligned trajectory assessment. Experimental results show that our method substantially improves planning performance while remaining compatible with existing architectures. Notably, KnowVal achieves the lowest collision rate on nuScenes and state-of-the-art results on Bench2Drive."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:08:00Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    8,
                    0,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Zhongyu Xia"
                    },
                    {
                        "name": "Wenhao Chen"
                    },
                    {
                        "name": "Yongtao Wang"
                    },
                    {
                        "name": "Ming-Hsuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Hsuan Yang"
                },
                "author": "Ming-Hsuan Yang"
            },
            {
                "id": "http://arxiv.org/abs/2512.20298v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20298v1",
                "title": "Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patterns vs. Patients: Evaluating LLMs against Mental Health Professionals on Personality Disorder Diagnosis through First-Person Narratives"
                },
                "updated": "2025-12-23T12:05:01Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    5,
                    1,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20298v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term \"narcissism.\" Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Growing reliance on LLMs for psychiatric self-assessment raises questions about their ability to interpret qualitative patient narratives. We present the first direct comparison between state-of-the-art LLMs and mental health professionals in diagnosing Borderline (BPD) and Narcissistic (NPD) Personality Disorders utilizing Polish-language first-person autobiographical accounts. We show that the top-performing Gemini Pro models surpassed human professionals in overall diagnostic accuracy by 21.91 percentage points (65.48% vs. 43.57%). While both models and human experts excelled at identifying BPD (F1 = 83.4 & F1 = 80.0, respectively), models severely underdiagnosed NPD (F1 = 6.7 vs. 50.0), showing a reluctance toward the value-laden term \"narcissism.\" Qualitatively, models provided confident, elaborate justifications focused on patterns and formal categories, while human experts remained concise and cautious, emphasizing the patient's sense of self and temporal experience. Our findings demonstrate that while LLMs are highly competent at interpreting complex first-person clinical data, they remain subject to critical reliability and bias issues."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:05:01Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    5,
                    1,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Karolina Drożdż"
                    },
                    {
                        "name": "Kacper Dudzic"
                    },
                    {
                        "name": "Anna Sterna"
                    },
                    {
                        "name": "Marcin Moskalewicz"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Moskalewicz"
                },
                "author": "Marcin Moskalewicz"
            },
            {
                "id": "http://arxiv.org/abs/2512.20293v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20293v1",
                "title": "AprielGuard",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AprielGuard"
                },
                "updated": "2025-12-23T12:01:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    1,
                    32,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20293v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safeguarding large language models (LLMs) against unsafe or adversarial behavior is critical as they are increasingly deployed in conversational and agentic settings. Existing moderation tools often treat safety risks (e.g. toxicity, bias) and adversarial threats (e.g. prompt injections, jailbreaks) as separate problems, limiting their robustness and generalizability. We introduce AprielGuard, an 8B parameter safeguard model that unify these dimensions within a single taxonomy and learning framework. AprielGuard is trained on a diverse mix of open and synthetic data covering standalone prompts, multi-turn conversations, and agentic workflows, augmented with structured reasoning traces to improve interpretability. Across multiple public and proprietary benchmarks, AprielGuard achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing opensource guardrails such as Llama-Guard and Granite Guardian, particularly in multi-step and reasoning intensive scenarios. By releasing the model, we aim to advance transparent and reproducible research on reliable safeguards for LLMs."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T12:01:32Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    12,
                    1,
                    32,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Jaykumar Kasundra"
                    },
                    {
                        "name": "Anjaneya Praharaj"
                    },
                    {
                        "name": "Sourabh Surana"
                    },
                    {
                        "name": "Lakshmi Sirisha Chodisetty"
                    },
                    {
                        "name": "Sourav Sharma"
                    },
                    {
                        "name": "Abhigya Verma"
                    },
                    {
                        "name": "Abhishek Bhardwaj"
                    },
                    {
                        "name": "Debasish Kanhar"
                    },
                    {
                        "name": "Aakash Bhagat"
                    },
                    {
                        "name": "Khalil Slimi"
                    },
                    {
                        "name": "Seganrasan Subramanian"
                    },
                    {
                        "name": "Sathwik Tejaswi Madhusudhan"
                    },
                    {
                        "name": "Ranga Prasad Chenna"
                    },
                    {
                        "name": "Srinivas Sunkara"
                    }
                ],
                "author_detail": {
                    "name": "Srinivas Sunkara"
                },
                "author": "Srinivas Sunkara"
            },
            {
                "id": "http://arxiv.org/abs/2512.20276v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20276v1",
                "title": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ActionFlow: A Pipelined Action Acceleration for Vision Language Models on Edge"
                },
                "updated": "2025-12-23T11:29:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    29,
                    3,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20276v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have emerged as a unified paradigm for robotic perception and control, enabling emergent generalization and long-horizon task execution. However, their deployment in dynamic, real-world environments is severely hin dered by high inference latency. While smooth robotic interaction requires control frequencies of 20 to 30 Hz, current VLA models typi cally operate at only 3-5 Hz on edge devices due to the memory bound nature of autoregressive decoding. Existing optimizations often require extensive retraining or compromise model accuracy. To bridge this gap, we introduce ActionFlow, a system-level inference framework tailored for resource-constrained edge plat forms. At the core of ActionFlow is a Cross-Request Pipelin ing strategy, a novel scheduler that redefines VLA inference as a macro-pipeline of micro-requests. The strategy intelligently batches memory-bound Decode phases with compute-bound Prefill phases across continuous time steps to maximize hardware utilization. Furthermore, to support this scheduling, we propose a Cross Request State Packed Forward operator and a Unified KV Ring Buffer, which fuse fragmented memory operations into efficient dense computations. Experimental results demonstrate that ActionFlow achieves a 2.55x improvement in FPS on the OpenVLA-7B model without retraining, enabling real-time dy namic manipulation on edge hardware. Our work is available at https://anonymous.4open.science/r/ActionFlow-1D47."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T11:29:03Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    29,
                    3,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Yuntao Dai"
                    },
                    {
                        "name": "Hang Gu"
                    },
                    {
                        "name": "Teng Wang"
                    },
                    {
                        "name": "Qianyu Cheng"
                    },
                    {
                        "name": "Yifei Zheng"
                    },
                    {
                        "name": "Zhiyong Qiu"
                    },
                    {
                        "name": "Lei Gong"
                    },
                    {
                        "name": "Wenqi Lou"
                    },
                    {
                        "name": "Xuehai Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xuehai Zhou"
                },
                "author": "Xuehai Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2512.20275v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20275v1",
                "title": "Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-Symbolic Policy Enforcement and Control (G-SPEC): A Neuro-Symbolic Framework for Safe Agentic AI in 5G Autonomous Networks"
                },
                "updated": "2025-12-23T11:27:17Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    27,
                    17,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20275v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As networks evolve toward 5G Standalone and 6G, operators face orchestration challenges that exceed the limits of static automation and Deep Reinforcement Learning. Although Large Language Model (LLM) agents offer a path toward intent-based networking, they introduce stochastic risks, including topology hallucinations and policy non-compliance. To mitigate this, we propose Graph-Symbolic Policy Enforcement and Control (G-SPEC), a neuro-symbolic framework that constrains probabilistic planning with deterministic verification. The architecture relies on a Governance Triad - a telecom-adapted agent (TSLAM-4B), a Network Knowledge Graph (NKG), and SHACL constraints. We evaluated G-SPEC on a simulated 450-node 5G Core, achieving zero safety violations and a 94.1% remediation success rate, significantly outperforming the 82.4% baseline. Ablation analysis indicates that NKG validation drives the majority of safety gains (68%), followed by SHACL policies (24%). Scalability tests on topologies ranging from 10K to 100K nodes demonstrate that validation latency scales as $O(k^{1.2})$ where $k$ is subgraph size. With a processing overhead of 142ms, G-SPEC is viable for SMO-layer operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As networks evolve toward 5G Standalone and 6G, operators face orchestration challenges that exceed the limits of static automation and Deep Reinforcement Learning. Although Large Language Model (LLM) agents offer a path toward intent-based networking, they introduce stochastic risks, including topology hallucinations and policy non-compliance. To mitigate this, we propose Graph-Symbolic Policy Enforcement and Control (G-SPEC), a neuro-symbolic framework that constrains probabilistic planning with deterministic verification. The architecture relies on a Governance Triad - a telecom-adapted agent (TSLAM-4B), a Network Knowledge Graph (NKG), and SHACL constraints. We evaluated G-SPEC on a simulated 450-node 5G Core, achieving zero safety violations and a 94.1% remediation success rate, significantly outperforming the 82.4% baseline. Ablation analysis indicates that NKG validation drives the majority of safety gains (68%), followed by SHACL policies (24%). Scalability tests on topologies ranging from 10K to 100K nodes demonstrate that validation latency scales as $O(k^{1.2})$ where $k$ is subgraph size. With a processing overhead of 142ms, G-SPEC is viable for SMO-layer operations."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T11:27:17Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    27,
                    17,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "15 pages, 3 figures, 3 tables",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Divya Vijay"
                    },
                    {
                        "name": "Vignesh Ethiraj"
                    }
                ],
                "author_detail": {
                    "name": "Vignesh Ethiraj"
                },
                "author": "Vignesh Ethiraj"
            },
            {
                "id": "http://arxiv.org/abs/2512.20268v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20268v1",
                "title": "DeepONet-accelerated Bayesian inversion for moving boundary problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepONet-accelerated Bayesian inversion for moving boundary problems"
                },
                "updated": "2025-12-23T11:22:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    22,
                    26,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20268v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "This work demonstrates that neural operator learning provides a powerful and flexible framework for building fast, accurate emulators of moving boundary systems, enabling their integration into digital twin platforms. To this end, a Deep Operator Network (DeepONet) architecture is employed to construct an efficient surrogate model for moving boundary problems in single-phase Darcy flow through porous media. The surrogate enables rapid and accurate approximation of complex flow dynamics and is coupled with an Ensemble Kalman Inversion (EKI) algorithm to solve Bayesian inverse problems.\n  The proposed inversion framework is demonstrated by estimating the permeability and porosity of fibre reinforcements for composite materials manufactured via the Resin Transfer Moulding (RTM) process. Using both synthetic and experimental in-process data, the DeepONet surrogate accelerates inversion by several orders of magnitude compared with full-model EKI. This computational efficiency enables real-time, accurate, high-resolution estimation of local variations in permeability, porosity, and other parameters, thereby supporting effective monitoring and control of RTM processes, as well as other applications involving moving boundary flows. Unlike prior approaches for RTM inversion that learn mesh-dependent mappings, the proposed neural operator generalises across spatial and temporal domains, enabling evaluation at arbitrary sensor configurations without retraining, and represents a significant step toward practical industrial deployment of digital twins.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work demonstrates that neural operator learning provides a powerful and flexible framework for building fast, accurate emulators of moving boundary systems, enabling their integration into digital twin platforms. To this end, a Deep Operator Network (DeepONet) architecture is employed to construct an efficient surrogate model for moving boundary problems in single-phase Darcy flow through porous media. The surrogate enables rapid and accurate approximation of complex flow dynamics and is coupled with an Ensemble Kalman Inversion (EKI) algorithm to solve Bayesian inverse problems.\n  The proposed inversion framework is demonstrated by estimating the permeability and porosity of fibre reinforcements for composite materials manufactured via the Resin Transfer Moulding (RTM) process. Using both synthetic and experimental in-process data, the DeepONet surrogate accelerates inversion by several orders of magnitude compared with full-model EKI. This computational efficiency enables real-time, accurate, high-resolution estimation of local variations in permeability, porosity, and other parameters, thereby supporting effective monitoring and control of RTM processes, as well as other applications involving moving boundary flows. Unlike prior approaches for RTM inversion that learn mesh-dependent mappings, the proposed neural operator generalises across spatial and temporal domains, enabling evaluation at arbitrary sensor configurations without retraining, and represents a significant step toward practical industrial deployment of digital twins."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T11:22:26Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    22,
                    26,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Marco A. Iglesias"
                    },
                    {
                        "name": "Michael. E. Causon"
                    },
                    {
                        "name": "Mikhail Y. Matveev"
                    },
                    {
                        "name": "Andreas Endruweit"
                    },
                    {
                        "name": "Michael . V. Tretyakov"
                    }
                ],
                "author_detail": {
                    "name": "Michael . V. Tretyakov"
                },
                "author": "Michael . V. Tretyakov"
            },
            {
                "id": "http://arxiv.org/abs/2512.20264v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20264v1",
                "title": "The AI Scaling Wall of Diminishing Returns: Of LLMs, Electric Dogs, and General Relativity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI Scaling Wall of Diminishing Returns: Of LLMs, Electric Dogs, and General Relativity"
                },
                "updated": "2025-12-23T11:18:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    18,
                    3,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20264v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLMs are hitting the scaling wall - compute grows 10-100x while accuracy barely moves. This note quantifies the slowdown and argues that the next leap in AI will come not from bigger models, but from smarter, more efficient ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs are hitting the scaling wall - compute grows 10-100x while accuracy barely moves. This note quantifies the slowdown and argues that the next leap in AI will come not from bigger models, but from smarter, more efficient ones."
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T11:18:03Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    18,
                    3,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM"
                },
                "authors": [
                    {
                        "name": "Hemant Shukla"
                    }
                ],
                "author_detail": {
                    "name": "Hemant Shukla"
                },
                "author": "Hemant Shukla"
            },
            {
                "id": "http://arxiv.org/abs/2512.19126v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19126v2",
                "title": "AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AWPO: Enhancing Tool-Use of Large Language Models through Explicit Integration of Reasoning Rewards"
                },
                "updated": "2025-12-23T11:15:52Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    11,
                    15,
                    52,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19126v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While reinforcement learning (RL) shows promise in training tool-use large language models (LLMs) using verifiable outcome rewards, existing methods largely overlook the potential of explicit reasoning rewards to bolster reasoning and tool utilization. Furthermore, natively combining reasoning and outcome rewards may yield suboptimal performance or conflict with the primary optimization objective. To address this, we propose advantage-weighted policy optimization (AWPO) -- a principled RL framework that effectively integrates explicit reasoning rewards to enhance tool-use capability. AWPO incorporates variance-aware gating and difficulty-aware weighting to adaptively modulate advantages from reasoning signals based on group-relative statistics, alongside a tailored clipping mechanism for stable optimization. Extensive experiments demonstrate that AWPO achieves state-of-the-art performance across standard tool-use benchmarks, significantly outperforming strong baselines and leading closed-source models in challenging multi-turn scenarios. Notably, with exceptional parameter efficiency, our 4B model surpasses Grok-4 by 16.0 percent in multi-turn accuracy while preserving generalization capability on the out-of-distribution MMLU-Pro benchmark."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T08:07:00Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    8,
                    7,
                    0,
                    0,
                    356,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Zihan Lin"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Hexiong Yang"
                    },
                    {
                        "name": "Jiajun Chai"
                    },
                    {
                        "name": "Jie Cao"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He"
            },
            {
                "id": "http://arxiv.org/abs/2512.20243v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20243v1",
                "title": "Post-Quantum Cryptography in the 5G Core",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-Quantum Cryptography in the 5G Core"
                },
                "updated": "2025-12-23T10:53:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    53,
                    32,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20243v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "In this work, the conventional cryptographic algorithms used in the 5G Core are replaced with post-quantum alternatives and the practical impact of this transition is evaluated. Using a simulation environment, we model the registration and deregistration of varying numbers of user equipments (UEs) and measure the resulting effects on bandwidth consumption and latency.\n  Our results show that the deployment of post-quantum cryptographic algorithms has a measurable effect on performance, but that this effect is small, and perhaps more crucially, that the extra overhead needed in terms of computation and bandwidth does not have any substantial impact on the usability of the network and the efficiency of its network functions.\n  Overall the experimental results in this work corroborate earlier research: the 5G Core is technically able to support post-quantum cryptography without any inherent issues connected to the increased computational overhead or larger message size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, the conventional cryptographic algorithms used in the 5G Core are replaced with post-quantum alternatives and the practical impact of this transition is evaluated. Using a simulation environment, we model the registration and deregistration of varying numbers of user equipments (UEs) and measure the resulting effects on bandwidth consumption and latency.\n  Our results show that the deployment of post-quantum cryptographic algorithms has a measurable effect on performance, but that this effect is small, and perhaps more crucially, that the extra overhead needed in terms of computation and bandwidth does not have any substantial impact on the usability of the network and the efficiency of its network functions.\n  Overall the experimental results in this work corroborate earlier research: the 5G Core is technically able to support post-quantum cryptography without any inherent issues connected to the increased computational overhead or larger message size."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T10:53:32Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    53,
                    32,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "11 pages, 7 figures, 2 tables",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Thomas Attema"
                    },
                    {
                        "name": "Bor de Kock"
                    },
                    {
                        "name": "Sandesh Manganahalli Jayaprakash"
                    },
                    {
                        "name": "Dimitrios Schoinianakis"
                    },
                    {
                        "name": "Thom Sijpesteijn"
                    },
                    {
                        "name": "Rintse van de Vlasakker"
                    }
                ],
                "author_detail": {
                    "name": "Rintse van de Vlasakker"
                },
                "author": "Rintse van de Vlasakker"
            },
            {
                "id": "http://arxiv.org/abs/2512.20237v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20237v1",
                "title": "MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemR$^3$: Memory Retrieval via Reflective Reasoning for LLM Agents"
                },
                "updated": "2025-12-23T10:49:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    49,
                    42,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20237v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory systems have been designed to leverage past experiences in Large Language Model (LLM) agents. However, many deployed memory systems primarily optimize compression and storage, with comparatively less emphasis on explicit, closed-loop control of memory retrieval. From this observation, we build memory retrieval as an autonomous, accurate, and compatible agent system, named MemR$^3$, which has two core mechanisms: 1) a router that selects among retrieve, reflect, and answer actions to optimize answer quality; 2) a global evidence-gap tracker that explicitly renders the answering process transparent and tracks the evidence collection process. This design departs from the standard retrieve-then-answer pipeline by introducing a closed-loop control mechanism that enables autonomous decision-making. Empirical results on the LoCoMo benchmark demonstrate that MemR$^3$ surpasses strong baselines on LLM-as-a-Judge score, and particularly, it improves existing retrievers across four categories with an overall improvement on RAG (+7.29%) and Zep (+1.94%) using GPT-4.1-mini backend, offering a plug-and-play controller for existing memory stores."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T10:49:42Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    49,
                    42,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "16 pages, 6 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Xingbo Du"
                    },
                    {
                        "name": "Loka Li"
                    },
                    {
                        "name": "Duzhen Zhang"
                    },
                    {
                        "name": "Le Song"
                    }
                ],
                "author_detail": {
                    "name": "Le Song"
                },
                "author": "Le Song"
            },
            {
                "id": "http://arxiv.org/abs/2512.20234v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20234v1",
                "title": "Achieving Flexible and Secure Authentication with Strong Privacy in Decentralized Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achieving Flexible and Secure Authentication with Strong Privacy in Decentralized Networks"
                },
                "updated": "2025-12-23T10:49:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    49,
                    5,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20234v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20234v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Anonymous credentials (ACs) are a crucial cryptographic tool for privacy-preserving authentication in decentralized networks, allowing holders to prove eligibility without revealing their identity. However, a major limitation of standard ACs is the disclosure of the issuer's identity, which can leak sensitive contextual information about the holder. Issuer-hiding ACs address this by making a credential's origin indistinguishable among a set of approved issuers. Despite this advancement, existing solutions suffer from practical limitations that hinder their deployment in decentralized environments: unflexible credential models that restrict issuer and holder autonomy, flawed revocation mechanisms that compromise security, and weak attribute hiding that fails to meet data minimization principles. This paper introduces a new scheme called IRAC to overcome these challenges. We propose a flexible credential model that employs vector commitments with a padding strategy to unify credentials from heterogeneous issuers, enabling privacy-preserving authentication without enforcing a global static attribute set or verifier-defined policies. Furthermore, we design a secure decentralized revocation mechanism where holders prove non-revocation by demonstrating their credential's hash lies within a gap in the issuer's sorted revocation list, effectively decoupling revocation checks from verifier policies while maintaining issuer anonymity. IRAC also strengthens attribute hiding by utilizing zk-SNARKs and vector commitments, allowing holders to prove statements about their attributes without disclosing the attributes themselves or the credential structure. Security analysis and performance evaluations demonstrate its practical feasibility for decentralized networks, where presenting a credential can be finished in 1s.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anonymous credentials (ACs) are a crucial cryptographic tool for privacy-preserving authentication in decentralized networks, allowing holders to prove eligibility without revealing their identity. However, a major limitation of standard ACs is the disclosure of the issuer's identity, which can leak sensitive contextual information about the holder. Issuer-hiding ACs address this by making a credential's origin indistinguishable among a set of approved issuers. Despite this advancement, existing solutions suffer from practical limitations that hinder their deployment in decentralized environments: unflexible credential models that restrict issuer and holder autonomy, flawed revocation mechanisms that compromise security, and weak attribute hiding that fails to meet data minimization principles. This paper introduces a new scheme called IRAC to overcome these challenges. We propose a flexible credential model that employs vector commitments with a padding strategy to unify credentials from heterogeneous issuers, enabling privacy-preserving authentication without enforcing a global static attribute set or verifier-defined policies. Furthermore, we design a secure decentralized revocation mechanism where holders prove non-revocation by demonstrating their credential's hash lies within a gap in the issuer's sorted revocation list, effectively decoupling revocation checks from verifier policies while maintaining issuer anonymity. IRAC also strengthens attribute hiding by utilizing zk-SNARKs and vector commitments, allowing holders to prove statements about their attributes without disclosing the attributes themselves or the credential structure. Security analysis and performance evaluations demonstrate its practical feasibility for decentralized networks, where presenting a credential can be finished in 1s."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T10:49:05Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    49,
                    5,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Bin Xie"
                    },
                    {
                        "name": "Rui Song"
                    },
                    {
                        "name": "Xuyuan Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xuyuan Cai"
                },
                "author": "Xuyuan Cai"
            },
            {
                "id": "http://arxiv.org/abs/2512.20218v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20218v1",
                "title": "Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cost-TrustFL: Cost-Aware Hierarchical Federated Learning with Lightweight Reputation Evaluation across Multi-Cloud"
                },
                "updated": "2025-12-23T10:16:43Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    16,
                    43,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20218v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Federated learning across multi-cloud environments faces critical challenges, including non-IID data distributions, malicious participant detection, and substantial cross-cloud communication costs (egress fees). Existing Byzantine-robust methods focus primarily on model accuracy while overlooking the economic implications of data transfer across cloud providers. This paper presents Cost-TrustFL, a hierarchical federated learning framework that jointly optimizes model performance and communication costs while providing robust defense against poisoning attacks. We propose a gradient-based approximate Shapley value computation method that reduces the complexity from exponential to linear, enabling lightweight reputation evaluation. Our cost-aware aggregation strategy prioritizes intra-cloud communication to minimize expensive cross-cloud data transfers. Experiments on CIFAR-10 and FEMNIST datasets demonstrate that Cost-TrustFL achieves 86.7% accuracy under 30% malicious clients while reducing communication costs by 32% compared to baseline methods. The framework maintains stable performance across varying non-IID degrees and attack intensities, making it practical for real-world multi-cloud deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning across multi-cloud environments faces critical challenges, including non-IID data distributions, malicious participant detection, and substantial cross-cloud communication costs (egress fees). Existing Byzantine-robust methods focus primarily on model accuracy while overlooking the economic implications of data transfer across cloud providers. This paper presents Cost-TrustFL, a hierarchical federated learning framework that jointly optimizes model performance and communication costs while providing robust defense against poisoning attacks. We propose a gradient-based approximate Shapley value computation method that reduces the complexity from exponential to linear, enabling lightweight reputation evaluation. Our cost-aware aggregation strategy prioritizes intra-cloud communication to minimize expensive cross-cloud data transfers. Experiments on CIFAR-10 and FEMNIST datasets demonstrate that Cost-TrustFL achieves 86.7% accuracy under 30% malicious clients while reducing communication costs by 32% compared to baseline methods. The framework maintains stable performance across varying non-IID degrees and attack intensities, making it practical for real-world multi-cloud deployments."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T10:16:43Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    16,
                    43,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jixiao Yang"
                    },
                    {
                        "name": "Jinyu Chen"
                    },
                    {
                        "name": "Zixiao Huang"
                    },
                    {
                        "name": "Chengda Xu"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Sijia Li"
                    }
                ],
                "author_detail": {
                    "name": "Sijia Li"
                },
                "author": "Sijia Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.20217v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20217v1",
                "title": "LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteFusion: Taming 3D Object Detectors from Vision-Based to Multi-Modal with Minimal Adaptation"
                },
                "updated": "2025-12-23T10:16:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    16,
                    33,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20217v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D object detection is fundamental for safe and robust intelligent transportation systems. Current multi-modal 3D object detectors often rely on complex architectures and training strategies to achieve higher detection accuracy. However, these methods heavily rely on the LiDAR sensor so that they suffer from large performance drops when LiDAR is absent, which compromises the robustness and safety of autonomous systems in practical scenarios. Moreover, existing multi-modal detectors face difficulties in deployment on diverse hardware platforms, such as NPUs and FPGAs, due to their reliance on 3D sparse convolution operators, which are primarily optimized for NVIDIA GPUs. To address these challenges, we reconsider the role of LiDAR in the camera-LiDAR fusion paradigm and introduce a novel multi-modal 3D detector, LiteFusion. Instead of treating LiDAR point clouds as an independent modality with a separate feature extraction backbone, LiteFusion utilizes LiDAR data as a complementary source of geometric information to enhance camera-based detection. This straightforward approach completely eliminates the reliance on a 3D backbone, making the method highly deployment-friendly. Specifically, LiteFusion integrates complementary features from LiDAR points into image features within a quaternion space, where the orthogonal constraints are well-preserved during network training. This helps model domain-specific relations across modalities, yielding a compact cross-modal embedding. Experiments on the nuScenes dataset show that LiteFusion improves the baseline vision-based detector by +20.4% mAP and +19.7% NDS with a minimal increase in parameters (1.1%) without using dedicated LiDAR encoders. Notably, even in the absence of LiDAR input, LiteFusion maintains strong results , highlighting its favorable robustness and effectiveness across diverse fusion paradigms and deployment scenarios."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T10:16:33Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    16,
                    33,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "13 pages, 9 figures, 8 tables",
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Xiangxuan Ren"
                    },
                    {
                        "name": "Zhongdao Wang"
                    },
                    {
                        "name": "Pin Tang"
                    },
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Jilai Zheng"
                    },
                    {
                        "name": "Chao Ma"
                    }
                ],
                "author_detail": {
                    "name": "Chao Ma"
                },
                "author": "Chao Ma"
            },
            {
                "id": "http://arxiv.org/abs/2512.20210v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20210v1",
                "title": "Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive-LoRA: A Proactive and Fragmentation-Aware Serverless Inference System for LLMs"
                },
                "updated": "2025-12-23T10:03:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    3,
                    47,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20210v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20210v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The serverless computing paradigm offers compelling advantages for deploying Large Language Model (LLM) inference services, including elastic scaling and pay-per-use billing. However, serving multiple fine-tuned LLMs via Low-Rank Adaptation (LoRA) in serverless environments faces critical challenges: reactive adapter loading causes significant cold start latency, and frequent adapter swapping leads to severe GPU memory fragmentation. In this paper, we present Predictive-LoRA (P-LoRA), a proactive and fragmentation-aware serverless inference system for LoRA-based LLMs. P-LoRA introduces two key innovations: (1) a lightweight LSTM-based traffic predictor that forecasts adapter demand and proactively prefetches hot adapters from host memory to GPU, reducing cold start latency by up to 68%; and (2) a page-based adapter memory management mechanism inspired by operating system virtual memory, which keeps GPU memory utilization above 87% even under heterogeneous adapter ranks. We evaluate P-LoRA using production-like workloads derived from the Azure Functions trace. Experimental results demonstrate that P-LoRA achieves 1.52x higher throughput than S-LoRA while reducing the average Time-To-First-Token (TTFT) by 35% under high concurrency scenarios."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T10:03:47Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    10,
                    3,
                    47,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Yinan Ni"
                    },
                    {
                        "name": "Xiao Yang"
                    },
                    {
                        "name": "Yuqi Tang"
                    },
                    {
                        "name": "Zhimin Qiu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Tingzhou Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Tingzhou Yuan"
                },
                "author": "Tingzhou Yuan"
            },
            {
                "id": "http://arxiv.org/abs/2508.16580v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.16580v2",
                "title": "Adaptive Command: Real-Time Policy Adjustment via Language Models in StarCraft II",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Command: Real-Time Policy Adjustment via Language Models in StarCraft II"
                },
                "updated": "2025-12-23T09:58:30Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    58,
                    30,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.16580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.16580v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We present Adaptive Command, a novel framework integrating large language models (LLMs) with behavior trees for real-time strategic decision-making in StarCraft II. Our system focuses on enhancing human-AI collaboration in complex, dynamic environments through natural language interactions. The framework comprises: (1) an LLM-based strategic advisor, (2) a behavior tree for action execution, and (3) a natural language interface with speech capabilities. User studies demonstrate significant improvements in player decision-making and strategic adaptability, particularly benefiting novice players and those with disabilities. This work contributes to the field of real-time human-AI collaborative decision-making, offering insights applicable beyond RTS games to various complex decision-making scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Adaptive Command, a novel framework integrating large language models (LLMs) with behavior trees for real-time strategic decision-making in StarCraft II. Our system focuses on enhancing human-AI collaboration in complex, dynamic environments through natural language interactions. The framework comprises: (1) an LLM-based strategic advisor, (2) a behavior tree for action execution, and (3) a natural language interface with speech capabilities. User studies demonstrate significant improvements in player decision-making and strategic adaptability, particularly benefiting novice players and those with disabilities. This work contributes to the field of real-time human-AI collaborative decision-making, offering insights applicable beyond RTS games to various complex decision-making scenarios."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-05T03:26:58Z",
                "published_parsed": [
                    2025,
                    8,
                    5,
                    3,
                    26,
                    58,
                    1,
                    217,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Weiyu Ma"
                    },
                    {
                        "name": "Dongyu Xu"
                    },
                    {
                        "name": "Shu Lin"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang"
            },
            {
                "id": "http://arxiv.org/abs/2512.20203v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20203v1",
                "title": "Well Begun is Half Done: Location-Aware and Trace-Guided Iterative Automated Vulnerability Repair",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Well Begun is Half Done: Location-Aware and Trace-Guided Iterative Automated Vulnerability Repair"
                },
                "updated": "2025-12-23T09:54:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    54,
                    22,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20203v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3744916.3773218",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "The advances of large language models (LLMs) have paved the way for automated software vulnerability repair approaches, which iteratively refine the patch until it becomes plausible. Nevertheless, existing LLM-based vulnerability repair approaches face notable limitations: 1) they ignore the concern of locations that need to be patched and focus solely on the repair content. 2) they lack quality assessment for generated candidate patches in the iterative process.\n  To tackle the two limitations, we propose \\sysname, an LLM-based approach that provides information about where should be patched first. Furthermore, \\sysname improves the iterative repair strategy by assessing the quality of test-failing patches and selecting the best patch for the next iteration. We introduce two dimensions to assess the quality of patches: whether they introduce new vulnerabilities and the taint statement coverage. We evaluated \\sysname on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability. The experimental results demonstrate that \\sysname exhibits substantial improvements compared with the Neural Machine Translation-based, Program Analysis-based, and LLM-based state-of-the-art vulnerability repair approaches. Specifically, \\sysname is able to generate 27 plausible patches, which is comparable to or even 8 to 22 more plausible patches than the baselines. In terms of correct patch generation, \\sysname repairs 8 to 13 additional vulnerabilities compared with existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advances of large language models (LLMs) have paved the way for automated software vulnerability repair approaches, which iteratively refine the patch until it becomes plausible. Nevertheless, existing LLM-based vulnerability repair approaches face notable limitations: 1) they ignore the concern of locations that need to be patched and focus solely on the repair content. 2) they lack quality assessment for generated candidate patches in the iterative process.\n  To tackle the two limitations, we propose \\sysname, an LLM-based approach that provides information about where should be patched first. Furthermore, \\sysname improves the iterative repair strategy by assessing the quality of test-failing patches and selecting the best patch for the next iteration. We introduce two dimensions to assess the quality of patches: whether they introduce new vulnerabilities and the taint statement coverage. We evaluated \\sysname on a real-world C/C++ vulnerability repair dataset VulnLoc+, which contains 40 vulnerabilities and their Proofs-of-Vulnerability. The experimental results demonstrate that \\sysname exhibits substantial improvements compared with the Neural Machine Translation-based, Program Analysis-based, and LLM-based state-of-the-art vulnerability repair approaches. Specifically, \\sysname is able to generate 27 plausible patches, which is comparable to or even 8 to 22 more plausible patches than the baselines. In terms of correct patch generation, \\sysname repairs 8 to 13 additional vulnerabilities compared with existing approaches."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:54:22Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    54,
                    22,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "Accepted by ICSE 2026",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Zhenlei Ye"
                    },
                    {
                        "name": "Xiaobing Sun"
                    },
                    {
                        "name": "Sicong Cao"
                    },
                    {
                        "name": "Lili Bo"
                    },
                    {
                        "name": "Bin Li"
                    }
                ],
                "author_detail": {
                    "name": "Bin Li"
                },
                "author": "Bin Li",
                "arxiv_doi": "10.1145/3744916.3773218"
            },
            {
                "id": "http://arxiv.org/abs/2512.20198v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20198v1",
                "title": "Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing Spatial Architectures for Sparse Attention: STAR Accelerator via Cross-Stage Tiling"
                },
                "updated": "2025-12-23T09:43:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    43,
                    32,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20198v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20198v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) rely on self-attention for contextual understanding, demanding high-throughput inference and large-scale token parallelism (LTPP). Existing dynamic sparsity accelerators falter under LTPP scenarios due to stage-isolated optimizations. Revisiting the end-to-end sparsity acceleration flow, we identify an overlooked opportunity: cross-stage coordination can substantially reduce redundant computation and memory access. We propose STAR, a cross-stage compute- and memory-efficient algorithm-hardware co-design tailored for Transformer inference under LTPP. STAR introduces a leading-zero-based sparsity prediction using log-domain add-only operations to minimize prediction overhead. It further employs distributed sorting and a sorted updating FlashAttention mechanism, guided by a coordinated tiling strategy that enables fine-grained stage interaction for improved memory efficiency and latency. These optimizations are supported by a dedicated STAR accelerator architecture, achieving up to 9.2$\\times$ speedup and 71.2$\\times$ energy efficiency over A100, and surpassing SOTA accelerators by up to 16.1$\\times$ energy and 27.1$\\times$ area efficiency gains. Further, we deploy STAR onto a multi-core spatial architecture, optimizing dataflow and execution orchestration for ultra-long sequence processing. Architectural evaluation shows that, compared to the baseline design, Spatial-STAR achieves a 20.1$\\times$ throughput improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on self-attention for contextual understanding, demanding high-throughput inference and large-scale token parallelism (LTPP). Existing dynamic sparsity accelerators falter under LTPP scenarios due to stage-isolated optimizations. Revisiting the end-to-end sparsity acceleration flow, we identify an overlooked opportunity: cross-stage coordination can substantially reduce redundant computation and memory access. We propose STAR, a cross-stage compute- and memory-efficient algorithm-hardware co-design tailored for Transformer inference under LTPP. STAR introduces a leading-zero-based sparsity prediction using log-domain add-only operations to minimize prediction overhead. It further employs distributed sorting and a sorted updating FlashAttention mechanism, guided by a coordinated tiling strategy that enables fine-grained stage interaction for improved memory efficiency and latency. These optimizations are supported by a dedicated STAR accelerator architecture, achieving up to 9.2$\\times$ speedup and 71.2$\\times$ energy efficiency over A100, and surpassing SOTA accelerators by up to 16.1$\\times$ energy and 27.1$\\times$ area efficiency gains. Further, we deploy STAR onto a multi-core spatial architecture, optimizing dataflow and execution orchestration for ultra-long sequence processing. Architectural evaluation shows that, compared to the baseline design, Spatial-STAR achieves a 20.1$\\times$ throughput improvement."
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:43:32Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    43,
                    32,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "Accepted for publication in IEEE Transactions on Computers",
                "arxiv_primary_category": {
                    "term": "cs.AR"
                },
                "authors": [
                    {
                        "name": "Huizheng Wang"
                    },
                    {
                        "name": "Taiquan Wei"
                    },
                    {
                        "name": "Hongbin Wang"
                    },
                    {
                        "name": "Zichuan Wang"
                    },
                    {
                        "name": "Xinru Tang"
                    },
                    {
                        "name": "Zhiheng Yue"
                    },
                    {
                        "name": "Shaojun Wei"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shouyi Yin"
                },
                "author": "Shouyi Yin"
            },
            {
                "id": "http://arxiv.org/abs/2511.16193v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.16193v3",
                "title": "Fast LLM Post-training via Decoupled and Fastest-of-N Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast LLM Post-training via Decoupled and Fastest-of-N Speculation"
                },
                "updated": "2025-12-23T09:31:34Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    31,
                    34,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.16193v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.16193v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. This work, SpecActor, achieves fast rollout with speculative decoding that deploys a fast draft path to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges that hinder speculation efficiency: (1) a Decoupled speculation method that overcomes the computation inefficiency issue when executing speculative decoding with relative large per-worker batch size -- a common configuration in training but unfriendly to speculation, and (2) a Fastest-of-N speculation method that selects and combines different draft methods according to the rollout progress to approximate the optimal draft method even when the best one is unknown a priori. Extensive evaluations on production traces show that SpecActor accelerates mean rollout speed by 2.0--2.4x, with up to 2.7x speedup, over common post-training baselines. The results are consistent across both dense and MoE models and across different RL algorithms. Notably, SpecActor is 1.1--2.6x faster compared to vanilla speculative rollout in different traces. The accelerated rollout achieves 1.4--2.3x faster end-to-end training time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. This work, SpecActor, achieves fast rollout with speculative decoding that deploys a fast draft path to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges that hinder speculation efficiency: (1) a Decoupled speculation method that overcomes the computation inefficiency issue when executing speculative decoding with relative large per-worker batch size -- a common configuration in training but unfriendly to speculation, and (2) a Fastest-of-N speculation method that selects and combines different draft methods according to the rollout progress to approximate the optimal draft method even when the best one is unknown a priori. Extensive evaluations on production traces show that SpecActor accelerates mean rollout speed by 2.0--2.4x, with up to 2.7x speedup, over common post-training baselines. The results are consistent across both dense and MoE models and across different RL algorithms. Notably, SpecActor is 1.1--2.6x faster compared to vanilla speculative rollout in different traces. The accelerated rollout achieves 1.4--2.3x faster end-to-end training time."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-20T10:00:03Z",
                "published_parsed": [
                    2025,
                    11,
                    20,
                    10,
                    0,
                    3,
                    3,
                    324,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Rongxin Cheng"
                    },
                    {
                        "name": "Kai Zhou"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Mingcong Han"
                    },
                    {
                        "name": "Mingjing Ai"
                    },
                    {
                        "name": "Yeju Zhou"
                    },
                    {
                        "name": "Baoquan Zhong"
                    },
                    {
                        "name": "Wencong Xiao"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.20188v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20188v1",
                "title": "Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asynchronous Fast-Slow Vision-Language-Action Policies for Whole-Body Robotic Manipulation"
                },
                "updated": "2025-12-23T09:28:20Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    28,
                    20,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20188v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20188v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most Vision-Language-Action (VLA) systems integrate a Vision-Language Model (VLM) for semantic reasoning with an action expert generating continuous action signals, yet both typically run at a single unified frequency. As a result, policy performance is constrained by the low inference speed of large VLMs. This mandatory synchronous execution severely limits control stability and real-time performance in whole-body robotic manipulation, which involves more joints, larger motion spaces, and dynamically changing views. We introduce a truly asynchronous Fast-Slow VLA framework (DuoCore-FS), organizing the system into a fast pathway for high-frequency action generation and a slow pathway for rich VLM reasoning. The system is characterized by two key features. First, a latent representation buffer bridges the slow and fast systems. It stores instruction semantics and action-reasoning representation aligned with the scene-instruction context, providing high-level guidance to the fast pathway. Second, a whole-body action tokenizer provides a compact, unified representation of whole-body actions. Importantly, the VLM and action expert are still jointly trained end-to-end, preserving unified policy learning while enabling asynchronous execution. DuoCore-FS supports a 3B-parameter VLM while achieving 30 Hz whole-body action-chunk generation, approximately three times as fast as prior VLA models with comparable model sizes. Real-world whole-body manipulation experiments demonstrate improved task success rates and significantly enhanced responsiveness compared to synchronous Fast-Slow VLA baselines. The implementation of DuoCore-FS, including training, inference, and deployment, is provided to commercial users by Astribot as part of the Astribot robotic platform."
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:28:20Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    28,
                    20,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO"
                },
                "authors": [
                    {
                        "name": "Teqiang Zou"
                    },
                    {
                        "name": "Hongliang Zeng"
                    },
                    {
                        "name": "Yuxuan Nong"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Kehui Liu"
                    },
                    {
                        "name": "Haotian Yang"
                    },
                    {
                        "name": "Xinyang Ling"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Lianyang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lianyang Ma"
                },
                "author": "Lianyang Ma"
            },
            {
                "id": "http://arxiv.org/abs/2512.20184v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20184v1",
                "title": "Reaching Agreement Among Reasoning LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reaching Agreement Among Reasoning LLM Agents"
                },
                "updated": "2025-12-23T09:20:42Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    20,
                    42,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20184v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.\n  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems have extended the capability of agentic AI. Instead of single inference passes, multiple agents perform collective reasoning to derive high quality answers. However, existing multi-agent orchestration relies on static heuristic workflows such as fixed loop limits and barrier synchronization. These ad-hoc approaches waste computational resources, incur high latency due to stragglers, and risk finalizing transient agreements. We argue that reliable multi-agent reasoning requires a formal foundation analogous to classical distributed consensus problem.\n  To that end, we propose a formal model of the multi-agent refinement problem. The model includes definitions of the correctness guarantees and formal semantics of agent reasoning. We then introduce Aegean, a consensus protocol designed for stochastic reasoning agents that solves multi-agent refinement. We implement the protocol in Aegean-Serve, a consensus-aware serving engine that performs incremental quorum detection across concurrent agent executions, enabling early termination when sufficient agents converge. Evaluation using four mathematical reasoning benchmarks shows that Aegean provides provable safety and liveness guarantees while reducing latency by 1.2--20$\\times$ compared to state-of-the-art baselines, maintaining answer quality within 2.5%. Consistent gains across both local GPU deployments and commercial API providers validate that consensus-based orchestration eliminates straggler delays without sacrificing correctness."
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:20:42Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    20,
                    42,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC"
                },
                "authors": [
                    {
                        "name": "Chaoyi Ruan"
                    },
                    {
                        "name": "Yiliang Wang"
                    },
                    {
                        "name": "Ziji Shi"
                    },
                    {
                        "name": "Jialin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Li"
                },
                "author": "Jialin Li"
            },
            {
                "id": "http://arxiv.org/abs/2512.20182v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20182v1",
                "title": "FaithLens: Detecting and Explaining Faithfulness Hallucination",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FaithLens: Detecting and Explaining Faithfulness Hallucination"
                },
                "updated": "2025-12-23T09:20:32Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    20,
                    32,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20182v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognizing whether outputs from large language models (LLMs) contain faithfulness hallucination is crucial for real-world applications, e.g., retrieval-augmented generation and summarization. In this paper, we introduce FaithLens, a cost-efficient and effective faithfulness hallucination detection model that can jointly provide binary predictions and corresponding explanations to improve trustworthiness. To achieve this, we first synthesize training data with explanations via advanced LLMs and apply a well-defined data filtering strategy to ensure label correctness, explanation quality, and data diversity. Subsequently, we fine-tune the model on these well-curated training data as a cold start and further optimize it with rule-based reinforcement learning, using rewards for both prediction correctness and explanation quality. Results on 12 diverse tasks show that the 8B-parameter FaithLens outperforms advanced models such as GPT-4.1 and o3. Also, FaithLens can produce high-quality explanations, delivering a distinctive balance of trustworthiness, efficiency, and effectiveness."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:20:32Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    20,
                    32,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shuzheng Si"
                    },
                    {
                        "name": "Qingyi Wang"
                    },
                    {
                        "name": "Haozhe Zhao"
                    },
                    {
                        "name": "Yuzhuo Bai"
                    },
                    {
                        "name": "Guanqiao Chen"
                    },
                    {
                        "name": "Kangyang Luo"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Fanchao Qi"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Baobao Chang"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun"
            },
            {
                "id": "http://arxiv.org/abs/2512.20179v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20179v1",
                "title": "RESPOND: Risk-Enhanced Structured Pattern for LLM-driven Online Node-level Decision-making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RESPOND: Risk-Enhanced Structured Pattern for LLM-driven Online Node-level Decision-making"
                },
                "updated": "2025-12-23T09:17:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    17,
                    44,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20179v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Current LLM-based driving agents that rely on unstructured plain-text memory suffer from low-precision scene retrieval and inefficient reflection. To address this limitation, we present RESPOND, a structured decision-making framework for LLM-driven agents grounded in explicit risk patterns. RESPOND represents each ego-centric scene using a unified 5 by 3 matrix that encodes spatial topology and road constraints, enabling consistent and reliable retrieval of spatial risk configurations. Based on this representation, a hybrid rule and LLM decision pipeline is developed with a two-tier memory mechanism. In high-risk contexts, exact pattern matching enables rapid and safe reuse of verified actions, while in low-risk contexts, sub-pattern matching supports personalized driving style adaptation. In addition, a pattern-aware reflection mechanism abstracts tactical corrections from crash and near-miss frames to update structured memory, achieving one-crash-to-generalize learning. Extensive experiments demonstrate the effectiveness of RESPOND. In highway-env, RESPOND outperforms state-of-the-art LLM-based and reinforcement learning based driving agents while producing substantially fewer collisions. With step-wise human feedback, the agent acquires a Sporty driving style within approximately 20 decision steps through sub-pattern abstraction. For real-world validation, RESPOND is evaluated on 53 high-risk cut-in scenarios extracted from the HighD dataset. For each event, intervention is applied immediately before the cut-in and RESPOND re-decides the driving action. Compared to recorded human behavior, RESPOND reduces subsequent risk in 84.9 percent of scenarios, demonstrating its practical feasibility under real-world driving conditions. These results highlight RESPONDs potential for autonomous driving, personalized driving assistance, and proactive hazard mitigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLM-based driving agents that rely on unstructured plain-text memory suffer from low-precision scene retrieval and inefficient reflection. To address this limitation, we present RESPOND, a structured decision-making framework for LLM-driven agents grounded in explicit risk patterns. RESPOND represents each ego-centric scene using a unified 5 by 3 matrix that encodes spatial topology and road constraints, enabling consistent and reliable retrieval of spatial risk configurations. Based on this representation, a hybrid rule and LLM decision pipeline is developed with a two-tier memory mechanism. In high-risk contexts, exact pattern matching enables rapid and safe reuse of verified actions, while in low-risk contexts, sub-pattern matching supports personalized driving style adaptation. In addition, a pattern-aware reflection mechanism abstracts tactical corrections from crash and near-miss frames to update structured memory, achieving one-crash-to-generalize learning. Extensive experiments demonstrate the effectiveness of RESPOND. In highway-env, RESPOND outperforms state-of-the-art LLM-based and reinforcement learning based driving agents while producing substantially fewer collisions. With step-wise human feedback, the agent acquires a Sporty driving style within approximately 20 decision steps through sub-pattern abstraction. For real-world validation, RESPOND is evaluated on 53 high-risk cut-in scenarios extracted from the HighD dataset. For each event, intervention is applied immediately before the cut-in and RESPOND re-decides the driving action. Compared to recorded human behavior, RESPOND reduces subsequent risk in 84.9 percent of scenarios, demonstrating its practical feasibility under real-world driving conditions. These results highlight RESPONDs potential for autonomous driving, personalized driving assistance, and proactive hazard mitigation."
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:17:44Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    17,
                    44,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "28 pages, 8 figures",
                "arxiv_primary_category": {
                    "term": "cs.HC"
                },
                "authors": [
                    {
                        "name": "Dan Chen"
                    },
                    {
                        "name": "Heye Huang"
                    },
                    {
                        "name": "Tiantian Chen"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Yongji Li"
                    },
                    {
                        "name": "Yuhui Xu"
                    },
                    {
                        "name": "Sikai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Sikai Chen"
                },
                "author": "Sikai Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.20176v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20176v1",
                "title": "Optimistic TEE-Rollups: A Hybrid Architecture for Scalable and Verifiable Generative AI Inference on Blockchain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimistic TEE-Rollups: A Hybrid Architecture for Scalable and Verifiable Generative AI Inference on Blockchain"
                },
                "updated": "2025-12-23T09:16:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    16,
                    41,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20176v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "The rapid integration of Large Language Models (LLMs) into decentralized physical infrastructure networks (DePIN) is currently bottlenecked by the Verifiability Trilemma, which posits that a decentralized inference system cannot simultaneously achieve high computational integrity, low latency, and low cost. Existing cryptographic solutions, such as Zero-Knowledge Machine Learning (ZKML), suffer from superlinear proving overheads (O(k NlogN)) that render them infeasible for billionparameter models. Conversely, optimistic approaches (opML) impose prohibitive dispute windows, preventing real-time interactivity, while recent \"Proof of Quality\" (PoQ) paradigms sacrifice cryptographic integrity for subjective semantic evaluation, leaving networks vulnerable to model downgrade attacks and reward hacking. In this paper, we introduce Optimistic TEE-Rollups (OTR), a hybrid verification protocol that harmonizes these constraints. OTR leverages NVIDIA H100 Confidential Computing Trusted Execution Environments (TEEs) to provide sub-second Provisional Finality, underpinned by an optimistic fraud-proof mechanism and stochastic Zero-Knowledge spot-checks to mitigate hardware side-channel risks. We formally define Proof of Efficient Attribution (PoEA), a consensus mechanism that cryptographically binds execution traces to hardware attestations, thereby guaranteeing model authenticity. Extensive simulations demonstrate that OTR achieves 99% of the throughput of centralized baselines with a marginal cost overhead of $0.07 per query, maintaining Byzantine fault tolerance against rational adversaries even in the presence of transient hardware vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid integration of Large Language Models (LLMs) into decentralized physical infrastructure networks (DePIN) is currently bottlenecked by the Verifiability Trilemma, which posits that a decentralized inference system cannot simultaneously achieve high computational integrity, low latency, and low cost. Existing cryptographic solutions, such as Zero-Knowledge Machine Learning (ZKML), suffer from superlinear proving overheads (O(k NlogN)) that render them infeasible for billionparameter models. Conversely, optimistic approaches (opML) impose prohibitive dispute windows, preventing real-time interactivity, while recent \"Proof of Quality\" (PoQ) paradigms sacrifice cryptographic integrity for subjective semantic evaluation, leaving networks vulnerable to model downgrade attacks and reward hacking. In this paper, we introduce Optimistic TEE-Rollups (OTR), a hybrid verification protocol that harmonizes these constraints. OTR leverages NVIDIA H100 Confidential Computing Trusted Execution Environments (TEEs) to provide sub-second Provisional Finality, underpinned by an optimistic fraud-proof mechanism and stochastic Zero-Knowledge spot-checks to mitigate hardware side-channel risks. We formally define Proof of Efficient Attribution (PoEA), a consensus mechanism that cryptographically binds execution traces to hardware attestations, thereby guaranteeing model authenticity. Extensive simulations demonstrate that OTR achieves 99% of the throughput of centralized baselines with a marginal cost overhead of $0.07 per query, maintaining Byzantine fault tolerance against rational adversaries even in the presence of transient hardware vulnerabilities."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:16:41Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    16,
                    41,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Aaron Chan"
                    },
                    {
                        "name": "Alex Ding"
                    },
                    {
                        "name": "Frank Chen"
                    },
                    {
                        "name": "Alan Wu"
                    },
                    {
                        "name": "Bruce Zhang"
                    },
                    {
                        "name": "Arther Tian"
                    }
                ],
                "author_detail": {
                    "name": "Arther Tian"
                },
                "author": "Arther Tian"
            },
            {
                "id": "http://arxiv.org/abs/2512.20173v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20173v1",
                "title": "Offline Safe Policy Optimization From Heterogeneous Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Safe Policy Optimization From Heterogeneous Feedback"
                },
                "updated": "2025-12-23T09:07:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    7,
                    53,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20173v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \\textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \\textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Preference-based Reinforcement Learning (PbRL) learns rewards and policies aligned with human preferences without the need for extensive reward engineering and direct interaction with human annotators. However, ensuring safety remains a critical challenge across many domains and tasks. Previous works on safe RL from human feedback (RLHF) first learn reward and cost models from offline data, then use constrained RL to optimize a safe policy. While such an approach works in the contextual bandits settings (LLMs), in long horizon continuous control tasks, errors in rewards and costs accumulate, leading to impairment in performance when used with constrained RL methods. To address these challenges, (a) instead of indirectly learning policies (from rewards and costs), we introduce a framework that learns a policy directly based on pairwise preferences regarding the agent's behavior in terms of rewards, as well as binary labels indicating the safety of trajectory segments; (b) we propose \\textsc{PreSa} (Preference and Safety Alignment), a method that combines preference learning module with safety alignment in a constrained optimization problem. This optimization problem is solved within a Lagrangian paradigm that directly learns reward-maximizing safe policy \\textit{without explicitly learning reward and cost models}, avoiding the need for constrained RL; (c) we evaluate our approach on continuous control tasks with both synthetic and real human feedback. Empirically, our method successfully learns safe policies with high rewards, outperforming state-of-the-art baselines, and offline safe RL approaches with ground-truth reward and cost."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T09:07:53Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    9,
                    7,
                    53,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "Accepted at AAMAS 2026 (Extended Abstract)",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Ze Gong"
                    },
                    {
                        "name": "Pradeep Varakantham"
                    },
                    {
                        "name": "Akshat Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Akshat Kumar"
                },
                "author": "Akshat Kumar"
            },
            {
                "id": "http://arxiv.org/abs/2512.20169v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20169v1",
                "title": "Learning to Reason in LLMs by Expectation Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason in LLMs by Expectation Maximization"
                },
                "updated": "2025-12-23T08:56:49Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    56,
                    49,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20169v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20169v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) solve reasoning problems by first generating a rationale and then answering. We formalize reasoning as a latent variable model and derive an expectation-maximization (EM) objective for learning to reason. This view connects EM and modern reward-based optimization, and shows that the main challenge lies in designing a sampling distribution that generates rationales that justify correct answers. We instantiate and compare several sampling schemes: rejection sampling with a budget, self-taught reasoner (STaR), and prompt posterior sampling (PPS), which only keeps the rationalization stage of STaR. Our experiments on the ARC, MMLU, and OpenBookQA datasets with the Llama and Qwen models show that the sampling scheme can significantly affect the accuracy of learned reasoning models. Despite its simplicity, we observe that PPS outperforms the other sampling schemes."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T08:56:49Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    56,
                    49,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "12 pages, 3 figures, 1 table",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Junghyun Lee"
                    },
                    {
                        "name": "Branislav Kveton"
                    },
                    {
                        "name": "Sunav Choudhary"
                    },
                    {
                        "name": "Subhojyoti Mukherjee"
                    },
                    {
                        "name": "Anup Rao"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Alexa Siu"
                    }
                ],
                "author_detail": {
                    "name": "Alexa Siu"
                },
                "author": "Alexa Siu"
            },
            {
                "id": "http://arxiv.org/abs/2512.20168v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20168v1",
                "title": "Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Odysseus: Jailbreaking Commercial Multimodal LLM-integrated Systems via Dual Steganography"
                },
                "updated": "2025-12-23T08:53:36Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    53,
                    36,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20168v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20168v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "By integrating language understanding with perceptual modalities such as images, multimodal large language models (MLLMs) constitute a critical substrate for modern AI systems, particularly intelligent agents operating in open and interactive environments. However, their increasing accessibility also raises heightened risks of misuse, such as generating harmful or unsafe content. To mitigate these risks, alignment techniques are commonly applied to align model behavior with human values. Despite these efforts, recent studies have shown that jailbreak attacks can circumvent alignment and elicit unsafe outputs. Currently, most existing jailbreak methods are tailored for open-source models and exhibit limited effectiveness against commercial MLLM-integrated systems, which often employ additional filters. These filters can detect and prevent malicious input and output content, significantly reducing jailbreak threats. In this paper, we reveal that the success of these safety filters heavily relies on a critical assumption that malicious content must be explicitly visible in either the input or the output. This assumption, while often valid for traditional LLM-integrated systems, breaks down in MLLM-integrated systems, where attackers can leverage multiple modalities to conceal adversarial intent, leading to a false sense of security in existing MLLM-integrated systems. To challenge this assumption, we propose Odysseus, a novel jailbreak paradigm that introduces dual steganography to covertly embed malicious queries and responses into benign-looking images. Extensive experiments on benchmark datasets demonstrate that our Odysseus successfully jailbreaks several pioneering and realistic MLLM-integrated systems, achieving up to 99% attack success rate. It exposes a fundamental blind spot in existing defenses, and calls for rethinking cross-modal security in MLLM-integrated systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "By integrating language understanding with perceptual modalities such as images, multimodal large language models (MLLMs) constitute a critical substrate for modern AI systems, particularly intelligent agents operating in open and interactive environments. However, their increasing accessibility also raises heightened risks of misuse, such as generating harmful or unsafe content. To mitigate these risks, alignment techniques are commonly applied to align model behavior with human values. Despite these efforts, recent studies have shown that jailbreak attacks can circumvent alignment and elicit unsafe outputs. Currently, most existing jailbreak methods are tailored for open-source models and exhibit limited effectiveness against commercial MLLM-integrated systems, which often employ additional filters. These filters can detect and prevent malicious input and output content, significantly reducing jailbreak threats. In this paper, we reveal that the success of these safety filters heavily relies on a critical assumption that malicious content must be explicitly visible in either the input or the output. This assumption, while often valid for traditional LLM-integrated systems, breaks down in MLLM-integrated systems, where attackers can leverage multiple modalities to conceal adversarial intent, leading to a false sense of security in existing MLLM-integrated systems. To challenge this assumption, we propose Odysseus, a novel jailbreak paradigm that introduces dual steganography to covertly embed malicious queries and responses into benign-looking images. Extensive experiments on benchmark datasets demonstrate that our Odysseus successfully jailbreaks several pioneering and realistic MLLM-integrated systems, achieving up to 99% attack success rate. It exposes a fundamental blind spot in existing defenses, and calls for rethinking cross-modal security in MLLM-integrated systems."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T08:53:36Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    53,
                    36,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "This paper is accepted by Network and Distributed System Security Symposium (NDSS) 2026",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Jiameng Cheng"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao"
            },
            {
                "id": "http://arxiv.org/abs/2506.09349v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.09349v4",
                "title": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DrVoice: Parallel Speech-Text Voice Conversation Model via Dual-Resolution Speech Representations"
                },
                "updated": "2025-12-23T08:50:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    50,
                    59,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.09349v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.09349v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies on end-to-end (E2E) speech generation with large language models (LLMs) have attracted significant community attention, with multiple works extending text-based LLMs to generate discrete speech tokens. Existing E2E approaches primarily fall into two categories: (1) Methods that generate discrete speech tokens independently without incorporating them into the LLM's autoregressive process, resulting in text generation being unaware of concurrent speech synthesis. (2) Models that generate interleaved or parallel speech-text tokens through joint autoregressive modeling, enabling mutual modality awareness during generation. This paper presents DrVoice, a parallel speech-text voice conversation model based on joint autoregressive modeling, featuring dual-resolution speech representations. Notably, while current methods utilize mainly 12.5Hz input audio representation, our proposed dual-resolution mechanism reduces the input frequency for the LLM to 5Hz, significantly reducing computational cost and alleviating the frequency discrepancy between speech and text tokens and in turn better exploiting LLMs' capabilities. Experimental results demonstrate that DrVoice-7B establishes new state-of-the-art (SOTA) on prominent speech benchmarks including OpenAudioBench, VoiceBench, UltraEval-Audio and Big Bench Audio, making it a leading open-source speech foundation model in ~7B models."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-11T02:57:22Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    2,
                    57,
                    22,
                    2,
                    162,
                    0
                ],
                "arxiv_comment": "Work in progress",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Chao-Hong Tan"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Luyao Cheng"
                    },
                    {
                        "name": "Hai Yu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Xiang Lv"
                    },
                    {
                        "name": "Tianyu Zhao"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yukun Ma"
                    },
                    {
                        "name": "Yafeng Chen"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Xiangang Li"
                    },
                    {
                        "name": "Jieping Ye"
                    }
                ],
                "author_detail": {
                    "name": "Jieping Ye"
                },
                "author": "Jieping Ye"
            },
            {
                "id": "http://arxiv.org/abs/2510.23649v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.23649v3",
                "title": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Low Rank Attention for Long-Context Inference in Large Language Models"
                },
                "updated": "2025-12-23T08:47:31Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    47,
                    31,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.23649v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.23649v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As the length of input text increases, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. In this work, Low Rank Query and Key attention (LRQK) is introduced, a two-stage framework that jointly decomposes full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then employs these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism where only missing full-precision KV pairs are transferred, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal accuracy loss. Our code is available at https://github.com/tenghuilee/LRQK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of input text increases, the key-value (KV) cache in LLMs imposes prohibitive GPU memory costs and limits long-context inference on resource constrained devices. Existing approaches, such as KV quantization and pruning, reduce memory usage but suffer from numerical precision loss or suboptimal retention of key-value pairs. In this work, Low Rank Query and Key attention (LRQK) is introduced, a two-stage framework that jointly decomposes full-precision query and key matrices into compact rank-\\(r\\) factors during the prefill stage, and then employs these low-dimensional projections to compute proxy attention scores in \\(\\mathcal{O}(lr)\\) time at each decode step. By selecting only the top-\\(k\\) tokens and a small fixed set of recent tokens, LRQK employs a mixed GPU-CPU cache with a hit-and-miss mechanism where only missing full-precision KV pairs are transferred, thereby preserving exact attention outputs while reducing CPU-GPU data movement. Extensive experiments on the RULER and LongBench benchmarks with LLaMA-3-8B and Qwen2.5-7B demonstrate that LRQK matches or surpasses leading sparse-attention methods in long context settings, while delivering significant memory savings with minimal accuracy loss. Our code is available at https://github.com/tenghuilee/LRQK."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-25T11:43:27Z",
                "published_parsed": [
                    2025,
                    10,
                    25,
                    11,
                    43,
                    27,
                    5,
                    298,
                    0
                ],
                "arxiv_comment": "https://neurips.cc/virtual/2025/loc/san-diego/poster/118451",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "arxiv_journal_ref": "39th Conference on Neural Information Processing Systems (NeurIPS 2025)",
                "authors": [
                    {
                        "name": "Tenghui Li"
                    },
                    {
                        "name": "Guoxu Zhou"
                    },
                    {
                        "name": "Xuyang Zhao"
                    },
                    {
                        "name": "Yuning Qiu"
                    },
                    {
                        "name": "Qibin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Qibin Zhao"
                },
                "author": "Qibin Zhao"
            },
            {
                "id": "http://arxiv.org/abs/2512.20164v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20164v1",
                "title": "AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Security Beyond Core Domains: Resume Screening as a Case Study of Adversarial Vulnerabilities in Specialized LLM Applications"
                },
                "updated": "2025-12-23T08:42:09Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    42,
                    9,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20164v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by \"adversarial instructions\" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) excel at text comprehension and generation, making them ideal for automated tasks like code review and content moderation. However, our research identifies a vulnerability: LLMs can be manipulated by \"adversarial instructions\" hidden in input data, such as resumes or code, causing them to deviate from their intended task. Notably, while defenses may exist for mature domains such as code review, they are often absent in other common applications such as resume screening and peer review. This paper introduces a benchmark to assess this vulnerability in resume screening, revealing attack success rates exceeding 80% for certain attack types. We evaluate two defense mechanisms: prompt-based defenses achieve 10.1% attack reduction with 12.5% false rejection increase, while our proposed FIDS (Foreign Instruction Detection through Separation) using LoRA adaptation achieves 15.4% attack reduction with 10.4% false rejection increase. The combined approach provides 26.3% attack reduction, demonstrating that training-time defenses outperform inference-time mitigations in both security and utility preservation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T08:42:09Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    42,
                    9,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Honglin Mu"
                    },
                    {
                        "name": "Jinghao Liu"
                    },
                    {
                        "name": "Kaiyang Wan"
                    },
                    {
                        "name": "Rui Xing"
                    },
                    {
                        "name": "Xiuying Chen"
                    },
                    {
                        "name": "Timothy Baldwin"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che"
            },
            {
                "id": "http://arxiv.org/abs/2512.20162v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20162v1",
                "title": "Concept Generalization in Humans and Large Language Models: Insights from the Number Game",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept Generalization in Humans and Large Language Models: Insights from the Number Game"
                },
                "updated": "2025-12-23T08:41:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    41,
                    3,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20162v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20162v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We compare human and large language model (LLM) generalization in the number game, a concept inference task. Using a Bayesian model as an analytical framework, we examined the inductive biases and inference strategies of humans and LLMs. The Bayesian model captured human behavior better than LLMs in that humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules. Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize. These contrasts highlight the fundamental differences in how humans and LLMs infer and generalize mathematical concepts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We compare human and large language model (LLM) generalization in the number game, a concept inference task. Using a Bayesian model as an analytical framework, we examined the inductive biases and inference strategies of humans and LLMs. The Bayesian model captured human behavior better than LLMs in that humans flexibly infer rule-based and similarity-based concepts, whereas LLMs rely more on mathematical rules. Humans also demonstrated a few-shot generalization, even from a single example, while LLMs required more samples to generalize. These contrasts highlight the fundamental differences in how humans and LLMs infer and generalize mathematical concepts."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T08:41:03Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    41,
                    3,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Arghavan Bazigaran"
                    },
                    {
                        "name": "Hansem Sohn"
                    }
                ],
                "author_detail": {
                    "name": "Hansem Sohn"
                },
                "author": "Hansem Sohn"
            },
            {
                "id": "http://arxiv.org/abs/2512.20159v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20159v1",
                "title": "AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AXIOM: Benchmarking LLM-as-a-Judge for Code via Rule-Based Perturbation and Multisource Quality Calibration"
                },
                "updated": "2025-12-23T08:39:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    39,
                    22,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20159v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20159v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) have been increasingly deployed in real-world software engineering, fostering the development of code evaluation metrics to study the quality of LLM-generated code. Conventional rule-based metrics merely score programs based on their surface-level similarities with reference programs instead of analyzing functionality and code quality in depth. To address this limitation, researchers have developed LLM-as-a-judge metrics, prompting LLMs to evaluate and score code, and curated various code evaluation benchmarks to validate their effectiveness. However, these benchmarks suffer from critical limitations, hindering reliable assessments of evaluation capability: Some feature coarse-grained binary labels, which reduce rich code behavior to a single bit of information, obscuring subtle errors. Others propose fine-grained but subjective, vaguely-defined evaluation criteria, introducing unreliability in manually-annotated scores, which is the ground-truth they rely on. Furthermore, they often use uncontrolled data synthesis methods, leading to unbalanced score distributions that poorly represent real-world code generation scenarios.\n  To curate a diverse benchmark with programs of well-balanced distributions across various quality levels and streamline the manual annotation procedure, we propose AXIOM, a novel perturbation-based framework for synthesizing code evaluation benchmarks at scale. It reframes program scores as the refinement effort needed for deployment, consisting of two stages: (1) Rule-guided perturbation, which prompts LLMs to apply sequences of predefined perturbation rules to existing high-quality programs to modify their functionality and code quality, enabling us to precisely control each program's target score to achieve balanced score distributions. (2) Multisource quality calibration, which first selects a subset of...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly deployed in real-world software engineering, fostering the development of code evaluation metrics to study the quality of LLM-generated code. Conventional rule-based metrics merely score programs based on their surface-level similarities with reference programs instead of analyzing functionality and code quality in depth. To address this limitation, researchers have developed LLM-as-a-judge metrics, prompting LLMs to evaluate and score code, and curated various code evaluation benchmarks to validate their effectiveness. However, these benchmarks suffer from critical limitations, hindering reliable assessments of evaluation capability: Some feature coarse-grained binary labels, which reduce rich code behavior to a single bit of information, obscuring subtle errors. Others propose fine-grained but subjective, vaguely-defined evaluation criteria, introducing unreliability in manually-annotated scores, which is the ground-truth they rely on. Furthermore, they often use uncontrolled data synthesis methods, leading to unbalanced score distributions that poorly represent real-world code generation scenarios.\n  To curate a diverse benchmark with programs of well-balanced distributions across various quality levels and streamline the manual annotation procedure, we propose AXIOM, a novel perturbation-based framework for synthesizing code evaluation benchmarks at scale. It reframes program scores as the refinement effort needed for deployment, consisting of two stages: (1) Rule-guided perturbation, which prompts LLMs to apply sequences of predefined perturbation rules to existing high-quality programs to modify their functionality and code quality, enabling us to precisely control each program's target score to achieve balanced score distributions. (2) Multisource quality calibration, which first selects a subset of..."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T08:39:22Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    39,
                    22,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Ruiqi Wang"
                    },
                    {
                        "name": "Xinchen Wang"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Chun Yong Chong"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "Qing Liao"
                    }
                ],
                "author_detail": {
                    "name": "Qing Liao"
                },
                "author": "Qing Liao"
            },
            {
                "id": "http://arxiv.org/abs/2512.20156v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20156v1",
                "title": "Fun-Audio-Chat Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fun-Audio-Chat Technical Report"
                },
                "updated": "2025-12-23T08:35:27Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    35,
                    27,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20156v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20156v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T08:35:27Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    35,
                    27,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "21 pages, https://github.com/FunAudioLLM/Fun-Audio-Chat",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Luyao Cheng"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Xiangang Li"
                    },
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Chao-Hong Tan"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Junhao Xu"
                    },
                    {
                        "name": "Jieping Ye"
                    },
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Qiquan Zhang"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou"
            },
            {
                "id": "http://arxiv.org/abs/2512.20154v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20154v1",
                "title": "Target Classification for Integrated Sensing and Communication in Industrial Deployments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Target Classification for Integrated Sensing and Communication in Industrial Deployments"
                },
                "updated": "2025-12-23T08:32:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    32,
                    40,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20154v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Integrated Sensing and Communication (ISAC) systems enable cellular networks to jointly operate as communication technology and sense the environment. While opportunities and potential performance have been largely investigated in simulations, few experimental works have showcased Automatic Target Recognition (ATR) effectiveness in a real-world deployment based on cellular radio units. To bridge this gap, this paper presents an initial study investigating the feasibility of ATR for ISAC. Our ATR solution uses a Deep Learning (DL)-based detector to infer the target class directly from the radar images generated by the ISAC system. The DL detector is evaluated with experimental data from a ISAC testbed based on commercially available mmWave radio units in the ARENA 2036 industrial research campus located in Stuttgart, Germany. Experimental results demonstrate accurate classification performance, demonstrating the feasibility of ATR ISAC with cellular hardware in our setup. We finally provide insights about the open generalization challenges, that will fuel future work on the topic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrated Sensing and Communication (ISAC) systems enable cellular networks to jointly operate as communication technology and sense the environment. While opportunities and potential performance have been largely investigated in simulations, few experimental works have showcased Automatic Target Recognition (ATR) effectiveness in a real-world deployment based on cellular radio units. To bridge this gap, this paper presents an initial study investigating the feasibility of ATR for ISAC. Our ATR solution uses a Deep Learning (DL)-based detector to infer the target class directly from the radar images generated by the ISAC system. The DL detector is evaluated with experimental data from a ISAC testbed based on commercially available mmWave radio units in the ARENA 2036 industrial research campus located in Stuttgart, Germany. Experimental results demonstrate accurate classification performance, demonstrating the feasibility of ATR ISAC with cellular hardware in our setup. We finally provide insights about the open generalization challenges, that will fuel future work on the topic."
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T08:32:40Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    32,
                    40,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP"
                },
                "authors": [
                    {
                        "name": "Luca Barbieri"
                    },
                    {
                        "name": "Marcus Henninger"
                    },
                    {
                        "name": "Paolo Tosi"
                    },
                    {
                        "name": "Artjom Grudnitsky"
                    },
                    {
                        "name": "Mattia Brambilla"
                    },
                    {
                        "name": "Monica Nicoli"
                    },
                    {
                        "name": "Silvio Mandelli"
                    }
                ],
                "author_detail": {
                    "name": "Silvio Mandelli"
                },
                "author": "Silvio Mandelli"
            },
            {
                "id": "http://arxiv.org/abs/2506.05594v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2506.05594v3",
                "title": "SoK: Are Watermarks in LLMs Ready for Deployment?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Are Watermarks in LLMs Ready for Deployment?"
                },
                "updated": "2025-12-23T08:31:59Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    31,
                    59,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2506.05594v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2506.05594v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs.\n  To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed natural language processing, demonstrating impressive capabilities across diverse tasks. However, deploying these models introduces critical risks related to intellectual property violations and potential misuse, particularly as adversaries can imitate these models to steal services or generate misleading outputs. We specifically focus on model stealing attacks, as they are highly relevant to proprietary LLMs and pose a serious threat to their security, revenue, and ethical deployment. While various watermarking techniques have emerged to mitigate these risks, it remains unclear how far the community and industry have progressed in developing and deploying watermarks in LLMs.\n  To bridge this gap, we aim to develop a comprehensive systematization for watermarks in LLMs by 1) presenting a detailed taxonomy for watermarks in LLMs, 2) proposing a novel intellectual property classifier to explore the effectiveness and impacts of watermarks on LLMs under both attack and attack-free environments, 3) analyzing the limitations of existing watermarks in LLMs, and 4) discussing practical challenges and potential future directions for watermarks in LLMs. Through extensive experiments, we show that despite promising research outcomes and significant attention from leading companies and community to deploy watermarks, these techniques have yet to reach their full potential in real-world applications due to their unfavorable impacts on model utility of LLMs and downstream tasks. Our findings provide an insightful understanding of watermarks in LLMs, highlighting the need for practical watermarks solutions tailored to LLM deployment."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-06-05T21:12:51Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    21,
                    12,
                    51,
                    3,
                    156,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Kieu Dang"
                    },
                    {
                        "name": "Phung Lai"
                    },
                    {
                        "name": "NhatHai Phan"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Ruoming Jin"
                    },
                    {
                        "name": "Abdallah Khreishah"
                    },
                    {
                        "name": "My T. Thai"
                    }
                ],
                "author_detail": {
                    "name": "My T. Thai"
                },
                "author": "My T. Thai"
            },
            {
                "id": "http://arxiv.org/abs/2512.01457v4",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.01457v4",
                "title": "Zero-Overhead Introspection for Adaptive Test-Time Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-Overhead Introspection for Adaptive Test-Time Compute"
                },
                "updated": "2025-12-23T08:18:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    18,
                    3,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.01457v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.01457v4",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, which equips models with zero-overhead introspective predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models excel at reasoning but lack key aspects of introspection, including anticipating their own success and the computation required to achieve it. Humans use real-time introspection to decide how much effort to invest, when to make multiple attempts, when to stop, and when to signal success or failure. Without this, LLMs struggle to make intelligent meta-cognition decisions. Test-time scaling methods like Best-of-N drive up cost and latency by using a fixed budget of samples regardless of the marginal benefit of each one at any point in generation, and the absence of confidence signals can mislead people, prevent appropriate escalation to better tools, and undermine trustworthiness. Learned verifiers or reward models can provide confidence estimates, but do not enable adaptive inference and add substantial cost by requiring extra models or forward passes. We present ZIP-RC, which equips models with zero-overhead introspective predictions of reward and cost. At every token, ZIP-RC reuses reserved or unused logits in the same forward pass as next-token prediction to output a joint distribution over final reward and remaining length -- no extra models, architecture change, or inference overhead. This full joint distribution is used to compute a sampling utility which is the linear combination of the expected maximum reward, total compute, and latency of set of samples if generated to completion. During inference, we maximize this utility with meta-actions that determine which prefix of tokens to continue or initiate sampling from. On mixed-difficulty mathematical benchmarks, ZIP-RC improves accuracy by up to 12% over majority voting at equal or lower average cost, and traces smooth Pareto frontiers between quality, compute, and latency. By providing real-time reward-cost introspection, ZIP-RC enables adaptive, efficient reasoning."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-01T09:44:31Z",
                "published_parsed": [
                    2025,
                    12,
                    1,
                    9,
                    44,
                    31,
                    0,
                    335,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Rohin Manvi"
                    },
                    {
                        "name": "Joey Hong"
                    },
                    {
                        "name": "Tim Seyde"
                    },
                    {
                        "name": "Maxime Labonne"
                    },
                    {
                        "name": "Mathias Lechner"
                    },
                    {
                        "name": "Sergey Levine"
                    }
                ],
                "author_detail": {
                    "name": "Sergey Levine"
                },
                "author": "Sergey Levine"
            },
            {
                "id": "http://arxiv.org/abs/2512.20144v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20144v1",
                "title": "Multi-hop Reasoning via Early Knowledge Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hop Reasoning via Early Knowledge Alignment"
                },
                "updated": "2025-12-23T08:14:44Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    14,
                    44,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20144v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \\href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for Large Language Models (LLMs) to address knowledge-intensive queries requiring domain-specific or up-to-date information. To handle complex multi-hop questions that are challenging for single-step retrieval, iterative RAG approaches incorporating reinforcement learning have been proposed. However, existing iterative RAG systems typically plan to decompose questions without leveraging information about the available retrieval corpus, leading to inefficient retrieval and reasoning chains that cascade into suboptimal performance. In this paper, we introduce Early Knowledge Alignment (EKA), a simple but effective module that aligns LLMs with retrieval set before planning in iterative RAG systems with contextually relevant retrieved knowledge. Extensive experiments on six standard RAG datasets demonstrate that by establishing a stronger reasoning foundation, EKA significantly improves retrieval precision, reduces cascading errors, and enhances both performance and efficiency. Our analysis from an entropy perspective demonstrate that incorporating early knowledge reduces unnecessary exploration during the reasoning process, enabling the model to focus more effectively on relevant information subsets. Moreover, EKA proves effective as a versatile, training-free inference strategy that scales seamlessly to large models. Generalization tests across diverse datasets and retrieval corpora confirm the robustness of our approach. Overall, EKA advances the state-of-the-art in iterative RAG systems while illuminating the critical interplay between structured reasoning and efficient exploration in reinforcement learning-augmented frameworks. The code is released at \\href{https://github.com/yxzwang/EarlyKnowledgeAlignment}{Github}."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T08:14:44Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    14,
                    44,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "16 pages",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Shicheng Fang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Qi Luo"
                    },
                    {
                        "name": "Xuanjing Huang"
                    },
                    {
                        "name": "Yining Zheng"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu"
            },
            {
                "id": "http://arxiv.org/abs/2512.20140v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20140v1",
                "title": "Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Zero-Shot Time Series Forecasting in Off-the-Shelf LLMs via Noise Injection"
                },
                "updated": "2025-12-23T08:02:33Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    2,
                    33,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20140v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated effectiveness as zero-shot time series (TS) forecasters. The key challenge lies in tokenizing TS data into textual representations that align with LLMs' pre-trained knowledge. While existing work often relies on fine-tuning specialized modules to bridge this gap, a distinct, yet challenging, paradigm aims to leverage truly off-the-shelf LLMs without any fine-tuning whatsoever, relying solely on strategic tokenization of numerical sequences. The performance of these fully frozen models is acutely sensitive to the textual representation of the input data, as their parameters cannot adapt to distribution shifts. In this paper, we introduce a simple yet highly effective strategy to overcome this brittleness: injecting noise into the raw time series before tokenization. This non-invasive intervention acts as a form of inference-time augmentation, compelling the frozen LLM to extrapolate based on robust underlying temporal patterns rather than superficial numerical artifacts. We theoretically analyze this phenomenon and empirically validate its effectiveness across diverse benchmarks. Notably, to fully eliminate potential biases from data contamination during LLM pre-training, we introduce two novel TS datasets that fall outside all utilized LLMs' pre-training scopes, and consistently observe improved performance. This study provides a further step in directly leveraging off-the-shelf LLMs for time series forecasting."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T08:02:33Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    2,
                    33,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "9 pages,3 figures",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Xingyou Yin"
                    },
                    {
                        "name": "Ceyao Zhang"
                    },
                    {
                        "name": "Min Hu"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen"
            },
            {
                "id": "http://arxiv.org/abs/2512.17814v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.17814v2",
                "title": "LLM-based Behaviour Driven Development for Hardware Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based Behaviour Driven Development for Hardware Design"
                },
                "updated": "2025-12-23T08:00:41Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    8,
                    0,
                    41,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.17814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.17814v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.\n  Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test and verification are essential activities in hardware and system design, but their complexity grows significantly with increasing system sizes. While Behavior Driven Development (BDD) has proven effective in software engineering, it is not yet well established in hardware design, and its practical use remains limited. One contributing factor is the manual effort required to derive precise behavioral scenarios from textual specifications.\n  Recent advances in Large Language Models (LLMs) offer new opportunities to automate this step. In this paper, we investigate the use of LLM-based techniques to support BDD in the context of hardware design."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-19T17:19:08Z",
                "published_parsed": [
                    2025,
                    12,
                    19,
                    17,
                    19,
                    8,
                    4,
                    353,
                    0
                ],
                "arxiv_comment": "7 pages, keynote given at 2nd International Symposium on Artificial Intelligence and Internet of Things (AIIoT-25), December 22-24th, 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Rolf Drechsler"
                    },
                    {
                        "name": "Qian Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qian Liu"
                },
                "author": "Qian Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.20135v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20135v1",
                "title": "MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MolAct: An Agentic RL Framework for Molecular Editing and Property Optimization"
                },
                "updated": "2025-12-23T07:53:57Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    7,
                    53,
                    57,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20135v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20135v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed \"thinking\" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open \"thinking\" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed \"thinking\" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Molecular editing and optimization are multi-step problems that require iteratively improving properties while keeping molecules chemically valid and structurally similar. We frame both tasks as sequential, tool-guided decisions and introduce MolAct, an agentic reinforcement learning framework that employs a two-stage training paradigm: first building editing capability, then optimizing properties while reusing the learned editing behaviors. To the best of our knowledge, this is the first work to formalize molecular design as an Agentic Reinforcement Learning problem, where an LLM agent learns to interleave reasoning, tool-use, and molecular optimization. The framework enables agents to interact in multiple turns, invoking chemical tools for validity checking, property assessment, and similarity control, and leverages their feedback to refine subsequent edits. We instantiate the MolAct framework to train two model families: MolEditAgent for molecular editing tasks and MolOptAgent for molecular optimization tasks. In molecular editing, MolEditAgent-7B delivers 100, 95, and 98 valid add, delete, and substitute edits, outperforming strong closed \"thinking\" baselines such as DeepSeek-R1; MolEditAgent-3B approaches the performance of much larger open \"thinking\" models like Qwen3-32B-think. In molecular optimization, MolOptAgent-7B (trained on MolEditAgent-7B) surpasses the best closed \"thinking\" baseline (e.g., Claude 3.7) on LogP and remains competitive on solubility, while maintaining balanced performance across other objectives. These results highlight that treating molecular design as a multi-step, tool-augmented process is key to reliable and interpretable improvements."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T07:53:57Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    7,
                    53,
                    57,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Zhuo Yang"
                    },
                    {
                        "name": "Yeyun chen"
                    },
                    {
                        "name": "Jiaqing Xie"
                    },
                    {
                        "name": "Ben Gao"
                    },
                    {
                        "name": "Shuaike Shen"
                    },
                    {
                        "name": "Wanhao Liu"
                    },
                    {
                        "name": "Liujia Yang"
                    },
                    {
                        "name": "Beilun Wang"
                    },
                    {
                        "name": "Tianfan Fu"
                    },
                    {
                        "name": "Yuqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuqiang Li"
                },
                "author": "Yuqiang Li"
            },
            {
                "id": "http://arxiv.org/abs/2510.06187v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2510.06187v3",
                "title": "Automated Program Repair of Uncompilable Student Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Program Repair of Uncompilable Student Code"
                },
                "updated": "2025-12-23T07:41:07Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    7,
                    41,
                    7,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2510.06187v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2510.06187v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    },
                    {
                        "rel": "related",
                        "href": "https://doi.org/10.1145/3770761.3777323",
                        "title": "doi",
                        "type": "text/html"
                    }
                ],
                "summary": "A significant portion of student programming submissions in CS1 learning environments are uncompilable, limiting their use in student modeling and downstream knowledge tracing. Traditional modeling pipelines often exclude these cases, discarding observations of student learning. This study investigates automated program repair as a strategy to recover uncompilable code while preserving students' structural intent for use in student modeling. Within this framework, we assess large language models (LLMs) as repair agents under high- and low-context prompting conditions. Repairs were evaluated for compilability, edit distance, and preservation of students' original structure and logic. While all models produced compilable repairs, they differed in how well they preserve students' control flow and code structure, affecting their pedagogical utility. By recovering uncompilable submissions, this work enables richer and more comprehensive analyses of learners' coding processes and development over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A significant portion of student programming submissions in CS1 learning environments are uncompilable, limiting their use in student modeling and downstream knowledge tracing. Traditional modeling pipelines often exclude these cases, discarding observations of student learning. This study investigates automated program repair as a strategy to recover uncompilable code while preserving students' structural intent for use in student modeling. Within this framework, we assess large language models (LLMs) as repair agents under high- and low-context prompting conditions. Repairs were evaluated for compilability, edit distance, and preservation of students' original structure and logic. While all models produced compilable repairs, they differed in how well they preserve students' control flow and code structure, affecting their pedagogical utility. By recovering uncompilable submissions, this work enables richer and more comprehensive analyses of learners' coding processes and development over time."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-10-07T17:46:33Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    46,
                    33,
                    1,
                    280,
                    0
                ],
                "arxiv_comment": "In Proceedings of the 57th ACM Technical Symposium on Computer Science Education V.2 (SIGCSE TS 2026)",
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Griffin Pitts"
                    },
                    {
                        "name": "Aum Pandya"
                    },
                    {
                        "name": "Darsh Rank"
                    },
                    {
                        "name": "Tirth Bhatt"
                    },
                    {
                        "name": "Muntasir Hoq"
                    },
                    {
                        "name": "Bita Akram"
                    }
                ],
                "author_detail": {
                    "name": "Bita Akram"
                },
                "author": "Bita Akram",
                "arxiv_doi": "10.1145/3770761.3777323"
            },
            {
                "id": "http://arxiv.org/abs/2512.20120v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20120v1",
                "title": "HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEART-VIT: Hessian-Guided Efficient Dynamic Attention and Token Pruning in Vision Transformer"
                },
                "updated": "2025-12-23T07:23:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    7,
                    23,
                    16,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20120v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Vision Transformers (ViTs) deliver state-of-the-art accuracy but their quadratic attention cost and redundant computations severely hinder deployment on latency and resource-constrained platforms. Existing pruning approaches treat either tokens or heads in isolation, relying on heuristics or first-order signals, which often sacrifice accuracy or fail to generalize across inputs. We introduce HEART-ViT, a Hessian-guided efficient dynamic attention and token pruning framework for vision transformers, which to the best of our knowledge is the first unified, second-order, input-adaptive framework for ViT optimization. HEART-ViT estimates curvature-weighted sensitivities of both tokens and attention heads using efficient Hessian-vector products, enabling principled pruning decisions under explicit loss budgets.This dual-view sensitivity reveals an important structural insight: token pruning dominates computational savings, while head pruning provides fine-grained redundancy removal, and their combination achieves a superior trade-off. On ImageNet-100 and ImageNet-1K with ViT-B/16 and DeiT-B/16, HEART-ViT achieves up to 49.4 percent FLOPs reduction, 36 percent lower latency, and 46 percent higher throughput, while consistently matching or even surpassing baseline accuracy after fine-tuning, for example 4.7 percent recovery at 40 percent token pruning. Beyond theoretical benchmarks, we deploy HEART-ViT on different edge devices such as AGX Orin, demonstrating that our reductions in FLOPs and latency translate directly into real-world gains in inference speed and energy efficiency. HEART-ViT bridges the gap between theory and practice, delivering the first unified, curvature-driven pruning framework that is both accuracy-preserving and edge-efficient.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) deliver state-of-the-art accuracy but their quadratic attention cost and redundant computations severely hinder deployment on latency and resource-constrained platforms. Existing pruning approaches treat either tokens or heads in isolation, relying on heuristics or first-order signals, which often sacrifice accuracy or fail to generalize across inputs. We introduce HEART-ViT, a Hessian-guided efficient dynamic attention and token pruning framework for vision transformers, which to the best of our knowledge is the first unified, second-order, input-adaptive framework for ViT optimization. HEART-ViT estimates curvature-weighted sensitivities of both tokens and attention heads using efficient Hessian-vector products, enabling principled pruning decisions under explicit loss budgets.This dual-view sensitivity reveals an important structural insight: token pruning dominates computational savings, while head pruning provides fine-grained redundancy removal, and their combination achieves a superior trade-off. On ImageNet-100 and ImageNet-1K with ViT-B/16 and DeiT-B/16, HEART-ViT achieves up to 49.4 percent FLOPs reduction, 36 percent lower latency, and 46 percent higher throughput, while consistently matching or even surpassing baseline accuracy after fine-tuning, for example 4.7 percent recovery at 40 percent token pruning. Beyond theoretical benchmarks, we deploy HEART-ViT on different edge devices such as AGX Orin, demonstrating that our reductions in FLOPs and latency translate directly into real-world gains in inference speed and energy efficiency. HEART-ViT bridges the gap between theory and practice, delivering the first unified, curvature-driven pruning framework that is both accuracy-preserving and edge-efficient."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T07:23:16Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    7,
                    23,
                    16,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Mohammad Helal Uddin"
                    },
                    {
                        "name": "Liam Seymour"
                    },
                    {
                        "name": "Sabur Baidya"
                    }
                ],
                "author_detail": {
                    "name": "Sabur Baidya"
                },
                "author": "Sabur Baidya"
            },
            {
                "id": "http://arxiv.org/abs/2512.20113v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20113v1",
                "title": "Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi Modal Attention Networks with Uncertainty Quantification for Automated Concrete Bridge Deck Delamination Detection"
                },
                "updated": "2025-12-23T07:16:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    7,
                    16,
                    18,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20113v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Deteriorating civil infrastructure requires automated inspection techniques overcoming limitations of visual assessment. While Ground Penetrating Radar and Infrared Thermography enable subsurface defect detection, single modal approaches face complementary constraints radar struggles with moisture and shallow defects, while thermography exhibits weather dependency and limited depth. This paper presents a multi modal attention network fusing radar temporal patterns with thermal spatial signatures for bridge deck delamination detection. Our architecture introduces temporal attention for radar processing, spatial attention for thermal features, and cross modal fusion with learnable embeddings discovering complementary defect patterns invisible to individual sensors. We incorporate uncertainty quantification through Monte Carlo dropout and learned variance estimation, decomposing uncertainty into epistemic and aleatoric components for safety critical decisions. Experiments on five bridge datasets reveal that on balanced to moderately imbalanced data, our approach substantially outperforms baselines in accuracy and AUC representing meaningful improvements over single modal and concatenation based fusion. Ablation studies demonstrate cross modal attention provides critical gains beyond within modality attention, while multi head mechanisms achieve improved calibration. Uncertainty quantification reduces calibration error, enabling selective prediction by rejecting uncertain cases. However, under extreme class imbalance, attention mechanisms show vulnerability to majority class collapse. These findings provide actionable guidance: attention based architecture performs well across typical scenarios, while extreme imbalance requires specialized techniques. Our system maintains deployment efficiency, enabling real time inspection with characterized capabilities and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deteriorating civil infrastructure requires automated inspection techniques overcoming limitations of visual assessment. While Ground Penetrating Radar and Infrared Thermography enable subsurface defect detection, single modal approaches face complementary constraints radar struggles with moisture and shallow defects, while thermography exhibits weather dependency and limited depth. This paper presents a multi modal attention network fusing radar temporal patterns with thermal spatial signatures for bridge deck delamination detection. Our architecture introduces temporal attention for radar processing, spatial attention for thermal features, and cross modal fusion with learnable embeddings discovering complementary defect patterns invisible to individual sensors. We incorporate uncertainty quantification through Monte Carlo dropout and learned variance estimation, decomposing uncertainty into epistemic and aleatoric components for safety critical decisions. Experiments on five bridge datasets reveal that on balanced to moderately imbalanced data, our approach substantially outperforms baselines in accuracy and AUC representing meaningful improvements over single modal and concatenation based fusion. Ablation studies demonstrate cross modal attention provides critical gains beyond within modality attention, while multi head mechanisms achieve improved calibration. Uncertainty quantification reduces calibration error, enabling selective prediction by rejecting uncertain cases. However, under extreme class imbalance, attention mechanisms show vulnerability to majority class collapse. These findings provide actionable guidance: attention based architecture performs well across typical scenarios, while extreme imbalance requires specialized techniques. Our system maintains deployment efficiency, enabling real time inspection with characterized capabilities and limitations."
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T07:16:18Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    7,
                    16,
                    18,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Alireza Moayedikia"
                    },
                    {
                        "name": "Sattar Dorafshan"
                    }
                ],
                "author_detail": {
                    "name": "Sattar Dorafshan"
                },
                "author": "Sattar Dorafshan"
            },
            {
                "id": "http://arxiv.org/abs/2512.16465v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.16465v2",
                "title": "cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cuPilot: A Strategy-Coordinated Multi-agent Framework for CUDA Kernel Evolution"
                },
                "updated": "2025-12-23T07:16:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    7,
                    16,
                    16,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.16465v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.16465v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CUDA kernels is a challenging and labor-intensive task, given the need for hardware-software co-design expertise and the proprietary nature of high-performance kernel libraries. While recent large language models (LLMs) combined with evolutionary algorithms show promise in automatic kernel optimization, existing approaches often fall short in performance due to their suboptimal agent designs and mismatched evolution representations. This work identifies these mismatches and proposes cuPilot, a strategy-coordinated multi-agent framework that introduces strategy as an intermediate semantic representation for kernel evolution. Key contributions include a strategy-coordinated evolution algorithm, roofline-guided prompting, and strategy-level population initialization. Experimental results show that the generated kernels by cuPilot achieve an average speed up of 3.09$\\times$ over PyTorch on a benchmark of 100 kernels. On the GEMM tasks, cuPilot showcases sophisticated optimizations and achieves high utilization of critical hardware units. The generated kernels are open-sourced at https://github.com/champloo2878/cuPilot-Kernels.git."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-18T12:34:00Z",
                "published_parsed": [
                    2025,
                    12,
                    18,
                    12,
                    34,
                    0,
                    3,
                    352,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Jinwu Chen"
                    },
                    {
                        "name": "Qidie Wu"
                    },
                    {
                        "name": "Bin Li"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Xin Si"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    },
                    {
                        "name": "Jun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Yang"
                },
                "author": "Jun Yang"
            },
            {
                "id": "http://arxiv.org/abs/2512.20111v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20111v1",
                "title": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABBEL: LLM Agents Acting through Belief Bottlenecks Expressed in Language"
                },
                "updated": "2025-12-23T07:11:26Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    7,
                    11,
                    26,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20111v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20111v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the length of sequential decision-making tasks increases, it becomes computationally impractical to keep full interaction histories in context. We introduce a general framework for LLM agents to maintain concise contexts through multi-step interaction: Acting through Belief Bottlenecks Expressed in Language (ABBEL), and methods to further improve ABBEL agents with RL post-training. ABBEL replaces long multi-step interaction history by a belief state, i.e., a natural language summary of what has been discovered about task-relevant unknowns. Under ABBEL, at each step the agent first updates a prior belief with the most recent observation from the environment to form a posterior belief, then uses only the posterior to select an action. We systematically evaluate frontier models under ABBEL across six diverse multi-step environments, finding that ABBEL supports generating interpretable beliefs while maintaining near-constant memory use over interaction steps. However, bottleneck approaches are generally prone to error propagation, which we observe causing inferior performance when compared to the full context setting due to errors in belief updating. Therefore, we train LLMs to generate and act on beliefs within the ABBEL framework via reinforcement learning (RL). We experiment with belief grading, to reward higher quality beliefs, as well as belief length penalties to reward more compressed beliefs. Our experiments demonstrate the ability of RL to improve ABBEL's performance beyond the full context setting, while using less memory than contemporaneous approaches."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T07:11:26Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    7,
                    11,
                    26,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Aly Lidayan"
                    },
                    {
                        "name": "Jakob Bjorner"
                    },
                    {
                        "name": "Satvik Golechha"
                    },
                    {
                        "name": "Kartik Goyal"
                    },
                    {
                        "name": "Alane Suhr"
                    }
                ],
                "author_detail": {
                    "name": "Alane Suhr"
                },
                "author": "Alane Suhr"
            },
            {
                "id": "http://arxiv.org/abs/2505.17266v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2505.17266v3",
                "title": "Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Select2Reason: Efficient Instruction-Tuning Data Selection for Long-CoT Reasoning"
                },
                "updated": "2025-12-23T07:05:40Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    7,
                    5,
                    40,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2505.17266v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2505.17266v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "A practical approach to activate long chain-of-thoughts reasoning ability in pre-trained large language models is to perform supervised fine-tuning on instruction datasets synthesized by strong Large Reasoning Models such as DeepSeek-R1, offering a cost-effective alternative to reinforcement learning. However, large-scale instruction sets with more than 100k samples incur significant training overhead, while effective strategies for automatic long-CoT instruction selection still remain unexplored. In this work, we propose Select2Reason, a novel and efficient instruction-tuning data selection framework for long-CoT reasoning. From the perspective of emergence of rethinking behaviors like self-correction and backtracking, we investigate common metrics that may determine the quality of long-CoT reasoning instructions. Select2Reason leverages a quantifier to estimate difficulty of question and jointly incorporates a reasoning trace length-based heuristic through a weighted scheme for ranking to prioritize high-utility examples. Empirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only 10% of the data selected by Select2Reason achieves performance competitive with or superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across three competition-level and six comprehensive mathematical benchmarks. Further experiments highlight the scalability in varying data size, efficiency during inference, and its adaptability to other instruction pools with minimal cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A practical approach to activate long chain-of-thoughts reasoning ability in pre-trained large language models is to perform supervised fine-tuning on instruction datasets synthesized by strong Large Reasoning Models such as DeepSeek-R1, offering a cost-effective alternative to reinforcement learning. However, large-scale instruction sets with more than 100k samples incur significant training overhead, while effective strategies for automatic long-CoT instruction selection still remain unexplored. In this work, we propose Select2Reason, a novel and efficient instruction-tuning data selection framework for long-CoT reasoning. From the perspective of emergence of rethinking behaviors like self-correction and backtracking, we investigate common metrics that may determine the quality of long-CoT reasoning instructions. Select2Reason leverages a quantifier to estimate difficulty of question and jointly incorporates a reasoning trace length-based heuristic through a weighted scheme for ranking to prioritize high-utility examples. Empirical results on OpenR1-Math-220k demonstrate that fine-tuning LLM on only 10% of the data selected by Select2Reason achieves performance competitive with or superior to full-data tuning and open-source baseline OpenR1-Qwen-7B across three competition-level and six comprehensive mathematical benchmarks. Further experiments highlight the scalability in varying data size, efficiency during inference, and its adaptability to other instruction pools with minimal cost."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-05-22T20:24:08Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    20,
                    24,
                    8,
                    3,
                    142,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Cehao Yang"
                    },
                    {
                        "name": "Xueyuan Lin"
                    },
                    {
                        "name": "Xiaojun Wu"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Honghao Liu"
                    },
                    {
                        "name": "Hui Xiong"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo"
            },
            {
                "id": "http://arxiv.org/abs/2508.20615v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.20615v2",
                "title": "EmoCAST: Emotional Talking Portrait via Emotive Text Description",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoCAST: Emotional Talking Portrait via Emotive Text Description"
                },
                "updated": "2025-12-23T07:03:05Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    7,
                    3,
                    5,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.20615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.20615v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Emotional talking head synthesis aims to generate talking portrait videos with vivid expressions. Existing methods still exhibit limitations in control flexibility, motion naturalness, and expression quality. Moreover, currently available datasets are mainly collected in lab settings, further exacerbating these shortcomings and hindering real-world deployment. To address these challenges, we propose EmoCAST, a diffusion-based talking head framework for precise, text-driven emotional synthesis. Its contributions are threefold: (1) architectural modules that enable effective text control; (2) an emotional talking-head dataset that expands the framework's ability; and (3) training strategies that further improve performance. Specifically, for appearance modeling, emotional prompts are integrated through a text-guided emotive attention module, enhancing spatial knowledge to improve emotion understanding. To strengthen audio-emotion alignment, we introduce an emotive audio attention module to capture the interplay between controlled emotion and driving audio, generating emotion-aware features to guide precise facial motion synthesis. Additionally, we construct a large-scale, in-the-wild emotional talking head dataset with emotive text descriptions to optimize the framework's performance. Based on this dataset, we propose an emotion-aware sampling strategy and a progressive functional training strategy that improve the model's ability to capture nuanced expressive features and achieve accurate lip-sync. Overall, EmoCAST achieves state-of-the-art performance in generating realistic, emotionally expressive, and audio-synchronized talking-head videos. Project Page: https://github.com/GVCLab/EmoCAST",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotional talking head synthesis aims to generate talking portrait videos with vivid expressions. Existing methods still exhibit limitations in control flexibility, motion naturalness, and expression quality. Moreover, currently available datasets are mainly collected in lab settings, further exacerbating these shortcomings and hindering real-world deployment. To address these challenges, we propose EmoCAST, a diffusion-based talking head framework for precise, text-driven emotional synthesis. Its contributions are threefold: (1) architectural modules that enable effective text control; (2) an emotional talking-head dataset that expands the framework's ability; and (3) training strategies that further improve performance. Specifically, for appearance modeling, emotional prompts are integrated through a text-guided emotive attention module, enhancing spatial knowledge to improve emotion understanding. To strengthen audio-emotion alignment, we introduce an emotive audio attention module to capture the interplay between controlled emotion and driving audio, generating emotion-aware features to guide precise facial motion synthesis. Additionally, we construct a large-scale, in-the-wild emotional talking head dataset with emotive text descriptions to optimize the framework's performance. Based on this dataset, we propose an emotion-aware sampling strategy and a progressive functional training strategy that improve the model's ability to capture nuanced expressive features and achieve accurate lip-sync. Overall, EmoCAST achieves state-of-the-art performance in generating realistic, emotionally expressive, and audio-synchronized talking-head videos. Project Page: https://github.com/GVCLab/EmoCAST"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-28T10:02:06Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    10,
                    2,
                    6,
                    3,
                    240,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV"
                },
                "authors": [
                    {
                        "name": "Yiguo Jiang"
                    },
                    {
                        "name": "Xiaodong Cun"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Yudian Zheng"
                    },
                    {
                        "name": "Fan Tang"
                    },
                    {
                        "name": "Chi-Man Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chi-Man Pun"
                },
                "author": "Chi-Man Pun"
            },
            {
                "id": "http://arxiv.org/abs/2511.09586v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.09586v3",
                "title": "Environment Scaling for Interactive Agentic Experience Collection: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Environment Scaling for Interactive Agentic Experience Collection: A Survey"
                },
                "updated": "2025-12-23T06:43:03Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    6,
                    43,
                    3,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.09586v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.09586v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior and long-term decision-making, training on static datasets built from human-level knowledge is insufficient. These datasets are costly to construct and lack both dynamism and realism. A growing consensus is that agents should instead interact directly with environments and learn from experience through reinforcement learning. We formalize this iterative process as the Generation-Execution-Feedback (GEF) loop, where environments generate tasks to challenge agents, return observations in response to agents' actions during task execution, and provide evaluative feedback on rollouts for subsequent learning. Under this paradigm, environments function as indispensable producers of experiential data, highlighting the need to scale them toward greater complexity, realism, and interactivity. In this survey, we systematically review representative methods for environment scaling from a pioneering environment-centric perspective and organize them along the stages of the GEF loop, namely task generation, task execution, and feedback. We further analyze implementation frameworks, challenges, and applications, consolidating fragmented advances and outlining future research directions for agent intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior and long-term decision-making, training on static datasets built from human-level knowledge is insufficient. These datasets are costly to construct and lack both dynamism and realism. A growing consensus is that agents should instead interact directly with environments and learn from experience through reinforcement learning. We formalize this iterative process as the Generation-Execution-Feedback (GEF) loop, where environments generate tasks to challenge agents, return observations in response to agents' actions during task execution, and provide evaluative feedback on rollouts for subsequent learning. Under this paradigm, environments function as indispensable producers of experiential data, highlighting the need to scale them toward greater complexity, realism, and interactivity. In this survey, we systematically review representative methods for environment scaling from a pioneering environment-centric perspective and organize them along the stages of the GEF loop, namely task generation, task execution, and feedback. We further analyze implementation frameworks, challenges, and applications, consolidating fragmented advances and outlining future research directions for agent intelligence."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-12T12:56:25Z",
                "published_parsed": [
                    2025,
                    11,
                    12,
                    12,
                    56,
                    25,
                    2,
                    316,
                    0
                ],
                "arxiv_comment": "22 pages, 5 figures, SEA Workshop @ NeurIPS 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Yuchen Huang"
                    },
                    {
                        "name": "Sijia Li"
                    },
                    {
                        "name": "Minghao Liu"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Shijue Huang"
                    },
                    {
                        "name": "Zhiyuan Fan"
                    },
                    {
                        "name": "Hou Pong Chan"
                    },
                    {
                        "name": "Yi R. Fung"
                    }
                ],
                "author_detail": {
                    "name": "Yi R. Fung"
                },
                "author": "Yi R. Fung"
            },
            {
                "id": "http://arxiv.org/abs/2512.20086v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20086v1",
                "title": "Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-Temporal Graphs Beyond Grids: Benchmark for Maritime Anomaly Detection"
                },
                "updated": "2025-12-23T06:28:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    6,
                    28,
                    12,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20086v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Spatio-temporal graph neural networks (ST-GNNs) have achieved notable success in structured domains such as road traffic and public transportation, where spatial entities can be naturally represented as fixed nodes. In contrast, many real-world systems including maritime traffic lack such fixed anchors, making the construction of spatio-temporal graphs a fundamental challenge. Anomaly detection in these non-grid environments is particularly difficult due to the absence of canonical reference points, the sparsity and irregularity of trajectories, and the fact that anomalies may manifest at multiple granularities. In this work, we introduce a novel benchmark dataset for anomaly detection in the maritime domain, extending the Open Maritime Traffic Analysis Dataset (OMTAD) into a benchmark tailored for graph-based anomaly detection. Our dataset enables systematic evaluation across three different granularities: node-level, edge-level, and graph-level anomalies. We plan to employ two specialized LLM-based agents: \\emph{Trajectory Synthesizer} and \\emph{Anomaly Injector} to construct richer interaction contexts and generate semantically meaningful anomalies. We expect this benchmark to promote reproducibility and to foster methodological advances in anomaly detection for non-grid spatio-temporal systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatio-temporal graph neural networks (ST-GNNs) have achieved notable success in structured domains such as road traffic and public transportation, where spatial entities can be naturally represented as fixed nodes. In contrast, many real-world systems including maritime traffic lack such fixed anchors, making the construction of spatio-temporal graphs a fundamental challenge. Anomaly detection in these non-grid environments is particularly difficult due to the absence of canonical reference points, the sparsity and irregularity of trajectories, and the fact that anomalies may manifest at multiple granularities. In this work, we introduce a novel benchmark dataset for anomaly detection in the maritime domain, extending the Open Maritime Traffic Analysis Dataset (OMTAD) into a benchmark tailored for graph-based anomaly detection. Our dataset enables systematic evaluation across three different granularities: node-level, edge-level, and graph-level anomalies. We plan to employ two specialized LLM-based agents: \\emph{Trajectory Synthesizer} and \\emph{Anomaly Injector} to construct richer interaction contexts and generate semantically meaningful anomalies. We expect this benchmark to promote reproducibility and to foster methodological advances in anomaly detection for non-grid spatio-temporal systems."
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T06:28:12Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    6,
                    28,
                    12,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "Accepted at NeurIPS 2025 Workshop in AI for Science: The Reach and Limits of AI for Scientific Discovery",
                "arxiv_primary_category": {
                    "term": "cs.LG"
                },
                "authors": [
                    {
                        "name": "Jeehong Kim"
                    },
                    {
                        "name": "Youngseok Hwang"
                    },
                    {
                        "name": "Minchan Kim"
                    },
                    {
                        "name": "Sungho Bae"
                    },
                    {
                        "name": "Hyunwoo Park"
                    }
                ],
                "author_detail": {
                    "name": "Hyunwoo Park"
                },
                "author": "Hyunwoo Park"
            },
            {
                "id": "http://arxiv.org/abs/2512.20083v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20083v1",
                "title": "Detecting Non-Optimal Decisions of Embodied Agents via Diversity-Guided Metamorphic Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Non-Optimal Decisions of Embodied Agents via Diversity-Guided Metamorphic Testing"
                },
                "updated": "2025-12-23T06:27:18Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    6,
                    27,
                    18,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20083v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As embodied agents advance toward real-world deployment, ensuring optimal decisions becomes critical for resource-constrained applications. Current evaluation methods focus primarily on functional correctness, overlooking the non-functional optimality of generated plans. This gap can lead to significant performance degradation and resource waste. We identify and formalize the problem of Non-optimal Decisions (NoDs), where agents complete tasks successfully but inefficiently. We present NoD-DGMT, a systematic framework for detecting NoDs in embodied agent task planning via diversity-guided metamorphic testing. Our key insight is that optimal planners should exhibit invariant behavioral properties under specific transformations. We design four novel metamorphic relations capturing fundamental optimality properties: position detour suboptimality, action optimality completeness, condition refinement monotonicity, and scene perturbation invariance. To maximize detection efficiency, we introduce a diversity-guided selection strategy that actively selects test cases exploring different violation categories, avoiding redundant evaluations while ensuring comprehensive diversity coverage. Extensive experiments on the AI2-THOR simulator with four state-of-the-art planning models demonstrate that NoD-DGMT achieves violation detection rates of 31.9% on average, with our diversity-guided filter improving rates by 4.3% and diversity scores by 3.3 on average. NoD-DGMT significantly outperforms six baseline methods, with 16.8% relative improvement over the best baseline, and demonstrates consistent superiority across different model architectures and task complexities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As embodied agents advance toward real-world deployment, ensuring optimal decisions becomes critical for resource-constrained applications. Current evaluation methods focus primarily on functional correctness, overlooking the non-functional optimality of generated plans. This gap can lead to significant performance degradation and resource waste. We identify and formalize the problem of Non-optimal Decisions (NoDs), where agents complete tasks successfully but inefficiently. We present NoD-DGMT, a systematic framework for detecting NoDs in embodied agent task planning via diversity-guided metamorphic testing. Our key insight is that optimal planners should exhibit invariant behavioral properties under specific transformations. We design four novel metamorphic relations capturing fundamental optimality properties: position detour suboptimality, action optimality completeness, condition refinement monotonicity, and scene perturbation invariance. To maximize detection efficiency, we introduce a diversity-guided selection strategy that actively selects test cases exploring different violation categories, avoiding redundant evaluations while ensuring comprehensive diversity coverage. Extensive experiments on the AI2-THOR simulator with four state-of-the-art planning models demonstrate that NoD-DGMT achieves violation detection rates of 31.9% on average, with our diversity-guided filter improving rates by 4.3% and diversity scores by 3.3 on average. NoD-DGMT significantly outperforms six baseline methods, with 16.8% relative improvement over the best baseline, and demonstrates consistent superiority across different model architectures and task complexities."
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T06:27:18Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    6,
                    27,
                    18,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE"
                },
                "authors": [
                    {
                        "name": "Wenzhao Wu"
                    },
                    {
                        "name": "Yahui Tang"
                    },
                    {
                        "name": "Mingfei Cheng"
                    },
                    {
                        "name": "Wenbing Tang"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu"
            },
            {
                "id": "http://arxiv.org/abs/2512.20082v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20082v1",
                "title": "Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Financial Sentiment Analysis for NIFTY 50 via Instruction-Tuned LLMs , RAG and Reinforcement Learning Approaches"
                },
                "updated": "2025-12-23T06:27:12Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    6,
                    27,
                    12,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20082v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Financial sentiment analysis plays a crucial role in informing investment decisions, assessing market risk, and predicting stock price trends. Existing works in financial sentiment analysis have not considered the impact of stock prices or market feedback on sentiment analysis. In this paper, we propose an adaptive framework that integrates large language models (LLMs) with real-world stock market feedback to improve sentiment classification in the context of the Indian stock market. The proposed methodology fine-tunes the LLaMA 3.2 3B model using instruction-based learning on the SentiFin dataset. To enhance sentiment predictions, a retrieval-augmented generation (RAG) pipeline is employed that dynamically selects multi-source contextual information based on the cosine similarity of the sentence embeddings. Furthermore, a feedback-driven module is introduced that adjusts the reliability of the source by comparing predicted sentiment with actual next-day stock returns, allowing the system to iteratively adapt to market behavior. To generalize this adaptive mechanism across temporal data, a reinforcement learning agent trained using proximal policy optimization (PPO) is incorporated. The PPO agent learns to optimize source weighting policies based on cumulative reward signals from sentiment-return alignment. Experimental results on NIFTY 50 news headlines collected from 2024 to 2025 demonstrate that the proposed system significantly improves classification accuracy, F1-score, and market alignment over baseline models and static retrieval methods. The results validate the potential of combining instruction-tuned LLMs with dynamic feedback and reinforcement learning for robust, market-aware financial sentiment modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial sentiment analysis plays a crucial role in informing investment decisions, assessing market risk, and predicting stock price trends. Existing works in financial sentiment analysis have not considered the impact of stock prices or market feedback on sentiment analysis. In this paper, we propose an adaptive framework that integrates large language models (LLMs) with real-world stock market feedback to improve sentiment classification in the context of the Indian stock market. The proposed methodology fine-tunes the LLaMA 3.2 3B model using instruction-based learning on the SentiFin dataset. To enhance sentiment predictions, a retrieval-augmented generation (RAG) pipeline is employed that dynamically selects multi-source contextual information based on the cosine similarity of the sentence embeddings. Furthermore, a feedback-driven module is introduced that adjusts the reliability of the source by comparing predicted sentiment with actual next-day stock returns, allowing the system to iteratively adapt to market behavior. To generalize this adaptive mechanism across temporal data, a reinforcement learning agent trained using proximal policy optimization (PPO) is incorporated. The PPO agent learns to optimize source weighting policies based on cumulative reward signals from sentiment-return alignment. Experimental results on NIFTY 50 news headlines collected from 2024 to 2025 demonstrate that the proposed system significantly improves classification accuracy, F1-score, and market alignment over baseline models and static retrieval methods. The results validate the potential of combining instruction-tuned LLMs with dynamic feedback and reinforcement learning for robust, market-aware financial sentiment modeling."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T06:27:12Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    6,
                    27,
                    12,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "Accepted in CODS 2025",
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Chaithra"
                    },
                    {
                        "name": "Kamesh Kadimisetty"
                    },
                    {
                        "name": "Biju R Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Biju R Mohan"
                },
                "author": "Biju R Mohan"
            },
            {
                "id": "http://arxiv.org/abs/2512.20080v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20080v1",
                "title": "CBA: Communication-Bound-Aware Cross-Domain Resource Assignment for Pipeline-Parallel Distributed LLM Training in Dynamic Multi-DC Optical Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CBA: Communication-Bound-Aware Cross-Domain Resource Assignment for Pipeline-Parallel Distributed LLM Training in Dynamic Multi-DC Optical Networks"
                },
                "updated": "2025-12-23T06:26:20Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    6,
                    26,
                    20,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20080v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "We propose a communication-bound-aware cross-domain resource assignment framework for pipeline-parallel distributed training over multi-datacenter optical networks, which lowers iteration time by 31.25% and reduces 13.20% blocking requests compared to baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a communication-bound-aware cross-domain resource assignment framework for pipeline-parallel distributed training over multi-datacenter optical networks, which lowers iteration time by 31.25% and reduces 13.20% blocking requests compared to baselines."
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T06:26:20Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    6,
                    26,
                    20,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI"
                },
                "authors": [
                    {
                        "name": "Dianxuan Fu"
                    },
                    {
                        "name": "Xiaomin Liu"
                    },
                    {
                        "name": "Yihao Zhang"
                    },
                    {
                        "name": "Shikui Shen"
                    },
                    {
                        "name": "Weisheng Hu"
                    },
                    {
                        "name": "Qunbi Zhuge"
                    }
                ],
                "author_detail": {
                    "name": "Qunbi Zhuge"
                },
                "author": "Qunbi Zhuge"
            },
            {
                "id": "http://arxiv.org/abs/2512.19012v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.19012v2",
                "title": "DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DramaBench: A Six-Dimensional Evaluation Framework for Drama Script Continuation"
                },
                "updated": "2025-12-23T06:23:22Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    6,
                    23,
                    22,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.19012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.19012v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Drama script continuation requires models to maintain character consistency, advance plot coherently, and preserve dramatic structurecapabilities that existing benchmarks fail to evaluate comprehensively. We present DramaBench, the first large-scale benchmark for evaluating drama script continuation across six independent dimensions: Format Standards, Narrative Efficiency, Character Consistency, Emotional Depth, Logic Consistency, and Conflict Handling. Our framework combines rulebased analysis with LLM-based labeling and statistical metrics, ensuring objective and reproducible evaluation. We conduct comprehensive evaluation of 8 state-of-the-art language models on 1,103 scripts (8,824 evaluations total), with rigorous statistical significance testing (252 pairwise comparisons, 65.9% significant) and human validation (188 scripts, substantial agreement on 3/5 dimensions). Our ablation studies confirm all six dimensions capture independent quality aspects (mean | r | = 0.020). DramaBench provides actionable, dimensionspecific feedback for model improvement and establishes a rigorous standard for creative writing evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drama script continuation requires models to maintain character consistency, advance plot coherently, and preserve dramatic structurecapabilities that existing benchmarks fail to evaluate comprehensively. We present DramaBench, the first large-scale benchmark for evaluating drama script continuation across six independent dimensions: Format Standards, Narrative Efficiency, Character Consistency, Emotional Depth, Logic Consistency, and Conflict Handling. Our framework combines rulebased analysis with LLM-based labeling and statistical metrics, ensuring objective and reproducible evaluation. We conduct comprehensive evaluation of 8 state-of-the-art language models on 1,103 scripts (8,824 evaluations total), with rigorous statistical significance testing (252 pairwise comparisons, 65.9% significant) and human validation (188 scripts, substantial agreement on 3/5 dimensions). Our ablation studies confirm all six dimensions capture independent quality aspects (mean | r | = 0.020). DramaBench provides actionable, dimensionspecific feedback for model improvement and establishes a rigorous standard for creative writing evaluation."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-22T04:03:01Z",
                "published_parsed": [
                    2025,
                    12,
                    22,
                    4,
                    3,
                    1,
                    0,
                    356,
                    0
                ],
                "arxiv_comment": "Project page: https://dramabench.pages.dev/",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Shijian Ma"
                    },
                    {
                        "name": "Yunqi Huang"
                    },
                    {
                        "name": "Yan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yan Lin"
                },
                "author": "Yan Lin"
            },
            {
                "id": "http://arxiv.org/abs/2512.20077v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20077v1",
                "title": "Fault Injection Attacks on Machine Learning-based Quantum Computer Readout Error Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fault Injection Attacks on Machine Learning-based Quantum Computer Readout Error Correction"
                },
                "updated": "2025-12-23T06:19:14Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    6,
                    19,
                    14,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20077v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20077v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Machine-learning (ML) classifiers are increasingly used in quantum computing systems to improve multi-qubit readout discrimination and to mitigate correlated readout errors. These ML classifiers are an integral component of today's quantum computer's control and readout stacks. This paper is the first to analyze the susceptibility of such ML classifiers to physical fault-injection which can result in generation of incorrect readout results from quantum computers. The study targets 5-qubit (thus 32-class) readout error-correction model. Using the ChipWhisperer Husky for physical voltage glitching, this work leverages an automated algorithm for scanning the fault injection parameter search space to find various successful faults in all the layers of the target ML model. Across repeated trials, this work finds that fault susceptibility is strongly layer-dependent: early-layers demonstrate higher rates of misprediction when faults are triggered in them, whereas later layers have smaller misprediction rates. This work further characterizes the resulting readout failures at the bitstring level using Hamming-distance and per-bit flip statistics, showing that single-shot glitches can induce structured readout corruption rather than purely random noise. These results motivate treating ML-based quantum computer readout and readout correction as a security-critical component of quantum systems and highlight the need for lightweight, deployment-friendly fault detection and redundancy mechanisms in the quantum computer readout pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine-learning (ML) classifiers are increasingly used in quantum computing systems to improve multi-qubit readout discrimination and to mitigate correlated readout errors. These ML classifiers are an integral component of today's quantum computer's control and readout stacks. This paper is the first to analyze the susceptibility of such ML classifiers to physical fault-injection which can result in generation of incorrect readout results from quantum computers. The study targets 5-qubit (thus 32-class) readout error-correction model. Using the ChipWhisperer Husky for physical voltage glitching, this work leverages an automated algorithm for scanning the fault injection parameter search space to find various successful faults in all the layers of the target ML model. Across repeated trials, this work finds that fault susceptibility is strongly layer-dependent: early-layers demonstrate higher rates of misprediction when faults are triggered in them, whereas later layers have smaller misprediction rates. This work further characterizes the resulting readout failures at the bitstring level using Hamming-distance and per-bit flip statistics, showing that single-shot glitches can induce structured readout corruption rather than purely random noise. These results motivate treating ML-based quantum computer readout and readout correction as a security-critical component of quantum systems and highlight the need for lightweight, deployment-friendly fault detection and redundancy mechanisms in the quantum computer readout pipelines."
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T06:19:14Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    6,
                    19,
                    14,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph"
                },
                "authors": [
                    {
                        "name": "Anthony Etim"
                    },
                    {
                        "name": "Jakub Szefer"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Szefer"
                },
                "author": "Jakub Szefer"
            },
            {
                "id": "http://arxiv.org/abs/2512.20074v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20074v1",
                "title": "Reason2Decide: Rationale-Driven Multi-Task Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reason2Decide: Rationale-Driven Multi-Task Learning"
                },
                "updated": "2025-12-23T05:58:47Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    5,
                    58,
                    47,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20074v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the wide adoption of Large Language Models (LLM)s, clinical decision support systems face a critical challenge: achieving high predictive accuracy while generating explanations aligned with the predictions. Current approaches suffer from exposure bias leading to misaligned explanations. We propose Reason2Decide, a two-stage training framework that addresses key challenges in self-rationalization, including exposure bias and task separation. In Stage-1, our model is trained on rationale generation, while in Stage-2, we jointly train on label prediction and rationale generation, applying scheduled sampling to gradually transition from conditioning on gold labels to model predictions. We evaluate Reason2Decide on three medical datasets, including a proprietary triage dataset and public biomedical QA datasets. Across model sizes, Reason2Decide outperforms other fine-tuning baselines and some zero-shot LLMs in prediction (F1) and rationale fidelity (BERTScore, BLEU, LLM-as-a-Judge). In triage, Reason2Decide is rationale source-robust across LLM-generated, nurse-authored, and nurse-post-processed rationales. In our experiments, while using only LLM-generated rationales in Stage-1, Reason2Decide outperforms other fine-tuning variants. This indicates that LLM-generated rationales are suitable for pretraining models, reducing reliance on human annotations. Remarkably, Reason2Decide achieves these gains with models 40x smaller than contemporary foundation models, making clinical reasoning more accessible for resource-constrained deployments while still providing explainable decision support."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T05:58:47Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    5,
                    58,
                    47,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "H M Quamran Hasan"
                    },
                    {
                        "name": "Housam Khalifa Bashier"
                    },
                    {
                        "name": "Jiayi Dai"
                    },
                    {
                        "name": "Mi-Young Kim"
                    },
                    {
                        "name": "Randy Goebel"
                    }
                ],
                "author_detail": {
                    "name": "Randy Goebel"
                },
                "author": "Randy Goebel"
            },
            {
                "id": "http://arxiv.org/abs/2408.02152v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2408.02152v3",
                "title": "Generative Retrieval with Few-shot Indexing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Retrieval with Few-shot Indexing"
                },
                "updated": "2025-12-23T05:50:45Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    5,
                    50,
                    45,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2408.02152v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2408.02152v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Existing generative retrieval (GR) methods rely on training-based indexing, which fine-tunes a model to memorise associations between queries and the document identifiers (docids) of relevant documents. Training-based indexing suffers from high training costs, under-utilisation of pre-trained knowledge in large language models (LLMs), and limited adaptability to dynamic document corpora. To address the issues, we propose a few-shot indexing-based GR framework (Few-Shot GR). It has a few-shot indexing process without any training, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Moreover, we devise few-shot indexing with one-to-many mapping to further enhance Few-Shot GR. Experiments show that Few-Shot GR achieves superior performance to state-of-the-art GR methods requiring heavy training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing generative retrieval (GR) methods rely on training-based indexing, which fine-tunes a model to memorise associations between queries and the document identifiers (docids) of relevant documents. Training-based indexing suffers from high training costs, under-utilisation of pre-trained knowledge in large language models (LLMs), and limited adaptability to dynamic document corpora. To address the issues, we propose a few-shot indexing-based GR framework (Few-Shot GR). It has a few-shot indexing process without any training, where we prompt an LLM to generate docids for all documents in a corpus, ultimately creating a docid bank for the entire corpus. During retrieval, we feed a query to the same LLM and constrain it to generate a docid within the docid bank created during indexing, and then map the generated docid back to its corresponding document. Moreover, we devise few-shot indexing with one-to-many mapping to further enhance Few-Shot GR. Experiments show that Few-Shot GR achieves superior performance to state-of-the-art GR methods requiring heavy training."
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2024-08-04T22:00:34Z",
                "published_parsed": [
                    2024,
                    8,
                    4,
                    22,
                    0,
                    34,
                    6,
                    217,
                    0
                ],
                "arxiv_comment": "Accepted for publication at the 48th European Conference on Information Retrieval (ECIR 2026)",
                "arxiv_primary_category": {
                    "term": "cs.IR"
                },
                "authors": [
                    {
                        "name": "Arian Askari"
                    },
                    {
                        "name": "Chuan Meng"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    },
                    {
                        "name": "Suzan Verberne"
                    }
                ],
                "author_detail": {
                    "name": "Suzan Verberne"
                },
                "author": "Suzan Verberne"
            },
            {
                "id": "http://arxiv.org/abs/2512.20062v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20062v1",
                "title": "On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Effectiveness of Instruction-Tuning Local LLMs for Identifying Software Vulnerabilities"
                },
                "updated": "2025-12-23T05:30:53Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    5,
                    30,
                    53,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20062v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large Language Models (LLMs) show significant promise in automating software vulnerability analysis, a critical task given the impact of security failure of modern software systems. However, current approaches in using LLMs to automate vulnerability analysis mostly rely on using online API-based LLM services, requiring the user to disclose the source code in development. Moreover, they predominantly frame the task as a binary classification(vulnerable or not vulnerable), limiting potential practical utility. This paper addresses these limitations by reformulating the problem as Software Vulnerability Identification (SVI), where LLMs are asked to output the type of weakness in Common Weakness Enumeration (CWE) IDs rather than simply indicating the presence or absence of a vulnerability. We also tackle the reliance on large, API-based LLMs by demonstrating that instruction-tuning smaller, locally deployable LLMs can achieve superior identification performance. In our analysis, instruct-tuning a local LLM showed better overall performance and cost trade-off than online API-based LLMs. Our findings indicate that instruct-tuned local models represent a more effective, secure, and practical approach for leveraging LLMs in real-world vulnerability management workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show significant promise in automating software vulnerability analysis, a critical task given the impact of security failure of modern software systems. However, current approaches in using LLMs to automate vulnerability analysis mostly rely on using online API-based LLM services, requiring the user to disclose the source code in development. Moreover, they predominantly frame the task as a binary classification(vulnerable or not vulnerable), limiting potential practical utility. This paper addresses these limitations by reformulating the problem as Software Vulnerability Identification (SVI), where LLMs are asked to output the type of weakness in Common Weakness Enumeration (CWE) IDs rather than simply indicating the presence or absence of a vulnerability. We also tackle the reliance on large, API-based LLMs by demonstrating that instruction-tuning smaller, locally deployable LLMs can achieve superior identification performance. In our analysis, instruct-tuning a local LLM showed better overall performance and cost trade-off than online API-based LLMs. Our findings indicate that instruct-tuned local models represent a more effective, secure, and practical approach for leveraging LLMs in real-world vulnerability management workflows."
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T05:30:53Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    5,
                    30,
                    53,
                    1,
                    357,
                    0
                ],
                "arxiv_comment": "The 9th International Conference on Mobile Internet Security (MobiSec 2025)",
                "arxiv_primary_category": {
                    "term": "cs.CR"
                },
                "authors": [
                    {
                        "name": "Sangryu Park"
                    },
                    {
                        "name": "Gihyuk Ko"
                    },
                    {
                        "name": "Homook Cho"
                    }
                ],
                "author_detail": {
                    "name": "Homook Cho"
                },
                "author": "Homook Cho"
            },
            {
                "id": "http://arxiv.org/abs/2512.20061v1",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2512.20061v1",
                "title": "Scaling Reinforcement Learning for Content Moderation with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Reinforcement Learning for Content Moderation with Large Language Models"
                },
                "updated": "2025-12-23T05:27:16Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    5,
                    27,
                    16,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2512.20061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2512.20061v1",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content moderation at scale remains one of the most pressing challenges in today's digital ecosystem, where billions of user- and AI-generated artifacts must be continuously evaluated for policy violations. Although recent advances in large language models (LLMs) have demonstrated strong potential for policy-grounded moderation, the practical challenges of training these systems to achieve expert-level accuracy in real-world settings remain largely unexplored, particularly in regimes characterized by label sparsity, evolving policy definitions, and the need for nuanced reasoning beyond shallow pattern matching. In this work, we present a comprehensive empirical investigation of scaling reinforcement learning (RL) for content classification, systematically evaluating multiple RL training recipes and reward-shaping strategies-including verifiable rewards and LLM-as-judge frameworks-to transform general-purpose language models into specialized, policy-aligned classifiers across three real-world content moderation tasks. Our findings provide actionable insights for industrial-scale moderation systems, demonstrating that RL exhibits sigmoid-like scaling behavior in which performance improves smoothly with increased training data, rollouts, and optimization steps before gradually saturating. Moreover, we show that RL substantially improves performance on tasks requiring complex policy-grounded reasoning while achieving up to 100x higher data efficiency than supervised fine-tuning, making it particularly effective in domains where expert annotations are scarce or costly."
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-12-23T05:27:16Z",
                "published_parsed": [
                    2025,
                    12,
                    23,
                    5,
                    27,
                    16,
                    1,
                    357,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI"
                },
                "authors": [
                    {
                        "name": "Hamed Firooz"
                    },
                    {
                        "name": "Rui Liu"
                    },
                    {
                        "name": "Yuchen Lu"
                    },
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Fangzhou Xiong"
                    },
                    {
                        "name": "Xiaoyang Zhang"
                    },
                    {
                        "name": "Changshu Jian"
                    },
                    {
                        "name": "Zhicheng Zhu"
                    },
                    {
                        "name": "Jiayuan Ma"
                    },
                    {
                        "name": "Jacob Tao"
                    },
                    {
                        "name": "Chaitali Gupta"
                    },
                    {
                        "name": "Xiaochang Peng"
                    },
                    {
                        "name": "Shike Mei"
                    },
                    {
                        "name": "Hang Cui"
                    },
                    {
                        "name": "Yang Qin"
                    },
                    {
                        "name": "Shuo Tang"
                    },
                    {
                        "name": "Jason Gaedtke"
                    },
                    {
                        "name": "Arpit Mittal"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Mittal"
                },
                "author": "Arpit Mittal"
            },
            {
                "id": "http://arxiv.org/abs/2508.04826v3",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2508.04826v3",
                "title": "Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Instability in LLM's Personality Measurements: Effects of Scale, Reasoning, and Conversation History"
                },
                "updated": "2025-12-23T05:07:19Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    5,
                    7,
                    19,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2508.04826v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2508.04826v3",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "Large language models require consistent behavioral patterns for safe deployment, yet there are indications of large variability that may lead to an instable expression of personality traits in these models. We present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive evaluation framework testing 25 open-source models (1B-685B parameters) across 2 million+ responses. Using traditional (BFI, SD3) and novel LLM-adapted personality questionnaires, we systematically vary model size, personas, reasoning modes, question order or paraphrasing, and conversation history. Our findings challenge fundamental assumptions: (1) Question reordering alone can introduce large shifts in personality measurements; (2) Scaling provides limited stability gains: even 400B+ models exhibit standard deviations >0.3 on 5-point scales; (3) Interventions expected to stabilize behavior, such as reasoning and inclusion of conversation history, can paradoxically increase variability; (4) Detailed persona instructions produce mixed effects, with misaligned personas showing significantly higher variability than the helpful assistant baseline; (5) The LLM-adapted questionnaires, despite their improved ecological validity, exhibit instability comparable to human-centric versions. This persistent instability across scales and mitigation strategies suggests that current LLMs lack the architectural foundations for genuine behavioral consistency. For safety-critical applications requiring predictable behavior, these findings indicate that current alignment strategies may be inadequate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models require consistent behavioral patterns for safe deployment, yet there are indications of large variability that may lead to an instable expression of personality traits in these models. We present PERSIST (PERsonality Stability in Synthetic Text), a comprehensive evaluation framework testing 25 open-source models (1B-685B parameters) across 2 million+ responses. Using traditional (BFI, SD3) and novel LLM-adapted personality questionnaires, we systematically vary model size, personas, reasoning modes, question order or paraphrasing, and conversation history. Our findings challenge fundamental assumptions: (1) Question reordering alone can introduce large shifts in personality measurements; (2) Scaling provides limited stability gains: even 400B+ models exhibit standard deviations >0.3 on 5-point scales; (3) Interventions expected to stabilize behavior, such as reasoning and inclusion of conversation history, can paradoxically increase variability; (4) Detailed persona instructions produce mixed effects, with misaligned personas showing significantly higher variability than the helpful assistant baseline; (5) The LLM-adapted questionnaires, despite their improved ecological validity, exhibit instability comparable to human-centric versions. This persistent instability across scales and mitigation strategies suggests that current LLMs lack the architectural foundations for genuine behavioral consistency. For safety-critical applications requiring predictable behavior, these findings indicate that current alignment strategies may be inadequate."
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-08-06T19:11:33Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    19,
                    11,
                    33,
                    2,
                    218,
                    0
                ],
                "arxiv_comment": "Accepted at AAAI 2026, Track on AI Alignment",
                "arxiv_primary_category": {
                    "term": "cs.CL"
                },
                "authors": [
                    {
                        "name": "Tommaso Tosato"
                    },
                    {
                        "name": "Saskia Helbling"
                    },
                    {
                        "name": "Yorguin-Jose Mantilla-Ramos"
                    },
                    {
                        "name": "Mahmood Hegazy"
                    },
                    {
                        "name": "Alberto Tosato"
                    },
                    {
                        "name": "David John Lemay"
                    },
                    {
                        "name": "Irina Rish"
                    },
                    {
                        "name": "Guillaume Dumas"
                    }
                ],
                "author_detail": {
                    "name": "Guillaume Dumas"
                },
                "author": "Guillaume Dumas"
            },
            {
                "id": "http://arxiv.org/abs/2511.06148v2",
                "guidislink": true,
                "link": "https://arxiv.org/abs/2511.06148v2",
                "title": "Large Language Models Develop Novel Social Biases Through Adaptive Exploration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models Develop Novel Social Biases Through Adaptive Exploration"
                },
                "updated": "2025-12-23T04:52:15Z",
                "updated_parsed": [
                    2025,
                    12,
                    23,
                    4,
                    52,
                    15,
                    1,
                    357,
                    0
                ],
                "links": [
                    {
                        "href": "https://arxiv.org/abs/2511.06148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "href": "https://arxiv.org/pdf/2511.06148v2",
                        "rel": "related",
                        "type": "application/pdf",
                        "title": "pdf"
                    }
                ],
                "summary": "As large language models (LLMs) are adopted into frameworks that grant them the capacity to make real decisions, it is increasingly important to ensure that they are unbiased. In this paper, we argue that the predominant approach of simply removing existing biases from models is not enough. Using a paradigm from the psychology literature, we demonstrate that LLMs can spontaneously develop novel social biases about artificial demographic groups even when no inherent differences exist. These biases result in highly stratified task allocations, which are less fair than assignments by human participants and are exacerbated by newer and larger models. In social science, emergent biases like these have been shown to result from exploration-exploitation trade-offs, where the decision-maker explores too little, allowing early observations to strongly influence impressions about entire demographic groups. To alleviate this effect, we examine a series of interventions targeting model inputs, problem structure, and explicit steering. We find that explicitly incentivizing exploration most robustly reduces stratification, highlighting the need for better multifaceted objectives to mitigate bias. These results reveal that LLMs are not merely passive mirrors of human social biases, but can actively create new ones from experience, raising urgent questions about how these systems will shape societies over time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are adopted into frameworks that grant them the capacity to make real decisions, it is increasingly important to ensure that they are unbiased. In this paper, we argue that the predominant approach of simply removing existing biases from models is not enough. Using a paradigm from the psychology literature, we demonstrate that LLMs can spontaneously develop novel social biases about artificial demographic groups even when no inherent differences exist. These biases result in highly stratified task allocations, which are less fair than assignments by human participants and are exacerbated by newer and larger models. In social science, emergent biases like these have been shown to result from exploration-exploitation trade-offs, where the decision-maker explores too little, allowing early observations to strongly influence impressions about entire demographic groups. To alleviate this effect, we examine a series of interventions targeting model inputs, problem structure, and explicit steering. We find that explicitly incentivizing exploration most robustly reduces stratification, highlighting the need for better multifaceted objectives to mitigate bias. These results reveal that LLMs are not merely passive mirrors of human social biases, but can actively create new ones from experience, raising urgent questions about how these systems will shape societies over time."
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ],
                "published": "2025-11-08T21:58:26Z",
                "published_parsed": [
                    2025,
                    11,
                    8,
                    21,
                    58,
                    26,
                    5,
                    312,
                    0
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY"
                },
                "authors": [
                    {
                        "name": "Addison J. Wu"
                    },
                    {
                        "name": "Ryan Liu"
                    },
                    {
                        "name": "Xuechunzi Bai"
                    },
                    {
                        "name": "Thomas L. Griffiths"
                    }
                ],
                "author_detail": {
                    "name": "Thomas L. Griffiths"
                },
                "author": "Thomas L. Griffiths"
            }
        ]
    }
]