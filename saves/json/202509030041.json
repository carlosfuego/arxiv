[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2506.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v2",
                "updated": "2025-08-29T09:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v2",
                "updated": "2025-08-29T07:40:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    40,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models"
                },
                "summary": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20865v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20865v1",
                "updated": "2025-08-28T14:58:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T14:58:47Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    58,
                    47,
                    3,
                    240,
                    0
                ],
                "title": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Multiple Quantization Network on Long Behavior Sequence for\n  Click-Through Rate Prediction"
                },
                "summary": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In Click-Through Rate (CTR) prediction, the long behavior sequence,\ncomprising the user's long period of historical interactions with items has a\nvital influence on assessing the user's interest in the candidate item.\nExisting approaches strike efficiency and effectiveness through a two-stage\nparadigm: first retrieving hundreds of candidate-related items and then\nextracting interest intensity vector through target attention. However, we\nargue that the discrepancy in target attention's relevance distribution between\nthe retrieved items and the full long behavior sequence inevitably leads to a\nperformance decline. To alleviate the discrepancy, we propose the Deep Multiple\nQuantization Network (DMQN) to process long behavior sequence end-to-end\nthrough compressing the long behavior sequence. Firstly, the entire spectrum of\nlong behavior sequence will be quantized into multiple codeword sequences based\non multiple independent codebooks. Hierarchical Sequential Transduction Unit is\nincorporated to facilitate the interaction of reduced codeword sequences. Then,\nattention between the candidate and multiple codeword sequences will output the\ninterest vector. To enable online serving, intermediate representations of the\ncodeword sequences are cached, significantly reducing latency. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMQN. The A/B test in our advertising system shows that DMQN\nimproves CTR by 3.5% and RPM by 2.0%."
                },
                "authors": [
                    {
                        "name": "Zhuoxing Wei"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Qingchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Qingchen Xie"
                },
                "author": "Qingchen Xie",
                "arxiv_doi": "10.1145/3726302.3730177",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730177",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.20865v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20865v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "5 pages, 1 figures, SIGIR 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18250v2",
                "updated": "2025-08-28T08:49:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    49,
                    24,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-25T17:41:13Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    17,
                    41,
                    13,
                    0,
                    237,
                    0
                ],
                "title": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SOT-MRAM Bitcell Scaling with BEOL Read Selectors: A DTCO Study"
                },
                "summary": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores the cross-node scaling potential of SOT-MRAM for\nlast-level caches (LLCs) under heterogeneous system scaling paradigm. We\nperform extensive Design-Technology Co-Optimization (DTCO) exercises to\nevaluate the bitcell footprint for different cell configurations at a\nrepresentative 7 nm technology and to assess their implications on read and\nwrite power-performance. We crucially identify the MTJ routing struggle in\nconventional two-transistor one-resistor (2T1R) SOT-MRAMs as the primary\nbitcell area scaling challenge and propose to use BEOL read selectors (BEOL\nRSs) that enable (10 -- 40) % bitcell area reduction and eventually match\nsub-N3 SRAM. On writability, we affirm that BEOL RS-based bitcells could meet\nthe required SOT switching current, provided the magnetic free layer properties\nbe engineered in line with LLC-specific, (0.1 -- 100) s retention targets. This\nis particularly to attribute to their (i) more available Si fins for write\ntransistor and (ii) lower bitline resistance at reduced cell width. We\nnevertheless underscore the read tradeoff associated with BEOL RSs, with the\nlow-drive IGZO-FET selector sacrificing the latency up to (3 -- 5) ns and the\nimperfectly rectifying diode selectors suffering (2.5 -- 5)$\\times$ energy cost\nrelative to 2T1R. This article thus highlights the realistic prospects and\nhurdles of BEOL RSs towards holistic power-performance-area scaling of\nSOT-MRAM."
                },
                "authors": [
                    {
                        "name": "Yang Xiang"
                    },
                    {
                        "name": "Fernando García-Redondo"
                    },
                    {
                        "name": "Arvind Sharma"
                    },
                    {
                        "name": "Van Dai Nguyen"
                    },
                    {
                        "name": "Andrea Fantini"
                    },
                    {
                        "name": "Philippe Matagne"
                    },
                    {
                        "name": "Siddharth Rao"
                    },
                    {
                        "name": "Subhali Subhechha"
                    },
                    {
                        "name": "Lynn Verschueren"
                    },
                    {
                        "name": "Mohammed Aftab Baig"
                    },
                    {
                        "name": "Marie Garcia Bardon"
                    },
                    {
                        "name": "Geert Hellings"
                    }
                ],
                "author_detail": {
                    "name": "Geert Hellings"
                },
                "author": "Geert Hellings",
                "arxiv_comment": "Manuscript submitted to IEEE Trans. Elec. Dev. Work enabled in part\n  by NanoIC pilot line; acquisition and operation jointly funded by Chips Joint\n  Undertaking, through EU's Digital Europe (101183266) and Horizon Europe\n  programs (101183277), as well as by the participating states\n  (Belgium-Flanders, France, Germany, Finland, Ireland, Romania)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20524v1",
                "updated": "2025-08-28T08:05:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T08:05:42Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    8,
                    5,
                    42,
                    3,
                    240,
                    0
                ],
                "title": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matrixed-Spectrum Decomposition Accelerated Linear Boltzmann Transport\n  Equation Solver for Fast Scatter Correction in Multi-Spectral CT"
                },
                "summary": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-ray scatter has been a serious concern in computed tomography (CT), leading\nto image artifacts and distortion of CT values. The linear Boltzmann transport\nequation (LBTE) is recognized as a fast and accurate approach for scatter\nestimation. However, for multi-spectral CT, it is cumbersome to compute\nmultiple scattering components for different spectra separately when applying\nLBTE-based scatter correction. In this work, we propose a Matrixed-Spectrum\nDecomposition accelerated LBTE solver (MSD-LBTE) that can be used to compute\nX-ray scatter distributions from CT acquisitions at two or more different\nspectra simultaneously, in a unified framework with no sacrifice in accuracy\nand nearly no increase in computation in theory. First, a matrixed-spectrum\nsolver of LBTE is obtained by introducing an additional label dimension to\nexpand the phase space. Then, we propose a ``spectrum basis'' for LBTE and a\nprinciple of selection of basis using the QR decomposition, along with the\nabove solver to construct the MSD-LBTE. Based on MSD-LBTE, a unified scatter\ncorrection method can be established for multi-spectral CT. We validate the\neffectiveness and accuracy of our method by comparing it with the Monte Carlo\nmethod, including the computational time. We also evaluate the scatter\ncorrection performance using two different phantoms for fast-kV switching based\ndual-energy CT, and using an elliptical phantom in a numerical simulation for\nkV-modulation enabled CT scans, validating that our proposed method can\nsignificantly reduce the computational cost at multiple spectra and effectively\nreduce scatter artifact in reconstructed CT images."
                },
                "authors": [
                    {
                        "name": "Guoxi Zhu"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Zhiqiang Chen"
                    },
                    {
                        "name": "Hewei Gao"
                    }
                ],
                "author_detail": {
                    "name": "Hewei Gao"
                },
                "author": "Hewei Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20433v1",
                "updated": "2025-08-28T05:22:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T05:22:25Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    5,
                    22,
                    25,
                    3,
                    240,
                    0
                ],
                "title": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MegaCacheX: Towards Cost-Effective Hierarchical Collaborative Content\n  Caching in Emerging Mega-Constellations"
                },
                "summary": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant latency in global content delivery primarily arises from\ninsufficient terrestrial infrastructure. Deploying space-based content delivery\nnetworks within emerging mega-constellations provides an effective means to\nbridge the digital divide. However, space-based caching faces constraints from\nphysical-layer dynamics, including dynamic topologies, time-varying\ninter-satellite link conditions, and limited onboard energy. In addition,\nexisting mechanisms often lack fine-grained content categorization and global\noptimization. This paper proposes MegaCacheX, a cost-effective hierarchical\nframework for collaborative content distribution that achieves\n\"Earth-independence\" by providing cloud services directly from space.\nSpecifically, data centers in Sun-synchronous orbit act as primary content\nsources, while caching nodes in mega-constellations and ground stations\ncollaboratively form a distributed edge layer. MegaCacheX optimizes caching\nstrategies by integrating content popularity, regional user distribution, and\nsatellite trajectory predictions. Multi-tier caching nodes serve as service\nanchors, enabling seamless content delivery with low latency. A prototype\nimplemented on a microservices-based, containerized testbed demonstrates that\nMegaCacheX reduces global content access latency by about 36% compared to\nbaseline approaches, while maintaining cost efficiency."
                },
                "authors": [
                    {
                        "name": "Haoyang Shi"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Sitong Li"
                    },
                    {
                        "name": "Minghang Li"
                    },
                    {
                        "name": "Xinming Lu"
                    },
                    {
                        "name": "Shaoxiang Xu"
                    },
                    {
                        "name": "Guoquan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guoquan Wang"
                },
                "author": "Guoquan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20424v1",
                "updated": "2025-08-28T04:46:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:46:44Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    46,
                    44,
                    3,
                    240,
                    0
                ],
                "title": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Diffusion with Cache: Exploiting Approximate Caches in\n  Diffusion Models"
                },
                "summary": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models are a powerful class of generative models that produce\ncontent, such as images, from user prompts, but they are computationally\nintensive. To mitigate this cost, recent academic and industry work has adopted\napproximate caching, which reuses intermediate states from similar prompts in a\ncache. While efficient, this optimization introduces new security risks by\nbreaking isolation among users. This work aims to comprehensively assess new\nsecurity vulnerabilities arising from approximate caching. First, we\ndemonstrate a remote covert channel established with the cache, where a sender\ninjects prompts with special keywords into the cache and a receiver can recover\nthat even after days, to exchange information. Second, we introduce a prompt\nstealing attack using the cache, where an attacker can recover existing cached\nprompts based on cache hit prompts. Finally, we introduce a poisoning attack\nthat embeds the attacker's logos into the previously stolen prompt, to render\nthem in future user prompts that hit the cache. These attacks are all performed\nremotely through the serving system, which indicates severe security\nvulnerabilities in approximate caching."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Shuncheng Jie"
                    },
                    {
                        "name": "Sihang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Sihang Liu"
                },
                "author": "Sihang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20407v1",
                "updated": "2025-08-28T04:10:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "published": "2025-08-28T04:10:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    10,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Transformer Connectivity: TLinFormer, A Path to Exact, Full\n  Context-Aware Linear Attention"
                },
                "summary": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has become a cornerstone of modern artificial\nintelligence, but its core self-attention mechanism suffers from a complexity\nbottleneck that scales quadratically with sequence length, severely limiting\nits application in long-sequence tasks. To address this challenge, existing\nlinear attention methods typically sacrifice model performance by relying on\ndata-agnostic kernel approximations or restrictive context selection. This\npaper returns to the first principles of connectionism, starting from the\ntopological structure of information flow, to introduce a novel linear\nattention architecture-\\textbf{TLinFormer}. By reconfiguring neuron connection\npatterns, TLinFormer achieves strict linear complexity while computing exact\nattention scores and ensuring information flow remains aware of the full\nhistorical context. This design aims to bridge the performance gap prevalent\nbetween existing efficient attention methods and standard attention. Through a\nseries of experiments, we systematically evaluate the performance of TLinFormer\nagainst a standard Transformer baseline on long-sequence inference tasks. The\nresults demonstrate that TLinFormer exhibits overwhelming advantages in key\nmetrics such as \\textbf{inference latency}, \\textbf{KV cache efficiency},\n\\textbf{memory footprint}, and \\textbf{overall speedup}."
                },
                "authors": [
                    {
                        "name": "Zhongpan Tang"
                    }
                ],
                "author_detail": {
                    "name": "Zhongpan Tang"
                },
                "author": "Zhongpan Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05821v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05821v5",
                "updated": "2025-08-28T03:57:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    3,
                    57,
                    52,
                    3,
                    240,
                    0
                ],
                "published": "2023-12-10T08:41:24Z",
                "published_parsed": [
                    2023,
                    12,
                    10,
                    8,
                    41,
                    24,
                    6,
                    344,
                    0
                ],
                "title": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASVD: Activation-aware Singular Value Decomposition for Compressing\n  Large Language Models"
                },
                "summary": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce a new post-training compression paradigm for\nLarge Language Models (LLMs) to facilitate their wider adoption. We delve into\nLLM weight low-rank decomposition, and find that the challenges of this task\nstem from (1) the distribution variance in the LLM activations and (2) the\nsensitivity difference among various kinds of layers. To address these issues,\nwe propose a training-free approach called Activation-aware Singular Value\nDecomposition (ASVD). Specifically, ASVD manages activation outliers by\ntransforming the weight matrix based on the activation distribution. This\ntransformation allows the outliers in the activation matrix to be absorbed into\nthe transformed weight matrix, thereby enhancing decomposition accuracy.\nAdditionally, we propose an efficient iterative calibration process to optimize\nlayer-specific decomposition by addressing the varying sensitivity of different\nLLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the\nsuccess of the low-rank decomposition of projection matrices in the\nself-attention module, we further introduce ASVD to compress the KV cache. By\nreducing the channel dimension of KV activations, memory requirements for KV\ncache can be largely reduced. ASVD can further achieve 50% KV cache reductions\nwithout performance drop in a training-free manner."
                },
                "authors": [
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Yuzhang Shang"
                    },
                    {
                        "name": "Yue Song"
                    },
                    {
                        "name": "Dawei Yang"
                    },
                    {
                        "name": "Qiang Wu"
                    },
                    {
                        "name": "Yan Yan"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05821v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05821v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09888v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09888v2",
                "updated": "2025-08-28T01:40:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    28,
                    1,
                    40,
                    30,
                    3,
                    240,
                    0
                ],
                "published": "2025-02-14T03:25:09Z",
                "published_parsed": [
                    2025,
                    2,
                    14,
                    3,
                    25,
                    9,
                    4,
                    45,
                    0
                ],
                "title": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Climber: Toward Efficient Scaling Laws for Large Recommendation Models"
                },
                "summary": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based generative models have achieved remarkable success across\ndomains with various scaling law manifestations. However, our extensive\nexperiments reveal persistent challenges when applying Transformer to\nrecommendation systems: (1) Transformer scaling is not ideal with increased\ncomputational resources, due to structural incompatibilities with\nrecommendation-specific features such as multi-source data heterogeneity; (2)\ncritical online inference latency constraints (tens of milliseconds) that\nintensify with longer user behavior sequences and growing computational\ndemands. We propose Climber, an efficient recommendation framework comprising\ntwo synergistic components: the model architecture for efficient scaling and\nthe co-designed acceleration techniques. Our proposed model adopts two core\ninnovations: (1) multi-scale sequence extraction that achieves a time\ncomplexity reduction by a constant factor, enabling more efficient scaling with\nsequence length; (2) dynamic temperature modulation adapting attention\ndistributions to the multi-scenario and multi-behavior patterns. Complemented\nby acceleration techniques, Climber achieves a 5.15$\\times$ throughput gain\nwithout performance degradation by adopting a \"single user, multiple item\"\nbatched processing and memory-efficient Key-Value caching. Comprehensive\noffline experiments on multiple datasets validate that Climber exhibits a more\nideal scaling curve. To our knowledge, this is the first publicly documented\nframework where controlled model scaling drives continuous online metric growth\n(12.19\\% overall lift) without prohibitive resource costs. Climber has been\nsuccessfully deployed on Netease Cloud Music, one of China's largest music\nstreaming platforms, serving tens of millions of users daily."
                },
                "authors": [
                    {
                        "name": "Songpei Xu"
                    },
                    {
                        "name": "Shijia Wang"
                    },
                    {
                        "name": "Da Guo"
                    },
                    {
                        "name": "Xianwen Guo"
                    },
                    {
                        "name": "Qiang Xiao"
                    },
                    {
                        "name": "Bin Huang"
                    },
                    {
                        "name": "Guanlin Wu"
                    },
                    {
                        "name": "Chuanjiang Luo"
                    }
                ],
                "author_detail": {
                    "name": "Chuanjiang Luo"
                },
                "author": "Chuanjiang Luo",
                "arxiv_doi": "10.1145/3746252.3761561",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761561",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.09888v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09888v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20272v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20272v1",
                "updated": "2025-08-27T21:05:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T21:05:05Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    21,
                    5,
                    5,
                    2,
                    239,
                    0
                ],
                "title": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRR-MDPF: A Queue Management Strategy Based on Dynamic Resource\n  Allocation and Markov Decision Process in Named Data Networking (NDN)"
                },
                "summary": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Named Data Networking (NDN) represents a transformative shift in network\narchitecture, prioritizing content names over host addresses to enhance data\ndissemination. Efficient queue and resource management are critical to NDN\nperformance, especially under dynamic and high-traffic conditions. This paper\nintroduces DRR-MDPF, a novel hybrid strategy that integrates the Markov\nDecision Process Forwarding (MDPF) model with the Deficit Round Robin (DRR)\nalgorithm. MDPF enables routers to intelligently predict optimal forwarding\ndecisions based on key metrics such as bandwidth, delay, and the number of\nunsatisfied Interests, while DRR ensures fair and adaptive bandwidth allocation\namong competing data flows. The proposed method models each router as a\nlearning agent capable of adjusting its strategies through continuous feedback\nand probabilistic updates. Simulation results using ndnSIM demonstrate that\nDRR-MDPF significantly outperforms state-of-the-art strategies including SAF,\nRFA, SMDPF, and LA-MDPF across various metrics such as throughput, Interest\nSatisfaction Rate (ISR), packet drop rate, content retrieval time, and load\nbalancing. Notably, DRR-MDPF maintains robustness under limited cache sizes and\nheavy traffic, offering enhanced adaptability and lower computational\ncomplexity due to its single-path routing design. Furthermore, its multi-metric\ndecision-making capability enables more accurate interface selection, leading\nto optimized network performance. Overall, DRR-MDPF serves as an intelligent,\nadaptive, and scalable queue management solution for NDN, effectively\naddressing core challenges such as resource allocation, congestion control, and\nroute optimization in dynamic networking environments."
                },
                "authors": [
                    {
                        "name": "Fatemeh Roshanzadeh"
                    },
                    {
                        "name": "Hamid Barati"
                    },
                    {
                        "name": "Ali Barati"
                    }
                ],
                "author_detail": {
                    "name": "Ali Barati"
                },
                "author": "Ali Barati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20272v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20272v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20253v1",
                "updated": "2025-08-27T20:18:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T20:18:37Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    20,
                    18,
                    37,
                    2,
                    239,
                    0
                ],
                "title": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeedMalloc: Improving Multi-threaded Applications via a Lightweight\n  Core for Memory Allocation"
                },
                "summary": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocation, though constituting only a small portion of the executed\ncode, can have a \"butterfly effect\" on overall program performance, leading to\nsignificant and far-reaching impacts. Despite accounting for just approximately\n5% of total instructions, memory allocation can result in up to a 2.7x\nperformance variation depending on the allocator used. This effect arises from\nthe complexity of memory allocation in modern multi-threaded multi-core\nsystems, where allocator metadata becomes intertwined with user data, leading\nto cache pollution or increased cross-thread synchronization overhead.\nOffloading memory allocators to accelerators, e.g., Mallacc and Memento, is a\npotential direction to improve the allocator performance and mitigate cache\npollution. However, these accelerators currently have limited support for\nmulti-threaded applications, and synchronization between cores and accelerators\nremains a significant challenge.\n  We present SpeedMalloc, using a lightweight support-core to process memory\nallocation tasks in multi-threaded applications. The support-core is a\nlightweight programmable processor with efficient cross-core data\nsynchronization and houses all allocator metadata in its own caches. This\ndesign minimizes cache conflicts with user data and eliminates the need for\ncross-core metadata synchronization. In addition, using a general-purpose core\ninstead of domain-specific accelerators makes SpeedMalloc capable of adopting\nnew allocator designs. We compare SpeedMalloc with state-of-the-art software\nand hardware allocators, including Jemalloc, TCMalloc, Mimalloc, Mallacc, and\nMemento. SpeedMalloc achieves 1.75x, 1.18x, 1.15x, 1.23x, and 1.18x speedups on\nmultithreaded workloads over these five allocators, respectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Qinzhe Wu"
                    },
                    {
                        "name": "Krishna Kavi"
                    },
                    {
                        "name": "Gayatri Mehta"
                    },
                    {
                        "name": "Jonathan C. Beard"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    },
                    {
                        "name": "Lizy K. John"
                    }
                ],
                "author_detail": {
                    "name": "Lizy K. John"
                },
                "author": "Lizy K. John",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v3",
                "updated": "2025-08-27T16:34:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    16,
                    34,
                    47,
                    2,
                    239,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Sasha Sirovica"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Laurent Duchesne"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "David Güera"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Victoria MönchJuan Haladjian"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Raunak Manjani"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Ahmed Fakhry"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Abhishek Sundararajan"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Valentin Wolf"
                    },
                    {
                        "name": "Fernando Bermúdez-Medina"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Changyuan Zhang"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Mehrdad Farajtabar"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Lezhi L"
                    }
                ],
                "author_detail": {
                    "name": "Lezhi L"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Lezhi L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09570v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09570v2",
                "updated": "2025-08-27T12:13:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    13,
                    45,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-13T07:40:25Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    7,
                    40,
                    25,
                    2,
                    225,
                    0
                ],
                "title": "Re-thinking Memory-Bound Limitations in CGRAs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Re-thinking Memory-Bound Limitations in CGRAs"
                },
                "summary": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coarse-Grained Reconfigurable Arrays (CGRAs) are specialized accelerators\ncommonly employed to boost performance in workloads with iterative structures.\nExisting research typically focuses on compiler or architecture optimizations\naimed at improving CGRA performance, energy efficiency, flexibility, and area\nutilization, under the idealistic assumption that kernels can access all data\nfrom Scratchpad Memory (SPM). However, certain complex workloads-particularly\nin fields like graph analytics, irregular database operations, and specialized\nforms of high-performance computing (e.g., unstructured mesh\nsimulations)-exhibit irregular memory access patterns that hinder CGRA\nutilization, sometimes dropping below 1.5%, making the CGRA memory-bound. To\naddress this challenge, we conduct a thorough analysis of the underlying causes\nof performance degradation, then propose a redesigned memory subsystem and\nrefine the memory model. With both microarchitectural and theoretical\noptimization, our solution can effectively manage irregular memory accesses\nthrough CGRA-specific runahead execution mechanism and cache reconfiguration\ntechniques. Our results demonstrate that we can achieve performance comparable\nto the original SPM-only system while requiring only 1.27% of the storage size.\nThe runahead execution mechanism achieves an average 3.04x speedup (up to\n6.91x), with cache reconfiguration technique providing an additional 6.02%\nimprovement, significantly enhancing CGRA performance for irregular memory\naccess patterns."
                },
                "authors": [
                    {
                        "name": "Xiangfeng Liu"
                    },
                    {
                        "name": "Zhe Jiang"
                    },
                    {
                        "name": "Anzhen Zhu"
                    },
                    {
                        "name": "Xiaomeng Han"
                    },
                    {
                        "name": "Mingsong Lyu"
                    },
                    {
                        "name": "Qingxu Deng"
                    },
                    {
                        "name": "Nan Guan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Guan"
                },
                "author": "Nan Guan",
                "arxiv_doi": "10.1145/3760386",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3760386",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.09570v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09570v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "25 pages, 18 figures, CODES+ISSS 2025",
                "arxiv_journal_ref": "ACM Transactions on Embedded Computing Systems 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "B.3.0; B.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21091v1",
                "updated": "2025-08-27T10:37:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T10:37:24Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    37,
                    24,
                    2,
                    239,
                    0
                ],
                "title": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ERTACache: Error Rectification and Timesteps Adjustment for Efficient\n  Diffusion"
                },
                "summary": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models suffer from substantial computational overhead due to their\ninherently iterative inference process. While feature caching offers a\npromising acceleration strategy by reusing intermediate outputs across\ntimesteps, naive reuse often incurs noticeable quality degradation. In this\nwork, we formally analyze the cumulative error introduced by caching and\ndecompose it into two principal components: feature shift error, caused by\ninaccuracies in cached outputs, and step amplification error, which arises from\nerror propagation under fixed timestep schedules. To address these issues, we\npropose ERTACache, a principled caching framework that jointly rectifies both\nerror types. Our method employs an offline residual profiling stage to identify\nreusable steps, dynamically adjusts integration intervals via a\ntrajectory-aware correction coefficient, and analytically approximates\ncache-induced errors through a closed-form residual linearization model.\nTogether, these components enable accurate and efficient sampling under\naggressive cache reuse. Extensive experiments across standard image and video\ngeneration benchmarks show that ERTACache achieves up to 2x inference speedup\nwhile consistently preserving or even improving visual quality. Notably, on the\nstate-of-the-art Wan2.1 video diffusion model, ERTACache delivers 2x\nacceleration with minimal VBench degradation, effectively maintaining baseline\nfidelity while significantly improving efficiency. The code is available at\nhttps://github.com/bytedance/ERTACache."
                },
                "authors": [
                    {
                        "name": "Xurui Peng"
                    },
                    {
                        "name": "Hong Liu"
                    },
                    {
                        "name": "Chenqian Yan"
                    },
                    {
                        "name": "Rui Ma"
                    },
                    {
                        "name": "Fangmin Chen"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Zhihua Wu"
                    },
                    {
                        "name": "Songwei Liu"
                    },
                    {
                        "name": "Mingbao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Mingbao Lin"
                },
                "author": "Mingbao Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v1",
                "updated": "2025-08-27T10:11:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19670v1",
                "updated": "2025-08-27T08:30:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "published": "2025-08-27T08:30:33Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    8,
                    30,
                    33,
                    2,
                    239,
                    0
                ],
                "title": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Bermuda Triangle of Contention: IOMMU Interference in Mixed\n  Criticality Systems"
                },
                "summary": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Mixed Criticality Systems (MCSs) evolve, they increasingly integrate\nheterogeneous computing platforms, combining general-purpose processors with\nspecialized accelerators such as AI engines, GPUs, and high-speed networking\ninterfaces. This heterogeneity introduces challenges, as these accelerators and\nDMA-capable devices act as independent bus masters, directly accessing memory.\nConsequently, ensuring both security and timing predictability in such\nenvironments becomes critical. To address these concerns, the Input-Output\nMemory Management Unit (IOMMU) plays a key role in mediating and regulating\nmemory access, preventing unauthorized transactions while enforcing isolation\nand access control policies. While prior work has explored IOMMU-related\nside-channel vulnerabilities from a security standpoint, its role in\nperformance interference remains largely unexplored. Moreover, many of the same\narchitectural properties that enable side-channel leakage, such as shared TLBs,\ncaching effects, and translation overheads, can also introduce timing\nunpredictability. In this work, we analyze the contention effects within IOMMU\nstructures using the Xilinx UltraScale+ ZCU104 platform, demonstrating how\ntheir shared nature introduce unpredictable delays. Our findings reveal that\nIOMMU-induced interference primarily affects small memory transactions, where\ntranslation overheads significantly impact execution time. Additionally, we\nhypothesize that contention effects arising from IOTLBs exhibit similar\nbehavior across architectures due to shared caching principles, such as\nprefetching and hierarchical TLB structures. Notably, our experiments show that\nIOMMU interference can delay DMA transactions by up to 1.79x for lower-size\ntransfers on the Arm SMMUv2 implementation."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Jose Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v4",
                "updated": "2025-08-27T04:58:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    27,
                    4,
                    58,
                    58,
                    2,
                    239,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "arxiv_comment": "Accepted to EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19247v1",
                "updated": "2025-08-26T17:59:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T17:59:47Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    17,
                    59,
                    47,
                    1,
                    238,
                    0
                ],
                "title": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoxHammer: Training-Free Precise and Coherent 3D Editing in Native 3D\n  Space"
                },
                "summary": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D local editing of specified regions is crucial for game industry and robot\ninteraction. Recent methods typically edit rendered multi-view images and then\nreconstruct 3D models, but they face challenges in precisely preserving\nunedited regions and overall coherence. Inspired by structured 3D generative\nmodels, we propose VoxHammer, a novel training-free approach that performs\nprecise and coherent editing in 3D latent space. Given a 3D model, VoxHammer\nfirst predicts its inversion trajectory and obtains its inverted latents and\nkey-value tokens at each timestep. Subsequently, in the denoising and editing\nphase, we replace the denoising features of preserved regions with the\ncorresponding inverted latents and cached key-value tokens. By retaining these\ncontextual features, this approach ensures consistent reconstruction of\npreserved areas and coherent integration of edited parts. To evaluate the\nconsistency of preserved regions, we constructed Edit3D-Bench, a\nhuman-annotated dataset comprising hundreds of samples, each with carefully\nlabeled 3D editing regions. Experiments demonstrate that VoxHammer\nsignificantly outperforms existing methods in terms of both 3D consistency of\npreserved regions and overall quality. Our method holds promise for\nsynthesizing high-quality edited paired data, thereby laying the data\nfoundation for in-context 3D generation. See our project page at\nhttps://huanngzh.github.io/VoxHammer-Page/."
                },
                "authors": [
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Zehuan Huang"
                    },
                    {
                        "name": "Haoran Feng"
                    },
                    {
                        "name": "Gengxiong Zhuang"
                    },
                    {
                        "name": "Rui Chen"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Lu Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Sheng"
                },
                "author": "Lu Sheng",
                "arxiv_comment": "Project page: https://huanngzh.github.io/VoxHammer-Page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18983v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18983v1",
                "updated": "2025-08-26T12:32:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T12:32:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    12,
                    32,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling MoE on the Edge via Importance-Driven Expert Scheduling"
                },
                "summary": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has emerged as a key technique for\nscaling Large Language Models by activating only a subset of experts per query.\nDeploying MoE on consumer-grade edge hardware, however, is constrained by\nlimited device memory, making dynamic expert offloading essential. Unlike prior\nwork that treats offloading purely as a scheduling problem, we leverage expert\nimportance to guide decisions, substituting low-importance activated experts\nwith functionally similar ones already cached in GPU memory, thereby preserving\naccuracy. As a result, this design reduces memory usage and data transfer,\nwhile largely eliminating PCIe overhead. In addition, we introduce a scheduling\npolicy that maximizes the reuse ratio of GPU-cached experts, further boosting\nefficiency. Extensive evaluations show that our approach delivers 48% lower\ndecoding latency with over 60% expert cache hit rate, while maintaining nearly\nlossless accuracy."
                },
                "authors": [
                    {
                        "name": "Guoying Zhu"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Haipeng Dai"
                    },
                    {
                        "name": "Xuechen Liu"
                    },
                    {
                        "name": "Weijun Wang"
                    },
                    {
                        "name": "Keran Li"
                    },
                    {
                        "name": "Jun xiao"
                    },
                    {
                        "name": "Ligeng Chen"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18983v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18983v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18736v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18736v1",
                "updated": "2025-08-26T07:09:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T07:09:09Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    7,
                    9,
                    9,
                    1,
                    238,
                    0
                ],
                "title": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Caching for LLM Serving Systems: Beyond Traditional\n  Heuristics"
                },
                "summary": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) at scale requires meeting strict Service\nLevel Objectives (SLOs) under severe computational and memory constraints.\nNevertheless, traditional caching strategies fall short: exact-matching and\nprefix caches neglect query semantics, while state-of-the-art semantic caches\nremain confined to traditional intuitions, offering little conceptual\ndeparture. Building on this, we present SISO, a semantic caching system that\nredefines efficiency for LLM serving. SISO introduces centroid-based caching to\nmaximize coverage with minimal memory, locality-aware replacement to preserve\nhigh-value entries, and dynamic thresholding to balance accuracy and latency\nunder varying workloads. Across diverse datasets, SISO delivers up to\n1.71$\\times$ higher hit ratios and consistently stronger SLO attainment\ncompared to state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Jungwoo Kim"
                    },
                    {
                        "name": "Minsang Kim"
                    },
                    {
                        "name": "Jaeheon Lee"
                    },
                    {
                        "name": "Chanwoo Moon"
                    },
                    {
                        "name": "Heejin Kim"
                    },
                    {
                        "name": "Taeho Hwang"
                    },
                    {
                        "name": "Woosuk Chung"
                    },
                    {
                        "name": "Yeseong Kim"
                    },
                    {
                        "name": "Sungjin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Lee"
                },
                "author": "Sungjin Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18736v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18736v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.09822v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.09822v3",
                "updated": "2025-08-26T03:23:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    3,
                    23,
                    53,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-13T13:54:51Z",
                "published_parsed": [
                    2025,
                    8,
                    13,
                    13,
                    54,
                    51,
                    2,
                    225,
                    0
                ],
                "title": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical Autoregressive Model for Robotic Manipulation without Action\n  Pretraining"
                },
                "summary": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scarcity of manipulation data has motivated the use of pretrained large\nmodels from other modalities in robotics. In this work, we build upon\nautoregressive video generation models to propose a Physical Autoregressive\nModel (PAR), where physical tokens combine frames and actions to represent the\njoint evolution of the robot and its environment. PAR leverages the world\nknowledge embedded in video pretraining to understand physical dynamics without\nrequiring action pretraining, enabling accurate video prediction and consistent\naction trajectories. It also adopts a DiT-based de-tokenizer to model frames\nand actions as continuous tokens, mitigating quantization errors and\nfacilitating mutual enhancement. Furthermore, we incorporate a causal mask with\ninverse kinematics, parallel training, and the KV-cache mechanism to further\nimprove performance and efficiency. Experiments on the ManiSkill benchmark show\nthat PAR achieves a 100\\% success rate on the PushCube task, matches the\nperformance of action-pretrained baselines on other tasks, and accurately\npredicts future videos with tightly aligned action trajectories. These findings\nunderscore a promising direction for robotic manipulation by transferring world\nknowledge from autoregressive video pretraining. The project page is here:\nhttps://hcplab-sysu.github.io/PhysicalAutoregressiveModel/"
                },
                "authors": [
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sihan Qin"
                    },
                    {
                        "name": "Tianshui Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Guangrun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Guangrun Wang"
                },
                "author": "Guangrun Wang",
                "arxiv_comment": "16 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.09822v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.09822v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08045v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08045v2",
                "updated": "2025-08-26T01:55:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    55,
                    27,
                    1,
                    238,
                    0
                ],
                "published": "2025-07-10T01:51:17Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    1,
                    51,
                    17,
                    3,
                    191,
                    0
                ],
                "title": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Krul: Efficient State Restoration for Multi-turn Conversations with\n  Dynamic Cross-layer KV Sharing"
                },
                "summary": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient state restoration in multi-turn conversations with large language\nmodels (LLMs) remains a critical challenge, primarily due to the overhead of\nrecomputing or loading full key-value (KV) caches for all historical tokens. To\naddress this, existing approaches compress KV caches across adjacent layers\nwith highly similar attention patterns. However, these methods often apply a\nfixed compression scheme across all conversations, selecting the same layer\npairs for compression without considering conversation-specific attention\ndynamics. This static strategy overlooks variability in attention pattern\nsimilarity across different conversations, which can lead to noticeable\naccuracy degradation.\n  We present Krul, a multi-turn LLM inference system that enables accurate and\nefficient KV cache restoration. Krul dynamically selects compression strategies\nbased on attention similarity across layer pairs and uses a\nrecomputation-loading pipeline to restore the KV cache. It introduces three key\ninnovations: 1) a preemptive compression strategy selector to preserve critical\ncontext for future conversation turns and selects a customized strategy for the\nconversation; 2) a token-wise heterogeneous attention similarity estimator to\nmitigate the attention similarity computation and storage overhead during model\ngeneration; 3) a bubble-free restoration scheduler to reduce potential bubbles\nbrought by the imbalance of recomputing and loading stream due to compressed KV\ncaches. Empirical evaluations on real-world tasks demonstrate that Krul\nachieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x\nreduction in KV cache storage compared to state-of-the-art methods without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Junyi Wen"
                    },
                    {
                        "name": "Junyuan Liang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Ting Cai"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08045v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08045v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v3",
                "updated": "2025-08-26T01:45:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    1,
                    45,
                    34,
                    1,
                    238,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPUs are critical for compute-intensive applications, yet emerging workloads\nsuch as recommender systems, graph analytics, and data analytics often exceed\nGPU memory capacity. Existing solutions allow GPUs to use CPU DRAM or SSDs as\nexternal memory, and the GPU-centric approach enables GPU threads to directly\nissue NVMe requests, further avoiding CPU intervention. However, current\nGPU-centric approaches adopt synchronous I/O, forcing threads to stall during\nlong communication delays.\n  We propose AGILE, a lightweight asynchronous GPU-centric I/O library that\neliminates deadlock risks and integrates a flexible HBM-based software cache.\nAGILE overlaps computation and I/O, improving performance by up to 1.88$\\times$\nacross workloads with diverse computation-to-communication ratios. Compared to\nBaM on DLRM, AGILE achieves up to 1.75$\\times$ speedup through efficient design\nand overlapping; on graph applications, AGILE reduces software cache overhead\nby up to 3.12$\\times$ and NVMe I/O overhead by up to 2.85$\\times$; AGILE also\nlowers per-thread register usage by up to 1.32$\\times$."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18572v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18572v1",
                "updated": "2025-08-26T00:09:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "published": "2025-08-26T00:09:03Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    0,
                    9,
                    3,
                    1,
                    238,
                    0
                ],
                "title": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Strata: Hierarchical Context Caching for Long Context Language Model\n  Serving"
                },
                "summary": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with expanding context windows face significant\nperformance hurdles. While caching key-value (KV) states is critical for\navoiding redundant computation, the storage footprint of long-context caches\nquickly exceeds GPU memory capacity, forcing production systems to adopt\nhierarchical caching across memory hierarchies. However, transferring large\ncached contexts back to the GPU introduces severe performance bottlenecks:\nfragmented I/O from paged layouts prevents full bandwidth utilization, and\nexisting schedulers fail to account for cache-loading delays, leaving systems\nloading-bound rather than compute-bound. We present Strata, a hierarchical\ncontext caching framework designed for efficient long context LLM serving.\nStrata introduces GPU-assisted I/O to combat KV cache fragmentation, decoupling\nGPU and CPU memory layouts and employs cache-aware request scheduling to\nbalance compute with I/O latency and overlapping unavoidable stalls with\ncomplementary tasks. Built on SGLang and deployed in production, Strata\nachieves up to 5x lower Time-To-First-Token (TTFT) compared to vLLM + LMCache\nand 3.75x speedup over NVIDIA TensorRT-LLM on long-context benchmarks, without\ndegrading short-context performance."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Xie"
                    },
                    {
                        "name": "Ziyi Xu"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Vikram Sharma Mailthody"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Michael Garland"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "13 pages, 14 figures, under peer review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18572v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18572v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18540v1",
                "updated": "2025-08-25T22:21:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T22:21:04Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    22,
                    21,
                    4,
                    0,
                    237,
                    0
                ],
                "title": "Real-time 3D Visualization of Radiance Fields on Light Field Displays",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time 3D Visualization of Radiance Fields on Light Field Displays"
                },
                "summary": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radiance fields have revolutionized photo-realistic 3D scene visualization by\nenabling high-fidelity reconstruction of complex environments, making them an\nideal match for light field displays. However, integrating these technologies\npresents significant computational challenges, as light field displays require\nmultiple high-resolution renderings from slightly shifted viewpoints, while\nradiance fields rely on computationally intensive volume rendering. In this\npaper, we propose a unified and efficient framework for real-time radiance\nfield rendering on light field displays. Our method supports a wide range of\nradiance field representations, including NeRFs, 3D Gaussian Splatting, and\nSparse Voxels, within a shared architecture based on a single-pass plane\nsweeping strategy and caching of shared, non-directional components. The\nframework generalizes across different scene formats without retraining, and\navoids redundant computation across views. We further demonstrate a real-time\ninteractive application on a Looking Glass display, achieving 200+ FPS at 512p\nacross 45 views, enabling seamless, immersive 3D interaction. On standard\nbenchmarks, our method achieves up to 22x speedup compared to independently\nrendering each view, while preserving image quality."
                },
                "authors": [
                    {
                        "name": "Jonghyun Kim"
                    },
                    {
                        "name": "Cheng Sun"
                    },
                    {
                        "name": "Michael Stengel"
                    },
                    {
                        "name": "Matthew Chan"
                    },
                    {
                        "name": "Andrew Russell"
                    },
                    {
                        "name": "Jaehyun Jung"
                    },
                    {
                        "name": "Wil Braithwaite"
                    },
                    {
                        "name": "Shalini De Mello"
                    },
                    {
                        "name": "David Luebke"
                    }
                ],
                "author_detail": {
                    "name": "David Luebke"
                },
                "author": "David Luebke",
                "arxiv_comment": "10 pages, 14 figures. J. Kim, C. Sun, and M. Stengel contributed\n  equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v1",
                "updated": "2025-08-25T21:07:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09425v3",
                "updated": "2025-08-25T15:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    48,
                    28,
                    0,
                    237,
                    0
                ],
                "published": "2024-11-14T13:22:41Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    13,
                    22,
                    41,
                    3,
                    319,
                    0
                ],
                "title": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MARM: Unlocking the Future of Recommendation Systems through Memory\n  Augmentation and Scalable Complexity"
                },
                "summary": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling-law has guided the language model designing for past years, however,\nit is worth noting that the scaling laws of NLP cannot be directly applied to\nRecSys due to the following reasons: (1) The amount of training samples and\nmodel parameters is typically not the bottleneck for the model. Our\nrecommendation system can generate over 50 billion user samples daily, and such\na massive amount of training data can easily allow our model parameters to\nexceed 200 billion, surpassing many LLMs (about 100B). (2) To ensure the\nstability and robustness of the recommendation system, it is essential to\ncontrol computational complexity FLOPs carefully. Considering the above\ndifferences with LLM, we can draw a conclusion that: for a RecSys model,\ncompared to model parameters, the computational complexity FLOPs is a more\nexpensive factor that requires careful control. In this paper, we propose our\nmilestone work, MARM (Memory Augmented Recommendation Model), which explores a\nnew cache scaling-laws successfully."
                },
                "authors": [
                    {
                        "name": "Xiao Lv"
                    },
                    {
                        "name": "Jiangxia Cao"
                    },
                    {
                        "name": "Shijie Guan"
                    },
                    {
                        "name": "Xiaoyou Zhou"
                    },
                    {
                        "name": "Zhiguang Qi"
                    },
                    {
                        "name": "Yaqiang Zang"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Ben Wang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "arxiv_comment": "CIKM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "N/A",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v1",
                "updated": "2025-08-25T10:59:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$, but also achieves performance\ncomparable to or better than the full context in the long context scenarios.\nWithout additional post training or operator development, ILRe can process a\nsingle $1M$ tokens request in less than half a minute (speedup $\\approx\n180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with model\nLlama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17756v1",
                "updated": "2025-08-25T07:49:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T07:49:17Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    7,
                    49,
                    17,
                    0,
                    237,
                    0
                ],
                "title": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperGen: An Efficient Ultra-high-resolution Video Generation System\n  with Sketching and Tiling"
                },
                "summary": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have recently achieved remarkable success in generative\ntasks (e.g., image and video generation), and the demand for high-quality\ncontent (e.g., 2K/4K videos) is rapidly increasing across various domains.\nHowever, generating ultra-high-resolution videos on existing\nstandard-resolution (e.g., 720p) platforms remains challenging due to the\nexcessive re-training requirements and prohibitively high computational and\nmemory costs. To this end, we introduce SuperGen, an efficient tile-based\nframework for ultra-high-resolution video generation. SuperGen features a novel\ntraining-free algorithmic innovation with tiling to successfully support a wide\nrange of resolutions without additional training efforts while significantly\nreducing both memory footprint and computational complexity. Moreover, SuperGen\nincorporates a tile-tailored, adaptive, region-aware caching strategy that\naccelerates video generation by exploiting redundancy across denoising steps\nand spatial regions. SuperGen also integrates cache-guided,\ncommunication-minimized tile parallelism for enhanced throughput and minimized\nlatency. Evaluations demonstrate that SuperGen harvests the maximum performance\ngains while achieving high output quality across various benchmarks."
                },
                "authors": [
                    {
                        "name": "Fanjiang Ye"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yi Mu"
                    },
                    {
                        "name": "Jucheng Shen"
                    },
                    {
                        "name": "Renjie Li"
                    },
                    {
                        "name": "Kaijian Wang"
                    },
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Myungjin Lee"
                    },
                    {
                        "name": "Triston Cao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "T. S. Eugene Ng"
                    },
                    {
                        "name": "Zhengzhong Tu"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16212v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16212v2",
                "updated": "2025-08-25T03:07:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    7,
                    2,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-22T08:36:58Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    36,
                    58,
                    4,
                    234,
                    0
                ],
                "title": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniCache: A Trajectory-Oriented Global Perspective on Training-Free\n  Cache Reuse for Diffusion Transformer Models"
                },
                "summary": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a powerful paradigm for generative tasks\nsuch as image synthesis and video generation, with Transformer architectures\nfurther enhancing performance. However, the high computational cost of\ndiffusion Transformers-stemming from a large number of sampling steps and\ncomplex per-step computations-presents significant challenges for real-time\ndeployment. In this paper, we introduce OmniCache, a training-free acceleration\nmethod that exploits the global redundancy inherent in the denoising process.\nUnlike existing methods that determine caching strategies based on inter-step\nsimilarities and tend to prioritize reusing later sampling steps, our approach\noriginates from the sampling perspective of DIT models. We systematically\nanalyze the model's sampling trajectories and strategically distribute cache\nreuse across the entire sampling process. This global perspective enables more\neffective utilization of cached computations throughout the diffusion\ntrajectory, rather than concentrating reuse within limited segments of the\nsampling procedure. In addition, during cache reuse, we dynamically estimate\nthe corresponding noise and filter it out to reduce its impact on the sampling\ndirection. Extensive experiments demonstrate that our approach accelerates the\nsampling process while maintaining competitive generative quality, offering a\npromising and practical solution for efficient deployment of diffusion-based\ngenerative models."
                },
                "authors": [
                    {
                        "name": "Huanpeng Chu"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Guanyu Fen"
                    },
                    {
                        "name": "Yutao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yutao Zhang"
                },
                "author": "Yutao Zhang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16212v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16212v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17624v1",
                "updated": "2025-08-25T03:05:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T03:05:16Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    3,
                    5,
                    16,
                    0,
                    237,
                    0
                ],
                "title": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ExpertWeave: Efficiently Serving Expert-Specialized Fine-Tuned Adapters\n  at Scale"
                },
                "summary": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expert-Specialized Fine-Tuning (ESFT) adapts Mixture-of-Experts (MoE) large\nlanguage models to enhance their task-specific performance by selectively\ntuning the top-activated experts for the task. Serving these fine-tuned models\nat scale is challenging: deploying merged models in isolation is prohibitively\nresource-hungry, while existing multi-adapter serving systems with LoRA-style\nadditive updates are incompatible with ESFT's expert-oriented paradigm. We\npresent ExpertWeave, a system that serves multiple ESFT adapters concurrently\nover a single shared MoE base model, drastically reducing the memory footprint\nand improving resource utilization. To seamlessly integrate into existing\ninference pipelines for MoE models with non-intrusive modifications and minimal\nlatency overhead, ExpertWeave introduces a virtual-memory-assisted expert\nweight manager that co-locates base-model and adapter experts without incurring\nmemory overhead from fragmentation, and a fused kernel for batched rerouting to\nenable lightweight redirection of tokens to the appropriate experts at runtime.\nOur evaluations show that ExpertWeave can simultaneously serve multiple\nadapters of a 16B MoE model on a single accelerator where the baseline runs out\nof memory, or provides up to 94x more KV cache capacity and achieves up to 18%\nhigher throughput while using comparable resources, all without compromising\nmodel accuracy. ExpertWeave maintains low overhead even when scaling to 20\nadapters, with a 4-11% latency increase compared with serving the base model\nalone. Source code will be released soon."
                },
                "authors": [
                    {
                        "name": "Ge Shi"
                    },
                    {
                        "name": "Hanieh Sadri"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15881v2",
                "updated": "2025-08-25T02:24:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    2,
                    24,
                    20,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-21T15:25:40Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    40,
                    3,
                    233,
                    0
                ],
                "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill and Decode Inference"
                },
                "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration."
                },
                "authors": [
                    {
                        "name": "Xiaojuan Tang"
                    },
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Di Yin"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17593v1",
                "updated": "2025-08-25T01:33:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "published": "2025-08-25T01:33:18Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    1,
                    33,
                    18,
                    0,
                    237,
                    0
                ],
                "title": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zen-Attention: A Compiler Framework for Dynamic Attention Folding on AMD\n  NPUs"
                },
                "summary": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based deep learning models are increasingly deployed on energy,\nand DRAM bandwidth constrained devices such as laptops and gaming consoles,\nwhich presents significant challenges in meeting the latency requirements of\nthe models. The industry is turning to neural processing units (NPUs) for\nsuperior performance-per-watt (perf/watt); however, efficiently mapping dynamic\nattention layers to the NPUs remains a challenging task. For optimizing\nperf/watt, AMD XDNA NPUs employ software managed caches and share system memory\nwith host. This requires substantial engineering effort to unlock efficient\ntiling, buffer allocation, and data movement to extract the maximum efficiency\nfrom the device. This paper introduces Zen-Attention, a framework that\noptimizes DRAM bandwidth utilization in the attention layer of models by\nsystematically exploring the complex design space of layer folding, tiling, and\ndata-movement on the interconnect, and the tensor layouts to come up with an\noptimal solution. Our evaluation includes comparative analysis of end-to-end\nmodel latency and specific attention latency in each model. We demonstrate how\nthe framework enhances mapping capabilities by varying input dimensions, which\nrequire padding and masking in the attention block. For representative\ntransformer models, the Zen-Attention Framework achieves up to 4x improvement\nin the latency of the attention block and up to 32% improvement in end-to-end\nnetwork latency compared to the baseline Unfolded- approaches."
                },
                "authors": [
                    {
                        "name": "Aadesh Deshmukh"
                    },
                    {
                        "name": "Venkata Yaswanth Raparti"
                    },
                    {
                        "name": "Samuel Hsu"
                    }
                ],
                "author_detail": {
                    "name": "Samuel Hsu"
                },
                "author": "Samuel Hsu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v3",
                "updated": "2025-08-25T00:15:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    25,
                    0,
                    15,
                    27,
                    0,
                    237,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-Cache: Training-Free Retrieval for Real-Time Manipulation"
                },
                "summary": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real robots are expected to repeat the same behavior in new environments with\nvery little new data, yet modern controllers either incur heavy per-step\ninference or require deployment-time fine-tuning. We propose RT-Cache, a\ntraining-free retrieval-as-control pipeline that caches diverse image action\ntrajectories in a unified vector memory and, at test time, embeds the current\nframe to retrieve and replay multi-step snippets, replacing per-step model\ncalls. A hierarchical search keeps lookups sub-second at million scale,\nshifting cost from compute to storage and enabling real-time control on modest\nGPUs. Across real-robot tasks and large open logs, RT-Cache achieves higher\nsuccess and lower completion time than strong retrieval baselines\n(approximately x2 higher success and ~30% faster in our settings), and a\nsingle-episode anchoring study shows immediate adaptation to a more complex,\ncontact-rich task without fine-tuning. RT-Cache turns experience into an\nappend-only memory, offering a simple, scalable path to few-shot deployment\ntoday and a foundation for multimodal keys and optional integration with\nhigh-level policies. Project page: https://rt-cache.github.io/."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "8 pages, 6 figures. 2025 IEEE-RAS 24th International Conference on\n  Humanoid Robots",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v3",
                "updated": "2025-08-24T22:09:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    22,
                    9,
                    57,
                    6,
                    236,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Efficient Long-Range Reasoning With Short-Context LLMs"
                },
                "summary": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks demand reasoning over long inputs. However, existing\nsolutions are limited, e.g., long-context models require large compute budgets,\nparameter-efficient fine-tuning (PEFT) needs training data, and\nretrieval-augmented generation (RAG) entails complex task-specific designs.\nThough in-context approaches overcome many of these issues, methods with\nshort-context LLMs are inefficient, trading context for processing more tokens.\nWe introduce PRISM, a highly token-efficient in-context method based on\nstructured schemas that outperforms baselines on diverse tasks with 4x shorter\ncontexts. This approach produces concise outputs and efficiently leverages\nkey-value (KV) caches to reduce costs by up to 54%. PRISM scales down to tiny\ncontexts without increasing costs or sacrificing quality, and generalizes to\nnew tasks with minimal effort by generating schemas from task descriptions."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "Published as a conference paper at EMNLP 2025. 28 pages, 7 figures, 5\n  tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17518v1",
                "updated": "2025-08-24T20:51:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T20:51:06Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    20,
                    51,
                    6,
                    6,
                    236,
                    0
                ],
                "title": "Evaluating Compiler Optimization Impacts on zkVM Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Compiler Optimization Impacts on zkVM Performance"
                },
                "summary": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zero-knowledge proofs (ZKPs) are the cornerstone of programmable\ncryptography. They enable (1) privacy-preserving and verifiable computation\nacross blockchains, and (2) an expanding range of off-chain applications such\nas credential schemes. Zero-knowledge virtual machines (zkVMs) lower the\nbarrier by turning ZKPs into a drop-in backend for standard compilation\npipelines. This lets developers write proof-generating programs in conventional\nlanguages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits.\nHowever, these VMs inherit compiler infrastructures tuned for traditional\narchitectures rather than for proof systems. In particular, standard compiler\noptimizations assume features that are absent in zkVMs, including cache\nlocality, branch prediction, or instruction-level parallelism. Therefore, their\nimpact on proof generation is questionable.\n  We present the first systematic study of the impact of compiler optimizations\non zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an\nunoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero\nand SP1). While standard LLVM optimization levels do improve zkVM performance\n(over 40\\%), their impact is far smaller than on traditional CPUs, since their\ndecisions rely on hardware features rather than proof constraints. Guided by a\nfine-grained pass-level analysis, we~\\emph{slightly} refine a small set of LLVM\npasses to be zkVM-aware, improving zkVM execution time by up to 45\\% (average\n+4.6\\% on RISC Zero, +1\\% on SP1) and achieving consistent proving-time gains.\nOur work highlights the potential of compiler-level optimizations for zkVM\nperformance and opens new direction for zkVM-specific passes, backends, and\nsuperoptimizers."
                },
                "authors": [
                    {
                        "name": "Thomas Gassmann"
                    },
                    {
                        "name": "Stefanos Chaliasos"
                    },
                    {
                        "name": "Thodoris Sotiropoulos"
                    },
                    {
                        "name": "Zhendong Su"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Su"
                },
                "author": "Zhendong Su",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17496v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17496v1",
                "updated": "2025-08-24T19:28:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T19:28:22Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    19,
                    28,
                    22,
                    6,
                    236,
                    0
                ],
                "title": "Practical Insertion-Only Convex Hull",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Insertion-Only Convex Hull"
                },
                "summary": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convex hull data structures are fundamental in computational geometry. We\nstudy insertion-only data structures, supporting various containment and\nintersection queries. When $P$ is sorted by $x$- or $y$-coordinate, convex\nhulls can be constructed in linear time using classical algorithms such as\nGraham scan. We investigate a variety of methods tailored to the insertion-only\nsetting. We explore a broad selection of trade-offs involving robustness,\nmemory access patterns, and space usage, providing an extensive evaluation of\nboth existing and novel techniques. Logarithmic-time methods rely on\npointer-based tree structures, which suffer in practice due to poor memory\nlocality. Motivated by this, we develop a vector-based solution inspired by\nOvermars' logarithmic method. Our structure has worse asymptotic bounds,\nsupporting queries in $O(\\log^2 n)$ time, but stores data in $O(\\log n)$\ncontiguous vectors, greatly improving cache performance.\n  Through empirical evaluation on real-world and synthetic data sets, we\nuncover surprising trends. Let $h$ denote the size of the convex hull. We show\nthat a na\\\"ive $O(h)$ insertion-only algorithm based on Graham scan\nconsistently outperforms both theoretical and practical state-of-the-art\nmethods under realistic workloads, even on data sets with rather large convex\nhulls. While tree-based methods with $O(\\log h)$ update times offer solid\ntheoretical guarantees, they are never optimal in practice. In contrast, our\nvector-based logarithmic method, despite its theoretically inferior bounds, is\nhighly competitive across all tested scenarios. It is optimal whenever the\nconvex hull becomes large."
                },
                "authors": [
                    {
                        "name": "Ivor van der Hoog"
                    },
                    {
                        "name": "Henrik Reinstädtler"
                    },
                    {
                        "name": "Eva Rotenberg"
                    }
                ],
                "author_detail": {
                    "name": "Eva Rotenberg"
                },
                "author": "Eva Rotenberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17496v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17496v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17445v1",
                "updated": "2025-08-24T16:52:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:52:37Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    52,
                    37,
                    6,
                    236,
                    0
                ],
                "title": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreePO: Bridging the Gap of Policy Optimization and Efficacy and\n  Inference Efficiency with Heuristic Tree-based Modeling"
                },
                "summary": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in aligning large language models via reinforcement\nlearning have achieved remarkable gains in solving complex reasoning problems,\nbut at the cost of expensive on-policy rollouts and limited exploration of\ndiverse reasoning paths. In this work, we introduce TreePO, involving a\nself-guided rollout algorithm that views sequence generation as a\ntree-structured searching process. Composed of dynamic tree sampling policy and\nfixed-length segment decoding, TreePO leverages local uncertainty to warrant\nadditional branches. By amortizing computation across common prefixes and\npruning low-value paths early, TreePO essentially reduces the per-update\ncompute burden while preserving or enhancing exploration diversity. Key\ncontributions include: (1) a segment-wise sampling algorithm that alleviates\nthe KV cache burden through contiguous segments and spawns new branches along\nwith an early-stop mechanism; (2) a tree-based segment-level advantage\nestimation that considers both global and local proximal policy optimization.\nand (3) analysis on the effectiveness of probability and quality-driven dynamic\ndivergence and fallback strategy. We empirically validate the performance gain\nof TreePO on a set reasoning benchmarks and the efficiency saving of GPU hours\nfrom 22\\% up to 43\\% of the sampling design for the trained models, meanwhile\nshowing up to 40\\% reduction at trajectory-level and 35\\% at token-level\nsampling compute for the existing models. While offering a free lunch of\ninference efficiency, TreePO reveals a practical path toward scaling RL-based\npost-training with fewer samples and less compute. Home page locates at\nhttps://m-a-p.ai/TreePO."
                },
                "authors": [
                    {
                        "name": "Yizhi Li"
                    },
                    {
                        "name": "Qingshui Gu"
                    },
                    {
                        "name": "Zhoufutu Wen"
                    },
                    {
                        "name": "Ziniu Li"
                    },
                    {
                        "name": "Tianshun Xing"
                    },
                    {
                        "name": "Shuyue Guo"
                    },
                    {
                        "name": "Tianyu Zheng"
                    },
                    {
                        "name": "Xin Zhou"
                    },
                    {
                        "name": "Xingwei Qu"
                    },
                    {
                        "name": "Wangchunshu Zhou"
                    },
                    {
                        "name": "Zheng Zhang"
                    },
                    {
                        "name": "Wei Shen"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Chenghua Lin"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Wenhao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Wenhao Huang"
                },
                "author": "Wenhao Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17434v1",
                "updated": "2025-08-24T16:17:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T16:17:33Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    16,
                    17,
                    33,
                    6,
                    236,
                    0
                ],
                "title": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinySR: Pruning Diffusion for Real-World Image Super-Resolution"
                },
                "summary": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world image super-resolution (Real-ISR) focuses on recovering\nhigh-quality images from low-resolution inputs that suffer from complex\ndegradations like noise, blur, and compression. Recently, diffusion models\n(DMs) have shown great potential in this area by leveraging strong generative\npriors to restore fine details. However, their iterative denoising process\nincurs high computational overhead, posing challenges for real-time\napplications. Although one-step distillation methods, such as OSEDiff and\nTSD-SR, offer faster inference, they remain fundamentally constrained by their\nlarge, over-parameterized model architectures. In this work, we present TinySR,\na compact yet effective diffusion model specifically designed for Real-ISR that\nachieves real-time performance while maintaining perceptual quality. We\nintroduce a Dynamic Inter-block Activation and an Expansion-Corrosion Strategy\nto facilitate more effective decision-making in depth pruning. We achieve VAE\ncompression through channel pruning, attention removal and lightweight SepConv.\nWe eliminate time- and prompt-related modules and perform pre-caching\ntechniques to further speed up the model. TinySR significantly reduces\ncomputational cost and model size, achieving up to 5.68x speedup and 83%\nparameter reduction compared to its teacher TSD-SR, while still providing high\nquality results."
                },
                "authors": [
                    {
                        "name": "Linwei Dong"
                    },
                    {
                        "name": "Qingnan Fan"
                    },
                    {
                        "name": "Yuhang Yu"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Jinwei Chen"
                    },
                    {
                        "name": "Yawei Luo"
                    },
                    {
                        "name": "Changqing Zou"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zou"
                },
                "author": "Changqing Zou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17356v1",
                "updated": "2025-08-24T13:30:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T13:30:00Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    13,
                    30,
                    0,
                    6,
                    236,
                    0
                ],
                "title": "DiCache: Let Diffusion Model Determine Its Own Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiCache: Let Diffusion Model Determine Its Own Cache"
                },
                "summary": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine the timing of caching and utilizing handcrafted rules for\nleveraging multi-step caches. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail on\noutlier samples. In this paper, a strong correlation is revealed between the\nvariation patterns of the shallow-layer feature differences in the diffusion\nmodel and those of final model outputs. Moreover, we have observed that the\nfeatures from different model layers form similar trajectories. Based on these\nobservations, we present DiCache, a novel training-free adaptive caching\nstrategy for accelerating diffusion models at runtime, answering both when and\nhow to cache within a unified framework. Specifically, DiCache is composed of\ntwo principal components: (1) Online Probe Profiling Scheme leverages a\nshallow-layer online probe to obtain a stable prior for the caching error in\nreal time, enabling the model to autonomously determine caching schedules. (2)\nDynamic Cache Trajectory Alignment combines multi-step caches based on\nshallow-layer probe feature trajectory to better approximate the current\nfeature, facilitating higher visual quality. Extensive experiments validate\nDiCache's capability in achieving higher efficiency and improved visual\nfidelity over state-of-the-art methods on various leading diffusion models\nincluding WAN 2.1, HunyuanVideo for video generation, and Flux for image\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years have witnessed the rapid development of acceleration techniques\nfor diffusion models, especially caching-based acceleration methods. These\nstudies seek to answer two fundamental questions: \"When to cache\" and \"How to\nuse cache\", typically relying on predefined empirical laws or dataset-level\npriors to determine the timing of caching and utilizing handcrafted rules for\nleveraging multi-step caches. However, given the highly dynamic nature of the\ndiffusion process, they often exhibit limited generalizability and fail on\noutlier samples. In this paper, a strong correlation is revealed between the\nvariation patterns of the shallow-layer feature differences in the diffusion\nmodel and those of final model outputs. Moreover, we have observed that the\nfeatures from different model layers form similar trajectories. Based on these\nobservations, we present DiCache, a novel training-free adaptive caching\nstrategy for accelerating diffusion models at runtime, answering both when and\nhow to cache within a unified framework. Specifically, DiCache is composed of\ntwo principal components: (1) Online Probe Profiling Scheme leverages a\nshallow-layer online probe to obtain a stable prior for the caching error in\nreal time, enabling the model to autonomously determine caching schedules. (2)\nDynamic Cache Trajectory Alignment combines multi-step caches based on\nshallow-layer probe feature trajectory to better approximate the current\nfeature, facilitating higher visual quality. Extensive experiments validate\nDiCache's capability in achieving higher efficiency and improved visual\nfidelity over state-of-the-art methods on various leading diffusion models\nincluding WAN 2.1, HunyuanVideo for video generation, and Flux for image\ngeneration."
                },
                "authors": [
                    {
                        "name": "Jiazi Bu"
                    },
                    {
                        "name": "Pengyang Ling"
                    },
                    {
                        "name": "Yujie Zhou"
                    },
                    {
                        "name": "Yibin Wang"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Tong Wu"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiaqi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaqi Wang"
                },
                "author": "Jiaqi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17219v1",
                "updated": "2025-08-24T05:45:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    24,
                    5,
                    45,
                    16,
                    6,
                    236,
                    0
                ],
                "published": "2025-08-24T05:45:16Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    5,
                    45,
                    16,
                    6,
                    236,
                    0
                ],
                "title": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained\n  Elastic Long-Context LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenLake: A Unified Segment-level Prefix Cache Pool for Fine-grained\n  Elastic Long-Context LLM Serving"
                },
                "summary": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefix caching is crucial to accelerate multi-turn interactions and requests\nwith shared prefixes. At the cluster level, existing prefix caching systems are\ntightly coupled with request scheduling to optimize cache efficiency and\ncomputation performance together, leading to load imbalance, data redundancy,\nand memory fragmentation of caching systems across instances. To address these\nissues, memory pooling is promising to shield the scheduler from the underlying\ncache management so that it can focus on the computation optimization. However,\nbecause existing prefix caching systems only transfer increasingly longer\nprefix caches between instances, they cannot achieve low-latency memory\npooling.\n  To address these problems, we propose a unified segment-level prefix cache\npool, TokenLake. It uses a declarative cache interface to expose requests'\nquery tensors, prefix caches, and cache-aware operations to TokenLake for\nefficient pooling. Powered by this abstraction, TokenLake can manage prefix\ncache at the segment level with a heavy-hitter-aware load balancing algorithm\nto achieve better cache load balance, deduplication, and defragmentation.\nTokenLake also transparently minimizes the communication volume of query\ntensors and new caches. Based on TokenLake, the scheduler can schedule requests\nelastically by using existing techniques without considering prefix cache\nmanagement. Evaluations on real-world workloads show that TokenLake can improve\nthroughput by up to 2.6$\\times$ and 2.0$\\times$ and boost hit rate by\n2.0$\\times$ and 2.1$\\times$, compared to state-of-the-art cache-aware routing\nand cache-centric PD-disaggregation solutions, respectively."
                },
                "authors": [
                    {
                        "name": "Bingyang Wu"
                    },
                    {
                        "name": "Zili Zhang"
                    },
                    {
                        "name": "Yinmin Zhong"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Xuanzhe Liu"
                    },
                    {
                        "name": "Xin Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xin Jin"
                },
                "author": "Xin Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14148v2",
                "updated": "2025-08-23T20:28:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    45,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-19T16:56:51Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    16,
                    56,
                    51,
                    1,
                    231,
                    0
                ],
                "title": "DPad: Efficient Diffusion Language Models with Suffix Dropout",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DPad: Efficient Diffusion Language Models with Suffix Dropout"
                },
                "summary": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based Large Language Models (dLLMs) parallelize text generation by\nframing decoding as a denoising process, but suffer from high computational\noverhead since they predict all future suffix tokens at each step while\nretaining only a small fraction. We propose Diffusion Scratchpad (DPad), a\ntraining-free method that restricts attention to a small set of nearby suffix\ntokens, preserving fidelity while eliminating redundancy. DPad integrates two\nstrategies: (i) a sliding window, which maintains a fixed-length suffix window,\nand (ii) distance-decay dropout, which deterministically removes distant suffix\ntokens before attention computation. This simple design is compatible with\nexisting optimizations such as prefix caching and can be implemented with only\na few lines of code. Comprehensive evaluations across multiple benchmarks on\nLLaDA-1.5 and Dream models demonstrate that DPad delivers up to\n$\\mathbf{61.4\\times}$ speedup over vanilla dLLMs while maintaining comparable\naccuracy, highlighting its potential for efficient and scalable long-sequence\ninference. Our code is available at https://github.com/Crys-Chen/DPad."
                },
                "authors": [
                    {
                        "name": "Xinhua Chen"
                    },
                    {
                        "name": "Sitao Huang"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Yintao He"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17137v1",
                "updated": "2025-08-23T20:28:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    32,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T20:28:32Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    20,
                    28,
                    32,
                    5,
                    235,
                    0
                ],
                "title": "MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Beyond: Learning-Based Expert Activation Prediction on Edge Devices"
                },
                "summary": "The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices\npresents significant challenges due to memory constraints. While MoE\narchitectures enable efficient utilization of computational resources by\nactivating only a subset of experts per inference, they require careful memory\nmanagement to operate efficiently in resource-constrained environments.\nTraditional heuristic-based expert caching strategies such as MoE-Infinity\nstruggle to maintain high cache hit rates as models parameters scale. In this\nwork, we introduce MoE-Beyond, a learning-based expert activation predictor\ntrained to predict expert activations during autoregressive decoding. By\nframing the task as a multi-label sequence prediction problem, we train a\nlightweight transformer model on 66 million expert activation traces extracted\nfrom LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor\ngeneralizes effectively across unseen prompts from WebGLM-QA dataset [6],\nachieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that\nMoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts\nfit in GPU cache, outperforming heuristic baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of large-scale Mixture-of-Experts (MoE) models on edge devices\npresents significant challenges due to memory constraints. While MoE\narchitectures enable efficient utilization of computational resources by\nactivating only a subset of experts per inference, they require careful memory\nmanagement to operate efficiently in resource-constrained environments.\nTraditional heuristic-based expert caching strategies such as MoE-Infinity\nstruggle to maintain high cache hit rates as models parameters scale. In this\nwork, we introduce MoE-Beyond, a learning-based expert activation predictor\ntrained to predict expert activations during autoregressive decoding. By\nframing the task as a multi-label sequence prediction problem, we train a\nlightweight transformer model on 66 million expert activation traces extracted\nfrom LDJnr-Puffin dataset [5] using DeepSeek-V2-Chat-Lite MoE. Our predictor\ngeneralizes effectively across unseen prompts from WebGLM-QA dataset [6],\nachieving 97.5% accuracy and an 86.6% F1-score. Simulation results show that\nMoE-Beyond improves GPU cache hit rate from 17% to 72% when only 10% of experts\nfit in GPU cache, outperforming heuristic baselines."
                },
                "authors": [
                    {
                        "name": "Nishant Gavhane"
                    },
                    {
                        "name": "Arush Mehrotra"
                    },
                    {
                        "name": "Rohit Chawla"
                    },
                    {
                        "name": "Peter Proenca"
                    }
                ],
                "author_detail": {
                    "name": "Peter Proenca"
                },
                "author": "Peter Proenca",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17125v1",
                "updated": "2025-08-23T19:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    19,
                    58,
                    18,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T19:58:18Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    19,
                    58,
                    18,
                    5,
                    235,
                    0
                ],
                "title": "VQL: An End-to-End Context-Aware Vector Quantization Attention for\n  Ultra-Long User Behavior Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQL: An End-to-End Context-Aware Vector Quantization Attention for\n  Ultra-Long User Behavior Modeling"
                },
                "summary": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In large-scale recommender systems, ultra-long user behavior sequences encode\nrich signals of evolving interests. Extending sequence length generally\nimproves accuracy, but directly modeling such sequences in production is\ninfeasible due to latency and memory constraints. Existing solutions fall into\ntwo categories: (1) top-k retrieval, which truncates the sequence and may\ndiscard most attention mass when L >> k; and (2) encoder-based compression,\nwhich preserves coverage but often over-compresses and fails to incorporate key\ncontext such as temporal gaps or target-aware signals. Neither class achieves a\ngood balance of low-loss compression, context awareness, and efficiency.\n  We propose VQL, a context-aware Vector Quantization Attention framework for\nultra-long behavior modeling, with three innovations. (1) Key-only\nquantization: only attention keys are quantized, while values remain intact; we\nprove that softmax normalization yields an error bound independent of sequence\nlength, and a codebook loss directly supervises quantization quality. This also\nenables L-free inference via offline caches. (2) Multi-scale quantization:\nattention heads are partitioned into groups, each with its own small codebook,\nwhich reduces quantization error while keeping cache size fixed. (3) Efficient\ncontext injection: static features (e.g., item category, modality) are directly\nintegrated, and relative position is modeled via a separable temporal kernel.\nAll context is injected without enlarging the codebook, so cached\nrepresentations remain query-independent.\n  Experiments on three large-scale datasets (KuaiRand-1K, KuaiRec, TMALL) show\nthat VQL consistently outperforms strong baselines, achieving higher accuracy\nwhile reducing inference latency, establishing a new state of the art in\nbalancing accuracy and efficiency for ultra-long sequence recommendation."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Yongxiang Tang"
                    },
                    {
                        "name": "Yanhua Cheng"
                    },
                    {
                        "name": "Yong Bai"
                    },
                    {
                        "name": "Yanxiang Zeng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Xialong Liu"
                    },
                    {
                        "name": "Peng Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Jiang"
                },
                "author": "Peng Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17032v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17032v1",
                "updated": "2025-08-23T14:20:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T14:20:06Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    14,
                    20,
                    6,
                    5,
                    235,
                    0
                ],
                "title": "Learned Structure in CARTRIDGES: Keys as Shareable Routers in\n  Self-Studied Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned Structure in CARTRIDGES: Keys as Shareable Routers in\n  Self-Studied Representations"
                },
                "summary": "A bottleneck for long-context LLM inference is the linearly growing KV cache.\nRecent work has proposed CARTRIDGES, an approach which leverages offline\ncompute to train a much smaller KV cache than is typically required for a full\ndocument (up to 40x less memory usage at inference time). In this paper, we\npresent the first mechanistic exploration of the learned CARTRIDGE key-value\ncache structure. In particular, we propose that (1) CARTRIDGE keys act as\nstable, shareable retrieval routers for the compressed corpora and (2) most of\nthe learned compression occurs within the CARTRIDGE value vectors. We present\nempirical evidence of our routing theory across tasks, model families, and\nmodel sizes; for example, we can ablate the learned CARTRIDGE key vectors\nbetween tasks with little performance loss. Finally, we propose a slight\nimprovement in initialization called Sampled Chunk Initialization (SCI). We\nsuggest that SCI can lead to faster CARTRIDGE convergence than previously\ndemonstrated in the literature. Our findings lay the groundwork for broader\nempirical study of CARTRIDGE training optimization which may be crucial for\nfurther scaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A bottleneck for long-context LLM inference is the linearly growing KV cache.\nRecent work has proposed CARTRIDGES, an approach which leverages offline\ncompute to train a much smaller KV cache than is typically required for a full\ndocument (up to 40x less memory usage at inference time). In this paper, we\npresent the first mechanistic exploration of the learned CARTRIDGE key-value\ncache structure. In particular, we propose that (1) CARTRIDGE keys act as\nstable, shareable retrieval routers for the compressed corpora and (2) most of\nthe learned compression occurs within the CARTRIDGE value vectors. We present\nempirical evidence of our routing theory across tasks, model families, and\nmodel sizes; for example, we can ablate the learned CARTRIDGE key vectors\nbetween tasks with little performance loss. Finally, we propose a slight\nimprovement in initialization called Sampled Chunk Initialization (SCI). We\nsuggest that SCI can lead to faster CARTRIDGE convergence than previously\ndemonstrated in the literature. Our findings lay the groundwork for broader\nempirical study of CARTRIDGE training optimization which may be crucial for\nfurther scaling."
                },
                "authors": [
                    {
                        "name": "Maurizio Diaz"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Diaz"
                },
                "author": "Maurizio Diaz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17032v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17032v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16984v1",
                "updated": "2025-08-23T10:35:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    10,
                    35,
                    16,
                    5,
                    235,
                    0
                ],
                "published": "2025-08-23T10:35:16Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    10,
                    35,
                    16,
                    5,
                    235,
                    0
                ],
                "title": "HiCache: Training-free Acceleration of Diffusion Models via Hermite\n  Polynomial-based Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiCache: Training-free Acceleration of Diffusion Models via Hermite\n  Polynomial-based Feature Caching"
                },
                "summary": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have achieved remarkable success in content generation but\nsuffer from prohibitive computational costs due to iterative sampling. While\nrecent feature caching methods tend to accelerate inference through temporal\nextrapolation, these methods still suffer from server quality loss due to the\nfailure in modeling the complex dynamics of feature evolution. To solve this\nproblem, this paper presents HiCache, a training-free acceleration framework\nthat fundamentally improves feature prediction by aligning mathematical tools\nwith empirical properties. Our key insight is that feature derivative\napproximations in Diffusion Transformers exhibit multivariate Gaussian\ncharacteristics, motivating the use of Hermite polynomials-the potentially\ntheoretically optimal basis for Gaussian-correlated processes. Besides, We\nfurther introduce a dual-scaling mechanism that ensures numerical stability\nwhile preserving predictive accuracy. Extensive experiments demonstrate\nHiCache's superiority: achieving 6.24x speedup on FLUX.1-dev while exceeding\nbaseline quality, maintaining strong performance across text-to-image, video\ngeneration, and super-resolution tasks. Core implementation is provided in the\nappendix, with complete code to be released upon acceptance."
                },
                "authors": [
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.03182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.03182v2",
                "updated": "2025-08-23T08:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    23,
                    8,
                    40,
                    52,
                    5,
                    235,
                    0
                ],
                "published": "2025-03-05T04:54:50Z",
                "published_parsed": [
                    2025,
                    3,
                    5,
                    4,
                    54,
                    50,
                    2,
                    64,
                    0
                ],
                "title": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Memory Efficiency in Large Language Model Training Through\n  Chronos-aware Pipeline Parallelism"
                },
                "summary": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Larger model sizes and longer sequence lengths have empowered the Large\nLanguage Model (LLM) to achieve outstanding performance across various domains.\nHowever, this progress brings significant storage capacity challenges for LLM\npretraining. High Bandwidth Memory (HBM) is expensive and requires more\nadvanced packaging technologies for capacity expansion, creating an urgent need\nfor memory-efficient scheduling strategies. Yet, prior pipeline parallelism\nschedules have primarily focused on reducing bubble overhead, often neglecting\nmemory efficiency and lacking compatibility with other memory-efficient\nstrategies. Consequently, these methods struggle to meet the storage demands of\nstorage capacity for next-generation LLM. This work presents ChronosPipe, a\nChronos-aware pipeline parallelism for memory-efficient LLM pretraining. The\ncore insight of ChronosPipe is to treat HBM as a fast but small 'cache,'\noptimizing and exploiting temporal locality within LLM pretraining to enhance\nHBM utilization. ChronosPipe introduces a pipeline scheduling strategy,\nChronos-Pipe, to reduce the extrinsic overhead that disrupts the temporal\nlocality of activations. Additionally, it leverages Chronos-Recomp and\nChronos-Offload to efficiently harness the intrinsic temporal locality of\nactivations and weights in Deep Neural Networks. Experiment results show that\nChronosPipe can expand the trainable model size by 2.4x while maintaining\ncomparable throughput, achieving 1.5x better than the 1F1B strategy combined\nwith recomputation."
                },
                "authors": [
                    {
                        "name": "Xinyuan Lin"
                    },
                    {
                        "name": "Chenlu Li"
                    },
                    {
                        "name": "Zongle Huang"
                    },
                    {
                        "name": "Chunyu Wang"
                    },
                    {
                        "name": "Bo Xiao"
                    },
                    {
                        "name": "Huazhong Yang"
                    },
                    {
                        "name": "Shishi Duan"
                    },
                    {
                        "name": "Yongpan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yongpan Liu"
                },
                "author": "Yongpan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.03182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.03182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20776v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20776v2",
                "updated": "2025-08-22T08:45:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    45,
                    4,
                    4,
                    234,
                    0
                ],
                "published": "2025-05-27T06:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    6,
                    30,
                    0,
                    1,
                    147,
                    0
                ],
                "title": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long\n  Sequences"
                },
                "summary": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a widely adopted technique for accelerating inference\nin large language models (LLMs), but its performance degrades on long inputs\ndue to increased attention cost and reduced draft accuracy. We introduce\nSpecExtend, a drop-in enhancement that improves the performance of speculative\ndecoding on long sequences without any additional training. First, SpecExtend\nintegrates efficient attention mechanisms such as FlashAttention and Hybrid\nTree Attention into both the draft and target models. To improve draft accuracy\nand speed on long inputs without retraining, we propose Cross-model Retrieval,\na novel KV cache eviction strategy that uses the target model's attention\nscores to dynamically select relevant context for the draft model. Extensive\nevaluations on three long-context understanding datasets show that SpecExtend\naccelerates standard tree-based speculative decoding by up to 2.22x for inputs\nup to 16K tokens, providing an effective solution for speculative decoding of\nlong sequences. Our code is available at https://github.com/jycha98/SpecExtend ."
                },
                "authors": [
                    {
                        "name": "Jungyoub Cha"
                    },
                    {
                        "name": "Hyunjong Kim"
                    },
                    {
                        "name": "Sungzoon Cho"
                    }
                ],
                "author_detail": {
                    "name": "Sungzoon Cho"
                },
                "author": "Sungzoon Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20776v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20776v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16211v1",
                "updated": "2025-08-22T08:34:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    34,
                    3,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T08:34:03Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    8,
                    34,
                    3,
                    4,
                    234,
                    0
                ],
                "title": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecast then Calibrate: Feature Caching as ODE for Efficient Diffusion\n  Transformers"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated exceptional performance in\nhigh-fidelity image and video generation. To reduce their substantial\ncomputational costs, feature caching techniques have been proposed to\naccelerate inference by reusing hidden representations from previous timesteps.\nHowever, current methods often struggle to maintain generation quality at high\nacceleration ratios, where prediction errors increase sharply due to the\ninherent instability of long-step forecasting. In this work, we adopt an\nordinary differential equation (ODE) perspective on the hidden-feature\nsequence, modeling layer representations along the trajectory as a feature-ODE.\nWe attribute the degradation of existing caching strategies to their inability\nto robustly integrate historical features under large skipping intervals. To\naddress this, we propose FoCa (Forecast-then-Calibrate), which treats feature\ncaching as a feature-ODE solving problem. Extensive experiments on image\nsynthesis, video generation, and super-resolution tasks demonstrate the\neffectiveness of FoCa, especially under aggressive acceleration. Without\nadditional training, FoCa achieves near-lossless speedups of 5.50 times on\nFLUX, 6.45 times on HunyuanVideo, 3.17 times on Inf-DiT, and maintains high\nquality with a 4.53 times speedup on DiT."
                },
                "authors": [
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Liang Feng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Junjie Chen"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16184v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16184v1",
                "updated": "2025-08-22T07:57:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    57,
                    28,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T07:57:28Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    7,
                    57,
                    28,
                    4,
                    234,
                    0
                ],
                "title": "Joint Cache Placement and Routing in Satellite-Terrestrial Edge\n  Computing Network: A GNN-Enabled DRL Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Cache Placement and Routing in Satellite-Terrestrial Edge\n  Computing Network: A GNN-Enabled DRL Approach"
                },
                "summary": "In this letter, we investigate the problem of joint content caching and\nrouting in satellite-terrestrial edge computing networks (STECNs) to improve\ncaching service for geographically distributed users. To handle the challenges\narising from dynamic low Earth orbit (LEO) satellite topologies and\nheterogeneous content demands, we propose a learning-based framework that\nintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).\nThe satellite network is represented as a dynamic graph, where GNNs are\nembedded within the DRL agent to capture spatial and topological dependencies\nand support routing-aware decision-making. The caching strategy is optimized by\nformulating the problem as a Markov decision process (MDP) and applying soft\nactor-critic (SAC) algorithm. Simulation results demonstrate that our approach\nsignificantly improves the delivery success rate and reduces communication\ntraffic cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this letter, we investigate the problem of joint content caching and\nrouting in satellite-terrestrial edge computing networks (STECNs) to improve\ncaching service for geographically distributed users. To handle the challenges\narising from dynamic low Earth orbit (LEO) satellite topologies and\nheterogeneous content demands, we propose a learning-based framework that\nintegrates graph neural networks (GNNs) with deep reinforcement learning (DRL).\nThe satellite network is represented as a dynamic graph, where GNNs are\nembedded within the DRL agent to capture spatial and topological dependencies\nand support routing-aware decision-making. The caching strategy is optimized by\nformulating the problem as a Markov decision process (MDP) and applying soft\nactor-critic (SAC) algorithm. Simulation results demonstrate that our approach\nsignificantly improves the delivery success rate and reduces communication\ntraffic cost."
                },
                "authors": [
                    {
                        "name": "Yuhao Zheng"
                    },
                    {
                        "name": "Ting You"
                    },
                    {
                        "name": "Kejia Peng"
                    },
                    {
                        "name": "Chang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Liu"
                },
                "author": "Chang Liu",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16184v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16184v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16134v1",
                "updated": "2025-08-22T06:55:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    55,
                    45,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T06:55:45Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    55,
                    45,
                    4,
                    234,
                    0
                ],
                "title": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CommonKV: Compressing KV Cache with Cross-layer Parameter Sharing"
                },
                "summary": "Large Language Models (LLMs) confront significant memory challenges due to\nthe escalating KV cache with increasing sequence length. As a crucial\ntechnique, existing cross-layer KV cache sharing methods either necessitate\nmodified model architectures with subsequent pre-training or incur significant\nperformance degradation at high compression rates. To mitigate these\nchallenges, we propose CommonKV, a training-free method for cross-layer KV\ncache compression through adjacent parameters sharing. Inspired by the high\nsimilarity observed in cross-layer hidden states, we utilize Singular Value\nDecomposition (SVD) to achieve weight sharing across adjacent parameters,\nresulting in a more easily mergeable latent KV cache. Furthermore, we also\nintroduce an adaptive budget allocation strategy. It dynamically assigns\ncompression budgets based on cosine similarity, ensuring that dissimilar caches\nare not over-compressed. Experiments across multiple backbone models and\nbenchmarks including LongBench and Ruler demonstrate that the proposed method\nconsistently outperforms existing low-rank and cross-layer approaches at\nvarious compression ratios. Moreover, we find that the benefits of CommonKV are\northogonal to other quantization and eviction methods. By integrating these\napproaches, we can ultimately achieve a 98\\% compression ratio without\nsignificant performance loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) confront significant memory challenges due to\nthe escalating KV cache with increasing sequence length. As a crucial\ntechnique, existing cross-layer KV cache sharing methods either necessitate\nmodified model architectures with subsequent pre-training or incur significant\nperformance degradation at high compression rates. To mitigate these\nchallenges, we propose CommonKV, a training-free method for cross-layer KV\ncache compression through adjacent parameters sharing. Inspired by the high\nsimilarity observed in cross-layer hidden states, we utilize Singular Value\nDecomposition (SVD) to achieve weight sharing across adjacent parameters,\nresulting in a more easily mergeable latent KV cache. Furthermore, we also\nintroduce an adaptive budget allocation strategy. It dynamically assigns\ncompression budgets based on cosine similarity, ensuring that dissimilar caches\nare not over-compressed. Experiments across multiple backbone models and\nbenchmarks including LongBench and Ruler demonstrate that the proposed method\nconsistently outperforms existing low-rank and cross-layer approaches at\nvarious compression ratios. Moreover, we find that the benefits of CommonKV are\northogonal to other quantization and eviction methods. By integrating these\napproaches, we can ultimately achieve a 98\\% compression ratio without\nsignificant performance loss."
                },
                "authors": [
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoyu Qiao"
                    },
                    {
                        "name": "Lujun Li"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "10 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16121v1",
                "updated": "2025-08-22T06:28:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    28,
                    24,
                    4,
                    234,
                    0
                ],
                "published": "2025-08-22T06:28:24Z",
                "published_parsed": [
                    2025,
                    8,
                    22,
                    6,
                    28,
                    24,
                    4,
                    234,
                    0
                ],
                "title": "Lightweight and Fast Real-time Image Enhancement via Decomposition of\n  the Spatial-aware Lookup Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight and Fast Real-time Image Enhancement via Decomposition of\n  the Spatial-aware Lookup Tables"
                },
                "summary": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently\nreduce both model size and runtime by interpolating pre-calculated values at\nthe vertices. However, the 3D LUT methods have a limitation due to their lack\nof spatial information, as they convert color values on a point-by-point basis.\nAlthough spatial-aware 3D LUT methods address this limitation, they introduce\nadditional modules that require a substantial number of parameters, leading to\nincreased runtime as image resolution increases. To address this issue, we\npropose a method for generating image-adaptive LUTs by focusing on the\nredundant parts of the tables. Our efficient framework decomposes a 3D LUT into\na linear sum of low-dimensional LUTs and employs singular value decomposition\n(SVD). Furthermore, we enhance the modules for spatial feature fusion to be\nmore cache-efficient. Extensive experimental results demonstrate that our model\neffectively decreases both the number of parameters and runtime while\nmaintaining spatial awareness and performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The image enhancement methods based on 3D lookup tables (3D LUTs) efficiently\nreduce both model size and runtime by interpolating pre-calculated values at\nthe vertices. However, the 3D LUT methods have a limitation due to their lack\nof spatial information, as they convert color values on a point-by-point basis.\nAlthough spatial-aware 3D LUT methods address this limitation, they introduce\nadditional modules that require a substantial number of parameters, leading to\nincreased runtime as image resolution increases. To address this issue, we\npropose a method for generating image-adaptive LUTs by focusing on the\nredundant parts of the tables. Our efficient framework decomposes a 3D LUT into\na linear sum of low-dimensional LUTs and employs singular value decomposition\n(SVD). Furthermore, we enhance the modules for spatial feature fusion to be\nmore cache-efficient. Extensive experimental results demonstrate that our model\neffectively decreases both the number of parameters and runtime while\nmaintaining spatial awareness and performance."
                },
                "authors": [
                    {
                        "name": "Wontae Kim"
                    },
                    {
                        "name": "Keuntek Lee"
                    },
                    {
                        "name": "Nam Ik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Nam Ik Cho"
                },
                "author": "Nam Ik Cho",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v2",
                "updated": "2025-08-22T03:36:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    22,
                    3,
                    36,
                    44,
                    4,
                    234,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1].This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1].This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Yang Zhao"
                },
                "author": "Yang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10431v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10431v3",
                "updated": "2025-08-31T05:43:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    31,
                    5,
                    43,
                    55,
                    6,
                    243,
                    0
                ],
                "published": "2025-08-14T08:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    8,
                    4,
                    15,
                    3,
                    226,
                    0
                ],
                "title": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yet Another Mirage of Breaking MIRAGE: Debunking Occupancy-based\n  Side-Channel Attacks on Fully Associative Randomized Caches"
                },
                "summary": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent work presented at USENIX Security 2025 (SEC'25) claims that\noccupancy-based attacks can recover AES keys from the MIRAGE randomized cache.\nIn this paper, we examine these claims and find that they arise from a modeling\nflaw in the SEC'25 paper. Most critically, the SEC'25 paper's simulation of\nMIRAGE uses a constant seed to initialize the random number generator used for\nglobal evictions in MIRAGE, causing every AES encryption they trace to evict\nthe same deterministic sequence of cache lines. This artificially creates a\nhighly repeatable timing pattern that is not representative of a realistic\nimplementation of MIRAGE, where eviction sequences vary randomly between\nencryptions. When we instead randomize the eviction seed for each run,\nreflecting realistic operation, the correlation between AES T-table accesses\nand attacker runtimes disappears, and the attack fails. These findings show\nthat the reported leakage is an artifact of incorrect modeling, and not an\nactual vulnerability in MIRAGE."
                },
                "authors": [
                    {
                        "name": "Chris Cao"
                    },
                    {
                        "name": "Gururaj Saileshwar"
                    }
                ],
                "author_detail": {
                    "name": "Gururaj Saileshwar"
                },
                "author": "Gururaj Saileshwar",
                "arxiv_comment": "This version includes updated analysis of RCO Bugs (one additional\n  bug identified). Appendix added with code snippets for bug fixes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10431v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10431v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01225v2",
                "updated": "2025-08-21T20:13:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    20,
                    13,
                    40,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-02T06:43:43Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models"
                },
                "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance. Project Page available at:\nhttps://zhaihaotian.github.io/MCP-ICCV25/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance. Project Page available at:\nhttps://zhaihaotian.github.io/MCP-ICCV25/"
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Xiupeng Shi"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15919v1",
                "updated": "2025-08-21T18:40:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T18:40:20Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling"
                },
                "summary": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures.\n  We present \\textbf{HyperFlexis}, a unified LLM serving system that integrates\nalgorithmic and system-level innovations to jointly optimize scheduling and\nscaling under multiple SLOs. It features a multi-SLO-aware scheduler that\nleverages budget estimation and request prioritization to ensure proactive SLO\ncompliance for both new and ongoing requests. The system supports prefill- and\ndecode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV\ncache transfers. It also enables cost-effective scaling decisions,\nprefill-decode instance linking during scaling, and rapid P/D role transitions.\nTo accelerate scaling and reduce cold-start latency, a device-to-device (D2D)\nweight transfer mechanism is proposed that lowers weight loading overhead by up\nto \\textbf{19.39$\\times$}. These optimizations allow the system to achieve up\nto \\textbf{4.44$\\times$} higher SLO attainment, \\textbf{65.82\\%} lower request\nlatency, and cost parity with state-of-the-art baselines. The code will be\nreleased soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures.\n  We present \\textbf{HyperFlexis}, a unified LLM serving system that integrates\nalgorithmic and system-level innovations to jointly optimize scheduling and\nscaling under multiple SLOs. It features a multi-SLO-aware scheduler that\nleverages budget estimation and request prioritization to ensure proactive SLO\ncompliance for both new and ongoing requests. The system supports prefill- and\ndecode-stage multi-SLO scheduling for P/D-disaggregated architectures and KV\ncache transfers. It also enables cost-effective scaling decisions,\nprefill-decode instance linking during scaling, and rapid P/D role transitions.\nTo accelerate scaling and reduce cold-start latency, a device-to-device (D2D)\nweight transfer mechanism is proposed that lowers weight loading overhead by up\nto \\textbf{19.39$\\times$}. These optimizations allow the system to achieve up\nto \\textbf{4.44$\\times$} higher SLO attainment, \\textbf{65.82\\%} lower request\nlatency, and cost parity with state-of-the-art baselines. The code will be\nreleased soon."
                },
                "authors": [
                    {
                        "name": "Zahra Yousefijamarani"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Taha Shabani"
                    },
                    {
                        "name": "Niloofar Gholipour"
                    },
                    {
                        "name": "Parham Yassini"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Qiantao Zhang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15717v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15717v1",
                "updated": "2025-08-21T16:56:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    56,
                    29,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T16:56:29Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    56,
                    29,
                    3,
                    233,
                    0
                ],
                "title": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamMem: Query-Agnostic KV Cache Memory for Streaming Video\n  Understanding"
                },
                "summary": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have made significant progress in\nvisual-language reasoning, but their ability to efficiently handle long videos\nremains limited. Despite recent advances in long-context MLLMs, storing and\nattending to the key-value (KV) cache for long visual contexts incurs\nsubstantial memory and computational overhead. Existing visual compression\nmethods require either encoding the entire visual context before compression or\nhaving access to the questions in advance, which is impractical for long video\nunderstanding and multi-turn conversational settings. In this work, we propose\nStreamMem, a query-agnostic KV cache memory mechanism for streaming video\nunderstanding. Specifically, StreamMem encodes new video frames in a streaming\nmanner, compressing the KV cache using attention scores between visual tokens\nand generic query tokens, while maintaining a fixed-size KV memory to enable\nefficient question answering (QA) in memory-constrained, long-video scenarios.\nEvaluation on three long video understanding and two streaming video question\nanswering benchmarks shows that StreamMem achieves state-of-the-art performance\nin query-agnostic KV cache compression and is competitive with query-aware\ncompression approaches."
                },
                "authors": [
                    {
                        "name": "Yanlai Yang"
                    },
                    {
                        "name": "Zhuokai Zhao"
                    },
                    {
                        "name": "Satya Narayan Shukla"
                    },
                    {
                        "name": "Aashu Singh"
                    },
                    {
                        "name": "Shlok Kumar Mishra"
                    },
                    {
                        "name": "Lizhu Zhang"
                    },
                    {
                        "name": "Mengye Ren"
                    }
                ],
                "author_detail": {
                    "name": "Mengye Ren"
                },
                "author": "Mengye Ren",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15717v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15717v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15694v1",
                "updated": "2025-08-21T16:21:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    21,
                    46,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T16:21:46Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    16,
                    21,
                    46,
                    3,
                    233,
                    0
                ],
                "title": "GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector\n  Nearest Neighbor Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoVector: An I/O-Efficient Caching Strategy for High-Dimensional Vector\n  Nearest Neighbor Search"
                },
                "summary": "Graph-based high-dimensional vector indices have become a mainstream solution\nfor large-scale approximate nearest neighbor search (ANNS). However, their\nsubstantial memory footprint often requires storage on secondary devices, where\nfrequent on-demand loading of graph and vector data leads to I/O becoming the\ndominant bottleneck, accounting for over 90\\% of query latency. Existing static\ncaching strategies mitigate this issue only in the initial navigation phase by\npreloading entry points and multi-hop neighbors, but they fail in the second\nphase where query-dependent nodes must be dynamically accessed to achieve high\nrecall. We propose GoVector, an I/O-efficient caching strategy tailored for\ndisk-based graph indices. GoVector combines (1) a static cache that stores\nentry points and frequently accessed neighbors, and (2) a dynamic cache that\nadaptively captures nodes with high spatial locality during the second search\nphase. To further align storage layout with similarity-driven search patterns,\nGoVector reorders nodes on disk so that similar vectors are colocated on the\nsame or adjacent pages, thereby improving locality and reducing I/O overhead.\nExtensive experiments on multiple public datasets show that GoVector achieves\nsubstantial performance improvements. At 90% recall, it reduces I/O operations\nby 46% on average, increases query throughput by 1.73x, and lowers query\nlatency by 42% compared to state-of-the-art disk-based graph indexing systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based high-dimensional vector indices have become a mainstream solution\nfor large-scale approximate nearest neighbor search (ANNS). However, their\nsubstantial memory footprint often requires storage on secondary devices, where\nfrequent on-demand loading of graph and vector data leads to I/O becoming the\ndominant bottleneck, accounting for over 90\\% of query latency. Existing static\ncaching strategies mitigate this issue only in the initial navigation phase by\npreloading entry points and multi-hop neighbors, but they fail in the second\nphase where query-dependent nodes must be dynamically accessed to achieve high\nrecall. We propose GoVector, an I/O-efficient caching strategy tailored for\ndisk-based graph indices. GoVector combines (1) a static cache that stores\nentry points and frequently accessed neighbors, and (2) a dynamic cache that\nadaptively captures nodes with high spatial locality during the second search\nphase. To further align storage layout with similarity-driven search patterns,\nGoVector reorders nodes on disk so that similar vectors are colocated on the\nsame or adjacent pages, thereby improving locality and reducing I/O overhead.\nExtensive experiments on multiple public datasets show that GoVector achieves\nsubstantial performance improvements. At 90% recall, it reduces I/O operations\nby 46% on average, increases query throughput by 1.73x, and lowers query\nlatency by 42% compared to state-of-the-art disk-based graph indexing systems."
                },
                "authors": [
                    {
                        "name": "Yijie Zhou"
                    },
                    {
                        "name": "Shengyuan Lin"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shuhao Fan"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "arxiv_comment": "12 pages, 12 figures, this paper is the English version of our\n  Chinese paper accepted for publication in Journal of Software, Vol. 37, No.\n  3, 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15647v1",
                "updated": "2025-08-21T15:25:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    30,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T15:25:30Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    15,
                    25,
                    30,
                    3,
                    233,
                    0
                ],
                "title": "CausalMesh: A Formally Verified Causal Cache for Stateful Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CausalMesh: A Formally Verified Causal Cache for Stateful Serverless\n  Computing"
                },
                "summary": "Stateful serverless workflows consist of multiple serverless functions that\naccess state on a remote database. Developers sometimes add a cache layer\nbetween the serverless runtime and the database to improve I/O latency.\nHowever, in a serverless environment, functions in the same workflow may be\nscheduled to different nodes with different caches, which can cause\nnon-intuitive anomalies. This paper presents CausalMesh, a novel approach to\ncausally consistent caching in environments where a computation may migrate\nfrom one machine to another, such as in serverless computing. CausalMesh is the\nfirst cache system that supports coordination-free and abort-free read/write\noperations and read transactions when clients roam among multiple servers.\nCausalMesh also supports read-write transactional causal consistency in the\npresence of client roaming, but at the cost of abort-freedom.\n  We have formally verified CausalMesh's protocol in Dafny, and our\nexperimental evaluation shows that CausalMesh has lower latency and higher\nthroughput than existing proposals",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stateful serverless workflows consist of multiple serverless functions that\naccess state on a remote database. Developers sometimes add a cache layer\nbetween the serverless runtime and the database to improve I/O latency.\nHowever, in a serverless environment, functions in the same workflow may be\nscheduled to different nodes with different caches, which can cause\nnon-intuitive anomalies. This paper presents CausalMesh, a novel approach to\ncausally consistent caching in environments where a computation may migrate\nfrom one machine to another, such as in serverless computing. CausalMesh is the\nfirst cache system that supports coordination-free and abort-free read/write\noperations and read transactions when clients roam among multiple servers.\nCausalMesh also supports read-write transactional causal consistency in the\npresence of client roaming, but at the cost of abort-freedom.\n  We have formally verified CausalMesh's protocol in Dafny, and our\nexperimental evaluation shows that CausalMesh has lower latency and higher\nthroughput than existing proposals"
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Zihao Zhang"
                    },
                    {
                        "name": "Shuai Mu"
                    },
                    {
                        "name": "Sebastian Angel"
                    },
                    {
                        "name": "Vincent Liu"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Liu"
                },
                "author": "Vincent Liu",
                "arxiv_doi": "10.14778/3704965.3704969",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.14778/3704965.3704969",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.15647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Extended version from PVLDB Volume 17, Issue 13, 2024. This version\n  includes full proofs and formal verification in Dafny and fixes some small\n  bugs",
                "arxiv_journal_ref": "PVLDB Volume 17, Issue 13, 2024",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v2",
                "updated": "2025-08-21T14:58:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    58,
                    12,
                    3,
                    233,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy. To our knowledge, this is the first\nside-channel attack on AI privacy that exploits hardware optimizations."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15601v1",
                "updated": "2025-08-21T14:24:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    24,
                    52,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T14:24:52Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    14,
                    24,
                    52,
                    3,
                    233,
                    0
                ],
                "title": "Efficient Mixed-Precision Large Language Model Inference with TurboMind",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Mixed-Precision Large Language Model Inference with TurboMind"
                },
                "summary": "Mixed-precision inference techniques reduce the memory and computational\ndemands of Large Language Models (LLMs) by applying hybrid precision formats to\nmodel weights, activations, and KV caches. This work introduces mixed-precision\nLLM inference techniques that encompass (i) systematic memory and compute\noptimization across hierarchical storage and tensor core architectures, and\n(ii) comprehensive end-to-end mixed-precision optimization across diverse\nprecision formats and hardware configurations. Our approach features two novel\nmixed-precision pipelines designed for optimal hardware utilization: a General\nMatrix Multiply (GEMM) pipeline that optimizes matrix operations through\noffline weight packing and online acceleration, and an attention pipeline that\nenables efficient attention computation with arbitrary Query, Key, and Value\nprecision combinations. The key implementation of the pipelines includes (i)\nhardware-aware weight packing for automatic format optimization, (ii) adaptive\nhead alignment for efficient attention computation, (iii) instruction-level\nparallelism for memory hierarchy exploitation, and (iv) KV memory loading\npipeline for enhanced inference efficiency. We conduct comprehensive\nevaluations across 16 popular LLMs and 4 representative GPU architectures.\nResults demonstrate that our approach achieves up to 61% lower serving latency\n(30% on average) and up to 156% higher throughput (58% on average) in\nmixed-precision workloads compared to existing mixed-precision frameworks,\nestablishing consistent performance improvements across all tested\nconfigurations and hardware types. This work is integrated into TurboMind, a\nhigh-performance inference engine of the LMDeploy project, which is\nopen-sourced and publicly available at https://github.com/InternLM/lmdeploy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed-precision inference techniques reduce the memory and computational\ndemands of Large Language Models (LLMs) by applying hybrid precision formats to\nmodel weights, activations, and KV caches. This work introduces mixed-precision\nLLM inference techniques that encompass (i) systematic memory and compute\noptimization across hierarchical storage and tensor core architectures, and\n(ii) comprehensive end-to-end mixed-precision optimization across diverse\nprecision formats and hardware configurations. Our approach features two novel\nmixed-precision pipelines designed for optimal hardware utilization: a General\nMatrix Multiply (GEMM) pipeline that optimizes matrix operations through\noffline weight packing and online acceleration, and an attention pipeline that\nenables efficient attention computation with arbitrary Query, Key, and Value\nprecision combinations. The key implementation of the pipelines includes (i)\nhardware-aware weight packing for automatic format optimization, (ii) adaptive\nhead alignment for efficient attention computation, (iii) instruction-level\nparallelism for memory hierarchy exploitation, and (iv) KV memory loading\npipeline for enhanced inference efficiency. We conduct comprehensive\nevaluations across 16 popular LLMs and 4 representative GPU architectures.\nResults demonstrate that our approach achieves up to 61% lower serving latency\n(30% on average) and up to 156% higher throughput (58% on average) in\nmixed-precision workloads compared to existing mixed-precision frameworks,\nestablishing consistent performance improvements across all tested\nconfigurations and hardware types. This work is integrated into TurboMind, a\nhigh-performance inference engine of the LMDeploy project, which is\nopen-sourced and publicly available at https://github.com/InternLM/lmdeploy."
                },
                "authors": [
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Youhe Jiang"
                    },
                    {
                        "name": "Guoliang He"
                    },
                    {
                        "name": "Xin Chen"
                    },
                    {
                        "name": "Han Lv"
                    },
                    {
                        "name": "Qian Yao"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15583v1",
                "updated": "2025-08-21T13:57:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    57,
                    9,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T13:57:09Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    57,
                    9,
                    3,
                    233,
                    0
                ],
                "title": "Time-Optimal Directed q-Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Optimal Directed q-Analysis"
                },
                "summary": "Directed q-analysis is a recent extension of q-analysis, an established\nmethod for extracting structure from networks, to directed graphs. Until\nrecently, a lack of efficient algorithms heavily restricted the application of\nthis technique: Previous approaches scale with the square of the input size,\nwhich is also the maximal size of the output, rendering such approaches\nworst-case optimal. In practice, output sizes of relevant networks are usually\nfar from the worst case, a fact that could be exploited by an (efficient)\noutput-sensitive algorithm. We develop such an algorithm and formally describe\nit in detail. The key insight, obtained by carefully studying various\napproaches to directed q-analysis and how they relate to each other, is that\ninverting the order of computation leads to significant complexity gains.\nTargeted precomputation and caching tactics further reduce the introduced\noverhead, enough to achieve (under mild assumptions) a time complexity that is\nlinear in output size. The resulting algorithm for performing directed\nq-analysis is shown to be time-optimal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Directed q-analysis is a recent extension of q-analysis, an established\nmethod for extracting structure from networks, to directed graphs. Until\nrecently, a lack of efficient algorithms heavily restricted the application of\nthis technique: Previous approaches scale with the square of the input size,\nwhich is also the maximal size of the output, rendering such approaches\nworst-case optimal. In practice, output sizes of relevant networks are usually\nfar from the worst case, a fact that could be exploited by an (efficient)\noutput-sensitive algorithm. We develop such an algorithm and formally describe\nit in detail. The key insight, obtained by carefully studying various\napproaches to directed q-analysis and how they relate to each other, is that\ninverting the order of computation leads to significant complexity gains.\nTargeted precomputation and caching tactics further reduce the introduced\noverhead, enough to achieve (under mild assumptions) a time complexity that is\nlinear in output size. The resulting algorithm for performing directed\nq-analysis is shown to be time-optimal."
                },
                "authors": [
                    {
                        "name": "Felix Windisch"
                    },
                    {
                        "name": "Florian Unger"
                    }
                ],
                "author_detail": {
                    "name": "Florian Unger"
                },
                "author": "Florian Unger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15545v1",
                "updated": "2025-08-21T13:24:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    24,
                    13,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T13:24:13Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    24,
                    13,
                    3,
                    233,
                    0
                ],
                "title": "QVecOpt: An Efficient Storage and Computing Opti-mization Framework for\n  Large-scale Quantum State Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QVecOpt: An Efficient Storage and Computing Opti-mization Framework for\n  Large-scale Quantum State Simulation"
                },
                "summary": "In response to the challenges in large-scale quantum state simulation on\nclassical computing platforms, including memory limits, frequent disk I/O, and\nhigh computational complexity, this study builds upon a previously proposed\nhierarchical storage-based quantum simulation system and introduces an\noptimization framework, the Quantum Vector Optimization Framework (QVecOpt).\nQVecOpt integrates four strategies: amplitude pairing, cache optimization,\nblock storage optimization, and parallel optimization. These collectively\nenhance state vector storage and computational scheduling. The amplitude\npairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing\ntraversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache\noptimization pre-allocates buffers and loads only required data, cutting disk\nI/O. Block storage optimization partitions the state vector for on-demand\nloading and local updates, reducing redundant access. Parallel optimization\ndistributes the state vector across nodes for collaborative computation,\nachieving near-linear speedup. Complexity analysis shows that, compared with\nhierarchical storage simulation, the method reduces state vector traversals for\nsingle-qubit gates from $2^n$ to 1, removing the main bottleneck. It also\nlowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and\n$O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold,\nbreaking the memory bottleneck of existing tools and enabling high-bit quantum\ncircuit simulations beyond traditional methods. This work provides an\nefficient, scalable solution for classical simulation of large-scale quantum\ncomputation with significant academic and practical value.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In response to the challenges in large-scale quantum state simulation on\nclassical computing platforms, including memory limits, frequent disk I/O, and\nhigh computational complexity, this study builds upon a previously proposed\nhierarchical storage-based quantum simulation system and introduces an\noptimization framework, the Quantum Vector Optimization Framework (QVecOpt).\nQVecOpt integrates four strategies: amplitude pairing, cache optimization,\nblock storage optimization, and parallel optimization. These collectively\nenhance state vector storage and computational scheduling. The amplitude\npairing mechanism locates relevant amplitude pairs via bitwise XOR, reducing\ntraversal complexity of single-qubit gates from $O(2^n)$ to $O(1)$. Cache\noptimization pre-allocates buffers and loads only required data, cutting disk\nI/O. Block storage optimization partitions the state vector for on-demand\nloading and local updates, reducing redundant access. Parallel optimization\ndistributes the state vector across nodes for collaborative computation,\nachieving near-linear speedup. Complexity analysis shows that, compared with\nhierarchical storage simulation, the method reduces state vector traversals for\nsingle-qubit gates from $2^n$ to 1, removing the main bottleneck. It also\nlowers computational and I/O complexity from $O(2^n)$ to $O(2^n/C)$ and\n$O(2^n/B)$. In simulations of 16-29 qubits, efficiency improves nearly tenfold,\nbreaking the memory bottleneck of existing tools and enabling high-bit quantum\ncircuit simulations beyond traditional methods. This work provides an\nefficient, scalable solution for classical simulation of large-scale quantum\ncomputation with significant academic and practical value."
                },
                "authors": [
                    {
                        "name": "Mingyang Yu"
                    },
                    {
                        "name": "Haorui Yang"
                    },
                    {
                        "name": "Donglin Wang"
                    },
                    {
                        "name": "Desheng Kong"
                    },
                    {
                        "name": "Ji Du"
                    },
                    {
                        "name": "Yulong Fu"
                    },
                    {
                        "name": "Jing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jing Xu"
                },
                "author": "Jing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12892v2",
                "updated": "2025-08-21T12:52:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    12,
                    52,
                    11,
                    3,
                    233,
                    0
                ],
                "published": "2024-09-19T16:31:44Z",
                "published_parsed": [
                    2024,
                    9,
                    19,
                    16,
                    31,
                    44,
                    3,
                    263,
                    0
                ],
                "title": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3DGS-LM: Faster Gaussian-Splatting Optimization with Levenberg-Marquardt"
                },
                "summary": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 20% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present 3DGS-LM, a new method that accelerates the reconstruction of 3D\nGaussian Splatting (3DGS) by replacing its ADAM optimizer with a tailored\nLevenberg-Marquardt (LM). Existing methods reduce the optimization time by\ndecreasing the number of Gaussians or by improving the implementation of the\ndifferentiable rasterizer. However, they still rely on the ADAM optimizer to\nfit Gaussian parameters of a scene in thousands of iterations, which can take\nup to an hour. To this end, we change the optimizer to LM that runs in\nconjunction with the 3DGS differentiable rasterizer. For efficient GPU\nparallization, we propose a caching data structure for intermediate gradients\nthat allows us to efficiently calculate Jacobian-vector products in custom CUDA\nkernels. In every LM iteration, we calculate update directions from multiple\nimage subsets using these kernels and combine them in a weighted mean. Overall,\nour method is 20% faster than the original 3DGS while obtaining the same\nreconstruction quality. Our optimization is also agnostic to other methods that\nacclerate 3DGS, thus enabling even faster speedups compared to vanilla 3DGS."
                },
                "authors": [
                    {
                        "name": "Lukas Höllein"
                    },
                    {
                        "name": "Aljaž Božič"
                    },
                    {
                        "name": "Michael Zollhöfer"
                    },
                    {
                        "name": "Matthias Nießner"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Nießner"
                },
                "author": "Matthias Nießner",
                "arxiv_comment": "Accepted to ICCV 2025. Project page:\n  https://lukashoel.github.io/3DGS-LM, Video:\n  https://www.youtube.com/watch?v=tDiGuGMssg8, Code:\n  https://github.com/lukasHoel/3DGS-LM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.12892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14204v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14204v3",
                "updated": "2025-08-21T11:43:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    11,
                    43,
                    48,
                    3,
                    233,
                    0
                ],
                "published": "2024-04-22T14:13:36Z",
                "published_parsed": [
                    2024,
                    4,
                    22,
                    14,
                    13,
                    36,
                    0,
                    113,
                    0
                ],
                "title": "TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading"
                },
                "summary": "Next-generation mobile networks are expected to facilitate fast AI model\ndownloading to end users. By caching models on edge servers, mobile networks\ncan deliver models to end users with low latency, resulting in a paradigm of\nedge model caching. In this paper, we develop a novel model placement\nframework, called parameter-sharing model caching (TrimCaching). TrimCaching\nexploits the key observation that a wide range of AI models, such as\nconvolutional neural networks or large language models, can share a significant\nproportion of parameter blocks containing reusable knowledge, thereby improving\nstorage efficiency. To this end, we formulate a parameter-sharing model\nplacement problem to maximize the cache hit ratio in multi-edge wireless\nnetworks by balancing the fundamental tradeoff between storage efficiency and\nservice latency. We show that the formulated problem is a submodular\nmaximization problem with submodular constraints, for which no polynomial-time\napproximation algorithm exists. To tackle this challenge, we study an important\nspecial case, where a small fixed number of parameter blocks are shared across\nmodels, which often holds in practice. In such a case, a polynomial-time\nalgorithm with a $\\left(1-\\epsilon\\right)/2$-approximation guarantee is\ndeveloped. Subsequently, we address the original problem for the general case\nby developing a greedy algorithm. Simulation results demonstrate that the\nproposed TrimCaching framework significantly improves the cache hit ratio\ncompared with state-of-the-art content caching without exploiting shared\nparameters in AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation mobile networks are expected to facilitate fast AI model\ndownloading to end users. By caching models on edge servers, mobile networks\ncan deliver models to end users with low latency, resulting in a paradigm of\nedge model caching. In this paper, we develop a novel model placement\nframework, called parameter-sharing model caching (TrimCaching). TrimCaching\nexploits the key observation that a wide range of AI models, such as\nconvolutional neural networks or large language models, can share a significant\nproportion of parameter blocks containing reusable knowledge, thereby improving\nstorage efficiency. To this end, we formulate a parameter-sharing model\nplacement problem to maximize the cache hit ratio in multi-edge wireless\nnetworks by balancing the fundamental tradeoff between storage efficiency and\nservice latency. We show that the formulated problem is a submodular\nmaximization problem with submodular constraints, for which no polynomial-time\napproximation algorithm exists. To tackle this challenge, we study an important\nspecial case, where a small fixed number of parameter blocks are shared across\nmodels, which often holds in practice. In such a case, a polynomial-time\nalgorithm with a $\\left(1-\\epsilon\\right)/2$-approximation guarantee is\ndeveloped. Subsequently, we address the original problem for the general case\nby developing a greedy algorithm. Simulation results demonstrate that the\nproposed TrimCaching framework significantly improves the cache hit ratio\ncompared with state-of-the-art content caching without exploiting shared\nparameters in AI models."
                },
                "authors": [
                    {
                        "name": "Guanqiao Qu"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Jian Li"
                    },
                    {
                        "name": "Fangming Liu"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "arxiv_comment": "18 pages, 13 figures. Part of this work has been accepted by ICDCS\n  2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14204v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14204v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15290v1",
                "updated": "2025-08-21T06:26:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    6,
                    26,
                    18,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T06:26:18Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    6,
                    26,
                    18,
                    3,
                    233,
                    0
                ],
                "title": "Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional\n  Vector Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gorgeous: Revisiting the Data Layout for Disk-Resident High-Dimensional\n  Vector Search"
                },
                "summary": "Similarity-based vector search underpins many important applications, but a\nkey challenge is processing massive vector datasets (e.g., in TBs). To reduce\ncosts, some systems utilize SSDs as the primary data storage. They employ a\nproximity graph, which connects similar vectors to form a graph and is the\nstate-of-the-art index for vector search. However, these systems are hindered\nby sub-optimal data layouts that fail to effectively utilize valuable memory\nspace to reduce disk access and suffer from poor locality for accessing\ndisk-resident data. Through extensive profiling and analysis, we found that the\nstructure of the proximity graph index is accessed more frequently than the\nvectors themselves, yet existing systems do not distinguish between the two. To\naddress this problem, we design the Gorgeous system with the principle of\nprioritizing graph structure over vectors. Specifically, Gorgeous features a\nmemory cache that keeps the adjacency lists of graph nodes to improve cache\nhits and a disk block format that explicitly stores neighbors' adjacency lists\nalong with a vector to enhance data locality. Experimental results show that\nGorgeous consistently outperforms two state-of-the-art disk-based systems for\nvector search, boosting average query throughput by over 60% and reducing query\nlatency by over 35%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity-based vector search underpins many important applications, but a\nkey challenge is processing massive vector datasets (e.g., in TBs). To reduce\ncosts, some systems utilize SSDs as the primary data storage. They employ a\nproximity graph, which connects similar vectors to form a graph and is the\nstate-of-the-art index for vector search. However, these systems are hindered\nby sub-optimal data layouts that fail to effectively utilize valuable memory\nspace to reduce disk access and suffer from poor locality for accessing\ndisk-resident data. Through extensive profiling and analysis, we found that the\nstructure of the proximity graph index is accessed more frequently than the\nvectors themselves, yet existing systems do not distinguish between the two. To\naddress this problem, we design the Gorgeous system with the principle of\nprioritizing graph structure over vectors. Specifically, Gorgeous features a\nmemory cache that keeps the adjacency lists of graph nodes to improve cache\nhits and a disk block format that explicitly stores neighbors' adjacency lists\nalong with a vector to enhance data locality. Experimental results show that\nGorgeous consistently outperforms two state-of-the-art disk-based systems for\nvector search, boosting average query throughput by over 60% and reducing query\nlatency by over 35%."
                },
                "authors": [
                    {
                        "name": "Peiqi Yin"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Qihui Zhou"
                    },
                    {
                        "name": "Hui Li"
                    },
                    {
                        "name": "Xiaolu Li"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Meiling Wang"
                    },
                    {
                        "name": "Xin Yao"
                    },
                    {
                        "name": "James Cheng"
                    }
                ],
                "author_detail": {
                    "name": "James Cheng"
                },
                "author": "James Cheng",
                "arxiv_comment": "12 pages, 19 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15212v1",
                "updated": "2025-08-21T03:48:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "published": "2025-08-21T03:48:28Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    3,
                    48,
                    28,
                    3,
                    233,
                    0
                ],
                "title": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache\n  Channel Pruning"
                },
                "summary": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context inference in large language models (LLMs) is increasingly\nconstrained by the KV cache bottleneck: memory usage grows linearly with\nsequence length, while attention computation scales quadratically. Existing\napproaches address this issue by compressing the KV cache along the temporal\naxis through strategies such as token eviction or merging to reduce memory and\ncomputational overhead. However, these methods often neglect fine-grained\nimportance variations across feature dimensions (i.e., the channel axis),\nthereby limiting their ability to effectively balance efficiency and model\naccuracy. In reality, we observe that channel saliency varies dramatically\nacross both queries and positions: certain feature channels carry near-zero\ninformation for a given query, while others spike in relevance. To address this\noversight, we propose SPARK, a training-free plug-and-play method that applies\nunstructured sparsity by pruning KV at the channel level, while dynamically\nrestoring the pruned entries during attention score computation. Notably, our\napproach is orthogonal to existing KV compression and quantization techniques,\nmaking it compatible for integration with them to achieve further acceleration.\nBy reducing channel-level redundancy, SPARK enables processing of longer\nsequences within the same memory budget. For sequences of equal length, SPARK\nnot only preserves or improves model accuracy but also reduces KV cache storage\nby over 30% compared to eviction-based methods. Furthermore, even with an\naggressive pruning ratio of 80%, SPARK maintains performance with less\ndegradation than 5% compared to the baseline eviction method, demonstrating its\nrobustness and effectiveness. Our code will be available at\nhttps://github.com/Xnhyacinth/SparK."
                },
                "authors": [
                    {
                        "name": "Huanxuan Liao"
                    },
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shizhu He"
                    },
                    {
                        "name": "Guanchen Li"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Emad Barsoum"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15036v1",
                "updated": "2025-08-20T20:02:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    2,
                    35,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T20:02:35Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    20,
                    2,
                    35,
                    2,
                    232,
                    0
                ],
                "title": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in\n  Mixture-of-Experts LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEcho: Exploiting Side-Channel Attacks to Compromise User Privacy in\n  Mixture-of-Experts LLMs"
                },
                "summary": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer architecture has become a cornerstone of modern AI, fueling\nremarkable progress across applications in natural language processing,\ncomputer vision, and multimodal learning. As these models continue to scale\nexplosively for performance, implementation efficiency remains a critical\nchallenge. Mixture of Experts (MoE) architectures, selectively activating\nspecialized subnetworks (experts), offer a unique balance between model\naccuracy and computational cost. However, the adaptive routing in MoE\narchitectures, where input tokens are dynamically directed to specialized\nexperts based on their semantic meaning inadvertently opens up a new attack\nsurface for privacy breaches. These input-dependent activation patterns leave\ndistinctive temporal and spatial traces in hardware execution, which\nadversaries could exploit to deduce sensitive user data. In this work, we\npropose MoEcho, discovering a side channel analysis based attack surface that\ncompromises user privacy on MoE based systems. Specifically, in MoEcho, we\nintroduce four novel architectural side channels on different computing\nplatforms, including Cache Occupancy Channels and Pageout+Reload on CPUs, and\nPerformance Counter and TLB Evict+Reload on GPUs, respectively. Exploiting\nthese vulnerabilities, we propose four attacks that effectively breach user\nprivacy in large language models (LLMs) and vision language models (VLMs) based\non MoE architectures: Prompt Inference Attack, Response Reconstruction Attack,\nVisual Inference Attack, and Visual Reconstruction Attack. MoEcho is the first\nruntime architecture level security analysis of the popular MoE structure\ncommon in modern transformers, highlighting a serious security and privacy\nthreat and calling for effective and timely safeguards when harnessing MoE\nbased models for developing efficient large scale AI services."
                },
                "authors": [
                    {
                        "name": "Ruyi Ding"
                    },
                    {
                        "name": "Tianhong Xu"
                    },
                    {
                        "name": "Xinyi Shen"
                    },
                    {
                        "name": "Aidong Adam Ding"
                    },
                    {
                        "name": "Yunsi Fei"
                    }
                ],
                "author_detail": {
                    "name": "Yunsi Fei"
                },
                "author": "Yunsi Fei",
                "arxiv_comment": "This paper will appear in CCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15033v1",
                "updated": "2025-08-20T19:54:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    54,
                    41,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T19:54:41Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    19,
                    54,
                    41,
                    2,
                    232,
                    0
                ],
                "title": "Rethinking the Potential of Layer Freezing for Efficient DNN Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking the Potential of Layer Freezing for Efficient DNN Training"
                },
                "summary": "With the growing size of deep neural networks and datasets, the computational\ncosts of training have significantly increased. The layer-freezing technique\nhas recently attracted great attention as a promising method to effectively\nreduce the cost of network training. However, in traditional layer-freezing\nmethods, frozen layers are still required for forward propagation to generate\nfeature maps for unfrozen layers, limiting the reduction of computation costs.\nTo overcome this, prior works proposed a hypothetical solution, which caches\nfeature maps from frozen layers as a new dataset, allowing later layers to\ntrain directly on stored feature maps. While this approach appears to be\nstraightforward, it presents several major challenges that are severely\noverlooked by prior literature, such as how to effectively apply augmentations\nto feature maps and the substantial storage overhead introduced. If these\noverlooked challenges are not addressed, the performance of the caching method\nwill be severely impacted and even make it infeasible. This paper is the first\nto comprehensively explore these challenges and provides a systematic solution.\nTo improve training accuracy, we propose \\textit{similarity-aware channel\naugmentation}, which caches channels with high augmentation sensitivity with a\nminimum additional storage cost. To mitigate storage overhead, we incorporate\nlossy data compression into layer freezing and design a \\textit{progressive\ncompression} strategy, which increases compression rates as more layers are\nfrozen, effectively reducing storage costs. Finally, our solution achieves\nsignificant reductions in training cost while maintaining model accuracy, with\na minor time overhead. Additionally, we conduct a comprehensive evaluation of\nfreezing and compression strategies, providing insights into optimizing their\napplication for efficient DNN training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing size of deep neural networks and datasets, the computational\ncosts of training have significantly increased. The layer-freezing technique\nhas recently attracted great attention as a promising method to effectively\nreduce the cost of network training. However, in traditional layer-freezing\nmethods, frozen layers are still required for forward propagation to generate\nfeature maps for unfrozen layers, limiting the reduction of computation costs.\nTo overcome this, prior works proposed a hypothetical solution, which caches\nfeature maps from frozen layers as a new dataset, allowing later layers to\ntrain directly on stored feature maps. While this approach appears to be\nstraightforward, it presents several major challenges that are severely\noverlooked by prior literature, such as how to effectively apply augmentations\nto feature maps and the substantial storage overhead introduced. If these\noverlooked challenges are not addressed, the performance of the caching method\nwill be severely impacted and even make it infeasible. This paper is the first\nto comprehensively explore these challenges and provides a systematic solution.\nTo improve training accuracy, we propose \\textit{similarity-aware channel\naugmentation}, which caches channels with high augmentation sensitivity with a\nminimum additional storage cost. To mitigate storage overhead, we incorporate\nlossy data compression into layer freezing and design a \\textit{progressive\ncompression} strategy, which increases compression rates as more layers are\nfrozen, effectively reducing storage costs. Finally, our solution achieves\nsignificant reductions in training cost while maintaining model accuracy, with\na minor time overhead. Additionally, we conduct a comprehensive evaluation of\nfreezing and compression strategies, providing insights into optimizing their\napplication for efficient DNN training."
                },
                "authors": [
                    {
                        "name": "Chence Yang"
                    },
                    {
                        "name": "Ci Zhang"
                    },
                    {
                        "name": "Lei Lu"
                    },
                    {
                        "name": "Qitao Tan"
                    },
                    {
                        "name": "Sheng Li"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Xulong Tang"
                    },
                    {
                        "name": "Shaoyi Huang"
                    },
                    {
                        "name": "Jinzhen Wang"
                    },
                    {
                        "name": "Guoming Li"
                    },
                    {
                        "name": "Jundong Li"
                    },
                    {
                        "name": "Xiaoming Zhai"
                    },
                    {
                        "name": "Jin Lu"
                    },
                    {
                        "name": "Geng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Geng Yuan"
                },
                "author": "Geng Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19263v1",
                "updated": "2025-08-20T12:46:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    46,
                    50,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T12:46:50Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    12,
                    46,
                    50,
                    2,
                    232,
                    0
                ],
                "title": "Lossless Compression of Neural Network Components: Weights, Checkpoints,\n  and K/V Caches in Low-Precision Formats",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lossless Compression of Neural Network Components: Weights, Checkpoints,\n  and K/V Caches in Low-Precision Formats"
                },
                "summary": "As deep learning models grow and deployment becomes more widespread, reducing\nthe storage and transmission costs of neural network weights has become\nincreasingly important. While prior work such as ZipNN has shown that lossless\ncompression methods - particularly those based on Huffman encoding\nfloating-point exponents can significantly reduce model sizes, these techniques\nhave primarily been applied to higher-precision formats such as FP32 and BF16.\nIn this work, we extend the ZipNN approach to lower-precision floating-point\nformats, specifically FP8 and FP4, which are gaining popularity for efficient\ninference. We design a compression method that separates and compresses the\nexponent and mantissa components independently using entropy coding. Our\nevaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also\ninvestigate the compressibility of key-value (K/V) cache tensors used in large\nlanguage models (LLMs), finding that they, too, exhibit compressible patterns,\nenabling memory savings during deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As deep learning models grow and deployment becomes more widespread, reducing\nthe storage and transmission costs of neural network weights has become\nincreasingly important. While prior work such as ZipNN has shown that lossless\ncompression methods - particularly those based on Huffman encoding\nfloating-point exponents can significantly reduce model sizes, these techniques\nhave primarily been applied to higher-precision formats such as FP32 and BF16.\nIn this work, we extend the ZipNN approach to lower-precision floating-point\nformats, specifically FP8 and FP4, which are gaining popularity for efficient\ninference. We design a compression method that separates and compresses the\nexponent and mantissa components independently using entropy coding. Our\nevaluation shows compression ratios up to 62% for BF16 and 83% for FP8. We also\ninvestigate the compressibility of key-value (K/V) cache tensors used in large\nlanguage models (LLMs), finding that they, too, exhibit compressible patterns,\nenabling memory savings during deployment."
                },
                "authors": [
                    {
                        "name": "Anat Heilper"
                    },
                    {
                        "name": "Doron Singer"
                    }
                ],
                "author_detail": {
                    "name": "Doron Singer"
                },
                "author": "Doron Singer",
                "arxiv_comment": "16 pages 9 images",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14468v1",
                "updated": "2025-08-20T06:48:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    48,
                    54,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T06:48:54Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    6,
                    48,
                    54,
                    2,
                    232,
                    0
                ],
                "title": "Diverse Negative Sampling for Implicit Collaborative Filtering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diverse Negative Sampling for Implicit Collaborative Filtering"
                },
                "summary": "Implicit collaborative filtering recommenders are usually trained to learn\nuser positive preferences. Negative sampling, which selects informative\nnegative items to form negative training data, plays a crucial role in this\nprocess. Since items are often clustered in the latent space, existing negative\nsampling strategies normally oversample negative items from the dense regions.\nThis leads to homogeneous negative data and limited model expressiveness. In\nthis paper, we propose Diverse Negative Sampling (DivNS), a novel approach that\nexplicitly accounts for diversity in negative training data during the negative\nsampling process. DivNS first finds hard negative items with large preference\nscores and constructs user-specific caches that store unused but highly\ninformative negative samples. Then, its diversity-augmented sampler selects a\ndiverse subset of negative items from the cache while ensuring dissimilarity\nfrom the user's hard negatives. Finally, a synthetic negatives generator\ncombines the selected diverse negatives with hard negatives to form more\neffective training data. The resulting synthetic negatives are both informative\nand diverse, enabling recommenders to learn a broader item space and improve\ntheir generalisability. Extensive experiments on four public datasets\ndemonstrate the effectiveness of DivNS in improving recommendation quality\nwhile maintaining computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit collaborative filtering recommenders are usually trained to learn\nuser positive preferences. Negative sampling, which selects informative\nnegative items to form negative training data, plays a crucial role in this\nprocess. Since items are often clustered in the latent space, existing negative\nsampling strategies normally oversample negative items from the dense regions.\nThis leads to homogeneous negative data and limited model expressiveness. In\nthis paper, we propose Diverse Negative Sampling (DivNS), a novel approach that\nexplicitly accounts for diversity in negative training data during the negative\nsampling process. DivNS first finds hard negative items with large preference\nscores and constructs user-specific caches that store unused but highly\ninformative negative samples. Then, its diversity-augmented sampler selects a\ndiverse subset of negative items from the cache while ensuring dissimilarity\nfrom the user's hard negatives. Finally, a synthetic negatives generator\ncombines the selected diverse negatives with hard negatives to form more\neffective training data. The resulting synthetic negatives are both informative\nand diverse, enabling recommenders to learn a broader item space and improve\ntheir generalisability. Extensive experiments on four public datasets\ndemonstrate the effectiveness of DivNS in improving recommendation quality\nwhile maintaining computational efficiency."
                },
                "authors": [
                    {
                        "name": "Yueqing Xuan"
                    },
                    {
                        "name": "Kacper Sokol"
                    },
                    {
                        "name": "Mark Sanderson"
                    },
                    {
                        "name": "Jeffrey Chan"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Chan"
                },
                "author": "Jeffrey Chan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14420v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14420v1",
                "updated": "2025-08-20T04:36:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    36,
                    25,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T04:36:25Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    4,
                    36,
                    25,
                    2,
                    232,
                    0
                ],
                "title": "You Only Evaluate Once: A Tree-based Rerank Method at Meituan",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "You Only Evaluate Once: A Tree-based Rerank Method at Meituan"
                },
                "summary": "Reranking plays a crucial role in modern recommender systems by capturing the\nmutual influences within the list. Due to the inherent challenges of\ncombinatorial search spaces, most methods adopt a two-stage search paradigm: a\nsimple General Search Unit (GSU) efficiently reduces the candidate space, and\nan Exact Search Unit (ESU) effectively selects the optimal sequence. These\nmethods essentially involve making trade-offs between effectiveness and\nefficiency, while suffering from a severe \\textbf{inconsistency problem}, that\nis, the GSU often misses high-value lists from ESU. To address this problem, we\npropose YOLOR, a one-stage reranking method that removes the GSU while\nretaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context\nExtraction Module (TCEM) that hierarchically aggregates multi-scale contextual\nfeatures to achieve \"list-level effectiveness\", and (2) a Context Cache Module\n(CCM) that enables efficient feature reuse across candidate permutations to\nachieve \"permutation-level efficiency\". Extensive experiments across public and\nindustry datasets validate YOLOR's performance, and we have successfully\ndeployed YOLOR on the Meituan food delivery platform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking plays a crucial role in modern recommender systems by capturing the\nmutual influences within the list. Due to the inherent challenges of\ncombinatorial search spaces, most methods adopt a two-stage search paradigm: a\nsimple General Search Unit (GSU) efficiently reduces the candidate space, and\nan Exact Search Unit (ESU) effectively selects the optimal sequence. These\nmethods essentially involve making trade-offs between effectiveness and\nefficiency, while suffering from a severe \\textbf{inconsistency problem}, that\nis, the GSU often misses high-value lists from ESU. To address this problem, we\npropose YOLOR, a one-stage reranking method that removes the GSU while\nretaining only the ESU. Specifically, YOLOR includes: (1) a Tree-based Context\nExtraction Module (TCEM) that hierarchically aggregates multi-scale contextual\nfeatures to achieve \"list-level effectiveness\", and (2) a Context Cache Module\n(CCM) that enables efficient feature reuse across candidate permutations to\nachieve \"permutation-level efficiency\". Extensive experiments across public and\nindustry datasets validate YOLOR's performance, and we have successfully\ndeployed YOLOR on the Meituan food delivery platform."
                },
                "authors": [
                    {
                        "name": "Shuli Wang"
                    },
                    {
                        "name": "Yinqiu Huang"
                    },
                    {
                        "name": "Changhao Li"
                    },
                    {
                        "name": "Yuan Zhou"
                    },
                    {
                        "name": "Yonggang Liu"
                    },
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Yinhua Zhu"
                    },
                    {
                        "name": "Haitao Wang"
                    },
                    {
                        "name": "Xingxing Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xingxing Wang"
                },
                "author": "Xingxing Wang",
                "arxiv_doi": "10.1145/3746252.3761539",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761539",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.14420v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14420v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by CIKM 2025",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16653v1",
                "updated": "2025-08-20T03:42:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    42,
                    37,
                    2,
                    232,
                    0
                ],
                "published": "2025-08-20T03:42:37Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    3,
                    42,
                    37,
                    2,
                    232,
                    0
                ],
                "title": "H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for\n  Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H2EAL: Hybrid-Bonding Architecture with Hybrid Sparse Attention for\n  Efficient Long-Context LLM Inference"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable proficiency in a\nwide range of natural language processing applications. However, the high\nenergy and latency overhead induced by the KV cache limits the edge deployment,\nespecially for long contexts. Emerging hybrid bonding (HB) technology has been\nproposed as a promising alternative to conventional near-memory processing\n(NMP) architectures, offering improved bandwidth efficiency and lower power\nconsumption while exhibiting characteristics of distributed memory. In this\npaper, we propose H2EAL, a hybrid bonding-based accelerator with sparse\nattention algorithm-hardware co-design for efficient LLM inference at the edge.\nAt the algorithm level, we propose a hybrid sparse attention scheme with static\nand dynamic sparsity for different heads to fully leverage the sparsity with\nhigh accuracy. At the hardware level, we co-design the hardware to support\nhybrid sparse attention and propose memory-compute co-placement to address the\ndistributed memory bottleneck. Since different attention heads exhibit\ndifferent sparse patterns and the attention structure often mismatches the HB\narchitecture, we further develop a load-balancing scheduler with parallel tiled\nattention to address workload imbalance and optimize the mapping strategy.\nExtensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and\n6.22~73.48x energy efficiency improvement over baseline HB implementation, with\na negligible average accuracy drop of 0.87% on multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable proficiency in a\nwide range of natural language processing applications. However, the high\nenergy and latency overhead induced by the KV cache limits the edge deployment,\nespecially for long contexts. Emerging hybrid bonding (HB) technology has been\nproposed as a promising alternative to conventional near-memory processing\n(NMP) architectures, offering improved bandwidth efficiency and lower power\nconsumption while exhibiting characteristics of distributed memory. In this\npaper, we propose H2EAL, a hybrid bonding-based accelerator with sparse\nattention algorithm-hardware co-design for efficient LLM inference at the edge.\nAt the algorithm level, we propose a hybrid sparse attention scheme with static\nand dynamic sparsity for different heads to fully leverage the sparsity with\nhigh accuracy. At the hardware level, we co-design the hardware to support\nhybrid sparse attention and propose memory-compute co-placement to address the\ndistributed memory bottleneck. Since different attention heads exhibit\ndifferent sparse patterns and the attention structure often mismatches the HB\narchitecture, we further develop a load-balancing scheduler with parallel tiled\nattention to address workload imbalance and optimize the mapping strategy.\nExtensive experiments demonstrate H2EAL achieves 5.20~48.21x speedup and\n6.22~73.48x energy efficiency improvement over baseline HB implementation, with\na negligible average accuracy drop of 0.87% on multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Zizhuo Fu"
                    },
                    {
                        "name": "Xiaotian Guo"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yadong Zhang"
                    },
                    {
                        "name": "Peiyu Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Le Ye"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "International Conference on Computer-Aided Design (ICCAD) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13935v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13935v1",
                "updated": "2025-08-19T15:26:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    26,
                    36,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:26:36Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    26,
                    36,
                    1,
                    231,
                    0
                ],
                "title": "Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated\n  LSM-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scavenger+: Revisiting Space-Time Tradeoffs in Key-Value Separated\n  LSM-trees"
                },
                "summary": "Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are\nwidely used in storage systems but face significant challenges, such as high\nwrite amplification caused by compaction. KV-separated LSM-trees address write\namplification but introduce significant space amplification, a critical concern\nin cost-sensitive scenarios. Garbage collection (GC) can reduce space\namplification, but existing strategies are often inefficient and fail to\naccount for workload characteristics. Moreover, current key-value (KV)\nseparated LSM-trees overlook the space amplification caused by the index\nLSM-tree. In this paper, we systematically analyze the sources of space\namplification in KV-separated LSM-trees and propose Scavenger+, which achieves\na better performance-space trade-off. Scavenger+ introduces (1) an\nI/O-efficient garbage collection scheme to reduce I/O overhead, (2) a\nspace-aware compaction strategy based on compensated size to mitigate\nindex-induced space amplification, and (3) a dynamic GC scheduler that adapts\nto system load to make better use of CPU and storage resources. Extensive\nexperiments demonstrate that Scavenger+ significantly improves write\nperformance and reduces space amplification compared to state-of-the-art\nKV-separated LSM-trees, including BlobDB, Titan, and TerarkDB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value Stores (KVS) based on log-structured merge-trees (LSM-trees) are\nwidely used in storage systems but face significant challenges, such as high\nwrite amplification caused by compaction. KV-separated LSM-trees address write\namplification but introduce significant space amplification, a critical concern\nin cost-sensitive scenarios. Garbage collection (GC) can reduce space\namplification, but existing strategies are often inefficient and fail to\naccount for workload characteristics. Moreover, current key-value (KV)\nseparated LSM-trees overlook the space amplification caused by the index\nLSM-tree. In this paper, we systematically analyze the sources of space\namplification in KV-separated LSM-trees and propose Scavenger+, which achieves\na better performance-space trade-off. Scavenger+ introduces (1) an\nI/O-efficient garbage collection scheme to reduce I/O overhead, (2) a\nspace-aware compaction strategy based on compensated size to mitigate\nindex-induced space amplification, and (3) a dynamic GC scheduler that adapts\nto system load to make better use of CPU and storage resources. Extensive\nexperiments demonstrate that Scavenger+ significantly improves write\nperformance and reduces space amplification compared to state-of-the-art\nKV-separated LSM-trees, including BlobDB, Titan, and TerarkDB."
                },
                "authors": [
                    {
                        "name": "Jianshun Zhang"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Jiaxin Ou"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Ming Zhao"
                    },
                    {
                        "name": "Sheng Qiu"
                    },
                    {
                        "name": "Junxun Huang"
                    },
                    {
                        "name": "Baoquan Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Dan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Feng"
                },
                "author": "Dan Feng",
                "arxiv_doi": "10.1109/TC.2025.3587513",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TC.2025.3587513",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13935v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13935v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by IEEE Transactions on Computers",
                "arxiv_journal_ref": "Year 2025, pp. 1-14,",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13909v1",
                "updated": "2025-08-19T15:08:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    8,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T15:08:39Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    15,
                    8,
                    39,
                    1,
                    231,
                    0
                ],
                "title": "Scavenger: Better Space-Time Trade-Offs for Key-Value Separated\n  LSM-trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scavenger: Better Space-Time Trade-Offs for Key-Value Separated\n  LSM-trees"
                },
                "summary": "Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)\nhave gained widespread acceptance in storage systems. Nonetheless, a\nsignificant challenge arises in the form of high write amplification due to the\ncompaction process. While KV-separated LSM-trees successfully tackle this\nissue, they also bring about substantial space amplification problems, a\nconcern that cannot be overlooked in cost-sensitive scenarios. Garbage\ncollection (GC) holds significant promise for space amplification reduction,\nyet existing GC strategies often fall short in optimization performance,\nlacking thorough consideration of workload characteristics. Additionally,\ncurrent KV-separated LSM-trees also ignore the adverse effect of the space\namplification in the index LSM-tree. In this paper, we systematically analyze\nthe sources of space amplification of KV-separated LSM-trees and introduce\nScavenger, which achieves a better trade-off between performance and space\namplification. Scavenger initially proposes an I/O-efficient garbage collection\nscheme to reduce I/O overhead and incorporates a space-aware compaction\nstrategy based on compensated size to minimize the space amplification of index\nLSM-trees. Extensive experiments show that Scavenger significantly improves\nwrite performance and achieves lower space amplification than other\nKV-separated LSM-trees (including BlobDB, Titan, and TerarkDB).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value Stores (KVS) implemented with log-structured merge-tree (LSM-tree)\nhave gained widespread acceptance in storage systems. Nonetheless, a\nsignificant challenge arises in the form of high write amplification due to the\ncompaction process. While KV-separated LSM-trees successfully tackle this\nissue, they also bring about substantial space amplification problems, a\nconcern that cannot be overlooked in cost-sensitive scenarios. Garbage\ncollection (GC) holds significant promise for space amplification reduction,\nyet existing GC strategies often fall short in optimization performance,\nlacking thorough consideration of workload characteristics. Additionally,\ncurrent KV-separated LSM-trees also ignore the adverse effect of the space\namplification in the index LSM-tree. In this paper, we systematically analyze\nthe sources of space amplification of KV-separated LSM-trees and introduce\nScavenger, which achieves a better trade-off between performance and space\namplification. Scavenger initially proposes an I/O-efficient garbage collection\nscheme to reduce I/O overhead and incorporates a space-aware compaction\nstrategy based on compensated size to minimize the space amplification of index\nLSM-trees. Extensive experiments show that Scavenger significantly improves\nwrite performance and achieves lower space amplification than other\nKV-separated LSM-trees (including BlobDB, Titan, and TerarkDB)."
                },
                "authors": [
                    {
                        "name": "Jianshun Zhang"
                    },
                    {
                        "name": "Fang Wang"
                    },
                    {
                        "name": "Sheng Qiu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Jiaxin Ou"
                    },
                    {
                        "name": "Junxun Huang"
                    },
                    {
                        "name": "Baoquan Li"
                    },
                    {
                        "name": "Peng Fang"
                    },
                    {
                        "name": "Dan Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dan Feng"
                },
                "author": "Dan Feng",
                "arxiv_doi": "10.1109/ICDE60146.2024.00312",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICDE60146.2024.00312",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, accepted by 2024 IEEE 40st International Conference on Data\n  Engineering (ICDE)",
                "arxiv_journal_ref": "Year: 2024, Pages: 4072-4085",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13863v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13863v1",
                "updated": "2025-08-19T14:30:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:30:41Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    30,
                    41,
                    1,
                    231,
                    0
                ],
                "title": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on\n  Multicore Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tight Inter-Core Cache Contention Analysis for WCET Estimation on\n  Multicore Systems"
                },
                "summary": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WCET (Worst-Case Execution Time) estimation on multicore architecture is\nparticularly challenging mainly due to the complex accesses over cache shared\nby multiple cores. Existing analysis identifies possible contentions between\nparallel tasks by leveraging the partial order of the tasks or their program\nregions. Unfortunately, they overestimate the number of cache misses caused by\na remote block access without considering the actual cache state and the number\nof accesses. This paper reports a new analysis for inter-core cache contention.\nBased on the order of program regions in a task, we first identify memory\nreferences that could be affected if a remote access occurs in a region.\nAfterwards, a fine-grained contention analysis is constructed that computes the\nnumber of cache misses based on the access quantity of local and remote blocks.\nWe demonstrate that the overall inter-core cache interference of a task can be\nobtained via dynamic programming. Experiments show that compared to existing\nmethods, the proposed analysis reduces inter-core cache interference and WCET\nestimations by 52.31% and 8.94% on average, without significantly increasing\ncomputation overhead."
                },
                "authors": [
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Shenlin Cai"
                    },
                    {
                        "name": "Yaowei Liang"
                    },
                    {
                        "name": "Chen Jie"
                    },
                    {
                        "name": "Yinjie Fang"
                    },
                    {
                        "name": "Wei Zhang"
                    },
                    {
                        "name": "Guoquan Zhang"
                    },
                    {
                        "name": "Yaoyao Gu"
                    },
                    {
                        "name": "Xiang Xiao"
                    },
                    {
                        "name": "Wei Qin"
                    },
                    {
                        "name": "Xiangzhen Ouyang"
                    },
                    {
                        "name": "Wanli Chang"
                    }
                ],
                "author_detail": {
                    "name": "Wanli Chang"
                },
                "author": "Wanli Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13863v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13863v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13859v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13859v1",
                "updated": "2025-08-19T14:18:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    18,
                    16,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T14:18:16Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    14,
                    18,
                    16,
                    1,
                    231,
                    0
                ],
                "title": "Zobrist Hash-based Duplicate Detection in Symbolic Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zobrist Hash-based Duplicate Detection in Symbolic Regression"
                },
                "summary": "Symbolic regression encompasses a family of search algorithms that aim to\ndiscover the best fitting function for a set of data without requiring an a\npriori specification of the model structure. The most successful and commonly\nused technique for symbolic regression is Genetic Programming (GP), an\nevolutionary search method that evolves a population of mathematical\nexpressions through the mechanism of natural selection. In this work we analyze\nthe efficiency of the evolutionary search in GP and show that many points in\nthe search space are re-visited and re-evaluated multiple times by the\nalgorithm, leading to wasted computational effort. We address this issue by\nintroducing a caching mechanism based on the Zobrist hash, a type of hashing\nfrequently used in abstract board games for the efficient construction and\nsubsequent update of transposition tables. We implement our caching approach\nusing the open-source framework Operon and demonstrate its performance on a\nselection of real-world regression problems, where we observe up to 34\\%\nspeedups without any detrimental effects on search quality. The hashing\napproach represents a straightforward way to improve runtime performance while\nalso offering some interesting possibilities for adjusting search strategy\nbased on cached information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symbolic regression encompasses a family of search algorithms that aim to\ndiscover the best fitting function for a set of data without requiring an a\npriori specification of the model structure. The most successful and commonly\nused technique for symbolic regression is Genetic Programming (GP), an\nevolutionary search method that evolves a population of mathematical\nexpressions through the mechanism of natural selection. In this work we analyze\nthe efficiency of the evolutionary search in GP and show that many points in\nthe search space are re-visited and re-evaluated multiple times by the\nalgorithm, leading to wasted computational effort. We address this issue by\nintroducing a caching mechanism based on the Zobrist hash, a type of hashing\nfrequently used in abstract board games for the efficient construction and\nsubsequent update of transposition tables. We implement our caching approach\nusing the open-source framework Operon and demonstrate its performance on a\nselection of real-world regression problems, where we observe up to 34\\%\nspeedups without any detrimental effects on search quality. The hashing\napproach represents a straightforward way to improve runtime performance while\nalso offering some interesting possibilities for adjusting search strategy\nbased on cached information."
                },
                "authors": [
                    {
                        "name": "Bogdan Burlacu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Burlacu"
                },
                "author": "Bogdan Burlacu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13859v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13859v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13756v1",
                "updated": "2025-08-19T11:54:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    30,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T11:54:30Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    11,
                    54,
                    30,
                    1,
                    231,
                    0
                ],
                "title": "INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "INDS: Incremental Named Data Streaming for Real-Time Point Cloud Video"
                },
                "summary": "Real-time streaming of point cloud video, characterized by massive data\nvolumes and high sensitivity to packet loss, remains a key challenge for\nimmersive applications under dynamic network conditions. While\nconnection-oriented protocols such as TCP and more modern alternatives like\nQUIC alleviate some transport-layer inefficiencies, including head-of-line\nblocking, they still retain a coarse-grained, segment-based delivery model and\na centralized control loop that limit fine-grained adaptation and effective\ncaching. We introduce INDS (Incremental Named Data Streaming), an adaptive\nstreaming framework based on Information-Centric Networking (ICN) that rethinks\ndelivery for hierarchical, layered media. INDS leverages the Octree structure\nof point cloud video and expressive content naming to support progressive,\npartial retrieval of enhancement layers based on consumer bandwidth and\ndecoding capability. By combining time-windows with Group-of-Frames (GoF),\nINDS's naming scheme supports fine-grained in-network caching and facilitates\nefficient multi-user data reuse. INDS can be deployed as an overlay, remaining\ncompatible with QUIC-based transport infrastructure as well as future\nMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IP\nnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%\nhigher throughput, and 20-30% increased cache hit rates compared to\nstate-of-the-art DASH-style systems. Together, these results establish INDS as\na scalable, cache-friendly solution for real-time point cloud streaming under\nvariable and lossy conditions, while its compatibility with MoQ overlays\nfurther positions it as a practical, forward-compatible architecture for\nemerging immersive media systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming of point cloud video, characterized by massive data\nvolumes and high sensitivity to packet loss, remains a key challenge for\nimmersive applications under dynamic network conditions. While\nconnection-oriented protocols such as TCP and more modern alternatives like\nQUIC alleviate some transport-layer inefficiencies, including head-of-line\nblocking, they still retain a coarse-grained, segment-based delivery model and\na centralized control loop that limit fine-grained adaptation and effective\ncaching. We introduce INDS (Incremental Named Data Streaming), an adaptive\nstreaming framework based on Information-Centric Networking (ICN) that rethinks\ndelivery for hierarchical, layered media. INDS leverages the Octree structure\nof point cloud video and expressive content naming to support progressive,\npartial retrieval of enhancement layers based on consumer bandwidth and\ndecoding capability. By combining time-windows with Group-of-Frames (GoF),\nINDS's naming scheme supports fine-grained in-network caching and facilitates\nefficient multi-user data reuse. INDS can be deployed as an overlay, remaining\ncompatible with QUIC-based transport infrastructure as well as future\nMedia-over-QUIC (MoQ) architectures, without requiring changes to underlying IP\nnetworks. Our prototype implementation shows up to 80% lower delay, 15-50%\nhigher throughput, and 20-30% increased cache hit rates compared to\nstate-of-the-art DASH-style systems. Together, these results establish INDS as\na scalable, cache-friendly solution for real-time point cloud streaming under\nvariable and lossy conditions, while its compatibility with MoQ overlays\nfurther positions it as a practical, forward-compatible architecture for\nemerging immersive media systems."
                },
                "authors": [
                    {
                        "name": "Ruonan Chai"
                    },
                    {
                        "name": "Yixiang Zhu"
                    },
                    {
                        "name": "Xinjiao Li"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Zili Meng"
                    },
                    {
                        "name": "Dirk Kutscher"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Kutscher"
                },
                "author": "Dirk Kutscher",
                "arxiv_comment": "9 pages, 9 figures, 2 tables. To appear in Proc. of the 33rd ACM\n  International Conference on Multimedia (MM '25), October 27--31, 2025,\n  Dublin, Ireland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.1; C.2.4; H.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13716v1",
                "updated": "2025-08-19T10:21:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T10:21:33Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    10,
                    21,
                    33,
                    1,
                    231,
                    0
                ],
                "title": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaPGNN: Optimizing Parallel Graph Neural Network Training with Joint\n  Caching and Resource-Aware Graph Partitioning"
                },
                "summary": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) have shown remarkable capabilities in processing\ngraph-structured data prevalent in various real-world applications. However,\nthe scalability of full-batch GNN training becomes severely limited by high\ncommunication overhead and load imbalance in distributed environments. In this\npaper, we present CaPGNN, a novel framework for efficient parallel full-batch\nGNN training on single-server with multi-GPU, designed specifically to reduce\nredundant inter-GPU communication and balance computational workloads. We\npropose a joint adaptive caching algorithm that leverages both CPU and GPU\nmemory to significantly reduce the repetitive transmission of vertex features\nacross partitions. Additionally, we introduce a resource-aware graph\npartitioning algorithm that adjusts subgraph sizes dynamically according to the\nheterogeneous computational and communication capacities of GPUs. Extensive\nexperiments on large-scale benchmark datasets demonstrate that CaPGNN\neffectively reduces communication costs by up to 96% and accelerates GNN\ntraining by up to 12.7 times compared to state-of-the-art approaches. Our\nresults highlight the potential of adaptive caching and resource-aware\npartitioning to facilitate scalable, efficient, and practical deployment of\nfull-batch GNN training in distributed computing environments."
                },
                "authors": [
                    {
                        "name": "Xianfeng Song"
                    },
                    {
                        "name": "Yi Zou"
                    },
                    {
                        "name": "Zheng Shi"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Shi"
                },
                "author": "Zheng Shi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v3",
                "updated": "2025-08-19T09:13:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    9,
                    13,
                    13,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13523v1",
                "updated": "2025-08-19T05:27:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "published": "2025-08-19T05:27:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures"
                },
                "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on all current US exascale\nmachines -- OLCF Frontier, and ALCF Aurora, and NNSA El Capitan -- for the\nthree potentials."
                },
                "authors": [
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "Evan Weinberg"
                    },
                    {
                        "name": "Christian R. Trott"
                    },
                    {
                        "name": "Megan J. McCarthy"
                    },
                    {
                        "name": "Stan G. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Stan G. Moore"
                },
                "author": "Stan G. Moore",
                "arxiv_comment": "14 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08422v2",
                "updated": "2025-08-19T03:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    3,
                    13,
                    39,
                    1,
                    231,
                    0
                ],
                "published": "2025-07-11T09:07:43Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    9,
                    7,
                    43,
                    4,
                    192,
                    0
                ],
                "title": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upsample What Matters: Region-Adaptive Latent Sampling for Accelerated\n  Diffusion Transformers"
                },
                "summary": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have emerged as an alternative to U-net-based\ndiffusion models for high-fidelity image and video generation, offering\nsuperior scalability. However, their heavy computation remains a major obstacle\nto real-world deployment. Existing acceleration methods primarily exploit the\ntemporal dimension such as reusing cached features across diffusion timesteps.\nHere, we propose Region-Adaptive Latent Upsampling (RALU), a training-free\nframework that accelerates inference along spatial dimension. RALU performs\nmixed-resolution sampling across three stages: 1) low-resolution denoising\nlatent diffusion to efficiently capture global semantic structure, 2)\nregion-adaptive upsampling on specific regions prone to artifacts at\nfull-resolution, and 3) all latent upsampling at full-resolution for detail\nrefinement. To stabilize generations across resolution transitions, we leverage\nnoise-timestep rescheduling to adapt the noise level across varying\nresolutions. Our method significantly reduces computation while preserving\nimage quality by achieving up to 7.0$\\times$ speed-up on FLUX and 3.0$\\times$\non Stable Diffusion 3 with minimal degradation. Furthermore, RALU is\ncomplementary to existing temporal accelerations such as caching methods, thus\ncan be seamlessly integrated to further reduce inference latency without\ncompromising generation quality."
                },
                "authors": [
                    {
                        "name": "Wongi Jeong"
                    },
                    {
                        "name": "Kyungryeol Lee"
                    },
                    {
                        "name": "Hoigi Seo"
                    },
                    {
                        "name": "Se Young Chun"
                    }
                ],
                "author_detail": {
                    "name": "Se Young Chun"
                },
                "author": "Se Young Chun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v2",
                "updated": "2025-08-19T01:38:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    19,
                    1,
                    38,
                    23,
                    1,
                    231,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal Coulomb interactions. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal Coulomb interactions. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13382v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13382v1",
                "updated": "2025-08-18T21:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    21,
                    58,
                    18,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T21:58:18Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    21,
                    58,
                    18,
                    0,
                    230,
                    0
                ],
                "title": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Datarus-R1: An Adaptive Multi-Step Reasoning LLM for Automated Data\n  Analysis"
                },
                "summary": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Datarus-R1-14B, a 14 B-parameter open-weights language model\nfine-tuned from Qwen 2.5-14B-Instruct to act as a virtual data analyst and\ngraduate-level problem solver. Datarus is trained not on isolated\nquestion-answer pairs but on full analytical trajectories including reasoning\nsteps, code execution, error traces, self-corrections, and final conclusions,\nall captured in a ReAct-style notebook format spanning finance, medicine,\nnumerical analysis, and other quantitative domains. Our training pipeline\ncombines (i) a trajectory-centric synthetic data generator that yielded 144 000\ntagged notebook episodes, (ii) a dual-reward framework blending a lightweight\ntag-based structural signal with a Hierarchical Reward Model (HRM) that scores\nboth single-step soundness and end-to-end coherence, and (iii) a\nmemory-optimized implementation of Group Relative Policy Optimization (GRPO)\nfeaturing KV-cache reuse, sequential generation, and reference-model sharding.\nA cosine curriculum smoothly shifts emphasis from structural fidelity to\nsemantic depth, reducing the format collapse and verbosity that often plague\nRL-aligned LLMs. A central design choice in Datarus is it dual reasoning\ninterface. In agentic mode the model produces ReAct-tagged steps that invoke\nPython tools to execute real code; in reflection mode it outputs compact\nChain-of-Thought (CoT) traces delimited by <think> and <answer> tags. On\ndemanding postgraduate-level problems, Datarus exhibits an \"AHA-moment\"\npattern: it sketches hypotheses, revises them once or twice, and converges\navoiding the circular, token-inflating loops common to contemporary systems.\nAcross standard public benchmarks Datarus surpasses similar size models and\neven reaches the level of larger reasoning models such as QwQ-32B achieving up\nto 30% higher accuracy on AIME 2024/2025 and LiveCodeBench while emitting\n18-49% fewer tokens per solution."
                },
                "authors": [
                    {
                        "name": "Ayoub Ben Chaliah"
                    },
                    {
                        "name": "Hela Dellagi"
                    }
                ],
                "author_detail": {
                    "name": "Hela Dellagi"
                },
                "author": "Hela Dellagi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13382v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13382v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.24584v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.24584v3",
                "updated": "2025-08-18T16:52:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    52,
                    22,
                    0,
                    230,
                    0
                ],
                "published": "2025-05-30T13:32:00Z",
                "published_parsed": [
                    2025,
                    5,
                    30,
                    13,
                    32,
                    0,
                    4,
                    150,
                    0
                ],
                "title": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoChemSchematic AI: Agentic Physics-Aware Automation for Chemical\n  Manufacturing Scale-Up"
                },
                "summary": "Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative AI have accelerated the discovery of novel\nchemicals and materials. However, scaling these discoveries to industrial\nproduction remains a major bottleneck due to the synthesis gap -- the need to\ndevelop entirely new manufacturing processes. This challenge requires detailed\nengineering blueprints: PFDs for equipment layouts and material/energy flows,\nand PIDs for process plant operations. Current AI systems cannot yet reliably\ngenerate these critical engineering schematics, creating a fundamental obstacle\nto manufacturing scale-up of novel discoveries. We present a closed-loop,\nphysics-aware framework for automated generation of industrially viable PFDs\nand PIDs. The framework integrates three key components: (1) domain-specialized\nsmall language models (SLMs) trained for auto-generation of PFDs and PIDs, (2)\na hierarchical knowledge graph containing process flow and instrumentation\ndescriptions for 1,020+ chemicals for Graph Retrieval-Augmented Generation\n(GRAG), and (3) an open-source chemical process simulator for modeling,\nsimulation, optimization, and analysis of novel chemical processes. The SLMs\nare trained through a multi-stage pipeline on synthetic datasets, with process\nsimulator-in-the-loop validation ensuring feasibility. To enhance computational\nefficiency, the framework implements structural pruning (width and depth)\nguided by importance heuristics to reduce language model size while preserving\naccuracy, followed by advanced inference optimizations including\nFlashAttention, Lookahead Decoding, PagedAttention with KV-cache quantization,\nand Test-Time Inference Scaling. Experimental results demonstrate that our\nframework generates simulator-validated process descriptions with high\nfidelity."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Shivam Gupta"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.24584v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.24584v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v2",
                "updated": "2025-08-18T16:06:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    16,
                    6,
                    9,
                    0,
                    230,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this paper, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, QwQ-32B, and Qwen3-8B. Our investigation covers weight, KV cache,\nand activation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes are open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "arxiv_comment": "COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12767v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12767v1",
                "updated": "2025-08-18T09:41:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    41,
                    28,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T09:41:28Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    41,
                    28,
                    0,
                    230,
                    0
                ],
                "title": "Some optimization possibilities in data plane programming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Some optimization possibilities in data plane programming"
                },
                "summary": "Software-defined networking (SDN) technology aims to create a highly flexible\nnetwork by decoupling control plane and the data plane and programming them\nindependently. There has been a lot of research on improving and optimizing the\ncontrol plane, and data plane programming is a relatively new concept, so study\non it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar,\nwell-known scientists on computer networking discussed challenges and problems\nin the field of data plane programming that need to be addressed over the next\n10 years. Based on this seminar issues and papers review, we suggested some\npossible solutions which are for optimizing data plane to improve packet\nprocessing performance and link utilization. The suggestions include (i)\nenriching data plane language with asynchronous external function, (ii)\ncompression based on payload size, (iii) in-network caching for fast packet\nprocessing, and (iv) offloading external functions to an additional thread,\nvirtual machine (VM) or server, etc. In addition, we implemented some of these\nin the P4 data plane language to illustrate the practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software-defined networking (SDN) technology aims to create a highly flexible\nnetwork by decoupling control plane and the data plane and programming them\nindependently. There has been a lot of research on improving and optimizing the\ncontrol plane, and data plane programming is a relatively new concept, so study\non it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar,\nwell-known scientists on computer networking discussed challenges and problems\nin the field of data plane programming that need to be addressed over the next\n10 years. Based on this seminar issues and papers review, we suggested some\npossible solutions which are for optimizing data plane to improve packet\nprocessing performance and link utilization. The suggestions include (i)\nenriching data plane language with asynchronous external function, (ii)\ncompression based on payload size, (iii) in-network caching for fast packet\nprocessing, and (iv) offloading external functions to an additional thread,\nvirtual machine (VM) or server, etc. In addition, we implemented some of these\nin the P4 data plane language to illustrate the practicality."
                },
                "authors": [
                    {
                        "name": "Altangerel Gereltsetseg"
                    },
                    {
                        "name": "Tejfel Máté"
                    }
                ],
                "author_detail": {
                    "name": "Tejfel Máté"
                },
                "author": "Tejfel Máté",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12767v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12767v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12743v1",
                "updated": "2025-08-18T09:06:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    6,
                    49,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T09:06:49Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    9,
                    6,
                    49,
                    0,
                    230,
                    0
                ],
                "title": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting CPU-GPU Unified Physical Memory on AMD MI300A APUs"
                },
                "summary": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discrete GPUs are a cornerstone of HPC and data center systems, requiring\nmanagement of separate CPU and GPU memory spaces. Unified Virtual Memory (UVM)\nhas been proposed to ease the burden of memory management; however, at a high\ncost in performance. The recent introduction of AMD's MI300A Accelerated\nProcessing Units (APUs)--as deployed in the El Capitan supercomputer--enables\nHPC systems featuring integrated CPU and GPU with Unified Physical Memory (UPM)\nfor the first time. This work presents the first comprehensive characterization\nof the UPM architecture on MI300A. We first analyze the UPM system properties,\nincluding memory latency, bandwidth, and coherence overhead. We then assess the\nefficiency of the system software in memory allocation, page fault handling,\nTLB management, and Infinity Cache utilization. We propose a set of porting\nstrategies for transforming applications for the UPM architecture and evaluate\nsix applications on the MI300A APU. Our results show that applications on UPM\nusing the unified memory model can match or outperform those in the explicitly\nmanaged model--while reducing memory costs by up to 44%."
                },
                "authors": [
                    {
                        "name": "Jacob Wahlgren"
                    },
                    {
                        "name": "Gabin Schieffer"
                    },
                    {
                        "name": "Ruimin Shi"
                    },
                    {
                        "name": "Edgar A. León"
                    },
                    {
                        "name": "Roger Pearce"
                    },
                    {
                        "name": "Maya Gokhale"
                    },
                    {
                        "name": "Ivy Peng"
                    }
                ],
                "author_detail": {
                    "name": "Ivy Peng"
                },
                "author": "Ivy Peng",
                "arxiv_comment": "To be published in IISWC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12691v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12691v1",
                "updated": "2025-08-18T07:49:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    18,
                    7,
                    49,
                    33,
                    0,
                    230,
                    0
                ],
                "published": "2025-08-18T07:49:33Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    7,
                    49,
                    33,
                    0,
                    230,
                    0
                ],
                "title": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixCache: Mixture-of-Cache for Video Diffusion Transformer Acceleration"
                },
                "summary": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging the Transformer architecture and the diffusion process, video DiT\nmodels have emerged as a dominant approach for high-quality video generation.\nHowever, their multi-step iterative denoising process incurs high computational\ncost and inference latency. Caching, a widely adopted optimization method in\nDiT models, leverages the redundancy in the diffusion process to skip\ncomputations in different granularities (e.g., step, cfg, block). Nevertheless,\nexisting caching methods are limited to single-granularity strategies,\nstruggling to balance generation quality and inference speed in a flexible\nmanner. In this work, we propose MixCache, a training-free caching-based\nframework for efficient video DiT inference. It first distinguishes the\ninterference and boundary between different caching strategies, and then\nintroduces a context-aware cache triggering strategy to determine when caching\nshould be enabled, along with an adaptive hybrid cache decision strategy for\ndynamically selecting the optimal caching granularity. Extensive experiments on\ndiverse models demonstrate that, MixCache can significantly accelerate video\ngeneration (e.g., 1.94$\\times$ speedup on Wan 14B, 1.97$\\times$ speedup on\nHunyuanVideo) while delivering both superior generation quality and inference\nefficiency compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Yuanxin Wei"
                    },
                    {
                        "name": "Lansong Diao"
                    },
                    {
                        "name": "Bujiao Chen"
                    },
                    {
                        "name": "Shenggan Cheng"
                    },
                    {
                        "name": "Zhengping Qian"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Nong Xiao"
                    },
                    {
                        "name": "Wei Lin"
                    },
                    {
                        "name": "Jiangsu Du"
                    }
                ],
                "author_detail": {
                    "name": "Jiangsu Du"
                },
                "author": "Jiangsu Du",
                "arxiv_comment": "7 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12691v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12691v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12485v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12485v1",
                "updated": "2025-08-17T20:01:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    20,
                    1,
                    12,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T20:01:12Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    20,
                    1,
                    12,
                    6,
                    229,
                    0
                ],
                "title": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for\n  NGINX",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for\n  NGINX"
                },
                "summary": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Web proxies such as NGINX commonly rely on least-recently-used (LRU)\neviction, which is size agnostic and can thrash under periodic bursts and mixed\nobject sizes. We introduce Cold-RL, a learned eviction policy for NGINX that\nreplaces LRU's forced-expire path with a dueling Deep Q-Network served by an\nONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL\nsamples the K least-recently-used objects, extracts six lightweight features\n(age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT),\nand requests a bitmask of victims; a hard timeout of 500 microseconds triggers\nimmediate fallback to native LRU. Policies are trained offline by replaying\nNGINX access logs through a cache simulator with a simple reward: a retained\nobject earns one point if it is hit again before TTL expiry. We compare against\nLRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial\nworkloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538,\na 146 percent improvement over the best classical baseline; at 100 MB, from\n0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods\n(about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th\npercentile eviction latency within budget. To our knowledge, this is the first\nreinforcement learning eviction policy integrated into NGINX with strict SLOs."
                },
                "authors": [
                    {
                        "name": "Aayush Gupta"
                    },
                    {
                        "name": "Arpit Bhayani"
                    }
                ],
                "author_detail": {
                    "name": "Arpit Bhayani"
                },
                "author": "Arpit Bhayani",
                "arxiv_comment": "8 pages, 4 figures (system architecture, eviction path, training\n  pipeline, and DQN algorithm), 2 tables. Code available at\n  https://github.com/ayushgupta4897/DRL-Cache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12485v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12485v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4; C.4; D.4.2; I.2.6",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v1",
                "updated": "2025-08-17T19:07:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12407v1",
                "updated": "2025-08-17T15:48:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    48,
                    50,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T15:48:50Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    15,
                    48,
                    50,
                    6,
                    229,
                    0
                ],
                "title": "ZigzagAttention: Efficient Long-Context Inference with Exclusive\n  Retrieval and Streaming Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigzagAttention: Efficient Long-Context Inference with Exclusive\n  Retrieval and Streaming Heads"
                },
                "summary": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of large language models (LLMs), handling long\ncontext has become one of the vital abilities in LLMs. Such long-context\nability is accompanied by difficulties in deployment, especially due to the\nincreased consumption of KV cache. There is certain work aiming to optimize the\nmemory footprint of KV cache, inspired by the observation that attention heads\ncan be categorized into retrieval heads that are of great significance and\nstreaming heads that are of less significance. Typically, identifying the\nstreaming heads and and waiving the KV cache in the streaming heads would\nlargely reduce the overhead without hurting the performance that much. However,\nsince employing both retrieval and streaming heads in one layer decomposes one\nlarge round of attention computation into two small ones, it may unexpectedly\nbring extra latency on accessing and indexing tensors. Based on this intuition,\nwe impose an important improvement to the identification process of retrieval\nand streaming heads, in which we design a criterion that enforces exclusively\nretrieval or streaming heads gathered in one unique layer. In this way, we\nfurther eliminate the extra latency and only incur negligible performance\ndegradation. Our method named \\textsc{ZigzagAttention} is competitive among\nconsidered baselines owing to reduced latency and comparable performance."
                },
                "authors": [
                    {
                        "name": "Zhuorui Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Dawei Song"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Song"
                },
                "author": "Dawei Song",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12357v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12357v1",
                "updated": "2025-08-17T13:05:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    17,
                    13,
                    5,
                    52,
                    6,
                    229,
                    0
                ],
                "published": "2025-08-17T13:05:52Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    13,
                    5,
                    52,
                    6,
                    229,
                    0
                ],
                "title": "Enhancement of the energy storage and electrocaloric effect performances\n  in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancement of the energy storage and electrocaloric effect performances\n  in 0.4 BCZT 0.6 BSTSn medium entropy ceramic prepared by sol gel method"
                },
                "summary": "Based on the traditional polycrystalline ferroelectric\nBa0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6\nBa0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and\nelectrocaloric effect performances is designed and synthesized by the solgel\nmethod. The structural, dielectric, energy storage and electrocaloric effect\nproperties of the prepared sample were studied. The findings demonstrate that\nthe 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic\nsimultaneously has a significant recoverable energy storage density of 255.4\nmJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a\nhigh ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm.\nMoreover, excellent temperature stability of Wrec (less than 10%) was achieved\nin the investigated sample 0.4BCZT 0.6BSTSn.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on the traditional polycrystalline ferroelectric\nBa0.85Ca0.15Zr0.10Ti0.90O3, the 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6\nBa0.9Sr0.1Ti0.9Sn0.1O3 medium entropy material with good energy storage and\nelectrocaloric effect performances is designed and synthesized by the solgel\nmethod. The structural, dielectric, energy storage and electrocaloric effect\nproperties of the prepared sample were studied. The findings demonstrate that\nthe 0.4 Ba0.85Ca0.15Zr0.10Ti0.90O3 0.6 Ba0.9Sr0.1Ti0.9Sn0.1O3 ceramic\nsimultaneously has a significant recoverable energy storage density of 255.4\nmJ/cm3, an efficiency of 67.9%, a large ECE temperature change of 1.36 K, and a\nhigh ECE responsivity of 0.453 K.mm/kV under a low electric field of 30 kV/cm.\nMoreover, excellent temperature stability of Wrec (less than 10%) was achieved\nin the investigated sample 0.4BCZT 0.6BSTSn."
                },
                "authors": [
                    {
                        "name": "S. Khardazi"
                    },
                    {
                        "name": "Z. Gargar"
                    },
                    {
                        "name": "A. Lyubchyk"
                    },
                    {
                        "name": "O. Zakir"
                    },
                    {
                        "name": "D. Mezzane"
                    },
                    {
                        "name": "M. Amjoud"
                    },
                    {
                        "name": "A. Alimoussa"
                    },
                    {
                        "name": "Z. Kutnjak"
                    }
                ],
                "author_detail": {
                    "name": "Z. Kutnjak"
                },
                "author": "Z. Kutnjak",
                "arxiv_doi": "10.1016/j.jssc.2025.125547",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jssc.2025.125547",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.12357v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12357v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14051v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14051v5",
                "updated": "2025-08-16T23:41:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    23,
                    41,
                    48,
                    5,
                    228,
                    0
                ],
                "published": "2025-04-18T19:46:54Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    19,
                    46,
                    54,
                    4,
                    108,
                    0
                ],
                "title": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAOTE: KV Cache Selection for LLMs via Attention Output Error-Based\n  Token Eviction"
                },
                "summary": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While long context support of large language models has extended their\nabilities, it also incurs challenges in memory and compute which becomes\ncrucial bottlenecks in resource-restricted devices. Token eviction, a widely\nadopted post-training methodology designed to alleviate the bottlenecks by\nevicting less important tokens from the cache, typically uses attention scores\nas proxy metrics for token importance. However, one major limitation of\nattention score as a token-wise importance metrics is that it lacks the\ninformation about contribution of tokens to the attention output. In this\npaper, we propose a simple eviction criterion based on the contribution of\ncached tokens to attention outputs. Our method, CAOTE, optimizes for eviction\nerror due to token eviction, by seamlessly integrating attention scores and\nvalue vectors. This is the first method which uses value tokens on top of\nattention-based eviction scores in closed-form. Additionally, CAOTE can act as\na meta-heuristic method with flexible usage with any token eviction method. We\nshow that CAOTE, when combined with the state-of-the-art attention score-based\nmethods, always improves accuracies on the downstream task, indicating the\nimportance of leveraging information from values during token eviction process."
                },
                "authors": [
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Junyoung Park"
                    },
                    {
                        "name": "Mukul Gagrani"
                    },
                    {
                        "name": "Dalton Jones"
                    },
                    {
                        "name": "Matthew Morse"
                    },
                    {
                        "name": "Harper Langston"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Chris Lott"
                    }
                ],
                "author_detail": {
                    "name": "Chris Lott"
                },
                "author": "Chris Lott",
                "arxiv_comment": "15 pages, 3 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14051v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14051v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.10824v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.10824v2",
                "updated": "2025-08-16T03:17:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    16,
                    3,
                    17,
                    35,
                    5,
                    228,
                    0
                ],
                "published": "2025-08-14T16:48:38Z",
                "published_parsed": [
                    2025,
                    8,
                    14,
                    16,
                    48,
                    38,
                    3,
                    226,
                    0
                ],
                "title": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Augmented Transformers: A Systematic Review from Neuroscience\n  Principles to Enhanced Model Architectures"
                },
                "summary": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory is fundamental to intelligence, enabling learning, reasoning, and\nadaptability across biological and artificial systems. While Transformer\narchitectures excel at sequence modeling, they face critical limitations in\nlong-range context retention, continual learning, and knowledge integration.\nThis review presents a unified framework bridging neuroscience principles,\nincluding dynamic multi-timescale memory, selective attention, and\nconsolidation, with engineering advances in Memory-Augmented Transformers. We\norganize recent progress through three taxonomic dimensions: functional\nobjectives (context extension, reasoning, knowledge integration, adaptation),\nmemory representations (parameter-encoded, state-based, explicit, hybrid), and\nintegration mechanisms (attention fusion, gated control, associative\nretrieval). Our analysis of core memory operations (reading, writing,\nforgetting, and capacity management) reveals a shift from static caches toward\nadaptive, test-time learning systems. We identify persistent challenges in\nscalability and interference, alongside emerging solutions including\nhierarchical buffering and surprise-gated updates. This synthesis provides a\nroadmap toward cognitively-inspired, lifelong-learning Transformer\narchitectures."
                },
                "authors": [
                    {
                        "name": "Parsa Omidi"
                    },
                    {
                        "name": "Xingshuai Huang"
                    },
                    {
                        "name": "Axel Laborieux"
                    },
                    {
                        "name": "Bahareh Nikpour"
                    },
                    {
                        "name": "Tianyu Shi"
                    },
                    {
                        "name": "Armaghan Eshaghi"
                    }
                ],
                "author_detail": {
                    "name": "Armaghan Eshaghi"
                },
                "author": "Armaghan Eshaghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.10824v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.10824v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11495v1",
                "updated": "2025-08-15T14:17:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    17,
                    24,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T14:17:24Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    14,
                    17,
                    24,
                    4,
                    227,
                    0
                ],
                "title": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV-Auditor: Auditing Local Differential Privacy for Correlated Key-Value\n  Estimation"
                },
                "summary": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To protect privacy for data-collection-based services, local differential\nprivacy (LDP) is widely adopted due to its rigorous theoretical bound on\nprivacy loss. However, mistakes in complex theoretical analysis or subtle\nimplementation errors may undermine its practical guarantee. To address this,\nauditing is crucial to confirm that LDP protocols truly protect user data.\nHowever, existing auditing methods, though, mainly target machine learning and\nfederated learning tasks based on centralized differentially privacy (DP), with\nlimited attention to LDP. Moreover, the few studies on LDP auditing focus\nsolely on simple frequency estimation task for discrete data, leaving\ncorrelated key-value data - which requires both discrete frequency estimation\nfor keys and continuous mean estimation for values - unexplored.\n  To bridge this gap, we propose KV-Auditor, a framework for auditing LDP-based\nkey-value estimation mechanisms by estimating their empirical privacy lower\nbounds. Rather than traditional LDP auditing methods that relies on binary\noutput predictions, KV-Auditor estimates this lower bound by analyzing\nunbounded output distributions, supporting continuous data. Specifically, we\nclassify state-of-the-art LDP key-value mechanisms into interactive and\nnon-interactive types. For non-interactive mechanisms, we propose horizontal\nKV-Auditor for small domains with sufficient samples and vertical KV-Auditor\nfor large domains with limited samples. For interactive mechanisms, we design a\nsegmentation strategy to capture incremental privacy leakage across iterations.\nFinally, we perform extensive experiments to validate the effectiveness of our\napproach, offering insights for optimizing LDP-based key-value estimators."
                },
                "authors": [
                    {
                        "name": "Jingnan Xu"
                    },
                    {
                        "name": "Leixia Wang"
                    },
                    {
                        "name": "Xiaofeng Meng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Meng"
                },
                "author": "Xiaofeng Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19257v1",
                "updated": "2025-08-15T12:03:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    3,
                    34,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T12:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    3,
                    34,
                    4,
                    227,
                    0
                ],
                "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates."
                },
                "authors": [
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Jiachen Zhang"
                    },
                    {
                        "name": "Chengxuan Li"
                    },
                    {
                        "name": "Zhimu Zhou"
                    },
                    {
                        "name": "Shixin Wu"
                    },
                    {
                        "name": "Songfang Huang"
                    },
                    {
                        "name": "Huiling Duan"
                    }
                ],
                "author_detail": {
                    "name": "Huiling Duan"
                },
                "author": "Huiling Duan",
                "arxiv_comment": "Manuscript submitted to AAAI 2026, currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11291v1",
                "updated": "2025-08-15T07:55:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T07:55:05Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    7,
                    55,
                    5,
                    4,
                    227,
                    0
                ],
                "title": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Quality-Latency Aware Routing for LLM Inference in Wireless\n  Edge-Device Networks"
                },
                "summary": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of wireless communications and Large Language Models (LLMs)\nis poised to unlock ubiquitous intelligent services, yet deploying them in\nwireless edge-device collaborative environments presents a critical trade-off\nbetween inference quality and end-to-end latency. A fundamental mismatch exists\nbetween task complexity and resource allocation: offloading simple queries\ninvites prohibitive latency, while on-device models lack the capacity for\ndemanding computations. To address this challenge, we propose a dynamic,\nquality-latency aware routing framework that orchestrates inference between a\nlightweight model on the mobile device and a powerful model on the edge server.\nOur framework employs two distinct cost models: for single-turn queries, it\nfuses a BERT-predicted semantic score with communication and computation\noverheads; for multi-turn dialogues, it further quantifies context-aware costs\narising from model switching and KV-cache management. While maintaining full\ninference quality, extensive experiments demonstrate that our framework cuts\naverage response latency by 5-15% and reduces large model invocations by 10-20%\nagainst competitive baselines on MMLU, GSM8K, and MT-Bench-101 benchmarks."
                },
                "authors": [
                    {
                        "name": "Rui Bao"
                    },
                    {
                        "name": "Nan Xue"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Zhiyong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Chen"
                },
                "author": "Zhiyong Chen",
                "arxiv_comment": "accepted by IEEE/CIC ICCC workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11260v1",
                "updated": "2025-08-15T06:53:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "published": "2025-08-15T06:53:28Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    6,
                    53,
                    28,
                    4,
                    227,
                    0
                ],
                "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?"
                },
                "summary": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages."
                },
                "authors": [
                    {
                        "name": "Mukund Choudhary"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Gaurja Aeron"
                    },
                    {
                        "name": "Antara Raaghavi Bhattacharya"
                    },
                    {
                        "name": "Dang Khoa Dang Dinh"
                    },
                    {
                        "name": "Ikhlasul Akmal Hanif"
                    },
                    {
                        "name": "Daria Kotova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    },
                    {
                        "name": "Monojit Choudhury"
                    }
                ],
                "author_detail": {
                    "name": "Monojit Choudhury"
                },
                "author": "Monojit Choudhury",
                "arxiv_comment": "Accepted to COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.21824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21824v1",
                "updated": "2025-08-29T17:59:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    59,
                    53,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:59:53Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    59,
                    53,
                    4,
                    241,
                    0
                ],
                "title": "DriveQA: Passing the Driving Knowledge Test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriveQA: Passing the Driving Knowledge Test"
                },
                "summary": "If a Large Language Model (LLM) were to take a driving knowledge test today,\nwould it pass? Beyond standard spatial and visual question-answering (QA) tasks\non current autonomous driving benchmarks, driving knowledge tests require a\ncomplete understanding of all traffic rules, signage, and right-of-way\nprinciples. To pass this test, human drivers must discern various edge cases\nthat rarely appear in real-world datasets. In this work, we present DriveQA, an\nextensive open-source text and vision-based benchmark that exhaustively covers\ntraffic regulations and scenarios. Through our experiments using DriveQA, we\nshow that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on\nbasic traffic rules but exhibit significant weaknesses in numerical reasoning\nand complex right-of-way scenarios, traffic sign variations, and spatial\nlayouts, (2) fine-tuning on DriveQA improves accuracy across multiple\ncategories, particularly in regulatory sign recognition and intersection\ndecision-making, (3) controlled variations in DriveQA-V provide insights into\nmodel sensitivity to environmental factors such as lighting, perspective,\ndistance, and weather conditions, and (4) pretraining on DriveQA enhances\ndownstream driving task performance, leading to improved results on real-world\ndatasets such as nuScenes and BDD, while also demonstrating that models can\ninternalize text and synthetic traffic knowledge to generalize effectively\nacross downstream QA tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If a Large Language Model (LLM) were to take a driving knowledge test today,\nwould it pass? Beyond standard spatial and visual question-answering (QA) tasks\non current autonomous driving benchmarks, driving knowledge tests require a\ncomplete understanding of all traffic rules, signage, and right-of-way\nprinciples. To pass this test, human drivers must discern various edge cases\nthat rarely appear in real-world datasets. In this work, we present DriveQA, an\nextensive open-source text and vision-based benchmark that exhaustively covers\ntraffic regulations and scenarios. Through our experiments using DriveQA, we\nshow that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on\nbasic traffic rules but exhibit significant weaknesses in numerical reasoning\nand complex right-of-way scenarios, traffic sign variations, and spatial\nlayouts, (2) fine-tuning on DriveQA improves accuracy across multiple\ncategories, particularly in regulatory sign recognition and intersection\ndecision-making, (3) controlled variations in DriveQA-V provide insights into\nmodel sensitivity to environmental factors such as lighting, perspective,\ndistance, and weather conditions, and (4) pretraining on DriveQA enhances\ndownstream driving task performance, leading to improved results on real-world\ndatasets such as nuScenes and BDD, while also demonstrating that models can\ninternalize text and synthetic traffic knowledge to generalize effectively\nacross downstream QA tasks."
                },
                "authors": [
                    {
                        "name": "Maolin Wei"
                    },
                    {
                        "name": "Wanzhou Liu"
                    },
                    {
                        "name": "Eshed Ohn-Bar"
                    }
                ],
                "author_detail": {
                    "name": "Eshed Ohn-Bar"
                },
                "author": "Eshed Ohn-Bar",
                "arxiv_comment": "Accepted by ICCV 2025. Project page: https://driveqaiccv.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21034v2",
                "updated": "2025-08-29T17:59:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    59,
                    50,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-27T23:10:00Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    23,
                    10,
                    0,
                    6,
                    117,
                    0
                ],
                "title": "SAGA: A Security Architecture for Governing AI Agentic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGA: A Security Architecture for Governing AI Agentic Systems"
                },
                "summary": "Large Language Model (LLM)-based agents increasingly interact, collaborate,\nand delegate tasks to one another autonomously with minimal human interaction.\nIndustry guidelines for agentic system governance emphasize the need for users\nto maintain comprehensive control over their agents, mitigating potential\ndamage from malicious agents. Several proposed agentic system designs address\nagent identity, authorization, and delegation, but remain purely theoretical,\nwithout concrete implementation and evaluation. Most importantly, they do not\nprovide user-controlled agent management.\n  To address this gap, we propose SAGA, a scalable Security Architecture for\nGoverning Agentic systems, that offers user oversight over their agents'\nlifecycle. In our design, users register their agents with a central entity,\nthe Provider, that maintains agent contact information, user-defined access\ncontrol policies, and helps agents enforce these policies on inter-agent\ncommunication. We introduce a cryptographic mechanism for deriving access\ncontrol tokens, that offers fine-grained control over an agent's interaction\nwith other agents, providing formal security guarantees. We evaluate SAGA on\nseveral agentic tasks, using agents in different geolocations, and multiple\non-device and cloud LLMs, demonstrating minimal performance overhead with no\nimpact on underlying task utility in a wide range of conditions. Our\narchitecture enables secure and trustworthy deployment of autonomous agents,\naccelerating the responsible adoption of this technology in sensitive\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents increasingly interact, collaborate,\nand delegate tasks to one another autonomously with minimal human interaction.\nIndustry guidelines for agentic system governance emphasize the need for users\nto maintain comprehensive control over their agents, mitigating potential\ndamage from malicious agents. Several proposed agentic system designs address\nagent identity, authorization, and delegation, but remain purely theoretical,\nwithout concrete implementation and evaluation. Most importantly, they do not\nprovide user-controlled agent management.\n  To address this gap, we propose SAGA, a scalable Security Architecture for\nGoverning Agentic systems, that offers user oversight over their agents'\nlifecycle. In our design, users register their agents with a central entity,\nthe Provider, that maintains agent contact information, user-defined access\ncontrol policies, and helps agents enforce these policies on inter-agent\ncommunication. We introduce a cryptographic mechanism for deriving access\ncontrol tokens, that offers fine-grained control over an agent's interaction\nwith other agents, providing formal security guarantees. We evaluate SAGA on\nseveral agentic tasks, using agents in different geolocations, and multiple\non-device and cloud LLMs, demonstrating minimal performance overhead with no\nimpact on underlying task utility in a wide range of conditions. Our\narchitecture enables secure and trustworthy deployment of autonomous agents,\naccelerating the responsible adoption of this technology in sensitive\nenvironments."
                },
                "authors": [
                    {
                        "name": "Georgios Syros"
                    },
                    {
                        "name": "Anshuman Suri"
                    },
                    {
                        "name": "Jacob Ginesin"
                    },
                    {
                        "name": "Cristina Nita-Rotaru"
                    },
                    {
                        "name": "Alina Oprea"
                    }
                ],
                "author_detail": {
                    "name": "Alina Oprea"
                },
                "author": "Alina Oprea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00631v2",
                "updated": "2025-08-29T17:58:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    58,
                    46,
                    4,
                    241,
                    0
                ],
                "published": "2024-12-01T01:01:09Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    1,
                    1,
                    9,
                    6,
                    336,
                    0
                ],
                "title": "ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific\n  Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific\n  Instruction Tuning"
                },
                "summary": "Instruction tuning has underscored the significant potential of large\nlanguage models (LLMs) in producing more human controllable and effective\noutputs in various domains. In this work, we focus on the data selection\nproblem for task-specific instruction tuning of LLMs. Prevailing methods\nprimarily rely on the crafted similarity metrics to select training data that\naligns with the test data distribution. The goal is to minimize instruction\ntuning loss on the test data, ultimately improving performance on the target\ntask. However, it has been widely observed that instruction tuning loss (i.e.,\ncross-entropy loss for next token prediction) in LLMs often fails to exhibit a\nmonotonic relationship with actual task performance. This misalignment\nundermines the effectiveness of current data selection methods for\ntask-specific instruction tuning. To address this issue, we introduce ROSE, a\nnovel Reward-Oriented inStruction data sElection method which leverages\npairwise preference loss as a reward signal to optimize data selection for\ntask-specific instruction tuning. Specifically, ROSE adapts an influence\nformulation to approximate the influence of training data points relative to a\nfew-shot preference validation set to select the most task-related training\ndata points. Experimental results show that by selecting just 5\\% of the\ntraining data using ROSE, our approach can achieve competitive results compared\nto fine-tuning with the full training dataset, and it surpasses other\nstate-of-the-art data selection methods for task-specific instruction tuning.\nOur qualitative analysis further confirms the robust generalizability of our\nmethod across multiple benchmark datasets and diverse model architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning has underscored the significant potential of large\nlanguage models (LLMs) in producing more human controllable and effective\noutputs in various domains. In this work, we focus on the data selection\nproblem for task-specific instruction tuning of LLMs. Prevailing methods\nprimarily rely on the crafted similarity metrics to select training data that\naligns with the test data distribution. The goal is to minimize instruction\ntuning loss on the test data, ultimately improving performance on the target\ntask. However, it has been widely observed that instruction tuning loss (i.e.,\ncross-entropy loss for next token prediction) in LLMs often fails to exhibit a\nmonotonic relationship with actual task performance. This misalignment\nundermines the effectiveness of current data selection methods for\ntask-specific instruction tuning. To address this issue, we introduce ROSE, a\nnovel Reward-Oriented inStruction data sElection method which leverages\npairwise preference loss as a reward signal to optimize data selection for\ntask-specific instruction tuning. Specifically, ROSE adapts an influence\nformulation to approximate the influence of training data points relative to a\nfew-shot preference validation set to select the most task-related training\ndata points. Experimental results show that by selecting just 5\\% of the\ntraining data using ROSE, our approach can achieve competitive results compared\nto fine-tuning with the full training dataset, and it surpasses other\nstate-of-the-art data selection methods for task-specific instruction tuning.\nOur qualitative analysis further confirms the robust generalizability of our\nmethod across multiple benchmark datasets and diverse model architectures."
                },
                "authors": [
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Huayi Zhang"
                    },
                    {
                        "name": "Yizheng Jiao"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    },
                    {
                        "name": "Jinhong Yu"
                    },
                    {
                        "name": "Dongyu Zhang"
                    },
                    {
                        "name": "Dezhi Yu"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21818v1",
                "updated": "2025-08-29T17:52:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    52,
                    48,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:52:48Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    52,
                    48,
                    4,
                    241,
                    0
                ],
                "title": "First constraints on the local ionization topology in front of two\n  quasars at z ~ 7.5",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "First constraints on the local ionization topology in front of two\n  quasars at z ~ 7.5"
                },
                "summary": "Thus far, Lyman-$\\alpha$ damping wings towards quasars have been used to\nprobe the \\textit{global} ionization state of the foreground intergalactic\nmedium (IGM). A new parameterization has demonstrated that the damping wing\nsignature also carries \\textit{local} information about the distribution of\nneutral hydrogen (HI) in front of the quasar before it started shining.\nLeveraging a recently introduced Bayesian \\texttt{JAX}-based Hamiltonian Monte\nCarlo (HMC) inference framework, we derive constraints on the\nLorentzian-weighted HI column density $N_\\mathrm{HI}^\\mathrm{DW}$, the quasar's\ndistance $r_\\mathrm{patch}$ to the first neutral patch and its lifetime\n$t_\\mathrm{Q}$ based on JWST/NIRSpec spectra of the two $z \\sim 7.5$ quasars\nJ1007+2115 and J1342+0928. After folding in model-dependent topology\ninformation, we find that J1007+2115 (and J1342+0928) is most likely to reside\nin a $\\langle x_\\mathrm{HI} \\rangle = 0.32_{-0.20}^{+0.22}$\n($0.58_{-0.23}^{+0.23}$) neutral IGM while shining for a remarkably short\nlifetime of $\\log_{10} t_\\mathrm{Q} /\\mathrm{yr} = 4.14_{-0.18}^{+0.74}$ (an\nintermediate lifetime of $5.64_{-0.43}^{+0.25}$) along a sightline with\n$\\log_{10} N_\\mathrm{HI}^\\mathrm{DW} /\\mathrm{cm}^{-2} = 19.70_{-0.86}^{+0.35}$\n($20.24_{-0.22}^{+0.25}$) and $r_\\mathrm{patch} = 28.9_{-14.4}^{+54.0}\n\\,\\mathrm{cMpc}$ ($10.9_{-5.9}^{+5.6} \\,\\mathrm{cMpc}$). In light of the\npotential presence of local absorbers in the foreground of J1342+0928 as has\nbeen recently suggested, we also demonstrate how the Lorentzian-weighted column\ndensity $N_\\mathrm{HI}^\\mathrm{DW}$ provides a natural means for quantifying\ntheir contribution to the observed damping wing signal.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thus far, Lyman-$\\alpha$ damping wings towards quasars have been used to\nprobe the \\textit{global} ionization state of the foreground intergalactic\nmedium (IGM). A new parameterization has demonstrated that the damping wing\nsignature also carries \\textit{local} information about the distribution of\nneutral hydrogen (HI) in front of the quasar before it started shining.\nLeveraging a recently introduced Bayesian \\texttt{JAX}-based Hamiltonian Monte\nCarlo (HMC) inference framework, we derive constraints on the\nLorentzian-weighted HI column density $N_\\mathrm{HI}^\\mathrm{DW}$, the quasar's\ndistance $r_\\mathrm{patch}$ to the first neutral patch and its lifetime\n$t_\\mathrm{Q}$ based on JWST/NIRSpec spectra of the two $z \\sim 7.5$ quasars\nJ1007+2115 and J1342+0928. After folding in model-dependent topology\ninformation, we find that J1007+2115 (and J1342+0928) is most likely to reside\nin a $\\langle x_\\mathrm{HI} \\rangle = 0.32_{-0.20}^{+0.22}$\n($0.58_{-0.23}^{+0.23}$) neutral IGM while shining for a remarkably short\nlifetime of $\\log_{10} t_\\mathrm{Q} /\\mathrm{yr} = 4.14_{-0.18}^{+0.74}$ (an\nintermediate lifetime of $5.64_{-0.43}^{+0.25}$) along a sightline with\n$\\log_{10} N_\\mathrm{HI}^\\mathrm{DW} /\\mathrm{cm}^{-2} = 19.70_{-0.86}^{+0.35}$\n($20.24_{-0.22}^{+0.25}$) and $r_\\mathrm{patch} = 28.9_{-14.4}^{+54.0}\n\\,\\mathrm{cMpc}$ ($10.9_{-5.9}^{+5.6} \\,\\mathrm{cMpc}$). In light of the\npotential presence of local absorbers in the foreground of J1342+0928 as has\nbeen recently suggested, we also demonstrate how the Lorentzian-weighted column\ndensity $N_\\mathrm{HI}^\\mathrm{DW}$ provides a natural means for quantifying\ntheir contribution to the observed damping wing signal."
                },
                "authors": [
                    {
                        "name": "Timo Kist"
                    },
                    {
                        "name": "Joseph F. Hennawi"
                    },
                    {
                        "name": "Frederick B. Davies"
                    },
                    {
                        "name": "Eduardo Bañados"
                    },
                    {
                        "name": "Sarah E. I. Bosman"
                    },
                    {
                        "name": "Zheng Cai"
                    },
                    {
                        "name": "Anna-Christina Eilers"
                    },
                    {
                        "name": "Xiaohui Fan"
                    },
                    {
                        "name": "Zoltán Haiman"
                    },
                    {
                        "name": "Hyunsung D. Jun"
                    },
                    {
                        "name": "Yichen Liu"
                    },
                    {
                        "name": "Jinyi Yang"
                    },
                    {
                        "name": "Feige Wang"
                    }
                ],
                "author_detail": {
                    "name": "Feige Wang"
                },
                "author": "Feige Wang",
                "arxiv_comment": "submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21812v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21812v1",
                "updated": "2025-08-29T17:50:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    50,
                    37,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:50:37Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    50,
                    37,
                    4,
                    241,
                    0
                ],
                "title": "Inferring local quasar IGM damping wing constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring local quasar IGM damping wing constraints"
                },
                "summary": "Lyman-$\\alpha$ damping wings towards quasars are a highly sensitive probe of\nthe neutral hydrogen (HI) content in the foreground intergalactic medium (IGM),\nnot only constraining the global timing of reionization but also the\n\\textit{local} ionization topology near the quasar. Near-optimal extraction of\nthis information is possible with the help of two recently introduced\nreionization model-independent summary statistics of the HI distribution in the\nIGM \\textit{before} the quasar started shining, complemented with the quasar's\nlifetime encoding the effect of its ionizing radiation as a third parameter. We\nintroduce a fully Bayesian JAX-based Hamiltonian Monte Carlo (HMC) inference\nframework that allows us to jointly reconstruct the quasar's unknown continuum\nand constrain these local damping wing statistics. We put forward a\nprobabilistic framework that allows us to tie these local constraints to any\nspecific reionization model and obtain model-dependent constraints on the\nglobal timing of reionization. We demonstrate that we are able to constrain the\n(Lorentzian-weighted) HI column density in front of the quasar to a precision\nof $0.69_{-0.30}^{+0.06}\\,\\mathrm{dex}$ and its original distance to the first\nneutral patch before the quasar started shining to\n$31.4_{-28.1}^{+10.7}\\,\\mathrm{cMpc}$ (if a noticeable damping wing is present\nin the spectrum), extracting hitherto unused local information from the IGM\ndamping wing imprint. Once tied to a specific reionization model, we find that\nthe statistical fidelity of our constraints on the global IGM neutral fraction\nand the lifetime of the quasar improves, while retaining the same precision as\nachieved by pipelines that infer these parameters directly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lyman-$\\alpha$ damping wings towards quasars are a highly sensitive probe of\nthe neutral hydrogen (HI) content in the foreground intergalactic medium (IGM),\nnot only constraining the global timing of reionization but also the\n\\textit{local} ionization topology near the quasar. Near-optimal extraction of\nthis information is possible with the help of two recently introduced\nreionization model-independent summary statistics of the HI distribution in the\nIGM \\textit{before} the quasar started shining, complemented with the quasar's\nlifetime encoding the effect of its ionizing radiation as a third parameter. We\nintroduce a fully Bayesian JAX-based Hamiltonian Monte Carlo (HMC) inference\nframework that allows us to jointly reconstruct the quasar's unknown continuum\nand constrain these local damping wing statistics. We put forward a\nprobabilistic framework that allows us to tie these local constraints to any\nspecific reionization model and obtain model-dependent constraints on the\nglobal timing of reionization. We demonstrate that we are able to constrain the\n(Lorentzian-weighted) HI column density in front of the quasar to a precision\nof $0.69_{-0.30}^{+0.06}\\,\\mathrm{dex}$ and its original distance to the first\nneutral patch before the quasar started shining to\n$31.4_{-28.1}^{+10.7}\\,\\mathrm{cMpc}$ (if a noticeable damping wing is present\nin the spectrum), extracting hitherto unused local information from the IGM\ndamping wing imprint. Once tied to a specific reionization model, we find that\nthe statistical fidelity of our constraints on the global IGM neutral fraction\nand the lifetime of the quasar improves, while retaining the same precision as\nachieved by pipelines that infer these parameters directly."
                },
                "authors": [
                    {
                        "name": "Timo Kist"
                    },
                    {
                        "name": "Joseph F. Hennawi"
                    },
                    {
                        "name": "Frederick B. Davies"
                    }
                ],
                "author_detail": {
                    "name": "Frederick B. Davies"
                },
                "author": "Frederick B. Davies",
                "arxiv_comment": "submitted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21812v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21812v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21810v1",
                "updated": "2025-08-29T17:47:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    47,
                    27,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:47:27Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    47,
                    27,
                    4,
                    241,
                    0
                ],
                "title": "QR-LoRA: QR-Based Low-Rank Adaptation for Efficient Fine-Tuning of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QR-LoRA: QR-Based Low-Rank Adaptation for Efficient Fine-Tuning of Large\n  Language Models"
                },
                "summary": "The growing scale of Large Language Models (LLMs) has necessitated the\ndevelopment of parameter-efficient fine-tuning techniques. Low-Rank Adaptation\n(LoRA) has emerged as a promising approach, reducing the number of trainable\nparameters by applying low-rank updates to pretrained weights. While standard\nLoRA learns both update factors directly, several recent variants first\ninitialize those matrices via an SVD of the pretrained weights -- an operation\nthat can be expensive on large models and yields singular vectors that are not\nalways easy to interpret. In this work, we extract an orthonormal basis from\nthe pretrained weight matrix using QR decomposition with column pivoting, and\nthen express the LoRA update as a linear combination of these basis vectors --\ntraining only the scalar coefficients, which imposes clear structure on\nadaptation and drastically reduces parameter count. Experiments across GLUE\ntasks show that QR-LoRA matches or exceeds the performance of full fine-tuning,\nstandard LoRA, and SVD-LoRA (LoRA with update matrices initialized via singular\nvalue decomposition) with as few as 601 parameters -- a reduction of over 1000x\ncompared to full fine-tuning and 77x fewer than typical LoRA setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing scale of Large Language Models (LLMs) has necessitated the\ndevelopment of parameter-efficient fine-tuning techniques. Low-Rank Adaptation\n(LoRA) has emerged as a promising approach, reducing the number of trainable\nparameters by applying low-rank updates to pretrained weights. While standard\nLoRA learns both update factors directly, several recent variants first\ninitialize those matrices via an SVD of the pretrained weights -- an operation\nthat can be expensive on large models and yields singular vectors that are not\nalways easy to interpret. In this work, we extract an orthonormal basis from\nthe pretrained weight matrix using QR decomposition with column pivoting, and\nthen express the LoRA update as a linear combination of these basis vectors --\ntraining only the scalar coefficients, which imposes clear structure on\nadaptation and drastically reduces parameter count. Experiments across GLUE\ntasks show that QR-LoRA matches or exceeds the performance of full fine-tuning,\nstandard LoRA, and SVD-LoRA (LoRA with update matrices initialized via singular\nvalue decomposition) with as few as 601 parameters -- a reduction of over 1000x\ncompared to full fine-tuning and 77x fewer than typical LoRA setups."
                },
                "authors": [
                    {
                        "name": "Jessica Liang"
                    },
                    {
                        "name": "Anirudh Bharadwaj"
                    }
                ],
                "author_detail": {
                    "name": "Anirudh Bharadwaj"
                },
                "author": "Anirudh Bharadwaj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17202v2",
                "updated": "2025-08-29T17:46:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    46,
                    43,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-24T03:34:40Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    3,
                    34,
                    40,
                    6,
                    236,
                    0
                ],
                "title": "Active Domain Knowledge Acquisition with 100-Dollar Budget: Enhancing\n  LLMs via Cost-Efficient, Expert-Involved Interaction in Sensitive Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Domain Knowledge Acquisition with 100-Dollar Budget: Enhancing\n  LLMs via Cost-Efficient, Expert-Involved Interaction in Sensitive Domains"
                },
                "summary": "Large Language Models (LLMs) have demonstrated an impressive level of general\nknowledge. However, they often struggle in highly specialized and\ncost-sensitive domains such as drug discovery and rare disease research due to\nthe lack of expert knowledge. In this paper, we propose a novel framework\n(PU-ADKA) designed to efficiently enhance domain-specific LLMs by actively\nengaging domain experts within a fixed budget. Unlike traditional fine-tuning\napproaches, PU-ADKA selectively identifies and queries the most appropriate\nexpert from a team, taking into account each expert's availability, knowledge\nboundaries, and consultation costs. We train PU-ADKA using simulations on\nPubMed data and validate it through both controlled expert interactions and\nreal-world deployment with a drug development team, demonstrating its\neffectiveness in enhancing LLM performance in specialized domains under strict\nbudget constraints. In addition to outlining our methodological innovations and\nexperimental results, we introduce a new benchmark dataset, CKAD, for\ncost-effective LLM domain knowledge acquisition to foster further research in\nthis challenging area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated an impressive level of general\nknowledge. However, they often struggle in highly specialized and\ncost-sensitive domains such as drug discovery and rare disease research due to\nthe lack of expert knowledge. In this paper, we propose a novel framework\n(PU-ADKA) designed to efficiently enhance domain-specific LLMs by actively\nengaging domain experts within a fixed budget. Unlike traditional fine-tuning\napproaches, PU-ADKA selectively identifies and queries the most appropriate\nexpert from a team, taking into account each expert's availability, knowledge\nboundaries, and consultation costs. We train PU-ADKA using simulations on\nPubMed data and validate it through both controlled expert interactions and\nreal-world deployment with a drug development team, demonstrating its\neffectiveness in enhancing LLM performance in specialized domains under strict\nbudget constraints. In addition to outlining our methodological innovations and\nexperimental results, we introduce a new benchmark dataset, CKAD, for\ncost-effective LLM domain knowledge acquisition to foster further research in\nthis challenging area."
                },
                "authors": [
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Raha Moraffah"
                    },
                    {
                        "name": "Rujing Yao"
                    },
                    {
                        "name": "Jinhong Yu"
                    },
                    {
                        "name": "Zhimin Tao"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaozhong Liu"
                },
                "author": "Xiaozhong Liu",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21804v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21804v1",
                "updated": "2025-08-29T17:32:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    32,
                    47,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:32:47Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    32,
                    47,
                    4,
                    241,
                    0
                ],
                "title": "Considerations for Estimating Causal Effects of Informatively Timed\n  Treatments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Considerations for Estimating Causal Effects of Informatively Timed\n  Treatments"
                },
                "summary": "Epidemiological studies are often concerned with estimating causal effects of\na sequence of treatment decisions on survival outcomes. In many settings,\ntreatment decisions do not occur at fixed, pre-specified followup times.\nRather, timing varies across subjects in ways that may be informative of\nsubsequent treatment decisions and potential outcomes. Awareness of the issue\nand its potential solutions is lacking in the literature, which motivate this\nwork. Here, we formalize the issue of informative timing, problems associated\nwith ignoring it, and show how g-methods can be used to analyze sequential\ntreatments that are informatively timed. As we describe, in such settings, the\nwaiting times between successive treatment decisions may be properly viewed as\na time-varying confounders. Using synthetic examples, we illustrate how\ng-methods that do not adjust for these waiting times may be biased and how\nadjustment can be done in scenarios where patients may die or be censored in\nbetween treatments. We draw connections between adjustment and identification\nwith discrete-time versus continuous-time models. Finally, we provide\nimplementation guidance and examples using publicly available software. Our\nconcluding message is that 1) considering timing is important for valid\ninference and 2) correcting for informative timing can be done with g-methods\nthat adjust for waiting times between treatments as time-varying confounders.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Epidemiological studies are often concerned with estimating causal effects of\na sequence of treatment decisions on survival outcomes. In many settings,\ntreatment decisions do not occur at fixed, pre-specified followup times.\nRather, timing varies across subjects in ways that may be informative of\nsubsequent treatment decisions and potential outcomes. Awareness of the issue\nand its potential solutions is lacking in the literature, which motivate this\nwork. Here, we formalize the issue of informative timing, problems associated\nwith ignoring it, and show how g-methods can be used to analyze sequential\ntreatments that are informatively timed. As we describe, in such settings, the\nwaiting times between successive treatment decisions may be properly viewed as\na time-varying confounders. Using synthetic examples, we illustrate how\ng-methods that do not adjust for these waiting times may be biased and how\nadjustment can be done in scenarios where patients may die or be censored in\nbetween treatments. We draw connections between adjustment and identification\nwith discrete-time versus continuous-time models. Finally, we provide\nimplementation guidance and examples using publicly available software. Our\nconcluding message is that 1) considering timing is important for valid\ninference and 2) correcting for informative timing can be done with g-methods\nthat adjust for waiting times between treatments as time-varying confounders."
                },
                "authors": [
                    {
                        "name": "Arman Oganisian"
                    }
                ],
                "author_detail": {
                    "name": "Arman Oganisian"
                },
                "author": "Arman Oganisian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21804v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21804v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21803v1",
                "updated": "2025-08-29T17:31:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    31,
                    24,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:31:24Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    31,
                    24,
                    4,
                    241,
                    0
                ],
                "title": "Automated Clinical Problem Detection from SOAP Notes using a\n  Collaborative Multi-Agent LLM Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Clinical Problem Detection from SOAP Notes using a\n  Collaborative Multi-Agent LLM Architecture"
                },
                "summary": "Accurate interpretation of clinical narratives is critical for patient care,\nbut the complexity of these notes makes automation challenging. While Large\nLanguage Models (LLMs) show promise, single-model approaches can lack the\nrobustness required for high-stakes clinical tasks. We introduce a\ncollaborative multi-agent system (MAS) that models a clinical consultation team\nto address this gap. The system is tasked with identifying clinical problems by\nanalyzing only the Subjective (S) and Objective (O) sections of SOAP notes,\nsimulating the diagnostic reasoning process of synthesizing raw data into an\nassessment. A Manager agent orchestrates a dynamically assigned team of\nspecialist agents who engage in a hierarchical, iterative debate to reach a\nconsensus. We evaluated our MAS against a single-agent baseline on a curated\ndataset of 420 MIMIC-III notes. The dynamic multi-agent configuration\ndemonstrated consistently improved performance in identifying congestive heart\nfailure, acute kidney injury, and sepsis. Qualitative analysis of the agent\ndebates reveals that this structure effectively surfaces and weighs conflicting\nevidence, though it can occasionally be susceptible to groupthink. By modeling\na clinical team's reasoning process, our system offers a promising path toward\nmore accurate, robust, and interpretable clinical decision support tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate interpretation of clinical narratives is critical for patient care,\nbut the complexity of these notes makes automation challenging. While Large\nLanguage Models (LLMs) show promise, single-model approaches can lack the\nrobustness required for high-stakes clinical tasks. We introduce a\ncollaborative multi-agent system (MAS) that models a clinical consultation team\nto address this gap. The system is tasked with identifying clinical problems by\nanalyzing only the Subjective (S) and Objective (O) sections of SOAP notes,\nsimulating the diagnostic reasoning process of synthesizing raw data into an\nassessment. A Manager agent orchestrates a dynamically assigned team of\nspecialist agents who engage in a hierarchical, iterative debate to reach a\nconsensus. We evaluated our MAS against a single-agent baseline on a curated\ndataset of 420 MIMIC-III notes. The dynamic multi-agent configuration\ndemonstrated consistently improved performance in identifying congestive heart\nfailure, acute kidney injury, and sepsis. Qualitative analysis of the agent\ndebates reveals that this structure effectively surfaces and weighs conflicting\nevidence, though it can occasionally be susceptible to groupthink. By modeling\na clinical team's reasoning process, our system offers a promising path toward\nmore accurate, robust, and interpretable clinical decision support tools."
                },
                "authors": [
                    {
                        "name": "Yeawon Lee"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Christopher C. Yang"
                    }
                ],
                "author_detail": {
                    "name": "Christopher C. Yang"
                },
                "author": "Christopher C. Yang",
                "arxiv_comment": "Accepted to The 16th ACM Conference on Bioinformatics, Computational\n  Biology, and Health Informatics (ACM-BCB 2025)(Poster Paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21802v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21802v1",
                "updated": "2025-08-29T17:31:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    31,
                    14,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:31:14Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    31,
                    14,
                    4,
                    241,
                    0
                ],
                "title": "An Adaptive Real-Time Forecasting Framework for Cryogenic Fluid\n  Management in Space Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Adaptive Real-Time Forecasting Framework for Cryogenic Fluid\n  Management in Space Systems"
                },
                "summary": "Accurate real-time forecasting of cryogenic tank behavior is essential for\nthe safe and efficient operation of propulsion and storage systems in future\ndeep-space missions. While cryogenic fluid management (CFM) systems\nincreasingly require autonomous capabilities, conventional simulation methods\nremain hindered by high computational cost, model imperfections, and\nsensitivity to unanticipated boundary condition changes. To address these\nlimitations, this study proposes an Adaptive Real-Time Forecasting Framework\nfor Cryogenic Propellant Management in Space Systems, featuring a lightweight,\nnon-intrusive method named ARCTIC (Adaptive Real-time Cryogenic Tank Inference\nand Correction). ARCTIC integrates real-time sensor data with precomputed nodal\nsimulations through a data-driven correction layer that dynamically refines\nforecast accuracy without modifying the underlying model. Two updating\nmechanisms, auto-calibration and observation and correction, enable continuous\nadaptation to evolving system states and transient disturbances. The method is\nfirst assessed through synthetic scenarios representing self-pressurization,\nsloshing, and periodic operations, then validated using experimental data from\nNASA's Multipurpose Hydrogen Test Bed and K-Site facilities. Results\ndemonstrate that ARCTIC significantly improves forecast accuracy under model\nimperfections, data noise, and boundary fluctuations, offering a robust\nreal-time forecasting capability to support autonomous CFM operations. The\nframework's compatibility with existing simulation tools and its low\ncomputational overhead make it especially suited for onboard implementation in\nspace systems requiring predictive autonomy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate real-time forecasting of cryogenic tank behavior is essential for\nthe safe and efficient operation of propulsion and storage systems in future\ndeep-space missions. While cryogenic fluid management (CFM) systems\nincreasingly require autonomous capabilities, conventional simulation methods\nremain hindered by high computational cost, model imperfections, and\nsensitivity to unanticipated boundary condition changes. To address these\nlimitations, this study proposes an Adaptive Real-Time Forecasting Framework\nfor Cryogenic Propellant Management in Space Systems, featuring a lightweight,\nnon-intrusive method named ARCTIC (Adaptive Real-time Cryogenic Tank Inference\nand Correction). ARCTIC integrates real-time sensor data with precomputed nodal\nsimulations through a data-driven correction layer that dynamically refines\nforecast accuracy without modifying the underlying model. Two updating\nmechanisms, auto-calibration and observation and correction, enable continuous\nadaptation to evolving system states and transient disturbances. The method is\nfirst assessed through synthetic scenarios representing self-pressurization,\nsloshing, and periodic operations, then validated using experimental data from\nNASA's Multipurpose Hydrogen Test Bed and K-Site facilities. Results\ndemonstrate that ARCTIC significantly improves forecast accuracy under model\nimperfections, data noise, and boundary fluctuations, offering a robust\nreal-time forecasting capability to support autonomous CFM operations. The\nframework's compatibility with existing simulation tools and its low\ncomputational overhead make it especially suited for onboard implementation in\nspace systems requiring predictive autonomy."
                },
                "authors": [
                    {
                        "name": "Qiyun Cheng"
                    },
                    {
                        "name": "Huihua Yang"
                    },
                    {
                        "name": "Wei Ji"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ji"
                },
                "author": "Wei Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21802v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21802v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21801v1",
                "updated": "2025-08-29T17:28:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    28,
                    7,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:28:07Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    28,
                    7,
                    4,
                    241,
                    0
                ],
                "title": "DMGIN: How Multimodal LLMs Enhance Large Recommendation Models for\n  Lifelong User Post-click Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DMGIN: How Multimodal LLMs Enhance Large Recommendation Models for\n  Lifelong User Post-click Behaviors"
                },
                "summary": "Modeling user interest based on lifelong user behavior sequences is crucial\nfor enhancing Click-Through Rate (CTR) prediction. However, long post-click\nbehavior sequences themselves pose severe performance issues: the sheer volume\nof data leads to high computational costs and inefficiencies in model training\nand inference. Traditional methods address this by introducing two-stage\napproaches, but this compromises model effectiveness due to incomplete\nutilization of the full sequence context. More importantly, integrating\nmultimodal embeddings into existing large recommendation models (LRM) presents\nsignificant challenges: These embeddings often exacerbate computational burdens\nand mismatch with LRM architectures. To address these issues and enhance the\nmodel's efficiency and accuracy, we introduce Deep Multimodal Group Interest\nNetwork (DMGIN). Given the observation that user post-click behavior sequences\ncontain a large number of repeated items with varying behaviors and timestamps,\nDMGIN employs Multimodal LLMs(MLLM) for grouping to reorganize complete\nlifelong post-click behavior sequences more effectively, with almost no\nadditional computational overhead, as opposed to directly introducing\nmultimodal embeddings. To mitigate the potential information loss from\ngrouping, we have implemented two key strategies. First, we analyze behaviors\nwithin each group using both interest statistics and intra-group transformers\nto capture group traits. Second, apply inter-group transformers to temporally\nordered groups to capture the evolution of user group interests. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMGIN. The A/B test in our LBS advertising system shows that\nDMGIN improves CTR by 4.7% and Revenue per Mile by 2.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling user interest based on lifelong user behavior sequences is crucial\nfor enhancing Click-Through Rate (CTR) prediction. However, long post-click\nbehavior sequences themselves pose severe performance issues: the sheer volume\nof data leads to high computational costs and inefficiencies in model training\nand inference. Traditional methods address this by introducing two-stage\napproaches, but this compromises model effectiveness due to incomplete\nutilization of the full sequence context. More importantly, integrating\nmultimodal embeddings into existing large recommendation models (LRM) presents\nsignificant challenges: These embeddings often exacerbate computational burdens\nand mismatch with LRM architectures. To address these issues and enhance the\nmodel's efficiency and accuracy, we introduce Deep Multimodal Group Interest\nNetwork (DMGIN). Given the observation that user post-click behavior sequences\ncontain a large number of repeated items with varying behaviors and timestamps,\nDMGIN employs Multimodal LLMs(MLLM) for grouping to reorganize complete\nlifelong post-click behavior sequences more effectively, with almost no\nadditional computational overhead, as opposed to directly introducing\nmultimodal embeddings. To mitigate the potential information loss from\ngrouping, we have implemented two key strategies. First, we analyze behaviors\nwithin each group using both interest statistics and intra-group transformers\nto capture group traits. Second, apply inter-group transformers to temporally\nordered groups to capture the evolution of user group interests. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMGIN. The A/B test in our LBS advertising system shows that\nDMGIN improves CTR by 4.7% and Revenue per Mile by 2.3%."
                },
                "authors": [
                    {
                        "name": "Zhuoxing Wei"
                    },
                    {
                        "name": "Qingchen Xie"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20489v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20489v3",
                "updated": "2025-08-29T17:23:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    23,
                    50,
                    4,
                    241,
                    0
                ],
                "published": "2025-02-27T19:53:59Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    19,
                    53,
                    59,
                    3,
                    58,
                    0
                ],
                "title": "Do Sell-side Analyst Reports Have Investment Value?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Sell-side Analyst Reports Have Investment Value?"
                },
                "summary": "This paper documents novel investment value in analyst report text. Using 1.2\nmillion reports from 2000-2023, I embed narratives with large language models\n(LLMs) and fit machine learning (ML) forecasts of future long-term returns.\nPortfolios formed on the report narrative forecasts earn sizable and\nsignificant performance that is incremental to analysts' numerical outputs and\nto a broad set of established factors and characteristic-based predictors. The\neffect is stronger after adverse news and is amplified for growth stocks with\naggressive investment. To open the black box, I apply a Shapley decomposition\nthat attributes portfolio performance to distinct topics. Analysts' strategic\noutlook contributes the most to portfolio performance, especially\nforward-looking fundamental assessments. Beyond providing direct evidence that\nanalyst narratives contain value-relevant assessments that diffuse into price\nover time, this study illustrates how interpretable LLM-plus-ML pipelines can\nscale and augment human judgment in investment decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper documents novel investment value in analyst report text. Using 1.2\nmillion reports from 2000-2023, I embed narratives with large language models\n(LLMs) and fit machine learning (ML) forecasts of future long-term returns.\nPortfolios formed on the report narrative forecasts earn sizable and\nsignificant performance that is incremental to analysts' numerical outputs and\nto a broad set of established factors and characteristic-based predictors. The\neffect is stronger after adverse news and is amplified for growth stocks with\naggressive investment. To open the black box, I apply a Shapley decomposition\nthat attributes portfolio performance to distinct topics. Analysts' strategic\noutlook contributes the most to portfolio performance, especially\nforward-looking fundamental assessments. Beyond providing direct evidence that\nanalyst narratives contain value-relevant assessments that diffuse into price\nover time, this study illustrates how interpretable LLM-plus-ML pipelines can\nscale and augment human judgment in investment decisions."
                },
                "authors": [
                    {
                        "name": "Linying Lv"
                    }
                ],
                "author_detail": {
                    "name": "Linying Lv"
                },
                "author": "Linying Lv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20489v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20489v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14488v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14488v4",
                "updated": "2025-08-29T17:17:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    17,
                    40,
                    4,
                    241,
                    0
                ],
                "published": "2024-03-21T15:36:26Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    15,
                    36,
                    26,
                    3,
                    81,
                    0
                ],
                "title": "COBRA-PPM: A Causal Bayesian Reasoning Architecture Using Probabilistic\n  Programming for Robot Manipulation Under Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COBRA-PPM: A Causal Bayesian Reasoning Architecture Using Probabilistic\n  Programming for Robot Manipulation Under Uncertainty"
                },
                "summary": "Manipulation tasks require robots to reason about cause and effect when\ninteracting with objects. Yet, many data-driven approaches lack causal\nsemantics and thus only consider correlations. We introduce COBRA-PPM, a novel\ncausal Bayesian reasoning architecture that combines causal Bayesian networks\nand probabilistic programming to perform interventional inference for robot\nmanipulation under uncertainty. We demonstrate its capabilities through\nhigh-fidelity Gazebo-based experiments on an exemplar block stacking task,\nwhere it predicts manipulation outcomes with high accuracy (Pred Acc: 88.6%)\nand performs greedy next-best action selection with a 94.2% task success rate.\nWe further demonstrate sim2real transfer on a domestic robot, showing\neffectiveness in handling real-world uncertainty from sensor noise and\nstochastic actions. Our generalised and extensible framework supports a wide\nrange of manipulation scenarios and lays a foundation for future work at the\nintersection of robotics and causality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manipulation tasks require robots to reason about cause and effect when\ninteracting with objects. Yet, many data-driven approaches lack causal\nsemantics and thus only consider correlations. We introduce COBRA-PPM, a novel\ncausal Bayesian reasoning architecture that combines causal Bayesian networks\nand probabilistic programming to perform interventional inference for robot\nmanipulation under uncertainty. We demonstrate its capabilities through\nhigh-fidelity Gazebo-based experiments on an exemplar block stacking task,\nwhere it predicts manipulation outcomes with high accuracy (Pred Acc: 88.6%)\nand performs greedy next-best action selection with a 94.2% task success rate.\nWe further demonstrate sim2real transfer on a domestic robot, showing\neffectiveness in handling real-world uncertainty from sensor noise and\nstochastic actions. Our generalised and extensible framework supports a wide\nrange of manipulation scenarios and lays a foundation for future work at the\nintersection of robotics and causality."
                },
                "authors": [
                    {
                        "name": "Ricardo Cannizzaro"
                    },
                    {
                        "name": "Michael Groom"
                    },
                    {
                        "name": "Jonathan Routley"
                    },
                    {
                        "name": "Robert Osazuwa Ness"
                    },
                    {
                        "name": "Lars Kunze"
                    }
                ],
                "author_detail": {
                    "name": "Lars Kunze"
                },
                "author": "Lars Kunze",
                "arxiv_comment": "8 pages, 7 figures, accepted to the 2025 IEEE European Conference on\n  Mobile Robots (ECMR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14488v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14488v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9; I.2.8; I.2.3; G.3; I.2.6; I.6.8; I.2.4; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06748v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06748v2",
                "updated": "2025-08-29T17:08:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    8,
                    59,
                    4,
                    241,
                    0
                ],
                "published": "2024-12-09T18:40:44Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    40,
                    44,
                    0,
                    344,
                    0
                ],
                "title": "Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refusal Tokens: A Simple Way to Calibrate Refusals in Large Language\n  Models"
                },
                "summary": "A key component of building safe and reliable language models is enabling the\nmodels to appropriately refuse to follow certain instructions or answer certain\nquestions. We may want models to output refusal messages for various categories\nof user queries, for example, ill-posed questions, instructions for committing\nillegal acts, or queries which require information past the model's knowledge\nhorizon. Engineering models that refuse to answer such questions is complicated\nby the fact that an individual may want their model to exhibit varying levels\nof sensitivity for refusing queries of various categories, and different users\nmay want different refusal rates. The current default approach involves\ntraining multiple models with varying proportions of refusal messages from each\ncategory to achieve the desired refusal rates, which is computationally\nexpensive and may require training a new model to accommodate each user's\ndesired preference over refusal rates. To address these challenges, we propose\nrefusal tokens, one such token for each refusal category or a single refusal\ntoken, which are prepended to the model's responses during training. We then\nshow how to increase or decrease the probability of generating the refusal\ntoken for each category during inference to steer the model's refusal behavior.\nRefusal tokens enable controlling a single model's refusal rates without the\nneed of any further fine-tuning, but only by selectively intervening during\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key component of building safe and reliable language models is enabling the\nmodels to appropriately refuse to follow certain instructions or answer certain\nquestions. We may want models to output refusal messages for various categories\nof user queries, for example, ill-posed questions, instructions for committing\nillegal acts, or queries which require information past the model's knowledge\nhorizon. Engineering models that refuse to answer such questions is complicated\nby the fact that an individual may want their model to exhibit varying levels\nof sensitivity for refusing queries of various categories, and different users\nmay want different refusal rates. The current default approach involves\ntraining multiple models with varying proportions of refusal messages from each\ncategory to achieve the desired refusal rates, which is computationally\nexpensive and may require training a new model to accommodate each user's\ndesired preference over refusal rates. To address these challenges, we propose\nrefusal tokens, one such token for each refusal category or a single refusal\ntoken, which are prepended to the model's responses during training. We then\nshow how to increase or decrease the probability of generating the refusal\ntoken for each category during inference to steer the model's refusal behavior.\nRefusal tokens enable controlling a single model's refusal rates without the\nneed of any further fine-tuning, but only by selectively intervening during\ngeneration."
                },
                "authors": [
                    {
                        "name": "Neel Jain"
                    },
                    {
                        "name": "Aditya Shrivastava"
                    },
                    {
                        "name": "Chenyang Zhu"
                    },
                    {
                        "name": "Daben Liu"
                    },
                    {
                        "name": "Alfy Samuel"
                    },
                    {
                        "name": "Ashwinee Panda"
                    },
                    {
                        "name": "Anoop Kumar"
                    },
                    {
                        "name": "Micah Goldblum"
                    },
                    {
                        "name": "Tom Goldstein"
                    }
                ],
                "author_detail": {
                    "name": "Tom Goldstein"
                },
                "author": "Tom Goldstein",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06748v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06748v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21788v1",
                "updated": "2025-08-29T17:04:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    4,
                    20,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:04:20Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    4,
                    20,
                    4,
                    241,
                    0
                ],
                "title": "Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing\n  Fine Web for Problematic Content Search and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing\n  Fine Web for Problematic Content Search and Retrieval"
                },
                "summary": "Large language models (LLMs) rely heavily on web-scale datasets like Common\nCrawl, which provides over 80\\% of training data for some modern models.\nHowever, the indiscriminate nature of web crawling raises challenges in data\nquality, safety, and ethics. Despite the critical importance of training data\nquality, prior research on harmful content has been limited to small samples\ndue to computational constraints. This project presents a framework for\nindexing and analyzing LLM training datasets using an ElasticSearch-based\npipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),\nachieving fast query performance--most searches in milliseconds, all under 2\nseconds. Our work demonstrates real-time dataset analysis, offering practical\ntools for safer, more accountable AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely heavily on web-scale datasets like Common\nCrawl, which provides over 80\\% of training data for some modern models.\nHowever, the indiscriminate nature of web crawling raises challenges in data\nquality, safety, and ethics. Despite the critical importance of training data\nquality, prior research on harmful content has been limited to small samples\ndue to computational constraints. This project presents a framework for\nindexing and analyzing LLM training datasets using an ElasticSearch-based\npipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),\nachieving fast query performance--most searches in milliseconds, all under 2\nseconds. Our work demonstrates real-time dataset analysis, offering practical\ntools for safer, more accountable AI systems."
                },
                "authors": [
                    {
                        "name": "Inés Altemir Marinas"
                    },
                    {
                        "name": "Anastasiia Kucherenko"
                    },
                    {
                        "name": "Andrei Kucharavy"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Kucharavy"
                },
                "author": "Andrei Kucharavy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21787v1",
                "updated": "2025-08-29T17:03:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    3,
                    47,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:03:47Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    3,
                    47,
                    4,
                    241,
                    0
                ],
                "title": "PiCSAR: Probabilistic Confidence Selection And Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiCSAR: Probabilistic Confidence Selection And Ranking"
                },
                "summary": "Best-of-n sampling improves the accuracy of large language models (LLMs) and\nlarge reasoning models (LRMs) by generating multiple candidate solutions and\nselecting the one with the highest reward. The key challenge for reasoning\ntasks is designing a scoring function that can identify correct reasoning\nchains without access to ground-truth answers. We propose Probabilistic\nConfidence Selection And Ranking (PiCSAR): a simple, training-free method that\nscores each candidate generation using the joint log-likelihood of the\nreasoning and final answer. The joint log-likelihood of the reasoning and final\nanswer naturally decomposes into reasoning confidence and answer confidence.\nPiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,\n+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in\n16 out of 20 comparisons. Our analysis reveals that correct reasoning chains\nexhibit significantly higher reasoning and answer confidence, justifying the\neffectiveness of PiCSAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best-of-n sampling improves the accuracy of large language models (LLMs) and\nlarge reasoning models (LRMs) by generating multiple candidate solutions and\nselecting the one with the highest reward. The key challenge for reasoning\ntasks is designing a scoring function that can identify correct reasoning\nchains without access to ground-truth answers. We propose Probabilistic\nConfidence Selection And Ranking (PiCSAR): a simple, training-free method that\nscores each candidate generation using the joint log-likelihood of the\nreasoning and final answer. The joint log-likelihood of the reasoning and final\nanswer naturally decomposes into reasoning confidence and answer confidence.\nPiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,\n+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in\n16 out of 20 comparisons. Our analysis reveals that correct reasoning chains\nexhibit significantly higher reasoning and answer confidence, justifying the\neffectiveness of PiCSAR."
                },
                "authors": [
                    {
                        "name": "Joshua Ong Jun Leang"
                    },
                    {
                        "name": "Zheng Zhao"
                    },
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Sohee Yang"
                    },
                    {
                        "name": "Wai-Chung Kwan"
                    },
                    {
                        "name": "Xuanli He"
                    },
                    {
                        "name": "Wenda Li"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Eleonora Giunchiglia"
                    },
                    {
                        "name": "Shay B. Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Shay B. Cohen"
                },
                "author": "Shay B. Cohen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21777v1",
                "updated": "2025-08-29T16:55:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    55,
                    25,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T16:55:25Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    55,
                    25,
                    4,
                    241,
                    0
                ],
                "title": "Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but\n  Persistent Need for Expert Oversight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but\n  Persistent Need for Expert Oversight"
                },
                "summary": "Introduction: Large language models (LLM) have shown great potential in\nclinical decision support. GPT-5 is a novel LLM system that has been\nspecifically marketed towards oncology use.\n  Methods: Performance was assessed using two complementary benchmarks: (i) the\nACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300\nmultiple-choice items, and (ii) a curated set of 60 authentic radiation\noncologic vignettes representing diverse disease sites and treatment\nindications. For the vignette evaluation, GPT-5 was instructed to generate\nconcise therapeutic plans. Four board-certified radiation oncologists rated\ncorrectness, comprehensiveness, and hallucinations. Inter-rater reliability was\nquantified using Fleiss' \\k{appa}.\n  Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%,\noutperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were\nmost pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5's\ntreatment recommendations were rated highly for correctness (mean 3.24/4, 95%\nCI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69).\nHallucinations were rare with no case reaching majority consensus for their\npresence. Inter-rater agreement was low (Fleiss' \\k{appa} 0.083 for\ncorrectness), reflecting inherent variability in clinical judgment. Errors\nclustered in complex scenarios requiring precise trial knowledge or detailed\nclinical adaptation.\n  Discussion: GPT-5 clearly outperformed prior model variants on the radiation\noncology multiple-choice benchmark. Although GPT-5 exhibited favorable\nperformance in generating real-world radiation oncology treatment\nrecommendations, correctness ratings indicate room for further improvement.\nWhile hallucinations were infrequent, the presence of substantive errors\nunderscores that GPT-5-generated recommendations require rigorous expert\noversight before clinical implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: Large language models (LLM) have shown great potential in\nclinical decision support. GPT-5 is a novel LLM system that has been\nspecifically marketed towards oncology use.\n  Methods: Performance was assessed using two complementary benchmarks: (i) the\nACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300\nmultiple-choice items, and (ii) a curated set of 60 authentic radiation\noncologic vignettes representing diverse disease sites and treatment\nindications. For the vignette evaluation, GPT-5 was instructed to generate\nconcise therapeutic plans. Four board-certified radiation oncologists rated\ncorrectness, comprehensiveness, and hallucinations. Inter-rater reliability was\nquantified using Fleiss' \\k{appa}.\n  Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%,\noutperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were\nmost pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5's\ntreatment recommendations were rated highly for correctness (mean 3.24/4, 95%\nCI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69).\nHallucinations were rare with no case reaching majority consensus for their\npresence. Inter-rater agreement was low (Fleiss' \\k{appa} 0.083 for\ncorrectness), reflecting inherent variability in clinical judgment. Errors\nclustered in complex scenarios requiring precise trial knowledge or detailed\nclinical adaptation.\n  Discussion: GPT-5 clearly outperformed prior model variants on the radiation\noncology multiple-choice benchmark. Although GPT-5 exhibited favorable\nperformance in generating real-world radiation oncology treatment\nrecommendations, correctness ratings indicate room for further improvement.\nWhile hallucinations were infrequent, the presence of substantive errors\nunderscores that GPT-5-generated recommendations require rigorous expert\noversight before clinical implementation."
                },
                "authors": [
                    {
                        "name": "Ugur Dinc"
                    },
                    {
                        "name": "Jibak Sarkar"
                    },
                    {
                        "name": "Philipp Schubert"
                    },
                    {
                        "name": "Sabine Semrau"
                    },
                    {
                        "name": "Thomas Weissmann"
                    },
                    {
                        "name": "Andre Karius"
                    },
                    {
                        "name": "Johann Brand"
                    },
                    {
                        "name": "Bernd-Niklas Axer"
                    },
                    {
                        "name": "Ahmed Gomaa"
                    },
                    {
                        "name": "Pluvio Stephan"
                    },
                    {
                        "name": "Ishita Sheth"
                    },
                    {
                        "name": "Sogand Beirami"
                    },
                    {
                        "name": "Annette Schwarz"
                    },
                    {
                        "name": "Udo Gaipl"
                    },
                    {
                        "name": "Benjamin Frey"
                    },
                    {
                        "name": "Christoph Bert"
                    },
                    {
                        "name": "Stefanie Corradini"
                    },
                    {
                        "name": "Rainer Fietkau"
                    },
                    {
                        "name": "Florian Putz"
                    }
                ],
                "author_detail": {
                    "name": "Florian Putz"
                },
                "author": "Florian Putz",
                "arxiv_comment": "Under review in Frontiers in Artificial Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13073v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13073v5",
                "updated": "2025-08-29T16:48:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    48,
                    23,
                    4,
                    241,
                    0
                ],
                "published": "2025-01-22T18:35:57Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    18,
                    35,
                    57,
                    2,
                    22,
                    0
                ],
                "title": "CHaRM: Conditioned Heatmap Regression Methodology for Accurate and Fast\n  Dental Landmark Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CHaRM: Conditioned Heatmap Regression Methodology for Accurate and Fast\n  Dental Landmark Localization"
                },
                "summary": "Identifying anatomical landmarks in 3D dental models is essential for\northodontic treatment, yet manual placement is labor-intensive and requires\nexpert knowledge. While machine learning methods have been proposed for\nautomatic landmark detection in 3D Intraoral Scans (IOS), none provide a fully\nend-to-end solution that avoids costly tooth segmentation.\n  We present CHaRM (Conditioned Heatmap Regression Methodology), the first\nfully end-to-end deep learning approach for tooth landmark detection in 3D IOS.\nCHaRM integrates four components: a point cloud encoder, a decoder with a\nheatmap regression head, a teeth-presence classification head, and the novel\nCHaR module. The CHaR module leverages teeth-presence information to adapt to\nmissing teeth, improving detection accuracy in complex dental cases. Unlike\ntwo-stage workflows that segment teeth before landmarking, CHaRM operates\ndirectly on IOS point clouds, reducing complexity, avoiding error propagation,\nand lowering computational cost.\n  We evaluated CHaRM with five point cloud learning backbones on\nIOSLandmarks-1k, a new dataset of 1,214 annotated 3D dental models. Both the\ndataset and code will be publicly released to address the scarcity of open data\nin orthodontics and foster reproducible research.\n  CHaRM with PointMLP, named CHaRNet, achieved the best accuracy and\nefficiency. Compared to state-of-the-art methods (TSMDL and ALIIOS), CHaRNet\nreduced mean Euclidean distance error to 0.56 mm on standard dental models and\n1.12 mm across all dentition type, while delivering up to 14.8x faster\ninference on GPU. This end-to-end approach streamlines orthodontic workflows,\nenhances the precision of 3D IOS analysis, and enables efficient\ncomputer-assisted treatment planning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying anatomical landmarks in 3D dental models is essential for\northodontic treatment, yet manual placement is labor-intensive and requires\nexpert knowledge. While machine learning methods have been proposed for\nautomatic landmark detection in 3D Intraoral Scans (IOS), none provide a fully\nend-to-end solution that avoids costly tooth segmentation.\n  We present CHaRM (Conditioned Heatmap Regression Methodology), the first\nfully end-to-end deep learning approach for tooth landmark detection in 3D IOS.\nCHaRM integrates four components: a point cloud encoder, a decoder with a\nheatmap regression head, a teeth-presence classification head, and the novel\nCHaR module. The CHaR module leverages teeth-presence information to adapt to\nmissing teeth, improving detection accuracy in complex dental cases. Unlike\ntwo-stage workflows that segment teeth before landmarking, CHaRM operates\ndirectly on IOS point clouds, reducing complexity, avoiding error propagation,\nand lowering computational cost.\n  We evaluated CHaRM with five point cloud learning backbones on\nIOSLandmarks-1k, a new dataset of 1,214 annotated 3D dental models. Both the\ndataset and code will be publicly released to address the scarcity of open data\nin orthodontics and foster reproducible research.\n  CHaRM with PointMLP, named CHaRNet, achieved the best accuracy and\nefficiency. Compared to state-of-the-art methods (TSMDL and ALIIOS), CHaRNet\nreduced mean Euclidean distance error to 0.56 mm on standard dental models and\n1.12 mm across all dentition type, while delivering up to 14.8x faster\ninference on GPU. This end-to-end approach streamlines orthodontic workflows,\nenhances the precision of 3D IOS analysis, and enables efficient\ncomputer-assisted treatment planning."
                },
                "authors": [
                    {
                        "name": "José Rodríguez-Ortega"
                    },
                    {
                        "name": "Francisco Pérez-Hernández"
                    },
                    {
                        "name": "Siham Tabik"
                    }
                ],
                "author_detail": {
                    "name": "Siham Tabik"
                },
                "arxiv_affiliation": "Department of Computer Science and Artificial Intelligence, University of Granada, Granada, Spain",
                "author": "Siham Tabik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13073v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13073v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05163v2",
                "updated": "2025-08-29T16:38:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    38,
                    16,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-07T15:08:03Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    8,
                    3,
                    0,
                    97,
                    0
                ],
                "title": "Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods\n  under Knowledge Incompleteness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods\n  under Knowledge Incompleteness"
                },
                "summary": "Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) is a technique\nthat enhances Large Language Model (LLM) inference in tasks like Question\nAnswering (QA) by retrieving relevant information from knowledge graphs (KGs).\nHowever, real-world KGs are often incomplete, meaning that essential\ninformation for answering questions may be missing. Existing benchmarks do not\nadequately capture the impact of KG incompleteness on KG-RAG performance. In\nthis paper, we systematically evaluate KG-RAG methods under incomplete KGs by\nremoving triples using different methods and analyzing the resulting effects.\nWe demonstrate that KG-RAG methods are sensitive to KG incompleteness,\nhighlighting the need for more robust approaches in realistic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) is a technique\nthat enhances Large Language Model (LLM) inference in tasks like Question\nAnswering (QA) by retrieving relevant information from knowledge graphs (KGs).\nHowever, real-world KGs are often incomplete, meaning that essential\ninformation for answering questions may be missing. Existing benchmarks do not\nadequately capture the impact of KG incompleteness on KG-RAG performance. In\nthis paper, we systematically evaluate KG-RAG methods under incomplete KGs by\nremoving triples using different methods and analyzing the resulting effects.\nWe demonstrate that KG-RAG methods are sensitive to KG incompleteness,\nhighlighting the need for more robust approaches in realistic settings."
                },
                "authors": [
                    {
                        "name": "Dongzhuoran Zhou"
                    },
                    {
                        "name": "Yuqicheng Zhu"
                    },
                    {
                        "name": "Xiaxia Wang"
                    },
                    {
                        "name": "Yuan He"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Steffen Staab"
                    },
                    {
                        "name": "Evgeny Kharlamov"
                    }
                ],
                "author_detail": {
                    "name": "Evgeny Kharlamov"
                },
                "author": "Evgeny Kharlamov",
                "arxiv_comment": "IRISAI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21762v1",
                "updated": "2025-08-29T16:37:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    37,
                    42,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T16:37:42Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    37,
                    42,
                    4,
                    241,
                    0
                ],
                "title": "Reasoning-Intensive Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-Intensive Regression"
                },
                "summary": "AI researchers and practitioners increasingly apply large language models\n(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing\nsubtle numerical properties from text. Unlike standard language regression\ntasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc\nproblems like rubric-based scoring or domain-specific retrieval, where much\ndeeper analysis of text is required while only limited task-specific training\ndata and computation are available. We cast three realistic problems as RiR\ntasks to establish an initial benchmark, and use that to test our hypothesis\nthat prompting frozen LLMs and finetuning Transformer encoders via gradient\ndescent will both often struggle in RiR. We then propose MENTAT, a simple and\nlightweight method that combines batch-reflective prompt optimization with\nneural ensemble learning. MENTAT achieves up to 65% improvement over both\nbaselines, though substantial room remains for future advances in RiR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI researchers and practitioners increasingly apply large language models\n(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing\nsubtle numerical properties from text. Unlike standard language regression\ntasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc\nproblems like rubric-based scoring or domain-specific retrieval, where much\ndeeper analysis of text is required while only limited task-specific training\ndata and computation are available. We cast three realistic problems as RiR\ntasks to establish an initial benchmark, and use that to test our hypothesis\nthat prompting frozen LLMs and finetuning Transformer encoders via gradient\ndescent will both often struggle in RiR. We then propose MENTAT, a simple and\nlightweight method that combines batch-reflective prompt optimization with\nneural ensemble learning. MENTAT achieves up to 65% improvement over both\nbaselines, though substantial room remains for future advances in RiR."
                },
                "authors": [
                    {
                        "name": "Diane Tchuindjo"
                    },
                    {
                        "name": "Omar Khattab"
                    }
                ],
                "author_detail": {
                    "name": "Omar Khattab"
                },
                "author": "Omar Khattab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21755v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21755v1",
                "updated": "2025-08-29T16:30:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    30,
                    16,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T16:30:16Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    30,
                    16,
                    4,
                    241,
                    0
                ],
                "title": "Analysis of Semantic Communication for Logic-based Hypothesis Deduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis of Semantic Communication for Logic-based Hypothesis Deduction"
                },
                "summary": "This work presents an analysis of semantic communication in the context of\nFirst-Order Logic (FOL)-based deduction. Specifically, the receiver holds a set\nof hypotheses about the State of the World (SotW), while the transmitter has\nincomplete evidence about the true SotW but lacks access to the ground truth.\nThe transmitter aims to communicate limited information to help the receiver\nidentify the hypothesis most consistent with true SotW. We formulate the\nobjective as approximating the posterior distribution at the transmitter to the\nreceiver. Using Stirling's approximation, this reduces to a constrained,\nfinite-horizon resource allocation problem. Applying the Karush-Kuhn-Tucker\nconditions yields a truncated water-filling solution. Despite the problem's\nnon-convexity, symmetry and permutation invariance ensure global optimality.\nBased on this, we design message selection strategies, both for single- and\nmulti-round communication, and model the receiver's inference as an m-ary\nBayesian hypothesis testing problem. Under the Maximum A Posteriori (MAP) rule,\nour communication strategy achieves optimal performance within budget\nconstraints. We further analyze convergence rates and validate the theoretical\nfindings through experiments, demonstrating reduced error over random selection\nand prior methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents an analysis of semantic communication in the context of\nFirst-Order Logic (FOL)-based deduction. Specifically, the receiver holds a set\nof hypotheses about the State of the World (SotW), while the transmitter has\nincomplete evidence about the true SotW but lacks access to the ground truth.\nThe transmitter aims to communicate limited information to help the receiver\nidentify the hypothesis most consistent with true SotW. We formulate the\nobjective as approximating the posterior distribution at the transmitter to the\nreceiver. Using Stirling's approximation, this reduces to a constrained,\nfinite-horizon resource allocation problem. Applying the Karush-Kuhn-Tucker\nconditions yields a truncated water-filling solution. Despite the problem's\nnon-convexity, symmetry and permutation invariance ensure global optimality.\nBased on this, we design message selection strategies, both for single- and\nmulti-round communication, and model the receiver's inference as an m-ary\nBayesian hypothesis testing problem. Under the Maximum A Posteriori (MAP) rule,\nour communication strategy achieves optimal performance within budget\nconstraints. We further analyze convergence rates and validate the theoretical\nfindings through experiments, demonstrating reduced error over random selection\nand prior methods."
                },
                "authors": [
                    {
                        "name": "Ahmet Faruk Saz"
                    },
                    {
                        "name": "Siheng Xiong"
                    },
                    {
                        "name": "Faramarz Fekri"
                    }
                ],
                "author_detail": {
                    "name": "Faramarz Fekri"
                },
                "author": "Faramarz Fekri",
                "arxiv_comment": "Accepted to 2025 IEEE Global Communications Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21755v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21755v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21748v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21748v1",
                "updated": "2025-08-29T16:20:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    20,
                    50,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T16:20:50Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    20,
                    50,
                    4,
                    241,
                    0
                ],
                "title": "A direct black hole mass measurement in a Little Red Dot at the Epoch of\n  Reionization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A direct black hole mass measurement in a Little Red Dot at the Epoch of\n  Reionization"
                },
                "summary": "Recent discoveries of faint active galactic nuclei (AGN) at the redshift\nfrontier have revealed a plethora of broad \\Halpha emitters with optically red\ncontinua, named Little Red Dots (LRDs), which comprise 15-30\\% of the high\nredshift broad line AGN population. Due to their peculiar spectral properties\nand X-ray weakness, modeling LRDs with standard AGN templates has proven\nchallenging. In particular, the validity of single-epoch virial mass estimates\nin determining the black hole (BH) masses of LRDs has been called into\nquestion, with some models claiming that masses might be overestimated by up to\n2 orders of magnitude, and other models claiming that LRDs may be entirely\nstellar in nature. We report the direct, dynamical BH mass measurement in a\nstrongly lensed LRD at $z = 7.04$. The combination of lensing with deep\nspectroscopic data reveals a rotation curve that is inconsistent with a nuclear\nstar cluster, yet can be well explained by Keplerian rotation around a point\nmass of 50 million Solar masses, consistent with virial BH mass estimates from\nthe Balmer lines. The Keplerian rotation leaves little room for any stellar\ncomponent in a host galaxy, as we conservatively infer $M_{\\rm BH}/M_{*}>2$.\nSuch a ''naked'' black hole, together with its near-pristine environment,\nindicates that this LRD is a massive black hole seed caught in its earliest\naccretion phase.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent discoveries of faint active galactic nuclei (AGN) at the redshift\nfrontier have revealed a plethora of broad \\Halpha emitters with optically red\ncontinua, named Little Red Dots (LRDs), which comprise 15-30\\% of the high\nredshift broad line AGN population. Due to their peculiar spectral properties\nand X-ray weakness, modeling LRDs with standard AGN templates has proven\nchallenging. In particular, the validity of single-epoch virial mass estimates\nin determining the black hole (BH) masses of LRDs has been called into\nquestion, with some models claiming that masses might be overestimated by up to\n2 orders of magnitude, and other models claiming that LRDs may be entirely\nstellar in nature. We report the direct, dynamical BH mass measurement in a\nstrongly lensed LRD at $z = 7.04$. The combination of lensing with deep\nspectroscopic data reveals a rotation curve that is inconsistent with a nuclear\nstar cluster, yet can be well explained by Keplerian rotation around a point\nmass of 50 million Solar masses, consistent with virial BH mass estimates from\nthe Balmer lines. The Keplerian rotation leaves little room for any stellar\ncomponent in a host galaxy, as we conservatively infer $M_{\\rm BH}/M_{*}>2$.\nSuch a ''naked'' black hole, together with its near-pristine environment,\nindicates that this LRD is a massive black hole seed caught in its earliest\naccretion phase."
                },
                "authors": [
                    {
                        "name": "Ignas Juodžbalis"
                    },
                    {
                        "name": "Cosimo Marconcini"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Alessandro Marconi"
                    },
                    {
                        "name": "Hannah Übler"
                    },
                    {
                        "name": "Jan Scholtz"
                    },
                    {
                        "name": "Xihan Ji"
                    },
                    {
                        "name": "Santiago Arribas"
                    },
                    {
                        "name": "Jake S. Bennett"
                    },
                    {
                        "name": "Volker Bromm"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Stéphane Charlot"
                    },
                    {
                        "name": "Giovanni Cresci"
                    },
                    {
                        "name": "Eiichi Egami"
                    },
                    {
                        "name": "Andrew Fabian"
                    },
                    {
                        "name": "Kohei Inayoshi"
                    },
                    {
                        "name": "Yuki Isobe"
                    },
                    {
                        "name": "Lucy Ivey"
                    },
                    {
                        "name": "Gareth C. Jones"
                    },
                    {
                        "name": "Sophie Koudmani"
                    },
                    {
                        "name": "Nicolas Laporte"
                    },
                    {
                        "name": "Boyuan Liu"
                    },
                    {
                        "name": "Jianwei Lyu"
                    },
                    {
                        "name": "Giovanni Mazzolari"
                    },
                    {
                        "name": "Stephanie Monty"
                    },
                    {
                        "name": "Eleonora Parlanti"
                    },
                    {
                        "name": "Pablo G. Pérez-González"
                    },
                    {
                        "name": "Michele Perna"
                    },
                    {
                        "name": "Brant Robertson"
                    },
                    {
                        "name": "Raffaella Schneider"
                    },
                    {
                        "name": "Debora Sijacki"
                    },
                    {
                        "name": "Sandro Tacchella"
                    },
                    {
                        "name": "Alessandro Trinca"
                    },
                    {
                        "name": "Rosa Valiante"
                    },
                    {
                        "name": "Marta Volonteri"
                    },
                    {
                        "name": "Joris Witstok"
                    },
                    {
                        "name": "Saiyang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Saiyang Zhang"
                },
                "author": "Saiyang Zhang",
                "arxiv_comment": "17 pages, 11 figures. Submitted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21748v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21748v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19028v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19028v4",
                "updated": "2025-08-29T16:20:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    20,
                    31,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-23T18:31:22Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    18,
                    31,
                    22,
                    0,
                    174,
                    0
                ],
                "title": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical\n  Perspective"
                },
                "summary": "Large Language Models (LLMs) often generate responses with inherent biases,\nundermining their reliability in real-world applications. Existing evaluation\nmethods often overlook biases in long-form responses and the intrinsic\nvariability of LLM outputs. To address these challenges, we propose FiSCo\n(Fine-grained Semantic Comparison), a novel statistical framework to evaluate\ngroup-level fairness in LLMs by detecting subtle semantic differences in\nlong-form responses across demographic groups. Unlike prior work focusing on\nsentiment or token-level comparisons, FiSCo goes beyond surface-level analysis\nby operating at the claim level, leveraging entailment checks to assess the\nconsistency of meaning across responses. We decompose model outputs into\nsemantically distinct claims and apply statistical hypothesis testing to\ncompare inter- and intra-group similarities, enabling robust detection of\nsubtle biases. We formalize a new group counterfactual fairness definition and\nvalidate FiSCo on both synthetic and human-annotated datasets spanning gender,\nrace, and age. Experiments show that FiSCo more reliably identifies nuanced\nbiases while reducing the impact of stochastic LLM variability, outperforming\nvarious evaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often generate responses with inherent biases,\nundermining their reliability in real-world applications. Existing evaluation\nmethods often overlook biases in long-form responses and the intrinsic\nvariability of LLM outputs. To address these challenges, we propose FiSCo\n(Fine-grained Semantic Comparison), a novel statistical framework to evaluate\ngroup-level fairness in LLMs by detecting subtle semantic differences in\nlong-form responses across demographic groups. Unlike prior work focusing on\nsentiment or token-level comparisons, FiSCo goes beyond surface-level analysis\nby operating at the claim level, leveraging entailment checks to assess the\nconsistency of meaning across responses. We decompose model outputs into\nsemantically distinct claims and apply statistical hypothesis testing to\ncompare inter- and intra-group similarities, enabling robust detection of\nsubtle biases. We formalize a new group counterfactual fairness definition and\nvalidate FiSCo on both synthetic and human-annotated datasets spanning gender,\nrace, and age. Experiments show that FiSCo more reliably identifies nuanced\nbiases while reducing the impact of stochastic LLM variability, outperforming\nvarious evaluation metrics."
                },
                "authors": [
                    {
                        "name": "Weijie Xu"
                    },
                    {
                        "name": "Yiwen Wang"
                    },
                    {
                        "name": "Chi Xue"
                    },
                    {
                        "name": "Xiangkun Hu"
                    },
                    {
                        "name": "Xi Fang"
                    },
                    {
                        "name": "Guimin Dong"
                    },
                    {
                        "name": "Chandan K. Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Chandan K. Reddy"
                },
                "author": "Chandan K. Reddy",
                "arxiv_comment": "29 pages, 9 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19028v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19028v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.14054v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.14054v3",
                "updated": "2025-08-29T16:14:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    14,
                    18,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-16T23:21:37Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    23,
                    21,
                    37,
                    0,
                    167,
                    0
                ],
                "title": "Scientifically-Interpretable Reasoning Network (ScIReN): Discovering\n  Hidden Relationships in the Carbon Cycle and Beyond",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientifically-Interpretable Reasoning Network (ScIReN): Discovering\n  Hidden Relationships in the Carbon Cycle and Beyond"
                },
                "summary": "Understanding how carbon flows through the soil is crucial for mitigating the\neffects of climate change. While soils have potential to sequester carbon from\nthe atmosphere, the soil carbon cycle remains poorly understood. Scientists\nhave developed mathematical process-based models of the soil carbon cycle based\non existing knowledge, but they contain numerous unknown parameters that must\nbe set in an ad-hoc manner, and often fit observations poorly. On the other\nhand, neural networks can learn patterns from data, but do not respect known\nscientific laws, nor can they reveal novel scientific relationships due to\ntheir black-box nature. We thus propose Scientifically-Interpretable Reasoning\nNetwork (ScIReN), a fully-transparent framework that combines interpretable\nneural and process-based reasoning. An interpretable encoder predicts\nscientifically-meaningful latent parameters, which are then passed through a\ndifferentiable process-based decoder to predict labeled output variables.\nScIReN leverages Kolmogorov-Arnold networks (KAN) to ensure the encoder is\nfully interpretable and reveals relationships between input features and latent\nparameters; it uses novel smoothness penalties to balance expressivity and\nsimplicity. ScIReN also uses a novel hard-sigmoid constraint layer to restrict\nlatent parameters to meaningful ranges defined by scientific prior knowledge.\nWhile the process-based decoder enforces established scientific knowledge, the\nKAN-based encoder reveals new scientific relationships hidden in conventional\nblack-box models. We apply ScIReN on two tasks: simulating the flow of organic\ncarbon through soils, and modeling ecosystem respiration from plants. In both\ntasks, ScIReN outperforms black-box networks in predictive accuracy while\nproviding substantial scientific interpretability -- it can infer latent\nscientific mechanisms and their relationships with input features.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how carbon flows through the soil is crucial for mitigating the\neffects of climate change. While soils have potential to sequester carbon from\nthe atmosphere, the soil carbon cycle remains poorly understood. Scientists\nhave developed mathematical process-based models of the soil carbon cycle based\non existing knowledge, but they contain numerous unknown parameters that must\nbe set in an ad-hoc manner, and often fit observations poorly. On the other\nhand, neural networks can learn patterns from data, but do not respect known\nscientific laws, nor can they reveal novel scientific relationships due to\ntheir black-box nature. We thus propose Scientifically-Interpretable Reasoning\nNetwork (ScIReN), a fully-transparent framework that combines interpretable\nneural and process-based reasoning. An interpretable encoder predicts\nscientifically-meaningful latent parameters, which are then passed through a\ndifferentiable process-based decoder to predict labeled output variables.\nScIReN leverages Kolmogorov-Arnold networks (KAN) to ensure the encoder is\nfully interpretable and reveals relationships between input features and latent\nparameters; it uses novel smoothness penalties to balance expressivity and\nsimplicity. ScIReN also uses a novel hard-sigmoid constraint layer to restrict\nlatent parameters to meaningful ranges defined by scientific prior knowledge.\nWhile the process-based decoder enforces established scientific knowledge, the\nKAN-based encoder reveals new scientific relationships hidden in conventional\nblack-box models. We apply ScIReN on two tasks: simulating the flow of organic\ncarbon through soils, and modeling ecosystem respiration from plants. In both\ntasks, ScIReN outperforms black-box networks in predictive accuracy while\nproviding substantial scientific interpretability -- it can infer latent\nscientific mechanisms and their relationships with input features."
                },
                "authors": [
                    {
                        "name": "Joshua Fan"
                    },
                    {
                        "name": "Haodi Xu"
                    },
                    {
                        "name": "Feng Tao"
                    },
                    {
                        "name": "Md Nasim"
                    },
                    {
                        "name": "Marc Grimson"
                    },
                    {
                        "name": "Yiqi Luo"
                    },
                    {
                        "name": "Carla P. Gomes"
                    }
                ],
                "author_detail": {
                    "name": "Carla P. Gomes"
                },
                "author": "Carla P. Gomes",
                "arxiv_comment": "18 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.14054v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.14054v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21742v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21742v1",
                "updated": "2025-08-29T16:08:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    8,
                    35,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T16:08:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    8,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "Orientability of Causal Relations in Time Series using Summary Causal\n  Graphs and Faithful Distributions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orientability of Causal Relations in Time Series using Summary Causal\n  Graphs and Faithful Distributions"
                },
                "summary": "Understanding causal relations between temporal variables is a central\nchallenge in time series analysis, particularly when the full causal structure\nis unknown. Even when the full causal structure cannot be fully specified,\nexperts often succeed in providing a high-level abstraction of the causal\ngraph, known as a summary causal graph, which captures the main causal\nrelations between different time series while abstracting away micro-level\ndetails. In this work, we present conditions that guarantee the orientability\nof micro-level edges between temporal variables given the background knowledge\nencoded in a summary causal graph and assuming having access to a faithful and\ncausally sufficient distribution with respect to the true unknown graph. Our\nresults provide theoretical guarantees for edge orientation at the micro-level,\neven in the presence of cycles or bidirected edges at the macro-level. These\nfindings offer practical guidance for leveraging SCGs to inform causal\ndiscovery in complex temporal systems and highlight the value of incorporating\nexpert knowledge to improve causal inference from observational time series\ndata.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding causal relations between temporal variables is a central\nchallenge in time series analysis, particularly when the full causal structure\nis unknown. Even when the full causal structure cannot be fully specified,\nexperts often succeed in providing a high-level abstraction of the causal\ngraph, known as a summary causal graph, which captures the main causal\nrelations between different time series while abstracting away micro-level\ndetails. In this work, we present conditions that guarantee the orientability\nof micro-level edges between temporal variables given the background knowledge\nencoded in a summary causal graph and assuming having access to a faithful and\ncausally sufficient distribution with respect to the true unknown graph. Our\nresults provide theoretical guarantees for edge orientation at the micro-level,\neven in the presence of cycles or bidirected edges at the macro-level. These\nfindings offer practical guidance for leveraging SCGs to inform causal\ndiscovery in complex temporal systems and highlight the value of incorporating\nexpert knowledge to improve causal inference from observational time series\ndata."
                },
                "authors": [
                    {
                        "name": "Timothée Loranchet"
                    },
                    {
                        "name": "Charles K. Assaad"
                    }
                ],
                "author_detail": {
                    "name": "Charles K. Assaad"
                },
                "author": "Charles K. Assaad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21742v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21742v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21741v1",
                "updated": "2025-08-29T16:07:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    7,
                    33,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T16:07:33Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    7,
                    33,
                    4,
                    241,
                    0
                ],
                "title": "Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning\n  Performance"
                },
                "summary": "Supervised fine-tuning (SFT) is a pivotal approach to adapting large language\nmodels (LLMs) for downstream tasks; however, performance often suffers from the\n``seesaw phenomenon'', where indiscriminate parameter updates yield progress on\ncertain tasks at the expense of others. To address this challenge, we propose a\nnovel \\emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.\nSpecifically, we first independently fine-tune the LLM on each task to identify\nits core parameter regions by quantifying parameter update magnitudes. Tasks\nwith similar core regions are then grouped based on region overlap, forming\nclusters for joint modeling. We further introduce a parameter fusion technique:\nfor each task, core parameters from its individually fine-tuned model are\ndirectly transplanted into a unified backbone, while non-core parameters from\ndifferent tasks are smoothly integrated via Spherical Linear Interpolation\n(SLERP), mitigating destructive interference. A lightweight, pipelined SFT\ntraining phase using mixed-task data is subsequently employed, while freezing\ncore regions from prior tasks to prevent catastrophic forgetting. Extensive\nexperiments on multiple public benchmarks demonstrate that our approach\nsignificantly alleviates task interference and forgetting, consistently\noutperforming vanilla multi-task and multi-stage fine-tuning baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) is a pivotal approach to adapting large language\nmodels (LLMs) for downstream tasks; however, performance often suffers from the\n``seesaw phenomenon'', where indiscriminate parameter updates yield progress on\ncertain tasks at the expense of others. To address this challenge, we propose a\nnovel \\emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.\nSpecifically, we first independently fine-tune the LLM on each task to identify\nits core parameter regions by quantifying parameter update magnitudes. Tasks\nwith similar core regions are then grouped based on region overlap, forming\nclusters for joint modeling. We further introduce a parameter fusion technique:\nfor each task, core parameters from its individually fine-tuned model are\ndirectly transplanted into a unified backbone, while non-core parameters from\ndifferent tasks are smoothly integrated via Spherical Linear Interpolation\n(SLERP), mitigating destructive interference. A lightweight, pipelined SFT\ntraining phase using mixed-task data is subsequently employed, while freezing\ncore regions from prior tasks to prevent catastrophic forgetting. Extensive\nexperiments on multiple public benchmarks demonstrate that our approach\nsignificantly alleviates task interference and forgetting, consistently\noutperforming vanilla multi-task and multi-stage fine-tuning baselines."
                },
                "authors": [
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Di Liang"
                    },
                    {
                        "name": "Minlong Peng"
                    }
                ],
                "author_detail": {
                    "name": "Minlong Peng"
                },
                "author": "Minlong Peng",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21740v1",
                "updated": "2025-08-29T16:06:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    6,
                    27,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T16:06:27Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    6,
                    27,
                    4,
                    241,
                    0
                ],
                "title": "Operational Validation of Large-Language-Model Agent Social Simulation:\n  Evidence from Voat v/technology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operational Validation of Large-Language-Model Agent Social Simulation:\n  Evidence from Voat v/technology"
                },
                "summary": "Large Language Models (LLMs) enable generative social simulations that can\ncapture culturally informed, norm-guided interaction on online social\nplatforms. We build a technology community simulation modeled on Voat, a\nReddit-like alt-right news aggregator and discussion platform active from 2014\nto 2020. Using the YSocial framework, we seed the simulation with a fixed\ncatalog of technology links sampled from Voat's shared URLs (covering 30+\ndomains) and calibrate parameters to Voat's v/technology using samples from the\nMADOC dataset. Agents use a base, uncensored model (Dolphin 3.0, based on Llama\n3.1 8B) and concise personas (demographics, political leaning, interests,\neducation, toxicity propensity) to generate posts, replies, and reactions under\nplatform rules for link and text submissions, threaded replies and daily\nactivity cycles. We run a 30-day simulation and evaluate operational validity\nby comparing distributions and structures with matched Voat data: activity\npatterns, interaction networks, toxicity, and topic coverage. Results indicate\nfamiliar online regularities: similar activity rhythms, heavy-tailed\nparticipation, sparse low-clustering interaction networks, core-periphery\nstructure, topical alignment with Voat, and elevated toxicity. Limitations of\nthe current study include the stateless agent design and evaluation based on a\nsingle 30-day run, which constrains external validity and variance estimates.\nThe simulation generates realistic discussions, often featuring toxic language,\nprimarily centered on technology topics such as Big Tech and AI. This approach\noffers a valuable method for examining toxicity dynamics and testing moderation\nstrategies within a controlled environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) enable generative social simulations that can\ncapture culturally informed, norm-guided interaction on online social\nplatforms. We build a technology community simulation modeled on Voat, a\nReddit-like alt-right news aggregator and discussion platform active from 2014\nto 2020. Using the YSocial framework, we seed the simulation with a fixed\ncatalog of technology links sampled from Voat's shared URLs (covering 30+\ndomains) and calibrate parameters to Voat's v/technology using samples from the\nMADOC dataset. Agents use a base, uncensored model (Dolphin 3.0, based on Llama\n3.1 8B) and concise personas (demographics, political leaning, interests,\neducation, toxicity propensity) to generate posts, replies, and reactions under\nplatform rules for link and text submissions, threaded replies and daily\nactivity cycles. We run a 30-day simulation and evaluate operational validity\nby comparing distributions and structures with matched Voat data: activity\npatterns, interaction networks, toxicity, and topic coverage. Results indicate\nfamiliar online regularities: similar activity rhythms, heavy-tailed\nparticipation, sparse low-clustering interaction networks, core-periphery\nstructure, topical alignment with Voat, and elevated toxicity. Limitations of\nthe current study include the stateless agent design and evaluation based on a\nsingle 30-day run, which constrains external validity and variance estimates.\nThe simulation generates realistic discussions, often featuring toxic language,\nprimarily centered on technology topics such as Big Tech and AI. This approach\noffers a valuable method for examining toxicity dynamics and testing moderation\nstrategies within a controlled environment."
                },
                "authors": [
                    {
                        "name": "Aleksandar Tomašević"
                    },
                    {
                        "name": "Darja Cvetković"
                    },
                    {
                        "name": "Sara Major"
                    },
                    {
                        "name": "Slobodan Maletić"
                    },
                    {
                        "name": "Miroslav Anđelković"
                    },
                    {
                        "name": "Ana Vranić"
                    },
                    {
                        "name": "Boris Stupovski"
                    },
                    {
                        "name": "Dušan Vudragović"
                    },
                    {
                        "name": "Aleksandar Bogojević"
                    },
                    {
                        "name": "Marija Mitrović Dankulov"
                    }
                ],
                "author_detail": {
                    "name": "Marija Mitrović Dankulov"
                },
                "author": "Marija Mitrović Dankulov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21739v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21739v1",
                "updated": "2025-08-29T16:04:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    4,
                    15,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T16:04:15Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    4,
                    15,
                    4,
                    241,
                    0
                ],
                "title": "Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL,\n  Rogue Software and Auto-SNL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Network Acceleration on MPSoC board: Integrating SLAC's SNL,\n  Rogue Software and Auto-SNL"
                },
                "summary": "The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline\nexperiments at rates of up to 1~MHz, with detectors producing data throughputs\nexceeding 1 TB/s. Managing such massive data streams presents significant\nchallenges, as transmission and storage infrastructures become prohibitively\nexpensive. Machine learning (ML) offers a promising solution for real-time data\nreduction, but conventional implementations introduce excessive latency, making\nthem unsuitable for high-speed experimental environments. To address these\nchallenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized\nframework designed to deploy real-time ML inference models on\nField-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to\ndynamically update model weights without requiring FPGA resynthesis, enhancing\nflexibility for adaptive learning applications. To further enhance usability\nand accessibility, we introduce Auto-SNL, a Python extension that streamlines\nthe process of converting Python-based neural network models into\nSNL-compatible high-level synthesis code. This paper presents a benchmark\ncomparison against hls4ml, the current state-of-the-art tool, across multiple\nneural network architectures, fixed-point precisions, and synthesis\nconfigurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL\nachieves competitive or superior latency in most tested architectures, while in\nsome cases also offering FPGA resource savings. This adaptation demonstrates\nSNL's versatility, opening new opportunities for researchers and academics in\nfields such as high-energy physics, medical imaging, robotics, and many more.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The LCLS-II Free Electron Laser (FEL) will generate X-ray pulses for beamline\nexperiments at rates of up to 1~MHz, with detectors producing data throughputs\nexceeding 1 TB/s. Managing such massive data streams presents significant\nchallenges, as transmission and storage infrastructures become prohibitively\nexpensive. Machine learning (ML) offers a promising solution for real-time data\nreduction, but conventional implementations introduce excessive latency, making\nthem unsuitable for high-speed experimental environments. To address these\nchallenges, SLAC developed the SLAC Neural Network Library (SNL), a specialized\nframework designed to deploy real-time ML inference models on\nField-Programmable Gate Arrays (FPGA). SNL's key feature is the ability to\ndynamically update model weights without requiring FPGA resynthesis, enhancing\nflexibility for adaptive learning applications. To further enhance usability\nand accessibility, we introduce Auto-SNL, a Python extension that streamlines\nthe process of converting Python-based neural network models into\nSNL-compatible high-level synthesis code. This paper presents a benchmark\ncomparison against hls4ml, the current state-of-the-art tool, across multiple\nneural network architectures, fixed-point precisions, and synthesis\nconfigurations targeting a Xilinx ZCU102 FPGA. The results showed that SNL\nachieves competitive or superior latency in most tested architectures, while in\nsome cases also offering FPGA resource savings. This adaptation demonstrates\nSNL's versatility, opening new opportunities for researchers and academics in\nfields such as high-energy physics, medical imaging, robotics, and many more."
                },
                "authors": [
                    {
                        "name": "Hamza Ezzaoui Rahali"
                    },
                    {
                        "name": "Abhilasha Dave"
                    },
                    {
                        "name": "Larry Ruckman"
                    },
                    {
                        "name": "Mohammad Mehdi Rahimifar"
                    },
                    {
                        "name": "Audrey C. Therrien"
                    },
                    {
                        "name": "James J. Russel"
                    },
                    {
                        "name": "Ryan T. Herbst"
                    }
                ],
                "author_detail": {
                    "name": "Ryan T. Herbst"
                },
                "author": "Ryan T. Herbst",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21739v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21739v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21727v1",
                "updated": "2025-08-29T15:50:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    15,
                    50,
                    59,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T15:50:59Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    15,
                    50,
                    59,
                    4,
                    241,
                    0
                ],
                "title": "OptMark: Robust Multi-bit Diffusion Watermarking via Inference Time\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OptMark: Robust Multi-bit Diffusion Watermarking via Inference Time\n  Optimization"
                },
                "summary": "Watermarking diffusion-generated images is crucial for copyright protection\nand user tracking. However, current diffusion watermarking methods face\nsignificant limitations: zero-bit watermarking systems lack the capacity for\nlarge-scale user tracking, while multi-bit methods are highly sensitive to\ncertain image transformations or generative attacks, resulting in a lack of\ncomprehensive robustness. In this paper, we propose OptMark, an\noptimization-based approach that embeds a robust multi-bit watermark into the\nintermediate latents of the diffusion denoising process. OptMark strategically\ninserts a structural watermark early to resist generative attacks and a detail\nwatermark late to withstand image transformations, with tailored regularization\nterms to preserve image quality and ensure imperceptibility. To address the\nchallenge of memory consumption growing linearly with the number of denoising\nsteps during optimization, OptMark incorporates adjoint gradient methods,\nreducing memory usage from O(N) to O(1). Experimental results demonstrate that\nOptMark achieves invisible multi-bit watermarking while ensuring robust\nresilience against valuemetric transformations, geometric transformations,\nediting, and regeneration attacks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Watermarking diffusion-generated images is crucial for copyright protection\nand user tracking. However, current diffusion watermarking methods face\nsignificant limitations: zero-bit watermarking systems lack the capacity for\nlarge-scale user tracking, while multi-bit methods are highly sensitive to\ncertain image transformations or generative attacks, resulting in a lack of\ncomprehensive robustness. In this paper, we propose OptMark, an\noptimization-based approach that embeds a robust multi-bit watermark into the\nintermediate latents of the diffusion denoising process. OptMark strategically\ninserts a structural watermark early to resist generative attacks and a detail\nwatermark late to withstand image transformations, with tailored regularization\nterms to preserve image quality and ensure imperceptibility. To address the\nchallenge of memory consumption growing linearly with the number of denoising\nsteps during optimization, OptMark incorporates adjoint gradient methods,\nreducing memory usage from O(N) to O(1). Experimental results demonstrate that\nOptMark achieves invisible multi-bit watermarking while ensuring robust\nresilience against valuemetric transformations, geometric transformations,\nediting, and regeneration attacks."
                },
                "authors": [
                    {
                        "name": "Jiazheng Xing"
                    },
                    {
                        "name": "Hai Ci"
                    },
                    {
                        "name": "Hongbin Xu"
                    },
                    {
                        "name": "Hangjie Yuan"
                    },
                    {
                        "name": "Yong Liu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    }
                ],
                "author_detail": {
                    "name": "Mike Zheng Shou"
                },
                "author": "Mike Zheng Shou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21722v1",
                "updated": "2025-08-29T15:38:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    15,
                    38,
                    27,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T15:38:27Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    15,
                    38,
                    27,
                    4,
                    241,
                    0
                ],
                "title": "Inferring Effects of Major Events through Discontinuity Forecasting of\n  Population Anxiety",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Effects of Major Events through Discontinuity Forecasting of\n  Population Anxiety"
                },
                "summary": "Estimating community-specific mental health effects of local events is vital\nfor public health policy. While forecasting mental health scores alone offers\nlimited insights into the impact of events on community well-being,\nquasi-experimental designs like the Longitudinal Regression Discontinuity\nDesign (LRDD) from econometrics help researchers derive more effects that are\nmore likely to be causal from observational data. LRDDs aim to extrapolate the\nsize of changes in an outcome (e.g. a discontinuity in running scores for\nanxiety) due to a time-specific event. Here, we propose adapting LRDDs beyond\ntraditional forecasting into a statistical learning framework whereby future\ndiscontinuities (i.e. time-specific shifts) and changes in slope (i.e. linear\ntrajectories) are estimated given a location's history of the score, dynamic\ncovariates (other running assessments), and exogenous variables (static\nrepresentations). Applying our framework to predict discontinuities in the\nanxiety of US counties from COVID-19 events, we found the task was difficult\nbut more achievable as the sophistication of models was increased, with the\nbest results coming from integrating exogenous and dynamic covariates. Our\napproach shows strong improvement ($r=+.46$ for discontinuity and $r = +.65$\nfor slope) over traditional static community representations. Discontinuity\nforecasting raises new possibilities for estimating the idiosyncratic effects\nof potential future or hypothetical events on specific communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating community-specific mental health effects of local events is vital\nfor public health policy. While forecasting mental health scores alone offers\nlimited insights into the impact of events on community well-being,\nquasi-experimental designs like the Longitudinal Regression Discontinuity\nDesign (LRDD) from econometrics help researchers derive more effects that are\nmore likely to be causal from observational data. LRDDs aim to extrapolate the\nsize of changes in an outcome (e.g. a discontinuity in running scores for\nanxiety) due to a time-specific event. Here, we propose adapting LRDDs beyond\ntraditional forecasting into a statistical learning framework whereby future\ndiscontinuities (i.e. time-specific shifts) and changes in slope (i.e. linear\ntrajectories) are estimated given a location's history of the score, dynamic\ncovariates (other running assessments), and exogenous variables (static\nrepresentations). Applying our framework to predict discontinuities in the\nanxiety of US counties from COVID-19 events, we found the task was difficult\nbut more achievable as the sophistication of models was increased, with the\nbest results coming from integrating exogenous and dynamic covariates. Our\napproach shows strong improvement ($r=+.46$ for discontinuity and $r = +.65$\nfor slope) over traditional static community representations. Discontinuity\nforecasting raises new possibilities for estimating the idiosyncratic effects\nof potential future or hypothetical events on specific communities."
                },
                "authors": [
                    {
                        "name": "Siddharth Mangalik"
                    },
                    {
                        "name": "Ojas Deshpande"
                    },
                    {
                        "name": "Adithya V. Ganesan"
                    },
                    {
                        "name": "Sean A. P. Clouston"
                    },
                    {
                        "name": "H. Andrew Schwartz"
                    }
                ],
                "author_detail": {
                    "name": "H. Andrew Schwartz"
                },
                "author": "H. Andrew Schwartz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05137v2",
                "updated": "2025-08-29T15:28:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    15,
                    28,
                    0,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-07T15:49:23Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    49,
                    23,
                    0,
                    188,
                    0
                ],
                "title": "Interpretable Mnemonic Generation for Kanji Learning via\n  Expectation-Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Mnemonic Generation for Kanji Learning via\n  Expectation-Maximization"
                },
                "summary": "Learning Japanese vocabulary is a challenge for learners from Roman alphabet\nbackgrounds due to script differences. Japanese combines syllabaries like\nhiragana with kanji, which are logographic characters of Chinese origin. Kanji\nare also complicated due to their complexity and volume. Keyword mnemonics are\na common strategy to aid memorization, often using the compositional structure\nof kanji to form vivid associations. Despite recent efforts to use large\nlanguage models (LLMs) to assist learners, existing methods for LLM-based\nkeyword mnemonic generation function as a black box, offering limited\ninterpretability. We propose a generative framework that explicitly models the\nmnemonic construction process as driven by a set of common rules, and learn\nthem using a novel Expectation-Maximization-type algorithm. Trained on\nlearner-authored mnemonics from an online platform, our method learns latent\nstructures and compositional rules, enabling interpretable and systematic\nmnemonics generation. Experiments show that our method performs well in the\ncold-start setting for new learners while providing insight into the mechanisms\nbehind effective mnemonic creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Japanese vocabulary is a challenge for learners from Roman alphabet\nbackgrounds due to script differences. Japanese combines syllabaries like\nhiragana with kanji, which are logographic characters of Chinese origin. Kanji\nare also complicated due to their complexity and volume. Keyword mnemonics are\na common strategy to aid memorization, often using the compositional structure\nof kanji to form vivid associations. Despite recent efforts to use large\nlanguage models (LLMs) to assist learners, existing methods for LLM-based\nkeyword mnemonic generation function as a black box, offering limited\ninterpretability. We propose a generative framework that explicitly models the\nmnemonic construction process as driven by a set of common rules, and learn\nthem using a novel Expectation-Maximization-type algorithm. Trained on\nlearner-authored mnemonics from an online platform, our method learns latent\nstructures and compositional rules, enabling interpretable and systematic\nmnemonics generation. Experiments show that our method performs well in the\ncold-start setting for new learners while providing insight into the mechanisms\nbehind effective mnemonic creation."
                },
                "authors": [
                    {
                        "name": "Jaewook Lee"
                    },
                    {
                        "name": "Alexander Scarlatos"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "arxiv_comment": "The Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21706v1",
                "updated": "2025-08-29T15:25:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    15,
                    25,
                    5,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T15:25:05Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    15,
                    25,
                    5,
                    4,
                    241,
                    0
                ],
                "title": "Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency\n  with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency\n  with Speculative Decoding"
                },
                "summary": "Recent advancements in Mixture of Experts (MoE) models have significantly\nincreased their parameter scale as well as model performance. Extensive\noffloading techniques have been proposed to address the GPU memory limitations\nof MoE inference. However, due to the I/O bottleneck and sparse computation of\nMoE models, existing offloading techniques still suffer from low hardware\nutilization. To fully utilize the hardware resources, we propose SpecMoEOff,\nwhich employs the speculative decoding technique to enlarge the workload of\neach expert. SpecMoEOff orchestrates the GPU and CPU by both theoretical and\nempirical roofline analysis. In addition, we develop a dedicated CPU chunked\nattention verification kernel to fit the speculative decoding in offloading\nscenarios as well as minimizing the additional overhead led by draft models.\nSpecMoEOff further integrates an optimizer to automatically tune the\nhyperparameters of speculative decoding under given hardware and workload.\nExperimental results show that SpecMoEOff achieves up to 2.5x decode throughput\nimprovement over the state-of-the-art MoE offloading techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Mixture of Experts (MoE) models have significantly\nincreased their parameter scale as well as model performance. Extensive\noffloading techniques have been proposed to address the GPU memory limitations\nof MoE inference. However, due to the I/O bottleneck and sparse computation of\nMoE models, existing offloading techniques still suffer from low hardware\nutilization. To fully utilize the hardware resources, we propose SpecMoEOff,\nwhich employs the speculative decoding technique to enlarge the workload of\neach expert. SpecMoEOff orchestrates the GPU and CPU by both theoretical and\nempirical roofline analysis. In addition, we develop a dedicated CPU chunked\nattention verification kernel to fit the speculative decoding in offloading\nscenarios as well as minimizing the additional overhead led by draft models.\nSpecMoEOff further integrates an optimizer to automatically tune the\nhyperparameters of speculative decoding under given hardware and workload.\nExperimental results show that SpecMoEOff achieves up to 2.5x decode throughput\nimprovement over the state-of-the-art MoE offloading techniques."
                },
                "authors": [
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Zhonghui Zhang"
                    },
                    {
                        "name": "Yuhang Zhou"
                    },
                    {
                        "name": "Zibo Wang"
                    },
                    {
                        "name": "Mo Zhou"
                    },
                    {
                        "name": "Peng Jiang"
                    },
                    {
                        "name": "Weilin Cai"
                    },
                    {
                        "name": "Chengying Huan"
                    },
                    {
                        "name": "Rong Gu"
                    },
                    {
                        "name": "Sheng Zhong"
                    },
                    {
                        "name": "Chen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Tian"
                },
                "author": "Chen Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22288v2",
                "updated": "2025-08-29T15:11:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    15,
                    11,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-28T12:27:32Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    12,
                    27,
                    32,
                    2,
                    148,
                    0
                ],
                "title": "Compression versus Accuracy: A Hierarchy of Lifted Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compression versus Accuracy: A Hierarchy of Lifted Models"
                },
                "summary": "Probabilistic graphical models that encode indistinguishable objects and\nrelations among them use first-order logic constructs to compress a\npropositional factorised model for more efficient (lifted) inference. To obtain\na lifted representation, the state-of-the-art algorithm Advanced Colour Passing\n(ACP) groups factors that represent matching distributions. In an approximate\nversion using $\\varepsilon$ as a hyperparameter, factors are grouped that\ndiffer by a factor of at most $(1\\pm \\varepsilon)$. However, finding a suitable\n$\\varepsilon$ is not obvious and may need a lot of exploration, possibly\nrequiring many ACP runs with different $\\varepsilon$ values. Additionally,\nvarying $\\varepsilon$ can yield wildly different models, leading to decreased\ninterpretability. Therefore, this paper presents a hierarchical approach to\nlifted model construction that is hyperparameter-free. It efficiently computes\na hierarchy of $\\varepsilon$ values that ensures a hierarchy of models, meaning\nthat once factors are grouped together given some $\\varepsilon$, these factors\nwill be grouped together for larger $\\varepsilon$ as well. The hierarchy of\n$\\varepsilon$ values also leads to a hierarchy of error bounds. This allows for\nexplicitly weighing compression versus accuracy when choosing specific\n$\\varepsilon$ values to run ACP with and enables interpretability between the\ndifferent models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probabilistic graphical models that encode indistinguishable objects and\nrelations among them use first-order logic constructs to compress a\npropositional factorised model for more efficient (lifted) inference. To obtain\na lifted representation, the state-of-the-art algorithm Advanced Colour Passing\n(ACP) groups factors that represent matching distributions. In an approximate\nversion using $\\varepsilon$ as a hyperparameter, factors are grouped that\ndiffer by a factor of at most $(1\\pm \\varepsilon)$. However, finding a suitable\n$\\varepsilon$ is not obvious and may need a lot of exploration, possibly\nrequiring many ACP runs with different $\\varepsilon$ values. Additionally,\nvarying $\\varepsilon$ can yield wildly different models, leading to decreased\ninterpretability. Therefore, this paper presents a hierarchical approach to\nlifted model construction that is hyperparameter-free. It efficiently computes\na hierarchy of $\\varepsilon$ values that ensures a hierarchy of models, meaning\nthat once factors are grouped together given some $\\varepsilon$, these factors\nwill be grouped together for larger $\\varepsilon$ as well. The hierarchy of\n$\\varepsilon$ values also leads to a hierarchy of error bounds. This allows for\nexplicitly weighing compression versus accuracy when choosing specific\n$\\varepsilon$ values to run ACP with and enables interpretability between the\ndifferent models."
                },
                "authors": [
                    {
                        "name": "Jan Speller"
                    },
                    {
                        "name": "Malte Luttermann"
                    },
                    {
                        "name": "Marcel Gehrke"
                    },
                    {
                        "name": "Tanya Braun"
                    }
                ],
                "author_detail": {
                    "name": "Tanya Braun"
                },
                "author": "Tanya Braun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.10989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.10989v2",
                "updated": "2025-08-29T14:52:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    52,
                    59,
                    4,
                    241,
                    0
                ],
                "published": "2023-10-17T04:23:31Z",
                "published_parsed": [
                    2023,
                    10,
                    17,
                    4,
                    23,
                    31,
                    1,
                    290,
                    0
                ],
                "title": "Mixed membership estimation for categorical data with weighted responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed membership estimation for categorical data with weighted responses"
                },
                "summary": "The Grade of Membership (GoM) model, which allows subjects to belong to\nmultiple latent classes, is a powerful tool for inferring latent classes in\ncategorical data. However, its application is limited to categorical data with\nnonnegative integer responses, as it assumes that the response matrix is\ngenerated from Bernoulli or Binomial distributions, making it inappropriate for\ndatasets with continuous or negative weighted responses. To address this, this\npaper proposes a novel model named the Weighted Grade of Membership (WGoM)\nmodel. Our WGoM is more general than GoM because it relaxes GoM's distribution\nconstraint by allowing the response matrix to be generated from distributions\nlike Bernoulli, Binomial, Normal, and Uniform as long as the expected response\nmatrix has a block structure related to subjects' mixed memberships under the\ndistribution. We show that WGoM can describe any response matrix with finite\ndistinct elements. We then propose an algorithm to estimate the latent mixed\nmemberships and other WGoM parameters. We derive the error bounds of the\nestimated parameters and show that the algorithm is statistically consistent.\nWe also propose an efficient method for determining the number of latent\nclasses $K$ for categorical data with weighted responses by maximizing fuzzy\nweighted modularity. The performance of our methods is validated through both\nsynthetic and real-world datasets. The results demonstrate the accuracy and\nefficiency of our algorithm for estimating latent mixed memberships, as well as\nthe high accuracy of our method for estimating $K$, indicating their high\npotential for practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Grade of Membership (GoM) model, which allows subjects to belong to\nmultiple latent classes, is a powerful tool for inferring latent classes in\ncategorical data. However, its application is limited to categorical data with\nnonnegative integer responses, as it assumes that the response matrix is\ngenerated from Bernoulli or Binomial distributions, making it inappropriate for\ndatasets with continuous or negative weighted responses. To address this, this\npaper proposes a novel model named the Weighted Grade of Membership (WGoM)\nmodel. Our WGoM is more general than GoM because it relaxes GoM's distribution\nconstraint by allowing the response matrix to be generated from distributions\nlike Bernoulli, Binomial, Normal, and Uniform as long as the expected response\nmatrix has a block structure related to subjects' mixed memberships under the\ndistribution. We show that WGoM can describe any response matrix with finite\ndistinct elements. We then propose an algorithm to estimate the latent mixed\nmemberships and other WGoM parameters. We derive the error bounds of the\nestimated parameters and show that the algorithm is statistically consistent.\nWe also propose an efficient method for determining the number of latent\nclasses $K$ for categorical data with weighted responses by maximizing fuzzy\nweighted modularity. The performance of our methods is validated through both\nsynthetic and real-world datasets. The results demonstrate the accuracy and\nefficiency of our algorithm for estimating latent mixed memberships, as well as\nthe high accuracy of our method for estimating $K$, indicating their high\npotential for practical applications."
                },
                "authors": [
                    {
                        "name": "Huan Qing"
                    }
                ],
                "author_detail": {
                    "name": "Huan Qing"
                },
                "author": "Huan Qing",
                "arxiv_doi": "10.1007/s11749-025-00973-x",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s11749-025-00973-x",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.10989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.10989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "TEST (2025): 1-48",
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17196v2",
                "updated": "2025-08-29T14:42:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    42,
                    16,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-24T03:17:50Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    3,
                    17,
                    50,
                    6,
                    236,
                    0
                ],
                "title": "BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have leveraged increased\ntest-time computation to enhance reasoning capabilities, a strategy that, while\neffective, incurs significant latency and resource costs, limiting their\napplicability in real-world time-constrained or cost-sensitive scenarios. This\npaper introduces BudgetThinker, a novel framework designed to empower LLMs with\nbudget-aware reasoning, enabling precise control over the length of their\nthought processes. We propose a methodology that periodically inserts special\ncontrol tokens during inference to continuously inform the model of its\nremaining token budget. This approach is coupled with a comprehensive two-stage\ntraining pipeline, beginning with Supervised Fine-Tuning (SFT) to familiarize\nthe model with budget constraints, followed by a curriculum-based Reinforcement\nLearning (RL) phase that utilizes a length-aware reward function to optimize\nfor both accuracy and budget adherence. We demonstrate that BudgetThinker\nsignificantly surpasses strong baselines in maintaining performance across a\nvariety of reasoning budgets on challenging mathematical benchmarks. Our method\nprovides a scalable and effective solution for developing efficient and\ncontrollable LLM reasoning, making advanced models more practical for\ndeployment in resource-constrained and real-time environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have leveraged increased\ntest-time computation to enhance reasoning capabilities, a strategy that, while\neffective, incurs significant latency and resource costs, limiting their\napplicability in real-world time-constrained or cost-sensitive scenarios. This\npaper introduces BudgetThinker, a novel framework designed to empower LLMs with\nbudget-aware reasoning, enabling precise control over the length of their\nthought processes. We propose a methodology that periodically inserts special\ncontrol tokens during inference to continuously inform the model of its\nremaining token budget. This approach is coupled with a comprehensive two-stage\ntraining pipeline, beginning with Supervised Fine-Tuning (SFT) to familiarize\nthe model with budget constraints, followed by a curriculum-based Reinforcement\nLearning (RL) phase that utilizes a length-aware reward function to optimize\nfor both accuracy and budget adherence. We demonstrate that BudgetThinker\nsignificantly surpasses strong baselines in maintaining performance across a\nvariety of reasoning budgets on challenging mathematical benchmarks. Our method\nprovides a scalable and effective solution for developing efficient and\ncontrollable LLM reasoning, making advanced models more practical for\ndeployment in resource-constrained and real-time environments."
                },
                "authors": [
                    {
                        "name": "Hao Wen"
                    },
                    {
                        "name": "Xinrui Wu"
                    },
                    {
                        "name": "Yi Sun"
                    },
                    {
                        "name": "Feifei Zhang"
                    },
                    {
                        "name": "Liye Chen"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Yunhao Liu"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Yuanchun Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Li"
                },
                "author": "Yuanchun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15066v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15066v3",
                "updated": "2025-08-29T14:41:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    41,
                    56,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-20T18:02:50Z",
                "published_parsed": [
                    2025,
                    7,
                    20,
                    18,
                    2,
                    50,
                    6,
                    201,
                    0
                ],
                "title": "Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback"
                },
                "summary": "Time series anomaly detection is critical across various domains, yet current\napproaches often limit analysis to mere binary anomaly classification without\ndetailed categorization or further explanatory reasoning. To address these\nlimitations, we propose a novel task, Time-series Reasoning for Anomaly\n(Time-RA) that transforms classical time series anomaly detection from a\ndiscriminative into a generative, reasoning-intensive task leveraging Large\nLanguage Models (LLMs). Also, we introduce the first real-world multimodal\nbenchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,\ncomprising approximately 40,000 samples across 10 real-world domains. Each\nsample includes numeric time series data, contextual text information, and\nvisual representations, each annotated with fine-grained categories (14 types\nfor univariate anomalies and 6 for multivariate anomalies) and structured\nexplanatory reasoning. We develop a sophisticated annotation framework\nutilizing ensemble-generated labels refined through GPT-4-driven feedback,\nensuring accuracy and interpretability. Extensive benchmarking of LLMs and\nmultimodal LLMs demonstrates the capabilities and limitations of current\nmodels, highlighting the critical role of supervised fine-tuning. Our dataset\nand task pave the way for significant advancements in interpretable time series\nanomaly detection and reasoning. The code\n(https://github.com/yyysjz1997/Time-RA) and dataset\n(https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourced\nto support and accelerate future research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series anomaly detection is critical across various domains, yet current\napproaches often limit analysis to mere binary anomaly classification without\ndetailed categorization or further explanatory reasoning. To address these\nlimitations, we propose a novel task, Time-series Reasoning for Anomaly\n(Time-RA) that transforms classical time series anomaly detection from a\ndiscriminative into a generative, reasoning-intensive task leveraging Large\nLanguage Models (LLMs). Also, we introduce the first real-world multimodal\nbenchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,\ncomprising approximately 40,000 samples across 10 real-world domains. Each\nsample includes numeric time series data, contextual text information, and\nvisual representations, each annotated with fine-grained categories (14 types\nfor univariate anomalies and 6 for multivariate anomalies) and structured\nexplanatory reasoning. We develop a sophisticated annotation framework\nutilizing ensemble-generated labels refined through GPT-4-driven feedback,\nensuring accuracy and interpretability. Extensive benchmarking of LLMs and\nmultimodal LLMs demonstrates the capabilities and limitations of current\nmodels, highlighting the critical role of supervised fine-tuning. Our dataset\nand task pave the way for significant advancements in interpretable time series\nanomaly detection and reasoning. The code\n(https://github.com/yyysjz1997/Time-RA) and dataset\n(https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourced\nto support and accelerate future research in this area."
                },
                "authors": [
                    {
                        "name": "Yiyuan Yang"
                    },
                    {
                        "name": "Zichuan Liu"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Kai Ying"
                    },
                    {
                        "name": "Zhiguang Wang"
                    },
                    {
                        "name": "Tom Bamford"
                    },
                    {
                        "name": "Svitlana Vyetrenko"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "Under review. 19 pages, 8 figures, 12 tables. Code and dataset are\n  publicly available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15066v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15066v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21669v1",
                "updated": "2025-08-29T14:32:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    32,
                    48,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T14:32:48Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    32,
                    48,
                    4,
                    241,
                    0
                ],
                "title": "Cybersecurity AI: Hacking the AI Hackers via Prompt Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cybersecurity AI: Hacking the AI Hackers via Prompt Injection"
                },
                "summary": "We demonstrate how AI-powered cybersecurity tools can be turned against\nthemselves through prompt injection attacks. Prompt injection is reminiscent of\ncross-site scripting (XSS): malicious text is hidden within seemingly trusted\ncontent, and when the system processes it, that text is transformed into\nunintended instructions. When AI agents designed to find and exploit\nvulnerabilities interact with malicious web servers, carefully crafted reponses\ncan hijack their execution flow, potentially granting attackers system access.\nWe present proof-of-concept exploits against the Cybersecurity AI (CAI)\nframework and its CLI tool, and detail our mitigations against such attacks in\na multi-layered defense implementation. Our findings indicate that prompt\ninjection is a recurring and systemic issue in LLM-based architectures, one\nthat will require dedicated work to address, much as the security community has\nhad to do with XSS in traditional web applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate how AI-powered cybersecurity tools can be turned against\nthemselves through prompt injection attacks. Prompt injection is reminiscent of\ncross-site scripting (XSS): malicious text is hidden within seemingly trusted\ncontent, and when the system processes it, that text is transformed into\nunintended instructions. When AI agents designed to find and exploit\nvulnerabilities interact with malicious web servers, carefully crafted reponses\ncan hijack their execution flow, potentially granting attackers system access.\nWe present proof-of-concept exploits against the Cybersecurity AI (CAI)\nframework and its CLI tool, and detail our mitigations against such attacks in\na multi-layered defense implementation. Our findings indicate that prompt\ninjection is a recurring and systemic issue in LLM-based architectures, one\nthat will require dedicated work to address, much as the security community has\nhad to do with XSS in traditional web applications."
                },
                "authors": [
                    {
                        "name": "Víctor Mayoral-Vilches"
                    },
                    {
                        "name": "Per Mannermaa Rynning"
                    }
                ],
                "author_detail": {
                    "name": "Per Mannermaa Rynning"
                },
                "author": "Per Mannermaa Rynning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17625v3",
                "updated": "2025-08-29T14:31:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    31,
                    3,
                    4,
                    241,
                    0
                ],
                "published": "2024-04-26T15:19:58Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    15,
                    19,
                    58,
                    4,
                    117,
                    0
                ],
                "title": "Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of\n  the Land",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of\n  the Land"
                },
                "summary": "Neural networks surround us, in the form of large language models, speech\ntranscription systems, molecular discovery algorithms, robotics, and much more.\nStripped of anything else, neural networks are compositions of differentiable\nprimitives, and studying them means learning how to program and how to interact\nwith these models, a particular example of what is called differentiable\nprogramming.\n  This primer is an introduction to this fascinating field imagined for\nsomeone, like Alice, who has just ventured into this strange differentiable\nwonderland. I overview the basics of optimizing a function via automatic\ndifferentiation, and a selection of the most common designs for handling\nsequences, graphs, texts, and audios. The focus is on a intuitive,\nself-contained introduction to the most important design techniques, including\nconvolutional, attentional, and recurrent blocks, hoping to bridge the gap\nbetween theory and code (PyTorch and JAX) and leaving the reader capable of\nunderstanding some of the most advanced models out there, such as large\nlanguage models (LLMs) and multimodal architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks surround us, in the form of large language models, speech\ntranscription systems, molecular discovery algorithms, robotics, and much more.\nStripped of anything else, neural networks are compositions of differentiable\nprimitives, and studying them means learning how to program and how to interact\nwith these models, a particular example of what is called differentiable\nprogramming.\n  This primer is an introduction to this fascinating field imagined for\nsomeone, like Alice, who has just ventured into this strange differentiable\nwonderland. I overview the basics of optimizing a function via automatic\ndifferentiation, and a selection of the most common designs for handling\nsequences, graphs, texts, and audios. The focus is on a intuitive,\nself-contained introduction to the most important design techniques, including\nconvolutional, attentional, and recurrent blocks, hoping to bridge the gap\nbetween theory and code (PyTorch and JAX) and leaving the reader capable of\nunderstanding some of the most advanced models out there, such as large\nlanguage models (LLMs) and multimodal architectures."
                },
                "authors": [
                    {
                        "name": "Simone Scardapane"
                    }
                ],
                "author_detail": {
                    "name": "Simone Scardapane"
                },
                "author": "Simone Scardapane",
                "arxiv_comment": "Companion website for additional chapters:\n  https://www.sscardapane.it/alice-book",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18124v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18124v3",
                "updated": "2025-08-29T14:28:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    28,
                    32,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-25T15:32:22Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    32,
                    22,
                    0,
                    237,
                    0
                ],
                "title": "CMPhysBench: A Benchmark for Evaluating Large Language Models in\n  Condensed Matter Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMPhysBench: A Benchmark for Evaluating Large Language Models in\n  Condensed Matter Physics"
                },
                "summary": "We introduce CMPhysBench, designed to assess the proficiency of Large\nLanguage Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.\nCMPhysBench is composed of more than 520 graduate-level meticulously curated\nquestions covering both representative subfields and foundational theoretical\nframeworks of condensed matter physics, such as magnetism, superconductivity,\nstrongly correlated systems, etc. To ensure a deep understanding of the\nproblem-solving process,we focus exclusively on calculation problems, requiring\nLLMs to independently generate comprehensive solutions. Meanwhile, leveraging\ntree-based representations of expressions, we introduce the Scalable Expression\nEdit Distance (SEED) score, which provides fine-grained (non-binary) partial\ncredit and yields a more accurate assessment of similarity between prediction\nand ground-truth. Our results show that even the best models, Grok-4, reach\nonly 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a\nsignificant capability gap, especially for this practical and frontier domain\nrelative to traditional physics. The code anddataset are publicly available at\nhttps://github.com/CMPhysBench/CMPhysBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce CMPhysBench, designed to assess the proficiency of Large\nLanguage Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.\nCMPhysBench is composed of more than 520 graduate-level meticulously curated\nquestions covering both representative subfields and foundational theoretical\nframeworks of condensed matter physics, such as magnetism, superconductivity,\nstrongly correlated systems, etc. To ensure a deep understanding of the\nproblem-solving process,we focus exclusively on calculation problems, requiring\nLLMs to independently generate comprehensive solutions. Meanwhile, leveraging\ntree-based representations of expressions, we introduce the Scalable Expression\nEdit Distance (SEED) score, which provides fine-grained (non-binary) partial\ncredit and yields a more accurate assessment of similarity between prediction\nand ground-truth. Our results show that even the best models, Grok-4, reach\nonly 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a\nsignificant capability gap, especially for this practical and frontier domain\nrelative to traditional physics. The code anddataset are publicly available at\nhttps://github.com/CMPhysBench/CMPhysBench."
                },
                "authors": [
                    {
                        "name": "Weida Wang"
                    },
                    {
                        "name": "Dongchen Huang"
                    },
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Tengchao Yang"
                    },
                    {
                        "name": "Ziyang Zheng"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Dong Han"
                    },
                    {
                        "name": "Benteng Chen"
                    },
                    {
                        "name": "Binzhao Luo"
                    },
                    {
                        "name": "Zhiyu Liu"
                    },
                    {
                        "name": "Kunling Liu"
                    },
                    {
                        "name": "Zhiyuan Gao"
                    },
                    {
                        "name": "Shiqi Geng"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Jiaming Su"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Shuchen Pu"
                    },
                    {
                        "name": "Yuhan Shui"
                    },
                    {
                        "name": "Qianjia Cheng"
                    },
                    {
                        "name": "Zhihao Dou"
                    },
                    {
                        "name": "Dongfei Cui"
                    },
                    {
                        "name": "Changyong He"
                    },
                    {
                        "name": "Jin Zeng"
                    },
                    {
                        "name": "Zeke Xie"
                    },
                    {
                        "name": "Mao Su"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yunqi Cai"
                    },
                    {
                        "name": "Xi Dai"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Jinguang Cheng"
                    },
                    {
                        "name": "Zhong Fang"
                    },
                    {
                        "name": "Hongming Weng"
                    }
                ],
                "author_detail": {
                    "name": "Hongming Weng"
                },
                "author": "Hongming Weng",
                "arxiv_comment": "29 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18124v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18124v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07210v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07210v2",
                "updated": "2025-08-29T14:23:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    23,
                    2,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-10T07:10:59Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    7,
                    10,
                    59,
                    6,
                    222,
                    0
                ],
                "title": "Uncertainty-Aware Semantic Decoding for LLM-Based Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware Semantic Decoding for LLM-Based Sequential\n  Recommendation"
                },
                "summary": "Large language models have been widely applied to sequential recommendation\ntasks, yet during inference, they continue to rely on decoding strategies\ndeveloped for natural language processing. This creates a mismatch between\ntext-generation objectives and recommendation next item selection objectives.\nThis paper addresses this limitation by proposing an Uncertainty-aware Semantic\nDecoding (USD) framework that combines logit-based clustering with adaptive\nscoring to improve next-item predictions. Our approach clusters items with\nsimilar logit vectors into semantic equivalence groups, then redistributes\nprobability mass within these clusters and computes entropy across them to\ncontrol item scoring and sampling temperature during recommendation inference.\nExperiments on Amazon Product datasets (six domains) gains of 18.5\\% in HR@3,\n11.9\\% in NDCG@3, and 10.8\\% in MRR@3 compared to state-of-the-art baselines.\nHyperparameter analysis confirms the optimal parameters among various settings,\nand experiments on H\\&M, and Netflix datasets indicate that the framework can\nadapt to differing recommendation domains. The experimental results confirm\nthat integrating semantic clustering and uncertainty assessment yields more\nreliable and accurate recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been widely applied to sequential recommendation\ntasks, yet during inference, they continue to rely on decoding strategies\ndeveloped for natural language processing. This creates a mismatch between\ntext-generation objectives and recommendation next item selection objectives.\nThis paper addresses this limitation by proposing an Uncertainty-aware Semantic\nDecoding (USD) framework that combines logit-based clustering with adaptive\nscoring to improve next-item predictions. Our approach clusters items with\nsimilar logit vectors into semantic equivalence groups, then redistributes\nprobability mass within these clusters and computes entropy across them to\ncontrol item scoring and sampling temperature during recommendation inference.\nExperiments on Amazon Product datasets (six domains) gains of 18.5\\% in HR@3,\n11.9\\% in NDCG@3, and 10.8\\% in MRR@3 compared to state-of-the-art baselines.\nHyperparameter analysis confirms the optimal parameters among various settings,\nand experiments on H\\&M, and Netflix datasets indicate that the framework can\nadapt to differing recommendation domains. The experimental results confirm\nthat integrating semantic clustering and uncertainty assessment yields more\nreliable and accurate recommendations."
                },
                "authors": [
                    {
                        "name": "Chenke Yin"
                    },
                    {
                        "name": "Li Fan"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Dongxiao Hu"
                    },
                    {
                        "name": "Haichao Zhang"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yang Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Xiang"
                },
                "author": "Yang Xiang",
                "arxiv_comment": "Accepted by APWeb 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07210v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07210v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12208v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12208v2",
                "updated": "2025-08-29T14:16:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    16,
                    16,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-18T02:49:47Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    2,
                    49,
                    47,
                    6,
                    138,
                    0
                ],
                "title": "Improved Bounds and Global Fit of Flavor-Violating Charged Lepton Yukawa\n  Couplings post LHC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Bounds and Global Fit of Flavor-Violating Charged Lepton Yukawa\n  Couplings post LHC"
                },
                "summary": "Higgs couplings to charged leptons form an important measurement to\nunderstand not only the Standard Model (SM), but also physics Beyond Standard\nModels (BSM). In this work, we update the bounds on the Flavor-Violating (FV)\nHiggs couplings to charged leptons. We find that the bounds on the size of the\ncouplings could range between $\\sim \\mathcal{O}(10^{-3}) -\n\\mathcal{O}(10^{-6})$. In fact, the direct constraints from LHC are much\nstronger than those inferred indirectly from rare decays in the $\\tau$-$\\mu$\nand $\\tau$-$e$ sector. We also match these bounds to the SM Effective Field\nTheory (SMEFT) and find lower limits on the scale of New Physics (NP). We find\nthat the scale of NP ranges between $\\sim \\mathcal{O}(10) - \\mathcal{O}(100)$\nTeV. We also present future projections for some upcoming experiments. We find\nthat the current bounds on the couplings to $\\mu$-$e$ are stronger than all\nfuture projections.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Higgs couplings to charged leptons form an important measurement to\nunderstand not only the Standard Model (SM), but also physics Beyond Standard\nModels (BSM). In this work, we update the bounds on the Flavor-Violating (FV)\nHiggs couplings to charged leptons. We find that the bounds on the size of the\ncouplings could range between $\\sim \\mathcal{O}(10^{-3}) -\n\\mathcal{O}(10^{-6})$. In fact, the direct constraints from LHC are much\nstronger than those inferred indirectly from rare decays in the $\\tau$-$\\mu$\nand $\\tau$-$e$ sector. We also match these bounds to the SM Effective Field\nTheory (SMEFT) and find lower limits on the scale of New Physics (NP). We find\nthat the scale of NP ranges between $\\sim \\mathcal{O}(10) - \\mathcal{O}(100)$\nTeV. We also present future projections for some upcoming experiments. We find\nthat the current bounds on the couplings to $\\mu$-$e$ are stronger than all\nfuture projections."
                },
                "authors": [
                    {
                        "name": "Fayez Abu-Ajamieh"
                    },
                    {
                        "name": "Suman Kumbhakar"
                    },
                    {
                        "name": "Ratan Sarkar"
                    },
                    {
                        "name": "Sudhir Vempati"
                    }
                ],
                "author_detail": {
                    "name": "Sudhir Vempati"
                },
                "author": "Sudhir Vempati",
                "arxiv_comment": "17 pages, 2 figures, Accepted for publication in EPJC",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12208v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12208v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21649v1",
                "updated": "2025-08-29T14:14:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    14,
                    3,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T14:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    14,
                    3,
                    4,
                    241,
                    0
                ],
                "title": "NExON-Bayes: A Bayesian approach to network estimation informed by\n  ordinal covariates",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NExON-Bayes: A Bayesian approach to network estimation informed by\n  ordinal covariates"
                },
                "summary": "In heterogeneous disease settings, accounting for intrinsic sample\nvariability is crucial for obtaining reliable and interpretable omic network\nestimates. However, most graphical model analyses of biomedical data assume\nhomogeneous conditional dependence structures, potentially leading to\nmisleading conclusions. To address this, we propose a joint Gaussian graphical\nmodel that leverages sample-level ordinal covariates (e.g., disease stage) to\naccount for heterogeneity and improve the estimation of partial correlation\nstructures. Our modelling framework, called NExON-Bayes, extends the graphical\nspike-and-slab framework to account for ordinal covariates, jointly estimating\ntheir relevance to the graph structure and leveraging them to improve the\naccuracy of network estimation. To scale to high-dimensional omic settings, we\ndevelop an efficient variational inference algorithm tailored to our model.\nThrough simulations, we demonstrate that our method outperforms the vanilla\ngraphical spike-and-slab (with no covariate information), as well as other\nstate-of-the-art network approaches which exploit covariate information.\nApplying our method to reverse phase protein array data from patients diagnosed\nwith stage I, II or III breast carcinoma, we estimate the behaviour of\nproteomic networks as breast carcinoma progresses. Our model provides insights\nnot only through inspection of the estimated proteomic networks, but also of\nthe estimated ordinal covariate dependencies of key groups of proteins within\nthose networks, offering a comprehensive understanding of how biological\npathways shift across disease stages.\n  Availability and Implementation: A user-friendly R package for NExON-Bayes\nwith tutorials is available on Github at github.com/jf687/NExON.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In heterogeneous disease settings, accounting for intrinsic sample\nvariability is crucial for obtaining reliable and interpretable omic network\nestimates. However, most graphical model analyses of biomedical data assume\nhomogeneous conditional dependence structures, potentially leading to\nmisleading conclusions. To address this, we propose a joint Gaussian graphical\nmodel that leverages sample-level ordinal covariates (e.g., disease stage) to\naccount for heterogeneity and improve the estimation of partial correlation\nstructures. Our modelling framework, called NExON-Bayes, extends the graphical\nspike-and-slab framework to account for ordinal covariates, jointly estimating\ntheir relevance to the graph structure and leveraging them to improve the\naccuracy of network estimation. To scale to high-dimensional omic settings, we\ndevelop an efficient variational inference algorithm tailored to our model.\nThrough simulations, we demonstrate that our method outperforms the vanilla\ngraphical spike-and-slab (with no covariate information), as well as other\nstate-of-the-art network approaches which exploit covariate information.\nApplying our method to reverse phase protein array data from patients diagnosed\nwith stage I, II or III breast carcinoma, we estimate the behaviour of\nproteomic networks as breast carcinoma progresses. Our model provides insights\nnot only through inspection of the estimated proteomic networks, but also of\nthe estimated ordinal covariate dependencies of key groups of proteins within\nthose networks, offering a comprehensive understanding of how biological\npathways shift across disease stages.\n  Availability and Implementation: A user-friendly R package for NExON-Bayes\nwith tutorials is available on Github at github.com/jf687/NExON."
                },
                "authors": [
                    {
                        "name": "Joseph Feest"
                    },
                    {
                        "name": "Hélène Ruffieux"
                    },
                    {
                        "name": "Camilla Lingjærde"
                    },
                    {
                        "name": "Xiaoyue Xi"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoyue Xi"
                },
                "author": "Xiaoyue Xi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23593v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23593v2",
                "updated": "2025-08-29T14:04:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    4,
                    49,
                    4,
                    241,
                    0
                ],
                "published": "2025-03-30T21:15:11Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    21,
                    15,
                    11,
                    6,
                    89,
                    0
                ],
                "title": "Measurement-induced back-action in a QD-based coherent spin-photon\n  interface",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement-induced back-action in a QD-based coherent spin-photon\n  interface"
                },
                "summary": "Polarization-encoded spin-photon interfaces constitute promising candidates\nfor the development of stationary nodes used as photon receivers, for quantum\ncommunication and distributed quantum computing. Here we introduce a\ntime-resolved tomography approach which allows observing the dynamics of an\nelectron spin, in a semiconductor quantum dot, mapped onto the dynamics of the\npolarization state of reflected photons. Through a single tomography\nexperiment, we infer all the relevant spin dynamics timescales, including\nprecession, decoherence and relaxation times. We also demonstrate and quantify\nthe measurement back-action induced, on the embedded spin qubit, by the\ndetection of a single reflected photon. We show that the induced population and\ncoherence of the spin state can be tuned by the chosen polarization basis of\nthe measurement. The control of the photon-induced back-action on the embedded\nspin qubit constitutes a crucial requirement for the use of spin-photon\ninterfaces as quantum receivers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polarization-encoded spin-photon interfaces constitute promising candidates\nfor the development of stationary nodes used as photon receivers, for quantum\ncommunication and distributed quantum computing. Here we introduce a\ntime-resolved tomography approach which allows observing the dynamics of an\nelectron spin, in a semiconductor quantum dot, mapped onto the dynamics of the\npolarization state of reflected photons. Through a single tomography\nexperiment, we infer all the relevant spin dynamics timescales, including\nprecession, decoherence and relaxation times. We also demonstrate and quantify\nthe measurement back-action induced, on the embedded spin qubit, by the\ndetection of a single reflected photon. We show that the induced population and\ncoherence of the spin state can be tuned by the chosen polarization basis of\nthe measurement. The control of the photon-induced back-action on the embedded\nspin qubit constitutes a crucial requirement for the use of spin-photon\ninterfaces as quantum receivers."
                },
                "authors": [
                    {
                        "name": "Adrià Medeiros"
                    },
                    {
                        "name": "Manuel Gundín"
                    },
                    {
                        "name": "Dario A. Fioretto"
                    },
                    {
                        "name": "Vincent Vinel"
                    },
                    {
                        "name": "Eliott Rambeau"
                    },
                    {
                        "name": "Elham Mehdi"
                    },
                    {
                        "name": "Niccolo Somaschi"
                    },
                    {
                        "name": "Aristide Lemaître"
                    },
                    {
                        "name": "Isabelle Sagnes"
                    },
                    {
                        "name": "Nadia Belabas"
                    },
                    {
                        "name": "Olivier Krebs"
                    },
                    {
                        "name": "Pascale Senellart"
                    },
                    {
                        "name": "Loïc Lanco"
                    }
                ],
                "author_detail": {
                    "name": "Loïc Lanco"
                },
                "author": "Loïc Lanco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23593v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23593v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21634v1",
                "updated": "2025-08-29T13:51:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    51,
                    28,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T13:51:28Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    51,
                    28,
                    4,
                    241,
                    0
                ],
                "title": "Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects,\n  Vulnerabilities, and Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects,\n  Vulnerabilities, and Complexity"
                },
                "summary": "As AI code assistants become increasingly integrated into software\ndevelopment workflows, understanding how their code compares to human-written\nprograms is critical for ensuring reliability, maintainability, and security.\nIn this paper, we present a large-scale comparison of code authored by human\ndevelopers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and\nQwen-Coder, on multiple dimensions of software quality: code defects, security\nvulnerabilities, and structural complexity. Our evaluation spans over 500k code\nsamples in two widely used languages, Python and Java, classifying defects via\nOrthogonal Defect Classification and security vulnerabilities using the Common\nWeakness Enumeration. We find that AI-generated code is generally simpler and\nmore repetitive, yet more prone to unused constructs and hardcoded debugging,\nwhile human-written code exhibits greater structural complexity and a higher\nconcentration of maintainability issues. Notably, AI-generated code also\ncontains more high-risk security vulnerabilities. These findings highlight the\ndistinct defect profiles of AI- and human-authored code and underscore the need\nfor specialized quality assurance practices in AI-assisted programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI code assistants become increasingly integrated into software\ndevelopment workflows, understanding how their code compares to human-written\nprograms is critical for ensuring reliability, maintainability, and security.\nIn this paper, we present a large-scale comparison of code authored by human\ndevelopers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and\nQwen-Coder, on multiple dimensions of software quality: code defects, security\nvulnerabilities, and structural complexity. Our evaluation spans over 500k code\nsamples in two widely used languages, Python and Java, classifying defects via\nOrthogonal Defect Classification and security vulnerabilities using the Common\nWeakness Enumeration. We find that AI-generated code is generally simpler and\nmore repetitive, yet more prone to unused constructs and hardcoded debugging,\nwhile human-written code exhibits greater structural complexity and a higher\nconcentration of maintainability issues. Notably, AI-generated code also\ncontains more high-risk security vulnerabilities. These findings highlight the\ndistinct defect profiles of AI- and human-authored code and underscore the need\nfor specialized quality assurance practices in AI-assisted programming."
                },
                "authors": [
                    {
                        "name": "Domenico Cotroneo"
                    },
                    {
                        "name": "Cristina Improta"
                    },
                    {
                        "name": "Pietro Liguori"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Liguori"
                },
                "author": "Pietro Liguori",
                "arxiv_comment": "Accepted to the 36th IEEE International Symposium on Software\n  Reliability Engineering (ISSRE, 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21632v1",
                "updated": "2025-08-29T13:47:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    47,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T13:47:22Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    47,
                    22,
                    4,
                    241,
                    0
                ],
                "title": "QZhou-Embedding Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QZhou-Embedding Technical Report"
                },
                "summary": "We present QZhou-Embedding, a general-purpose contextual text embedding model\nwith exceptional text representation capabilities. Built upon the\nQwen2.5-7B-Instruct foundation model, we designed a unified multi-task\nframework comprising specialized data transformation and training strategies.\nThe data transformation scheme enables the incorporation of more diverse\ntextual training datasets, while the task-specific training strategies enhance\nmodel learning efficiency. We developed a data synthesis pipeline leveraging\nLLM API, incorporating techniques such as paraphrasing, augmentation, and hard\nnegative example generation to improve the semantic richness and sample\ndifficulty of the training set. Additionally, we employ a two-stage training\nstrategy, comprising initial retrieval-focused pretraining followed by\nfull-task fine-tuning, enabling the embedding model to extend its capabilities\nbased on robust retrieval performance. Our model achieves state-of-the-art\nresults on the MTEB and CMTEB benchmarks, ranking first on both leaderboards\n(August 27 2025), and simultaneously achieves state-of-the-art performance on\ntasks including reranking, clustering, etc. Our findings demonstrate that\nhigher-quality, more diverse data is crucial for advancing retrieval model\nperformance, and that leveraging LLMs generative capabilities can further\noptimize data quality for embedding model breakthroughs. Our model weights are\nreleased on HuggingFace under Apache 2.0 license. For reproducibility, we\nprovide evaluation code and instructions on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present QZhou-Embedding, a general-purpose contextual text embedding model\nwith exceptional text representation capabilities. Built upon the\nQwen2.5-7B-Instruct foundation model, we designed a unified multi-task\nframework comprising specialized data transformation and training strategies.\nThe data transformation scheme enables the incorporation of more diverse\ntextual training datasets, while the task-specific training strategies enhance\nmodel learning efficiency. We developed a data synthesis pipeline leveraging\nLLM API, incorporating techniques such as paraphrasing, augmentation, and hard\nnegative example generation to improve the semantic richness and sample\ndifficulty of the training set. Additionally, we employ a two-stage training\nstrategy, comprising initial retrieval-focused pretraining followed by\nfull-task fine-tuning, enabling the embedding model to extend its capabilities\nbased on robust retrieval performance. Our model achieves state-of-the-art\nresults on the MTEB and CMTEB benchmarks, ranking first on both leaderboards\n(August 27 2025), and simultaneously achieves state-of-the-art performance on\ntasks including reranking, clustering, etc. Our findings demonstrate that\nhigher-quality, more diverse data is crucial for advancing retrieval model\nperformance, and that leveraging LLMs generative capabilities can further\noptimize data quality for embedding model breakthroughs. Our model weights are\nreleased on HuggingFace under Apache 2.0 license. For reproducibility, we\nprovide evaluation code and instructions on GitHub."
                },
                "authors": [
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "En Xu"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Haibiao Chen"
                    },
                    {
                        "name": "Yinfei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yinfei Xu"
                },
                "author": "Yinfei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19563v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19563v2",
                "updated": "2025-08-29T13:46:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    46,
                    29,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-27T04:46:05Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    4,
                    46,
                    5,
                    2,
                    239,
                    0
                ],
                "title": "Robustness is Important: Limitations of LLMs for Data Fitting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness is Important: Limitations of LLMs for Data Fitting"
                },
                "summary": "Large Language Models (LLMs) are being applied in a wide array of settings,\nwell beyond the typical language-oriented use cases. In particular, LLMs are\nincreasingly used as a plug-and-play method for fitting data and generating\npredictions. Prior work has shown that LLMs, via in-context learning or\nsupervised fine-tuning, can perform competitively with many tabular supervised\nlearning techniques in terms of predictive performance. However, we identify a\ncritical vulnerability of using LLMs for data fitting -- making changes to data\nrepresentation that are completely irrelevant to the underlying learning task\ncan drastically alter LLMs' predictions on the same data. For example, simply\nchanging variable names can sway the size of prediction error by as much as 82%\nin certain settings. Such prediction sensitivity with respect to\ntask-irrelevant variations manifests under both in-context learning and\nsupervised fine-tuning, for both close-weight and open-weight general-purpose\nLLMs. Moreover, by examining the attention scores of an open-weight LLM, we\ndiscover a non-uniform attention pattern: training examples and variable\nnames/values which happen to occupy certain positions in the prompt receive\nmore attention when output tokens are generated, even though different\npositions are expected to receive roughly the same attention. This partially\nexplains the sensitivity in the presence of task-irrelevant variations. We also\nconsider a state-of-the-art tabular foundation model (TabPFN) trained\nspecifically for data fitting. Despite being explicitly designed to achieve\nprediction robustness, TabPFN is still not immune to task-irrelevant\nvariations. Overall, despite LLMs' impressive predictive capabilities,\ncurrently they lack even the basic level of robustness to be used as a\nprincipled data-fitting tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are being applied in a wide array of settings,\nwell beyond the typical language-oriented use cases. In particular, LLMs are\nincreasingly used as a plug-and-play method for fitting data and generating\npredictions. Prior work has shown that LLMs, via in-context learning or\nsupervised fine-tuning, can perform competitively with many tabular supervised\nlearning techniques in terms of predictive performance. However, we identify a\ncritical vulnerability of using LLMs for data fitting -- making changes to data\nrepresentation that are completely irrelevant to the underlying learning task\ncan drastically alter LLMs' predictions on the same data. For example, simply\nchanging variable names can sway the size of prediction error by as much as 82%\nin certain settings. Such prediction sensitivity with respect to\ntask-irrelevant variations manifests under both in-context learning and\nsupervised fine-tuning, for both close-weight and open-weight general-purpose\nLLMs. Moreover, by examining the attention scores of an open-weight LLM, we\ndiscover a non-uniform attention pattern: training examples and variable\nnames/values which happen to occupy certain positions in the prompt receive\nmore attention when output tokens are generated, even though different\npositions are expected to receive roughly the same attention. This partially\nexplains the sensitivity in the presence of task-irrelevant variations. We also\nconsider a state-of-the-art tabular foundation model (TabPFN) trained\nspecifically for data fitting. Despite being explicitly designed to achieve\nprediction robustness, TabPFN is still not immune to task-irrelevant\nvariations. Overall, despite LLMs' impressive predictive capabilities,\ncurrently they lack even the basic level of robustness to be used as a\nprincipled data-fitting tool."
                },
                "authors": [
                    {
                        "name": "Hejia Liu"
                    },
                    {
                        "name": "Mochen Yang"
                    },
                    {
                        "name": "Gediminas Adomavicius"
                    }
                ],
                "author_detail": {
                    "name": "Gediminas Adomavicius"
                },
                "author": "Gediminas Adomavicius",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19563v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19563v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21628v1",
                "updated": "2025-08-29T13:42:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    42,
                    26,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T13:42:26Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    42,
                    26,
                    4,
                    241,
                    0
                ],
                "title": "Personality Matters: User Traits Predict LLM Preferences in Multi-Turn\n  Collaborative Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personality Matters: User Traits Predict LLM Preferences in Multi-Turn\n  Collaborative Tasks"
                },
                "summary": "As Large Language Models (LLMs) increasingly integrate into everyday\nworkflows, where users shape outcomes through multi-turn collaboration, a\ncritical question emerges: do users with different personality traits\nsystematically prefer certain LLMs over others? We conducted a study with 32\nparticipants evenly distributed across four Keirsey personality types,\nevaluating their interactions with GPT-4 and Claude 3.5 across four\ncollaborative tasks: data analysis, creative writing, information retrieval,\nand writing assistance. Results revealed significant personality-driven\npreferences: Rationals strongly preferred GPT-4, particularly for goal-oriented\ntasks, while idealists favored Claude 3.5, especially for creative and\nanalytical tasks. Other personality types showed task-dependent preferences.\nSentiment analysis of qualitative feedback confirmed these patterns. Notably,\naggregate helpfulness ratings were similar across models, showing how\npersonality-based analysis reveals LLM differences that traditional evaluations\nmiss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) increasingly integrate into everyday\nworkflows, where users shape outcomes through multi-turn collaboration, a\ncritical question emerges: do users with different personality traits\nsystematically prefer certain LLMs over others? We conducted a study with 32\nparticipants evenly distributed across four Keirsey personality types,\nevaluating their interactions with GPT-4 and Claude 3.5 across four\ncollaborative tasks: data analysis, creative writing, information retrieval,\nand writing assistance. Results revealed significant personality-driven\npreferences: Rationals strongly preferred GPT-4, particularly for goal-oriented\ntasks, while idealists favored Claude 3.5, especially for creative and\nanalytical tasks. Other personality types showed task-dependent preferences.\nSentiment analysis of qualitative feedback confirmed these patterns. Notably,\naggregate helpfulness ratings were similar across models, showing how\npersonality-based analysis reveals LLM differences that traditional evaluations\nmiss."
                },
                "authors": [
                    {
                        "name": "Sarfaroz Yunusov"
                    },
                    {
                        "name": "Kaige Chen"
                    },
                    {
                        "name": "Kazi Nishat Anwar"
                    },
                    {
                        "name": "Ali Emami"
                    }
                ],
                "author_detail": {
                    "name": "Ali Emami"
                },
                "author": "Ali Emami",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21622v1",
                "updated": "2025-08-29T13:34:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    34,
                    55,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T13:34:55Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    34,
                    55,
                    4,
                    241,
                    0
                ],
                "title": "Integrating Large Language Models with Network Optimization for\n  Interactive and Explainable Supply Chain Planning: A Real-World Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models with Network Optimization for\n  Interactive and Explainable Supply Chain Planning: A Real-World Case Study"
                },
                "summary": "This paper presents an integrated framework that combines traditional network\noptimization models with large language models (LLMs) to deliver interactive,\nexplainable, and role-aware decision support for supply chain planning. The\nproposed system bridges the gap between complex operations research outputs and\nbusiness stakeholder understanding by generating natural language summaries,\ncontextual visualizations, and tailored key performance indicators (KPIs). The\ncore optimization model addresses tactical inventory redistribution across a\nnetwork of distribution centers for multi-period and multi-item, using a\nmixed-integer formulation. The technical architecture incorporates AI agents,\nRESTful APIs, and a dynamic user interface to support real-time interaction,\nconfiguration updates, and simulation-based insights. A case study demonstrates\nhow the system improves planning outcomes by preventing stockouts, reducing\ncosts, and maintaining service levels. Future extensions include integrating\nprivate LLMs, transfer learning, reinforcement learning, and Bayesian neural\nnetworks to enhance explainability, adaptability, and real-time\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an integrated framework that combines traditional network\noptimization models with large language models (LLMs) to deliver interactive,\nexplainable, and role-aware decision support for supply chain planning. The\nproposed system bridges the gap between complex operations research outputs and\nbusiness stakeholder understanding by generating natural language summaries,\ncontextual visualizations, and tailored key performance indicators (KPIs). The\ncore optimization model addresses tactical inventory redistribution across a\nnetwork of distribution centers for multi-period and multi-item, using a\nmixed-integer formulation. The technical architecture incorporates AI agents,\nRESTful APIs, and a dynamic user interface to support real-time interaction,\nconfiguration updates, and simulation-based insights. A case study demonstrates\nhow the system improves planning outcomes by preventing stockouts, reducing\ncosts, and maintaining service levels. Future extensions include integrating\nprivate LLMs, transfer learning, reinforcement learning, and Bayesian neural\nnetworks to enhance explainability, adaptability, and real-time\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Saravanan Venkatachalam"
                    }
                ],
                "author_detail": {
                    "name": "Saravanan Venkatachalam"
                },
                "author": "Saravanan Venkatachalam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17464v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17464v3",
                "updated": "2025-08-29T13:34:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    34,
                    45,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-23T04:45:37Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    4,
                    45,
                    37,
                    4,
                    143,
                    0
                ],
                "title": "Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. Current hybrid RAG system retrieves evidence\nfrom both knowledge graphs (KGs) and text documents to support LLM reasoning.\nHowever, it faces challenges like handling multi-hop reasoning, multi-entity\nquestions, multi-source verification, and effective graph utilization. To\naddress these limitations, we present Hydra, a training-free framework that\nunifies graph topology, document semantics, and source reliability to support\ndeep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity\nproblems through agent-driven exploration that combines structured and\nunstructured retrieval, increasing both diversity and precision of evidence. To\ntackle multi-source verification, Hydra uses a tri-factor cross-source\nverification (source trustworthiness assessment, cross-source corroboration,\nand entity-path alignment), to balance topic relevance with cross-modal\nagreement. By leveraging graph structure, Hydra fuses heterogeneous sources,\nguides efficient exploration, and prunes noise early. Comprehensive experiments\non seven benchmark datasets show that Hydra achieves overall state-of-the-art\nresults on all benchmarks with GPT-3.5, outperforming the strong hybrid\nbaseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra\nenables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance\ncomparable to that of GPT-4-Turbo. The source code is available on\nhttps://stevetantan.github.io/Hydra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. Current hybrid RAG system retrieves evidence\nfrom both knowledge graphs (KGs) and text documents to support LLM reasoning.\nHowever, it faces challenges like handling multi-hop reasoning, multi-entity\nquestions, multi-source verification, and effective graph utilization. To\naddress these limitations, we present Hydra, a training-free framework that\nunifies graph topology, document semantics, and source reliability to support\ndeep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity\nproblems through agent-driven exploration that combines structured and\nunstructured retrieval, increasing both diversity and precision of evidence. To\ntackle multi-source verification, Hydra uses a tri-factor cross-source\nverification (source trustworthiness assessment, cross-source corroboration,\nand entity-path alignment), to balance topic relevance with cross-modal\nagreement. By leveraging graph structure, Hydra fuses heterogeneous sources,\nguides efficient exploration, and prunes noise early. Comprehensive experiments\non seven benchmark datasets show that Hydra achieves overall state-of-the-art\nresults on all benchmarks with GPT-3.5, outperforming the strong hybrid\nbaseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra\nenables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance\ncomparable to that of GPT-4-Turbo. The source code is available on\nhttps://stevetantan.github.io/Hydra/."
                },
                "authors": [
                    {
                        "name": "Xingyu Tan"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Qing Liu"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Liming Zhu"
                    },
                    {
                        "name": "Wenjie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Zhang"
                },
                "author": "Wenjie Zhang",
                "arxiv_comment": "Accepted by EMNLP2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17464v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17464v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21594v1",
                "updated": "2025-08-29T12:50:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    50,
                    4,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T12:50:04Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    50,
                    4,
                    4,
                    241,
                    0
                ],
                "title": "Quantum Sequential Universal Hypothesis Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Sequential Universal Hypothesis Testing"
                },
                "summary": "Quantum hypothesis testing (QHT) concerns the statistical inference of\nunknown quantum states. In the general setting of composite hypotheses, the\ngoal of QHT is to determine whether an unknown quantum state belongs to one or\nanother of two classes of states based on the measurement of a number of copies\nof the state. Prior art on QHT with composite hypotheses focused on a\nfixed-copy two-step protocol, with state estimation followed by an optimized\njoint measurement. However, this fixed-copy approach may be inefficient, using\nthe same number of copies irrespective of the inherent difficulty of the\ntesting task. To address these limitations, we introduce the quantum sequential\nuniversal test (QSUT), a novel framework for sequential QHT in the general case\nof composite hypotheses. QSUT builds on universal inference, and it alternates\nbetween adaptive local measurements aimed at exploring the hypothesis space and\njoint measurements optimized for maximal discrimination. QSUT is proven to\nrigorously control the type I error under minimal assumptions about the\nhypothesis structure. We present two practical instantiations of QSUT, one\nbased on the Helstrom-Holevo test and one leveraging shallow variational\nquantum circuits. Empirical results across a range of composite QHT tasks\ndemonstrate that QSUT consistently reduces copy complexity relative to\nstate-of-the-art fixed-copy strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum hypothesis testing (QHT) concerns the statistical inference of\nunknown quantum states. In the general setting of composite hypotheses, the\ngoal of QHT is to determine whether an unknown quantum state belongs to one or\nanother of two classes of states based on the measurement of a number of copies\nof the state. Prior art on QHT with composite hypotheses focused on a\nfixed-copy two-step protocol, with state estimation followed by an optimized\njoint measurement. However, this fixed-copy approach may be inefficient, using\nthe same number of copies irrespective of the inherent difficulty of the\ntesting task. To address these limitations, we introduce the quantum sequential\nuniversal test (QSUT), a novel framework for sequential QHT in the general case\nof composite hypotheses. QSUT builds on universal inference, and it alternates\nbetween adaptive local measurements aimed at exploring the hypothesis space and\njoint measurements optimized for maximal discrimination. QSUT is proven to\nrigorously control the type I error under minimal assumptions about the\nhypothesis structure. We present two practical instantiations of QSUT, one\nbased on the Helstrom-Holevo test and one leveraging shallow variational\nquantum circuits. Empirical results across a range of composite QHT tasks\ndemonstrate that QSUT consistently reduces copy complexity relative to\nstate-of-the-art fixed-copy strategies."
                },
                "authors": [
                    {
                        "name": "Matteo Zecchin"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    },
                    {
                        "name": "Aaditya Ramdas"
                    }
                ],
                "author_detail": {
                    "name": "Aaditya Ramdas"
                },
                "author": "Aaditya Ramdas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21589v1",
                "updated": "2025-08-29T12:47:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    47,
                    27,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T12:47:27Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    47,
                    27,
                    4,
                    241,
                    0
                ],
                "title": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM\n  Fine-Tuning via Closed-Loop Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM\n  Fine-Tuning via Closed-Loop Learning"
                },
                "summary": "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our \\method consistently enhances the quality of\nseed data and boosts LLM's performance with improving accuracy by 7.15% on\naverage while maintaining the original dataset scale. This work establishes a\nnew paradigm for sustainable LLM training through dynamic human-AI co-evolution\nof data and models. Our datasets, models, and code are coming soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our \\method consistently enhances the quality of\nseed data and boosts LLM's performance with improving accuracy by 7.15% on\naverage while maintaining the original dataset scale. This work establishes a\nnew paradigm for sustainable LLM training through dynamic human-AI co-evolution\nof data and models. Our datasets, models, and code are coming soon."
                },
                "authors": [
                    {
                        "name": "Zinan Tang"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Qizhi Pei"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Mengzhang Cai"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Lijun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Wu"
                },
                "author": "Lijun Wu",
                "arxiv_comment": "Accepted by EMNLP 2025 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21587v1",
                "updated": "2025-08-29T12:43:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    43,
                    6,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T12:43:06Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    43,
                    6,
                    4,
                    241,
                    0
                ],
                "title": "A Survey on Current Trends and Recent Advances in Text Anonymization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Current Trends and Recent Advances in Text Anonymization"
                },
                "summary": "The proliferation of textual data containing sensitive personal information\nacross various domains requires robust anonymization techniques to protect\nprivacy and comply with regulations, while preserving data usability for\ndiverse and crucial downstream tasks. This survey provides a comprehensive\noverview of current trends and recent advances in text anonymization\ntechniques. We begin by discussing foundational approaches, primarily centered\non Named Entity Recognition, before examining the transformative impact of\nLarge Language Models, detailing their dual role as sophisticated anonymizers\nand potent de-anonymization threats. The survey further explores\ndomain-specific challenges and tailored solutions in critical sectors such as\nhealthcare, law, finance, and education. We investigate advanced methodologies\nincorporating formal privacy models and risk-aware frameworks, and address the\nspecialized subfield of authorship anonymization. Additionally, we review\nevaluation frameworks, comprehensive metrics, benchmarks, and practical\ntoolkits for real-world deployment of anonymization solutions. This review\nconsolidates current knowledge, identifies emerging trends and persistent\nchallenges, including the evolving privacy-utility trade-off, the need to\naddress quasi-identifiers, and the implications of LLM capabilities, and aims\nto guide future research directions for both academics and practitioners in\nthis field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of textual data containing sensitive personal information\nacross various domains requires robust anonymization techniques to protect\nprivacy and comply with regulations, while preserving data usability for\ndiverse and crucial downstream tasks. This survey provides a comprehensive\noverview of current trends and recent advances in text anonymization\ntechniques. We begin by discussing foundational approaches, primarily centered\non Named Entity Recognition, before examining the transformative impact of\nLarge Language Models, detailing their dual role as sophisticated anonymizers\nand potent de-anonymization threats. The survey further explores\ndomain-specific challenges and tailored solutions in critical sectors such as\nhealthcare, law, finance, and education. We investigate advanced methodologies\nincorporating formal privacy models and risk-aware frameworks, and address the\nspecialized subfield of authorship anonymization. Additionally, we review\nevaluation frameworks, comprehensive metrics, benchmarks, and practical\ntoolkits for real-world deployment of anonymization solutions. This review\nconsolidates current knowledge, identifies emerging trends and persistent\nchallenges, including the evolving privacy-utility trade-off, the need to\naddress quasi-identifiers, and the implications of LLM capabilities, and aims\nto guide future research directions for both academics and practitioners in\nthis field."
                },
                "authors": [
                    {
                        "name": "Tobias Deußer"
                    },
                    {
                        "name": "Lorenz Sparrenberg"
                    },
                    {
                        "name": "Armin Berger"
                    },
                    {
                        "name": "Max Hahnbück"
                    },
                    {
                        "name": "Christian Bauckhage"
                    },
                    {
                        "name": "Rafet Sifa"
                    }
                ],
                "author_detail": {
                    "name": "Rafet Sifa"
                },
                "author": "Rafet Sifa",
                "arxiv_comment": "Accepted at IEEE DSAA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21583v1",
                "updated": "2025-08-29T12:34:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    34,
                    54,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T12:34:54Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    34,
                    54,
                    4,
                    241,
                    0
                ],
                "title": "Treatment effects at the margin: Everyone is marginal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Treatment effects at the margin: Everyone is marginal"
                },
                "summary": "This paper develops a framework for identifying treatment effects when a\npolicy simultaneously alters both the incentive to participate and the outcome\nof interest -- such as hiring decisions and wages in response to employment\nsubsidies; or working decisions and wages in response to job trainings. This\nframework was inspired by my PhD project on a Belgian reform that subsidised\nfirst-time hiring, inducing entry by marginal firms yet meanwhile changing the\nwages they pay. Standard methods addressing selection-into-treatment concepts\n(like Heckman selection equations and local average treatment effects), or\nbefore-after comparisons (including simple DiD or RDD), cannot isolate effects\nat this shifting margin where treatment defines who is observed. I introduce\nmarginality-weighted estimands that recover causal effects among policy-induced\nentrants, offering a policy-relevant alternative in settings with endogenous\nselection. This method can thus be applied widely to understanding the economic\nimpacts of public programmes, especially in fields largely relying on\nreduced-form causal inference estimation (e.g. labour economics, development\neconomics, health economics).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops a framework for identifying treatment effects when a\npolicy simultaneously alters both the incentive to participate and the outcome\nof interest -- such as hiring decisions and wages in response to employment\nsubsidies; or working decisions and wages in response to job trainings. This\nframework was inspired by my PhD project on a Belgian reform that subsidised\nfirst-time hiring, inducing entry by marginal firms yet meanwhile changing the\nwages they pay. Standard methods addressing selection-into-treatment concepts\n(like Heckman selection equations and local average treatment effects), or\nbefore-after comparisons (including simple DiD or RDD), cannot isolate effects\nat this shifting margin where treatment defines who is observed. I introduce\nmarginality-weighted estimands that recover causal effects among policy-induced\nentrants, offering a policy-relevant alternative in settings with endogenous\nselection. This method can thus be applied widely to understanding the economic\nimpacts of public programmes, especially in fields largely relying on\nreduced-form causal inference estimation (e.g. labour economics, development\neconomics, health economics)."
                },
                "authors": [
                    {
                        "name": "Haotian Deng"
                    }
                ],
                "author_detail": {
                    "name": "Haotian Deng"
                },
                "author": "Haotian Deng",
                "arxiv_comment": "13 pages; 2 figures; this is the note for my poster presentation at\n  the Royal Statistical Society 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21565v1",
                "updated": "2025-08-29T12:21:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    21,
                    57,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T12:21:57Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    21,
                    57,
                    4,
                    241,
                    0
                ],
                "title": "How Well Do Vision--Language Models Understand Cities? A Comparative\n  Study on Spatial Reasoning from Street-View Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Well Do Vision--Language Models Understand Cities? A Comparative\n  Study on Spatial Reasoning from Street-View Images"
                },
                "summary": "Effectively understanding urban scenes requires fine-grained spatial\nreasoning about objects, layouts, and depth cues. However, how well current\nvision-language models (VLMs), pretrained on general scenes, transfer these\nabilities to urban domain remains underexplored. To address this gap, we\nconduct a comparative study of three off-the-shelf VLMs-BLIP-2, InstructBLIP,\nand LLaVA-1.5-evaluating both zero-shot performance and the effects of\nfine-tuning with a synthetic VQA dataset specific to urban scenes. We construct\nsuch dataset from segmentation, depth, and object detection predictions of\nstreet-view images, pairing each question with LLM-generated Chain-of-Thought\n(CoT) answers for step-by-step reasoning supervision. Results show that while\nVLMs perform reasonably well in zero-shot settings, fine-tuning with our\nsynthetic CoT-supervised dataset substantially boosts performance, especially\nfor challenging question types such as negation and counterfactuals. This study\nintroduces urban spatial reasoning as a new challenge for VLMs and demonstrates\nsynthetic dataset construction as a practical path for adapting general-purpose\nmodels to specialized domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively understanding urban scenes requires fine-grained spatial\nreasoning about objects, layouts, and depth cues. However, how well current\nvision-language models (VLMs), pretrained on general scenes, transfer these\nabilities to urban domain remains underexplored. To address this gap, we\nconduct a comparative study of three off-the-shelf VLMs-BLIP-2, InstructBLIP,\nand LLaVA-1.5-evaluating both zero-shot performance and the effects of\nfine-tuning with a synthetic VQA dataset specific to urban scenes. We construct\nsuch dataset from segmentation, depth, and object detection predictions of\nstreet-view images, pairing each question with LLM-generated Chain-of-Thought\n(CoT) answers for step-by-step reasoning supervision. Results show that while\nVLMs perform reasonably well in zero-shot settings, fine-tuning with our\nsynthetic CoT-supervised dataset substantially boosts performance, especially\nfor challenging question types such as negation and counterfactuals. This study\nintroduces urban spatial reasoning as a new challenge for VLMs and demonstrates\nsynthetic dataset construction as a practical path for adapting general-purpose\nmodels to specialized domains."
                },
                "authors": [
                    {
                        "name": "Juneyoung Ro"
                    },
                    {
                        "name": "Namwoo Kim"
                    },
                    {
                        "name": "Yoonjin Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Yoonjin Yoon"
                },
                "author": "Yoonjin Yoon",
                "arxiv_comment": "Accepted to ICCV Workshop 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21561v1",
                "updated": "2025-08-29T12:16:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    16,
                    24,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T12:16:24Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    16,
                    24,
                    4,
                    241,
                    0
                ],
                "title": "Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers\n  LLMs for Few-shot Tabular Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers\n  LLMs for Few-shot Tabular Classification"
                },
                "summary": "Recent studies show the promise of large language models (LLMs) for few-shot\ntabular classification but highlight challenges due to the variability in\nstructured data. To address this, we propose distilling data into actionable\ninsights to enable robust and effective classification by LLMs. Drawing\ninspiration from human learning processes, we introduce InsightTab, an insight\ndistillation framework guided by principles of divide-and-conquer, easy-first,\nand reflective learning. Our approach integrates rule summarization, strategic\nexemplification, and insight reflection through deep collaboration between LLMs\nand data modeling techniques. The obtained insights enable LLMs to better align\ntheir general knowledge and capabilities with the particular requirements of\nspecific tabular tasks. We extensively evaluate InsightTab on nine datasets.\nThe results demonstrate consistent improvement over state-of-the-art methods.\nAblation studies further validate the principle-guided distillation process,\nwhile analyses emphasize InsightTab's effectiveness in leveraging labeled data\nand managing bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show the promise of large language models (LLMs) for few-shot\ntabular classification but highlight challenges due to the variability in\nstructured data. To address this, we propose distilling data into actionable\ninsights to enable robust and effective classification by LLMs. Drawing\ninspiration from human learning processes, we introduce InsightTab, an insight\ndistillation framework guided by principles of divide-and-conquer, easy-first,\nand reflective learning. Our approach integrates rule summarization, strategic\nexemplification, and insight reflection through deep collaboration between LLMs\nand data modeling techniques. The obtained insights enable LLMs to better align\ntheir general knowledge and capabilities with the particular requirements of\nspecific tabular tasks. We extensively evaluate InsightTab on nine datasets.\nThe results demonstrate consistent improvement over state-of-the-art methods.\nAblation studies further validate the principle-guided distillation process,\nwhile analyses emphasize InsightTab's effectiveness in leveraging labeled data\nand managing bias."
                },
                "authors": [
                    {
                        "name": "Yifei Yuan"
                    },
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Weijia Zhang"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    },
                    {
                        "name": "Renjun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Renjun Hu"
                },
                "author": "Renjun Hu",
                "arxiv_comment": "EMNLP 25 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21556v1",
                "updated": "2025-08-29T12:12:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    12,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T12:12:22Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    12,
                    22,
                    4,
                    241,
                    0
                ],
                "title": "ECHO: Ego-Centric modeling of Human-Object interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECHO: Ego-Centric modeling of Human-Object interactions"
                },
                "summary": "Modeling human-object interactions (HOI) from an egocentric perspective is a\nlargely unexplored yet important problem due to the increasing adoption of\nwearable devices, such as smart glasses and watches. We investigate how much\ninformation about interaction can be recovered from only head and wrists\ntracking. Our answer is ECHO (Ego-Centric modeling of Human-Object\ninteractions), which, for the first time, proposes a unified framework to\nrecover three modalities: human pose, object motion, and contact from such\nminimal observation. ECHO employs a Diffusion Transformer architecture and a\nunique three-variate diffusion process, which jointly models human motion,\nobject trajectory, and contact sequence, allowing for flexible input\nconfigurations. Our method operates in a head-centric canonical space,\nenhancing robustness to global orientation. We propose a conveyor-based\ninference, which progressively increases the diffusion timestamp with the frame\nposition, allowing us to process sequences of any length. Through extensive\nevaluation, we demonstrate that ECHO outperforms existing methods that do not\noffer the same flexibility, setting a state-of-the-art in egocentric HOI\nreconstruction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling human-object interactions (HOI) from an egocentric perspective is a\nlargely unexplored yet important problem due to the increasing adoption of\nwearable devices, such as smart glasses and watches. We investigate how much\ninformation about interaction can be recovered from only head and wrists\ntracking. Our answer is ECHO (Ego-Centric modeling of Human-Object\ninteractions), which, for the first time, proposes a unified framework to\nrecover three modalities: human pose, object motion, and contact from such\nminimal observation. ECHO employs a Diffusion Transformer architecture and a\nunique three-variate diffusion process, which jointly models human motion,\nobject trajectory, and contact sequence, allowing for flexible input\nconfigurations. Our method operates in a head-centric canonical space,\nenhancing robustness to global orientation. We propose a conveyor-based\ninference, which progressively increases the diffusion timestamp with the frame\nposition, allowing us to process sequences of any length. Through extensive\nevaluation, we demonstrate that ECHO outperforms existing methods that do not\noffer the same flexibility, setting a state-of-the-art in egocentric HOI\nreconstruction."
                },
                "authors": [
                    {
                        "name": "Ilya A. Petrov"
                    },
                    {
                        "name": "Vladimir Guzov"
                    },
                    {
                        "name": "Riccardo Marin"
                    },
                    {
                        "name": "Emre Aksan"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Daniel Cremers"
                    },
                    {
                        "name": "Thabo Beeler"
                    },
                    {
                        "name": "Gerard Pons-Moll"
                    }
                ],
                "author_detail": {
                    "name": "Gerard Pons-Moll"
                },
                "author": "Gerard Pons-Moll",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15689v2",
                "updated": "2025-08-29T12:03:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    3,
                    45,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-26T14:22:21Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    14,
                    22,
                    21,
                    0,
                    146,
                    0
                ],
                "title": "BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for\n  Large Language Models"
                },
                "summary": "Rotations have become essential to state-of-the-art quantization pipelines\nfor large language models (LLMs) by effectively smoothing outliers in weights\nand activations. However, further optimizing the rotation parameters offers\nonly limited performance gains and introduces significant training overhead:\ndue to rotation parameter sharing, full-model must be loaded simultaneously to\nenable backpropagation, resulting in substantial memory consumption and limited\npractical utility. In this work, we identify two fundamental limitations of\ncurrent rotational quantization methods: (i) rotation fails to align channel\nmeans, resulting in wider quantization bounds and increased rounding errors;\nand (ii) rotation makes the activation distribution more Gaussian-like,\nincreasing energy loss caused by clipping errors. To address these issues, we\nintroduce \\textbf{BASE-Q}, a simple yet powerful approach that combines bias\ncorrection and asymmetric scaling to effectively reduce rounding and clipping\nerrors. Furthermore, BASE-Q enables blockwise optimization, eliminating the\nneed for memory-intensive full-model backpropagation. Extensive experiments on\nvarious LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing\nthe accuracy gap to full-precision models by 50.5\\%, 42.9\\%, and 29.2\\%\ncompared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be\nreleased soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotations have become essential to state-of-the-art quantization pipelines\nfor large language models (LLMs) by effectively smoothing outliers in weights\nand activations. However, further optimizing the rotation parameters offers\nonly limited performance gains and introduces significant training overhead:\ndue to rotation parameter sharing, full-model must be loaded simultaneously to\nenable backpropagation, resulting in substantial memory consumption and limited\npractical utility. In this work, we identify two fundamental limitations of\ncurrent rotational quantization methods: (i) rotation fails to align channel\nmeans, resulting in wider quantization bounds and increased rounding errors;\nand (ii) rotation makes the activation distribution more Gaussian-like,\nincreasing energy loss caused by clipping errors. To address these issues, we\nintroduce \\textbf{BASE-Q}, a simple yet powerful approach that combines bias\ncorrection and asymmetric scaling to effectively reduce rounding and clipping\nerrors. Furthermore, BASE-Q enables blockwise optimization, eliminating the\nneed for memory-intensive full-model backpropagation. Extensive experiments on\nvarious LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing\nthe accuracy gap to full-precision models by 50.5\\%, 42.9\\%, and 29.2\\%\ncompared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be\nreleased soon."
                },
                "authors": [
                    {
                        "name": "Liulu He"
                    },
                    {
                        "name": "Shenli Zheng"
                    },
                    {
                        "name": "Karwei Sun"
                    },
                    {
                        "name": "Yijiang Liu"
                    },
                    {
                        "name": "Yufei Zhao"
                    },
                    {
                        "name": "Chongkang Tan"
                    },
                    {
                        "name": "Huanrui Yang"
                    },
                    {
                        "name": "Yuan Du"
                    },
                    {
                        "name": "Li Du"
                    }
                ],
                "author_detail": {
                    "name": "Li Du"
                },
                "author": "Li Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21547v1",
                "updated": "2025-08-29T12:01:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    1,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T12:01:17Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    1,
                    17,
                    4,
                    241,
                    0
                ],
                "title": "What Data is Really Necessary? A Feasibility Study of Inference Data\n  Minimization for Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Data is Really Necessary? A Feasibility Study of Inference Data\n  Minimization for Recommender Systems"
                },
                "summary": "Data minimization is a legal principle requiring personal data processing to\nbe limited to what is necessary for a specified purpose. Operationalizing this\nprinciple for recommender systems, which rely on extensive personal data,\nremains a significant challenge. This paper conducts a feasibility study on\nminimizing implicit feedback inference data for such systems. We propose a\nnovel problem formulation, analyze various minimization techniques, and\ninvestigate key factors influencing their effectiveness. We demonstrate that\nsubstantial inference data reduction is technically feasible without\nsignificant performance loss. However, its practicality is critically\ndetermined by two factors: the technical setting (e.g., performance targets,\nchoice of model) and user characteristics (e.g., history size, preference\ncomplexity). Thus, while we establish its technical feasibility, we conclude\nthat data minimization remains practically challenging and its dependence on\nthe technical and user context makes a universal standard for data `necessity'\ndifficult to implement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data minimization is a legal principle requiring personal data processing to\nbe limited to what is necessary for a specified purpose. Operationalizing this\nprinciple for recommender systems, which rely on extensive personal data,\nremains a significant challenge. This paper conducts a feasibility study on\nminimizing implicit feedback inference data for such systems. We propose a\nnovel problem formulation, analyze various minimization techniques, and\ninvestigate key factors influencing their effectiveness. We demonstrate that\nsubstantial inference data reduction is technically feasible without\nsignificant performance loss. However, its practicality is critically\ndetermined by two factors: the technical setting (e.g., performance targets,\nchoice of model) and user characteristics (e.g., history size, preference\ncomplexity). Thus, while we establish its technical feasibility, we conclude\nthat data minimization remains practically challenging and its dependence on\nthe technical and user context makes a universal standard for data `necessity'\ndifficult to implement."
                },
                "authors": [
                    {
                        "name": "Jens Leysen"
                    },
                    {
                        "name": "Marco Favier"
                    },
                    {
                        "name": "Bart Goethals"
                    }
                ],
                "author_detail": {
                    "name": "Bart Goethals"
                },
                "author": "Bart Goethals",
                "arxiv_doi": "10.1145/3746252.3761058",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761058",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.21547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication at the 34th ACM International Conference on\n  Information and Knowledge Management (CIKM '25), November 10-14, 2025, Seoul,\n  Republic of Korea",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21546v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21546v1",
                "updated": "2025-08-29T11:57:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    57,
                    31,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T11:57:31Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    57,
                    31,
                    4,
                    241,
                    0
                ],
                "title": "Measuring mutual friction in superfluids: the role of initial vortex\n  configuration fluctuations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring mutual friction in superfluids: the role of initial vortex\n  configuration fluctuations"
                },
                "summary": "The physical origin of mutual friction in quantum fluids is deeply connected\nto the fundamental nature of superfluidity. It stems from the interaction\nbetween the superfluid and normal components, mediated by the dynamics of\nquantized vortices that induce the exchange of momentum and energy. Despite the\ncomplexity of these interactions, their essential features can be effectively\ndescribed by the dissipative point vortex model, an extension of classical\nvortex dynamics that incorporates finite-temperature dissipation. Mutual\nfriction is parametrized by the longitudinal (dissipative) coefficient $\\alpha$\nand the transverse (reactive) coefficient $\\alpha'$. Accurate measurement of\nthese parameters provides critical insights into the microscopic mechanisms\ngoverning vortex motion and dissipation in quantum fluids, serving as a key\nbenchmark for theoretical models. In this work, we employ the dissipative point\nvortex model to study how fluctuations in the initial conditions influence the\ninference of $\\alpha$ and $\\alpha'$ from the time evolution of the vortex\ntrajectories. Using experimentally realistic parameters, we show that\nfluctuations can introduce significant biases in the extracted values of the\nmutual friction coefficients. We compare our findings with recent experimental\nmeasurements in strongly interacting atomic superfluids. Applying this analysis\nto our recent experimental results allowed us to account for fluctuations in\nthe correct determination of $\\alpha$ and $\\alpha'$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The physical origin of mutual friction in quantum fluids is deeply connected\nto the fundamental nature of superfluidity. It stems from the interaction\nbetween the superfluid and normal components, mediated by the dynamics of\nquantized vortices that induce the exchange of momentum and energy. Despite the\ncomplexity of these interactions, their essential features can be effectively\ndescribed by the dissipative point vortex model, an extension of classical\nvortex dynamics that incorporates finite-temperature dissipation. Mutual\nfriction is parametrized by the longitudinal (dissipative) coefficient $\\alpha$\nand the transverse (reactive) coefficient $\\alpha'$. Accurate measurement of\nthese parameters provides critical insights into the microscopic mechanisms\ngoverning vortex motion and dissipation in quantum fluids, serving as a key\nbenchmark for theoretical models. In this work, we employ the dissipative point\nvortex model to study how fluctuations in the initial conditions influence the\ninference of $\\alpha$ and $\\alpha'$ from the time evolution of the vortex\ntrajectories. Using experimentally realistic parameters, we show that\nfluctuations can introduce significant biases in the extracted values of the\nmutual friction coefficients. We compare our findings with recent experimental\nmeasurements in strongly interacting atomic superfluids. Applying this analysis\nto our recent experimental results allowed us to account for fluctuations in\nthe correct determination of $\\alpha$ and $\\alpha'$."
                },
                "authors": [
                    {
                        "name": "Nicola Grani"
                    },
                    {
                        "name": "Diego Hernández-Rajkov"
                    },
                    {
                        "name": "Marcia Frómeta Fernández"
                    },
                    {
                        "name": "Giulia Del Pace"
                    },
                    {
                        "name": "Giacomo Roati"
                    }
                ],
                "author_detail": {
                    "name": "Giacomo Roati"
                },
                "author": "Giacomo Roati",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21546v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21546v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.quant-gas",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.quant-gas",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21542v1",
                "updated": "2025-08-29T11:55:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    55,
                    47,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T11:55:47Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    55,
                    47,
                    4,
                    241,
                    0
                ],
                "title": "Complete Gaussian Splats from a Single Image with Denoising Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Complete Gaussian Splats from a Single Image with Denoising Diffusion\n  Models"
                },
                "summary": "Gaussian splatting typically requires dense observations of the scene and can\nfail to reconstruct occluded and unobserved areas. We propose a latent\ndiffusion model to reconstruct a complete 3D scene with Gaussian splats,\nincluding the occluded parts, from only a single image during inference.\nCompleting the unobserved surfaces of a scene is challenging due to the\nambiguity of the plausible surfaces. Conventional methods use a\nregression-based formulation to predict a single \"mode\" for occluded and\nout-of-frustum surfaces, leading to blurriness, implausibility, and failure to\ncapture multiple possible explanations. Thus, they often address this problem\npartially, focusing either on objects isolated from the background,\nreconstructing only visible surfaces, or failing to extrapolate far from the\ninput views. In contrast, we propose a generative formulation to learn a\ndistribution of 3D representations of Gaussian splats conditioned on a single\ninput image. To address the lack of ground-truth training data, we propose a\nVariational AutoReconstructor to learn a latent space only from 2D images in a\nself-supervised manner, over which a diffusion model is trained. Our method\ngenerates faithful reconstructions and diverse samples with the ability to\ncomplete the occluded surfaces for high-quality 360-degree renderings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gaussian splatting typically requires dense observations of the scene and can\nfail to reconstruct occluded and unobserved areas. We propose a latent\ndiffusion model to reconstruct a complete 3D scene with Gaussian splats,\nincluding the occluded parts, from only a single image during inference.\nCompleting the unobserved surfaces of a scene is challenging due to the\nambiguity of the plausible surfaces. Conventional methods use a\nregression-based formulation to predict a single \"mode\" for occluded and\nout-of-frustum surfaces, leading to blurriness, implausibility, and failure to\ncapture multiple possible explanations. Thus, they often address this problem\npartially, focusing either on objects isolated from the background,\nreconstructing only visible surfaces, or failing to extrapolate far from the\ninput views. In contrast, we propose a generative formulation to learn a\ndistribution of 3D representations of Gaussian splats conditioned on a single\ninput image. To address the lack of ground-truth training data, we propose a\nVariational AutoReconstructor to learn a latent space only from 2D images in a\nself-supervised manner, over which a diffusion model is trained. Our method\ngenerates faithful reconstructions and diverse samples with the ability to\ncomplete the occluded surfaces for high-quality 360-degree renderings."
                },
                "authors": [
                    {
                        "name": "Ziwei Liao"
                    },
                    {
                        "name": "Mohamed Sayed"
                    },
                    {
                        "name": "Steven L. Waslander"
                    },
                    {
                        "name": "Sara Vicente"
                    },
                    {
                        "name": "Daniyar Turmukhambetov"
                    },
                    {
                        "name": "Michael Firman"
                    }
                ],
                "author_detail": {
                    "name": "Michael Firman"
                },
                "author": "Michael Firman",
                "arxiv_comment": "Main paper: 11 pages; Supplementary materials: 7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21541v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21541v1",
                "updated": "2025-08-29T11:53:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    53,
                    49,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T11:53:49Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    53,
                    49,
                    4,
                    241,
                    0
                ],
                "title": "Refining fundamental constants with white dwarfs: machine learning\n  informed constraints on fine-structure constant and proton-to-electron mass\n  ratio",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining fundamental constants with white dwarfs: machine learning\n  informed constraints on fine-structure constant and proton-to-electron mass\n  ratio"
                },
                "summary": "We explore the potential variation of two fundamental constants, the\nfine-structure constant $\\alpha$ and the proton-to-electron mass ratio $\\mu$,\nwithin the framework of modified gravity theories and finite-temperature\neffects. Utilising high-precision white dwarf observations from the Gaia-DR3\nsurvey, we construct a robust mass--radius relation using a Bayesian-inspired\nmachine learning framework. This empirical relation is rigorously compared with\ntheoretical predictions derived from scalar-tensor gravity models and\ntemperature-dependent equations of state. Our results demonstrate that both\nunderlying gravitational theory and temperature substantially influence the\ninferred constraints on $\\alpha$ and $\\mu$. We obtain the strongest constraints\nas $|\\Delta\\alpha/\\alpha|=2.10^{+32.56}_{-39.26}\\times10^{-7}$ and\n$|\\Delta\\mu/\\mu|=1.61^{+37.16}_{-34.67}\\times10^{-7}$ for modified gravity\nparameter $\\gamma\\simeq -3.69\\times10^{13}\\,\\mathrm{cm}^2$, while for the\nfinite temperature case, these are\n$|\\Delta\\alpha/\\alpha|=1.60^{+37.31}_{-35.42}\\times10^{-7}$ and\n$|\\Delta\\mu/\\mu|=1.23^{+37.02}_{-35.71}\\times10^{-7}$ for $T \\simeq 1.1 \\times\n10^7\\rm\\, K$. These findings yield tighter constraints than those reported in\nearlier studies and underscore the critical roles of gravitational and thermal\nphysics in testing the constancy of fundamental parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We explore the potential variation of two fundamental constants, the\nfine-structure constant $\\alpha$ and the proton-to-electron mass ratio $\\mu$,\nwithin the framework of modified gravity theories and finite-temperature\neffects. Utilising high-precision white dwarf observations from the Gaia-DR3\nsurvey, we construct a robust mass--radius relation using a Bayesian-inspired\nmachine learning framework. This empirical relation is rigorously compared with\ntheoretical predictions derived from scalar-tensor gravity models and\ntemperature-dependent equations of state. Our results demonstrate that both\nunderlying gravitational theory and temperature substantially influence the\ninferred constraints on $\\alpha$ and $\\mu$. We obtain the strongest constraints\nas $|\\Delta\\alpha/\\alpha|=2.10^{+32.56}_{-39.26}\\times10^{-7}$ and\n$|\\Delta\\mu/\\mu|=1.61^{+37.16}_{-34.67}\\times10^{-7}$ for modified gravity\nparameter $\\gamma\\simeq -3.69\\times10^{13}\\,\\mathrm{cm}^2$, while for the\nfinite temperature case, these are\n$|\\Delta\\alpha/\\alpha|=1.60^{+37.31}_{-35.42}\\times10^{-7}$ and\n$|\\Delta\\mu/\\mu|=1.23^{+37.02}_{-35.71}\\times10^{-7}$ for $T \\simeq 1.1 \\times\n10^7\\rm\\, K$. These findings yield tighter constraints than those reported in\nearlier studies and underscore the critical roles of gravitational and thermal\nphysics in testing the constancy of fundamental parameters."
                },
                "authors": [
                    {
                        "name": "Akhil Uniyal"
                    },
                    {
                        "name": "Surajit Kalita"
                    },
                    {
                        "name": "Yosuke Mizuno"
                    },
                    {
                        "name": "Sayan Chakrabarti"
                    },
                    {
                        "name": "Yan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yan Lu"
                },
                "arxiv_affiliation": "Shanghai",
                "author": "Yan Lu",
                "arxiv_comment": "7 pages with 5 figures; accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21541v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21541v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21540v1",
                "updated": "2025-08-29T11:53:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    53,
                    16,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T11:53:16Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    53,
                    16,
                    4,
                    241,
                    0
                ],
                "title": "HealthProcessAI: A Technical Framework and Proof-of-Concept for\n  LLM-Enhanced Healthcare Process Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HealthProcessAI: A Technical Framework and Proof-of-Concept for\n  LLM-Enhanced Healthcare Process Mining"
                },
                "summary": "Process mining has emerged as a powerful analytical technique for\nunderstanding complex healthcare workflows. However, its application faces\nsignificant barriers, including technical complexity, a lack of standardized\napproaches, and limited access to practical training resources. We introduce\nHealthProcessAI, a GenAI framework designed to simplify process mining\napplications in healthcare and epidemiology by providing a comprehensive\nwrapper around existing Python (PM4PY) and R (bupaR) libraries. To address\nunfamiliarity and improve accessibility, the framework integrates multiple\nLarge Language Models (LLMs) for automated process map interpretation and\nreport generation, helping translate technical analyses into outputs that\ndiverse users can readily understand. We validated the framework using sepsis\nprogression data as a proof-of-concept example and compared the outputs of five\nstate-of-the-art LLM models through the OpenRouter platform. To test its\nfunctionality, the framework successfully processed sepsis data across four\nproof-of-concept scenarios, demonstrating robust technical performance and its\ncapability to generate reports through automated LLM analysis. LLM evaluation\nusing five independent LLMs as automated evaluators revealed distinct model\nstrengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency\nscores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By\nintegrating multiple Large Language Models (LLMs) for automated interpretation\nand report generation, the framework addresses widespread unfamiliarity with\nprocess mining outputs, making them more accessible to clinicians, data\nscientists, and researchers. This structured analytics and AI-driven\ninterpretation combination represents a novel methodological advance in\ntranslating complex process mining results into potentially actionable insights\nfor healthcare applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process mining has emerged as a powerful analytical technique for\nunderstanding complex healthcare workflows. However, its application faces\nsignificant barriers, including technical complexity, a lack of standardized\napproaches, and limited access to practical training resources. We introduce\nHealthProcessAI, a GenAI framework designed to simplify process mining\napplications in healthcare and epidemiology by providing a comprehensive\nwrapper around existing Python (PM4PY) and R (bupaR) libraries. To address\nunfamiliarity and improve accessibility, the framework integrates multiple\nLarge Language Models (LLMs) for automated process map interpretation and\nreport generation, helping translate technical analyses into outputs that\ndiverse users can readily understand. We validated the framework using sepsis\nprogression data as a proof-of-concept example and compared the outputs of five\nstate-of-the-art LLM models through the OpenRouter platform. To test its\nfunctionality, the framework successfully processed sepsis data across four\nproof-of-concept scenarios, demonstrating robust technical performance and its\ncapability to generate reports through automated LLM analysis. LLM evaluation\nusing five independent LLMs as automated evaluators revealed distinct model\nstrengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency\nscores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By\nintegrating multiple Large Language Models (LLMs) for automated interpretation\nand report generation, the framework addresses widespread unfamiliarity with\nprocess mining outputs, making them more accessible to clinicians, data\nscientists, and researchers. This structured analytics and AI-driven\ninterpretation combination represents a novel methodological advance in\ntranslating complex process mining results into potentially actionable insights\nfor healthcare applications."
                },
                "authors": [
                    {
                        "name": "Eduardo Illueca-Fernandez"
                    },
                    {
                        "name": "Kaile Chen"
                    },
                    {
                        "name": "Fernando Seoane"
                    },
                    {
                        "name": "Farhad Abtahi"
                    }
                ],
                "author_detail": {
                    "name": "Farhad Abtahi"
                },
                "author": "Farhad Abtahi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00863v2",
                "updated": "2025-08-29T11:51:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    51,
                    24,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-01T07:01:34Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    7,
                    1,
                    34,
                    6,
                    152,
                    0
                ],
                "title": "L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with\n  Synthetic Annotations using CoTR prompting and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with\n  Synthetic Annotations using CoTR prompting and Large Language Models"
                },
                "summary": "Emotion recognition in low-resource languages like Marathi remains\nchallenging due to limited annotated data. We present L3Cube-MahaEmotions, a\nhigh-quality Marathi emotion recognition dataset with 11 fine-grained emotion\nlabels. The training data is synthetically annotated using large language\nmodels (LLMs), while the validation and test sets are manually labeled to serve\nas a reliable gold-standard benchmark. Building on the MahaSent dataset, we\napply the Chain-of-Translation (CoTR) prompting technique, where Marathi\nsentences are translated into English and emotion labeled via a single prompt.\nGPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data\nannotation due to superior label quality. We evaluate model performance using\nstandard metrics and explore label aggregation strategies (e.g., Union,\nIntersection). While GPT-4 predictions outperform fine-tuned BERT models,\nBERT-based models trained on synthetic labels fail to surpass GPT-4. This\nhighlights both the importance of high-quality human-labeled data and the\ninherent complexity of emotion recognition. An important finding of this work\nis that generic LLMs like GPT-4 and Llama3-405B generalize better than\nfine-tuned BERT for complex low-resource emotion recognition tasks. The dataset\nand model are shared publicly at https://github.com/l3cube-pune/MarathiNLP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion recognition in low-resource languages like Marathi remains\nchallenging due to limited annotated data. We present L3Cube-MahaEmotions, a\nhigh-quality Marathi emotion recognition dataset with 11 fine-grained emotion\nlabels. The training data is synthetically annotated using large language\nmodels (LLMs), while the validation and test sets are manually labeled to serve\nas a reliable gold-standard benchmark. Building on the MahaSent dataset, we\napply the Chain-of-Translation (CoTR) prompting technique, where Marathi\nsentences are translated into English and emotion labeled via a single prompt.\nGPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data\nannotation due to superior label quality. We evaluate model performance using\nstandard metrics and explore label aggregation strategies (e.g., Union,\nIntersection). While GPT-4 predictions outperform fine-tuned BERT models,\nBERT-based models trained on synthetic labels fail to surpass GPT-4. This\nhighlights both the importance of high-quality human-labeled data and the\ninherent complexity of emotion recognition. An important finding of this work\nis that generic LLMs like GPT-4 and Llama3-405B generalize better than\nfine-tuned BERT for complex low-resource emotion recognition tasks. The dataset\nand model are shared publicly at https://github.com/l3cube-pune/MarathiNLP"
                },
                "authors": [
                    {
                        "name": "Nidhi Kowtal"
                    },
                    {
                        "name": "Raviraj Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Raviraj Joshi"
                },
                "author": "Raviraj Joshi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20417v2",
                "updated": "2025-08-29T11:37:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    37,
                    41,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-28T04:37:15Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    37,
                    15,
                    3,
                    240,
                    0
                ],
                "title": "KG-CQR: Leveraging Structured Relation Representations in Knowledge\n  Graphs for Contextual Query Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG-CQR: Leveraging Structured Relation Representations in Knowledge\n  Graphs for Contextual Query Retrieval"
                },
                "summary": "The integration of knowledge graphs (KGs) with large language models (LLMs)\noffers significant potential to improve the retrieval phase of\nretrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,\na novel framework for Contextual Query Retrieval (CQR) that enhances the\nretrieval phase by enriching the contextual representation of complex input\nqueries using a corpus-centric KG. Unlike existing methods that primarily\naddress corpus-level context loss, KG-CQR focuses on query enrichment through\nstructured relation representations, extracting and completing relevant KG\nsubgraphs to generate semantically rich query contexts. Comprising subgraph\nextraction, completion, and contextual generation modules, KG-CQR operates as a\nmodel-agnostic pipeline, ensuring scalability across LLMs of varying sizes\nwithout additional training. Experimental results on RAGBench and MultiHop-RAG\ndatasets demonstrate KG-CQR's superior performance, achieving a 4-6%\nimprovement in mAP and a 2-3% improvement in Recall@25 over strong baseline\nmodels. Furthermore, evaluations on challenging RAG tasks such as multi-hop\nquestion answering show that, by incorporating KG-CQR, the performance\nconsistently outperforms the existing baseline in terms of retrieval\neffectiveness",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of knowledge graphs (KGs) with large language models (LLMs)\noffers significant potential to improve the retrieval phase of\nretrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,\na novel framework for Contextual Query Retrieval (CQR) that enhances the\nretrieval phase by enriching the contextual representation of complex input\nqueries using a corpus-centric KG. Unlike existing methods that primarily\naddress corpus-level context loss, KG-CQR focuses on query enrichment through\nstructured relation representations, extracting and completing relevant KG\nsubgraphs to generate semantically rich query contexts. Comprising subgraph\nextraction, completion, and contextual generation modules, KG-CQR operates as a\nmodel-agnostic pipeline, ensuring scalability across LLMs of varying sizes\nwithout additional training. Experimental results on RAGBench and MultiHop-RAG\ndatasets demonstrate KG-CQR's superior performance, achieving a 4-6%\nimprovement in mAP and a 2-3% improvement in Recall@25 over strong baseline\nmodels. Furthermore, evaluations on challenging RAG tasks such as multi-hop\nquestion answering show that, by incorporating KG-CQR, the performance\nconsistently outperforms the existing baseline in terms of retrieval\neffectiveness"
                },
                "authors": [
                    {
                        "name": "Chi Minh Bui"
                    },
                    {
                        "name": "Ngoc Mai Thieu"
                    },
                    {
                        "name": "Van Vinh Nguyen"
                    },
                    {
                        "name": "Jason J. Jung"
                    },
                    {
                        "name": "Khac-Hoai Nam Bui"
                    }
                ],
                "author_detail": {
                    "name": "Khac-Hoai Nam Bui"
                },
                "author": "Khac-Hoai Nam Bui",
                "arxiv_comment": "Accepted at Main EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20133v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20133v2",
                "updated": "2025-08-29T11:20:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    20,
                    53,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-26T16:13:28Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    16,
                    13,
                    28,
                    1,
                    238,
                    0
                ],
                "title": "Proactive HIV Care: AI-Based Comorbidity Prediction from Routine EHR\n  Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive HIV Care: AI-Based Comorbidity Prediction from Routine EHR\n  Data"
                },
                "summary": "People living with HIV face a high burden of comorbidities, yet early\ndetection is often limited by symptom-driven screening. We evaluate the\npotential of AI to predict multiple comorbidities from routinely collected\nElectronic Health Records. Using data from 2,200 HIV-positive patients in South\nEast London, comprising 30 laboratory markers and 7 demographic/social\nattributes, we compare demographic-aware models (which use both\nlaboratory/social variables and demographic information as input) against\ndemographic-unaware models (which exclude all demographic information). Across\nall methods, demographic-aware models consistently outperformed unaware\ncounterparts. Demographic recoverability experiments revealed that gender and\nage can be accurately inferred from laboratory data, underscoring both the\npredictive value and fairness considerations of demographic features. These\nfindings show that combining demographic and laboratory data can improve\nautomated, multi-label comorbidity prediction in HIV care, while raising\nimportant questions about bias and interpretability in clinical AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "People living with HIV face a high burden of comorbidities, yet early\ndetection is often limited by symptom-driven screening. We evaluate the\npotential of AI to predict multiple comorbidities from routinely collected\nElectronic Health Records. Using data from 2,200 HIV-positive patients in South\nEast London, comprising 30 laboratory markers and 7 demographic/social\nattributes, we compare demographic-aware models (which use both\nlaboratory/social variables and demographic information as input) against\ndemographic-unaware models (which exclude all demographic information). Across\nall methods, demographic-aware models consistently outperformed unaware\ncounterparts. Demographic recoverability experiments revealed that gender and\nage can be accurately inferred from laboratory data, underscoring both the\npredictive value and fairness considerations of demographic features. These\nfindings show that combining demographic and laboratory data can improve\nautomated, multi-label comorbidity prediction in HIV care, while raising\nimportant questions about bias and interpretability in clinical AI."
                },
                "authors": [
                    {
                        "name": "Solomon Russom"
                    },
                    {
                        "name": "Dimitrios Kollias"
                    },
                    {
                        "name": "Qianni Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qianni Zhang"
                },
                "author": "Qianni Zhang",
                "arxiv_comment": "accepted at ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20133v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20133v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21520v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21520v1",
                "updated": "2025-08-29T11:13:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    13,
                    12,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T11:13:12Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    13,
                    12,
                    4,
                    241,
                    0
                ],
                "title": "Practically significant change points in high dimension -- measuring\n  signal strength pro active component",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practically significant change points in high dimension -- measuring\n  signal strength pro active component"
                },
                "summary": "We consider the change point testing problem for high-dimensional time\nseries. Unlike conventional approaches, where one tests whether the difference\n$\\delta$ of the mean vectors before and after the change point is equal to\nzero, we argue that the consideration of the null hypothesis\n$H_0:\\|\\delta\\|\\le\\Delta$, for some norm $\\|\\cdot\\|$ and a threshold\n$\\Delta>0$, is better suited. By the formulation of the null hypothesis as a\ncomposite hypothesis, the change point testing problem becomes significantly\nmore challenging. We develop pivotal inference for testing hypotheses of this\ntype in the setting of high-dimensional time series, first, measuring\ndeviations from the null vector by the $\\ell_2$-norm $\\|\\cdot\\|_2$ normalized\nby the dimension. Second, by measuring deviations using a sparsity adjusted\n$\\ell_2$-\"norm\" $\\|\\cdot \\|_2/\\sqrt{\\|\\cdot\\|_0} $, where $\\|\\cdot\\|_0$ denotes\nthe $\\ell_0$-\"norm,\" we propose a pivotal test procedure which intrinsically\nadapts to sparse alternatives in a data-driven way by pivotally estimating the\nset of nonzero entries of the vector $\\delta$. To establish the statistical\nvalidity of our approach, we derive tail bounds of certain classes of\ndistributions that frequently appear as limiting distributions of\nself-normalized statistics. As a theoretical foundation for all results, we\ndevelop a general weak invariance principle for the partial sum process\n$X_1^\\top\\xi +\\cdots +X_{\\lfloor\\lambda n\\rfloor}^\\top\\xi$ for a time series\n$(X_j)_{j\\in\\mathbb{Z}}$ and a contrast vector $\\xi\\in\\mathbb{R}^p$ under\nincreasing dimension $p$, which is of independent interest. Finally, we\ninvestigate the finite sample properties of the tests by means of a simulation\nstudy and illustrate its application in a data example.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the change point testing problem for high-dimensional time\nseries. Unlike conventional approaches, where one tests whether the difference\n$\\delta$ of the mean vectors before and after the change point is equal to\nzero, we argue that the consideration of the null hypothesis\n$H_0:\\|\\delta\\|\\le\\Delta$, for some norm $\\|\\cdot\\|$ and a threshold\n$\\Delta>0$, is better suited. By the formulation of the null hypothesis as a\ncomposite hypothesis, the change point testing problem becomes significantly\nmore challenging. We develop pivotal inference for testing hypotheses of this\ntype in the setting of high-dimensional time series, first, measuring\ndeviations from the null vector by the $\\ell_2$-norm $\\|\\cdot\\|_2$ normalized\nby the dimension. Second, by measuring deviations using a sparsity adjusted\n$\\ell_2$-\"norm\" $\\|\\cdot \\|_2/\\sqrt{\\|\\cdot\\|_0} $, where $\\|\\cdot\\|_0$ denotes\nthe $\\ell_0$-\"norm,\" we propose a pivotal test procedure which intrinsically\nadapts to sparse alternatives in a data-driven way by pivotally estimating the\nset of nonzero entries of the vector $\\delta$. To establish the statistical\nvalidity of our approach, we derive tail bounds of certain classes of\ndistributions that frequently appear as limiting distributions of\nself-normalized statistics. As a theoretical foundation for all results, we\ndevelop a general weak invariance principle for the partial sum process\n$X_1^\\top\\xi +\\cdots +X_{\\lfloor\\lambda n\\rfloor}^\\top\\xi$ for a time series\n$(X_j)_{j\\in\\mathbb{Z}}$ and a contrast vector $\\xi\\in\\mathbb{R}^p$ under\nincreasing dimension $p$, which is of independent interest. Finally, we\ninvestigate the finite sample properties of the tests by means of a simulation\nstudy and illustrate its application in a data example."
                },
                "authors": [
                    {
                        "name": "Pascal Quanz"
                    },
                    {
                        "name": "Holger Dette"
                    }
                ],
                "author_detail": {
                    "name": "Holger Dette"
                },
                "author": "Holger Dette",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21520v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21520v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21517v1",
                "updated": "2025-08-29T11:03:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    3,
                    44,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T11:03:44Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    3,
                    44,
                    4,
                    241,
                    0
                ],
                "title": "Modeling Wise Decision Making: A Z-Number Fuzzy Framework Inspired by\n  Phronesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling Wise Decision Making: A Z-Number Fuzzy Framework Inspired by\n  Phronesis"
                },
                "summary": "Background: Wisdom is a superordinate construct that embraces perspective\ntaking, reflectiveness, prosocial orientation, reflective empathetic action,\nand intellectual humility. Unlike conventional models of reasoning that are\nrigidly bound by binary thinking, wisdom unfolds in shades of ambiguity,\nrequiring both graded evaluation and self-reflective humility. Current measures\ndepend on self-reports and seldom reflect the humility and uncertainty inherent\nin wise reasoning. A computational framework that takes into account both\nmultidimensionality and confidence has the potential to improve psychological\nscience and allow humane AI. Method: We present a fuzzy inference system with Z\nnumbers, each of the decisions being expressed in terms of a wisdom score\n(restriction) and confidence score (certainty). As part of this study,\nparticipants (N = 100) were exposed to culturally neutral pictorial moral\ndilemma tasks to which they generated think-aloud linguistic responses, which\nwere mapped into five theoretically based components of wisdom. The scores of\neach individual component were combined using a base of 21 rules, with\nmembership functions tuned via Gaussian kernel density estimation. Results: In\na proof of concept study, the system produced dual attribute wisdom\nrepresentations that correlated modestly but significantly with established\nscales while showing negligible relations with unrelated traits, supporting\nconvergent and divergent validity. Contribution: The contribution is to\nformalize wisdom as a multidimensional, uncertainty-conscious construct,\noperationalized in the form of Z-numbers. In addition to progressing\nmeasurement in psychology, it calculates how fuzzy Z numbers can provide AI\nsystems with interpretable, confidence-sensitive reasoning that affords a safe,\nmiddle ground between rigorous computation and human-like judgment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Background: Wisdom is a superordinate construct that embraces perspective\ntaking, reflectiveness, prosocial orientation, reflective empathetic action,\nand intellectual humility. Unlike conventional models of reasoning that are\nrigidly bound by binary thinking, wisdom unfolds in shades of ambiguity,\nrequiring both graded evaluation and self-reflective humility. Current measures\ndepend on self-reports and seldom reflect the humility and uncertainty inherent\nin wise reasoning. A computational framework that takes into account both\nmultidimensionality and confidence has the potential to improve psychological\nscience and allow humane AI. Method: We present a fuzzy inference system with Z\nnumbers, each of the decisions being expressed in terms of a wisdom score\n(restriction) and confidence score (certainty). As part of this study,\nparticipants (N = 100) were exposed to culturally neutral pictorial moral\ndilemma tasks to which they generated think-aloud linguistic responses, which\nwere mapped into five theoretically based components of wisdom. The scores of\neach individual component were combined using a base of 21 rules, with\nmembership functions tuned via Gaussian kernel density estimation. Results: In\na proof of concept study, the system produced dual attribute wisdom\nrepresentations that correlated modestly but significantly with established\nscales while showing negligible relations with unrelated traits, supporting\nconvergent and divergent validity. Contribution: The contribution is to\nformalize wisdom as a multidimensional, uncertainty-conscious construct,\noperationalized in the form of Z-numbers. In addition to progressing\nmeasurement in psychology, it calculates how fuzzy Z numbers can provide AI\nsystems with interpretable, confidence-sensitive reasoning that affords a safe,\nmiddle ground between rigorous computation and human-like judgment."
                },
                "authors": [
                    {
                        "name": "Sweta Kaman"
                    },
                    {
                        "name": "Ankita Sharma"
                    },
                    {
                        "name": "Romi Banerjee"
                    }
                ],
                "author_detail": {
                    "name": "Romi Banerjee"
                },
                "author": "Romi Banerjee",
                "arxiv_comment": "total 17 pages, main manuscript 12 pages, supplementary 5 pages, 6\n  tables in main manuscript, 5 figures in main manuscript, 2 tables in\n  supplementary, and 3 figures in supplementary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21512v1",
                "updated": "2025-08-29T10:51:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    51,
                    41,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T10:51:41Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    51,
                    41,
                    4,
                    241,
                    0
                ],
                "title": "Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval\n  across Table-to-Text Serialization Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval\n  across Table-to-Text Serialization Approaches"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in high-stakes\ndecision-making tasks, such as loan approvals. While their applications expand\nacross domains, LLMs struggle to process tabular data, ensuring fairness and\ndelivering reliable predictions. In this work, we assess the performance and\nfairness of LLMs on serialized loan approval datasets from three geographically\ndistinct regions: Ghana, Germany, and the United States. Our evaluation focuses\non the model's zero-shot and in-context learning (ICL) capabilities. Our\nresults reveal that the choice of serialization (Serialization refers to the\nprocess of converting tabular data into text formats suitable for processing by\nLLMs.) format significantly affects both performance and fairness in LLMs, with\ncertain formats such as GReat and LIFT yielding higher F1 scores but\nexacerbating fairness disparities. Notably, while ICL improved model\nperformance by 4.9-59.6% relative to zero-shot baselines, its effect on\nfairness varied considerably across datasets. Our work underscores the\nimportance of effective tabular data representation methods and fairness-aware\nmodels to improve the reliability of LLMs in financial decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in high-stakes\ndecision-making tasks, such as loan approvals. While their applications expand\nacross domains, LLMs struggle to process tabular data, ensuring fairness and\ndelivering reliable predictions. In this work, we assess the performance and\nfairness of LLMs on serialized loan approval datasets from three geographically\ndistinct regions: Ghana, Germany, and the United States. Our evaluation focuses\non the model's zero-shot and in-context learning (ICL) capabilities. Our\nresults reveal that the choice of serialization (Serialization refers to the\nprocess of converting tabular data into text formats suitable for processing by\nLLMs.) format significantly affects both performance and fairness in LLMs, with\ncertain formats such as GReat and LIFT yielding higher F1 scores but\nexacerbating fairness disparities. Notably, while ICL improved model\nperformance by 4.9-59.6% relative to zero-shot baselines, its effect on\nfairness varied considerably across datasets. Our work underscores the\nimportance of effective tabular data representation methods and fairness-aware\nmodels to improve the reliability of LLMs in financial decision-making."
                },
                "authors": [
                    {
                        "name": "Israel Abebe Azime"
                    },
                    {
                        "name": "Deborah D. Kanubala"
                    },
                    {
                        "name": "Tejumade Afonja"
                    },
                    {
                        "name": "Mario Fritz"
                    },
                    {
                        "name": "Isabel Valera"
                    },
                    {
                        "name": "Dietrich Klakow"
                    },
                    {
                        "name": "Philipp Slusallek"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Slusallek"
                },
                "author": "Philipp Slusallek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21511v1",
                "updated": "2025-08-29T10:51:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    51,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T10:51:22Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    51,
                    22,
                    4,
                    241,
                    0
                ],
                "title": "Possible Coronal Geometry in the Hard and Soft State of Black Hole X-ray\n  Binaries from MONK Simulations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Possible Coronal Geometry in the Hard and Soft State of Black Hole X-ray\n  Binaries from MONK Simulations"
                },
                "summary": "Understanding the coronal geometry in different states of black hole X-ray\nbinaries is important for more accurate modeling of the system. However, it is\ndifficult to distinguish different geometries by fitting the observed\nComptonization spectra. In this work, we use the Monte Carlo ray-tracing code\nMONK to simulate the spectra for three widely proposed coronal geometries:\nsandwich, spherical, and lamppost, varying their optical depth and size\n(height). By fitting the simulated NuSTAR observations with the simplcut*kerrbb\nmodel, we infer the possible parameter space for the hard state and soft state\nof different coronal geometries. The influence of the disk inclination angle\nand black hole spin is discussed. We find that for the lamppost model the disk\nemission is always dominant, making it incompatible in the hard state. While\nthe sandwich and spherical models can produce similar spectra in both the hard\nand soft states, the simulated IXPE polarimetric spectra show the potential to\nbreak this degeneracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the coronal geometry in different states of black hole X-ray\nbinaries is important for more accurate modeling of the system. However, it is\ndifficult to distinguish different geometries by fitting the observed\nComptonization spectra. In this work, we use the Monte Carlo ray-tracing code\nMONK to simulate the spectra for three widely proposed coronal geometries:\nsandwich, spherical, and lamppost, varying their optical depth and size\n(height). By fitting the simulated NuSTAR observations with the simplcut*kerrbb\nmodel, we infer the possible parameter space for the hard state and soft state\nof different coronal geometries. The influence of the disk inclination angle\nand black hole spin is discussed. We find that for the lamppost model the disk\nemission is always dominant, making it incompatible in the hard state. While\nthe sandwich and spherical models can produce similar spectra in both the hard\nand soft states, the simulated IXPE polarimetric spectra show the potential to\nbreak this degeneracy."
                },
                "authors": [
                    {
                        "name": "Ningyue Fan"
                    },
                    {
                        "name": "Cosimo Bambi"
                    },
                    {
                        "name": "James F. Steiner"
                    },
                    {
                        "name": "Wenda Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenda Zhang"
                },
                "author": "Wenda Zhang",
                "arxiv_comment": "13 pages, 11 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17052v2",
                "updated": "2025-08-29T10:47:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    47,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-23T19:00:39Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    19,
                    0,
                    39,
                    2,
                    113,
                    0
                ],
                "title": "Testing Conviction: An Argumentative Framework for Measuring LLM\n  Political Stability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Conviction: An Argumentative Framework for Measuring LLM\n  Political Stability"
                },
                "summary": "Large Language Models (LLMs) increasingly shape political discourse, yet\nexhibit inconsistent responses when challenged. While prior research\ncategorizes LLMs as left- or right-leaning based on single-prompt responses, a\ncritical question remains: Do these classifications reflect stable ideologies\nor superficial mimicry? Existing methods cannot distinguish between genuine\nideological alignment and performative text generation. To address this, we\npropose a framework for evaluating ideological depth through (1) argumentative\nconsistency and (2) uncertainty quantification. Testing 12 LLMs on 19 economic\npolicies from the Political Compass Test, we classify responses as stable or\nperformative ideological positioning. Results show 95% of left-leaning models\nand 89% of right-leaning models demonstrate behavior consistent with our\nclassifications across different experimental conditions. Furthermore, semantic\nentropy strongly validates our classifications (AUROC=0.78), revealing\nuncertainty's relationship to ideological consistency. Our findings demonstrate\nthat ideological stability is topic-dependent and challenge the notion of\nmonolithic LLM ideologies, and offer a robust way to distinguish genuine\nalignment from performative behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly shape political discourse, yet\nexhibit inconsistent responses when challenged. While prior research\ncategorizes LLMs as left- or right-leaning based on single-prompt responses, a\ncritical question remains: Do these classifications reflect stable ideologies\nor superficial mimicry? Existing methods cannot distinguish between genuine\nideological alignment and performative text generation. To address this, we\npropose a framework for evaluating ideological depth through (1) argumentative\nconsistency and (2) uncertainty quantification. Testing 12 LLMs on 19 economic\npolicies from the Political Compass Test, we classify responses as stable or\nperformative ideological positioning. Results show 95% of left-leaning models\nand 89% of right-leaning models demonstrate behavior consistent with our\nclassifications across different experimental conditions. Furthermore, semantic\nentropy strongly validates our classifications (AUROC=0.78), revealing\nuncertainty's relationship to ideological consistency. Our findings demonstrate\nthat ideological stability is topic-dependent and challenge the notion of\nmonolithic LLM ideologies, and offer a robust way to distinguish genuine\nalignment from performative behavior."
                },
                "authors": [
                    {
                        "name": "Shariar Kabir"
                    },
                    {
                        "name": "Kevin Esterling"
                    },
                    {
                        "name": "Yue Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yue Dong"
                },
                "author": "Yue Dong",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08005v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08005v6",
                "updated": "2025-08-29T10:44:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    44,
                    32,
                    4,
                    241,
                    0
                ],
                "published": "2025-01-14T10:49:26Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    10,
                    49,
                    26,
                    1,
                    14,
                    0
                ],
                "title": "DisCoPatch: Taming Adversarially-driven Batch Statistics for Improved\n  Out-of-Distribution Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DisCoPatch: Taming Adversarially-driven Batch Statistics for Improved\n  Out-of-Distribution Detection"
                },
                "summary": "Out-of-distribution (OOD) detection holds significant importance across many\napplications. While semantic and domain-shift OOD problems are well-studied,\nthis work focuses on covariate shifts - subtle variations in the data\ndistribution that can degrade machine learning performance. We hypothesize that\ndetecting these subtle shifts can improve our understanding of in-distribution\nboundaries, ultimately improving OOD detection. In adversarial discriminators\ntrained with Batch Normalization (BN), real and adversarial samples form\ndistinct domains with unique batch statistics - a property we exploit for OOD\ndetection. We introduce DisCoPatch, an unsupervised Adversarial Variational\nAutoencoder (VAE) framework that harnesses this mechanism. During inference,\nbatches consist of patches from the same image, ensuring a consistent data\ndistribution that allows the model to rely on batch statistics. DisCoPatch uses\nthe VAE's suboptimal outputs (generated and reconstructed) as negative samples\nto train the discriminator, thereby improving its ability to delineate the\nboundary between in-distribution samples and covariate shifts. By tightening\nthis boundary, DisCoPatch achieves state-of-the-art results in public OOD\ndetection benchmarks. The proposed model not only excels in detecting covariate\nshifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior\nmethods on public Near-OOD (95.0%) benchmarks. With a compact model size of\n25MB, it achieves high OOD detection performance at notably lower latency than\nexisting methods, making it an efficient and practical solution for real-world\nOOD detection applications. The code is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection holds significant importance across many\napplications. While semantic and domain-shift OOD problems are well-studied,\nthis work focuses on covariate shifts - subtle variations in the data\ndistribution that can degrade machine learning performance. We hypothesize that\ndetecting these subtle shifts can improve our understanding of in-distribution\nboundaries, ultimately improving OOD detection. In adversarial discriminators\ntrained with Batch Normalization (BN), real and adversarial samples form\ndistinct domains with unique batch statistics - a property we exploit for OOD\ndetection. We introduce DisCoPatch, an unsupervised Adversarial Variational\nAutoencoder (VAE) framework that harnesses this mechanism. During inference,\nbatches consist of patches from the same image, ensuring a consistent data\ndistribution that allows the model to rely on batch statistics. DisCoPatch uses\nthe VAE's suboptimal outputs (generated and reconstructed) as negative samples\nto train the discriminator, thereby improving its ability to delineate the\nboundary between in-distribution samples and covariate shifts. By tightening\nthis boundary, DisCoPatch achieves state-of-the-art results in public OOD\ndetection benchmarks. The proposed model not only excels in detecting covariate\nshifts, achieving 95.5% AUROC on ImageNet-1K(-C) but also outperforms all prior\nmethods on public Near-OOD (95.0%) benchmarks. With a compact model size of\n25MB, it achieves high OOD detection performance at notably lower latency than\nexisting methods, making it an efficient and practical solution for real-world\nOOD detection applications. The code is publicly available."
                },
                "authors": [
                    {
                        "name": "Francisco Caetano"
                    },
                    {
                        "name": "Christiaan Viviers"
                    },
                    {
                        "name": "Luis A. Zavala-Mondragón"
                    },
                    {
                        "name": "Peter H. N. de With"
                    },
                    {
                        "name": "Fons van der Sommen"
                    }
                ],
                "author_detail": {
                    "name": "Fons van der Sommen"
                },
                "author": "Fons van der Sommen",
                "arxiv_comment": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08005v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08005v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21505v1",
                "updated": "2025-08-29T10:37:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    37,
                    37,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T10:37:37Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    37,
                    37,
                    4,
                    241,
                    0
                ],
                "title": "Spiking Decision Transformers: Local Plasticity, Phase-Coding, and\n  Dendritic Routing for Low-Power Sequence Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Decision Transformers: Local Plasticity, Phase-Coding, and\n  Dendritic Routing for Low-Power Sequence Control"
                },
                "summary": "Reinforcement learning agents based on Transformer architectures have\nachieved impressive performance on sequential decision-making tasks, but their\nreliance on dense matrix operations makes them ill-suited for\nenergy-constrained, edge-oriented platforms. Spiking neural networks promise\nultra-low-power, event-driven inference, yet no prior work has seamlessly\nmerged spiking dynamics with return-conditioned sequence modeling. We present\nthe Spiking Decision Transformer (SNN-DT), which embeds Leaky\nIntegrate-and-Fire neurons into each self-attention block, trains end-to-end\nvia surrogate gradients, and incorporates biologically inspired three-factor\nplasticity, phase-shifted spike-based positional encodings, and a lightweight\ndendritic routing module. Our implementation matches or exceeds standard\nDecision Transformer performance on classic control benchmarks (CartPole-v1,\nMountainCar-v0, Acrobot-v1, Pendulum-v1) while emitting fewer than ten spikes\nper decision, an energy proxy suggesting over four orders-of-magnitude\nreduction in per inference energy. By marrying sequence modeling with\nneuromorphic efficiency, SNN-DT opens a pathway toward real-time, low-power\ncontrol on embedded and wearable devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning agents based on Transformer architectures have\nachieved impressive performance on sequential decision-making tasks, but their\nreliance on dense matrix operations makes them ill-suited for\nenergy-constrained, edge-oriented platforms. Spiking neural networks promise\nultra-low-power, event-driven inference, yet no prior work has seamlessly\nmerged spiking dynamics with return-conditioned sequence modeling. We present\nthe Spiking Decision Transformer (SNN-DT), which embeds Leaky\nIntegrate-and-Fire neurons into each self-attention block, trains end-to-end\nvia surrogate gradients, and incorporates biologically inspired three-factor\nplasticity, phase-shifted spike-based positional encodings, and a lightweight\ndendritic routing module. Our implementation matches or exceeds standard\nDecision Transformer performance on classic control benchmarks (CartPole-v1,\nMountainCar-v0, Acrobot-v1, Pendulum-v1) while emitting fewer than ten spikes\nper decision, an energy proxy suggesting over four orders-of-magnitude\nreduction in per inference energy. By marrying sequence modeling with\nneuromorphic efficiency, SNN-DT opens a pathway toward real-time, low-power\ncontrol on embedded and wearable devices."
                },
                "authors": [
                    {
                        "name": "Vishal Pandey"
                    },
                    {
                        "name": "Debasmita Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Biswas"
                },
                "author": "Debasmita Biswas",
                "arxiv_comment": "Preprint (31 pages, 19 images, 7 tables)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21495v1",
                "updated": "2025-08-29T10:21:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    21,
                    46,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T10:21:46Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    21,
                    46,
                    4,
                    241,
                    0
                ],
                "title": "Failure Prediction Is a Better Performance Proxy for Early-Exit Networks\n  Than Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure Prediction Is a Better Performance Proxy for Early-Exit Networks\n  Than Calibration"
                },
                "summary": "Early-exit models speed up inference by attaching internal classifiers to\nintermediate layers of the model and allowing computation to stop once a\nprediction satisfies an exit criterion. Most early-exit methods rely on\nconfidence-based exit strategies, which motivated some works to calibrate\nintermediate classifiers to improve the performance of the entire model. In\nthis paper, we show that calibration measures can be misleading indicators of\nthe performance of multi-exit models: a well-calibrated classifier may still\nwaste computation, and common calibration methods do not preserve the sample\nranking within a classifier. We demonstrate empirical cases where miscalibrated\nnetworks outperform calibrated ones. As an alternative, we propose to use\nfailure prediction as a more useful proxy for early-exit model performance.\nUnlike calibration, failure prediction accounts for changes in the ranking of\nsamples and shows a strong correlation with efficiency improvements, making it\na more dependable basis for designing and evaluating early-exit models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Early-exit models speed up inference by attaching internal classifiers to\nintermediate layers of the model and allowing computation to stop once a\nprediction satisfies an exit criterion. Most early-exit methods rely on\nconfidence-based exit strategies, which motivated some works to calibrate\nintermediate classifiers to improve the performance of the entire model. In\nthis paper, we show that calibration measures can be misleading indicators of\nthe performance of multi-exit models: a well-calibrated classifier may still\nwaste computation, and common calibration methods do not preserve the sample\nranking within a classifier. We demonstrate empirical cases where miscalibrated\nnetworks outperform calibrated ones. As an alternative, we propose to use\nfailure prediction as a more useful proxy for early-exit model performance.\nUnlike calibration, failure prediction accounts for changes in the ranking of\nsamples and shows a strong correlation with efficiency improvements, making it\na more dependable basis for designing and evaluating early-exit models."
                },
                "authors": [
                    {
                        "name": "Piotr Kubaty"
                    },
                    {
                        "name": "Filip Szatkowski"
                    },
                    {
                        "name": "Metod Jazbec"
                    },
                    {
                        "name": "Bartosz Wójcik"
                    }
                ],
                "author_detail": {
                    "name": "Bartosz Wójcik"
                },
                "author": "Bartosz Wójcik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21493v1",
                "updated": "2025-08-29T10:19:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    19,
                    6,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T10:19:06Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    19,
                    6,
                    4,
                    241,
                    0
                ],
                "title": "SIRA: Scaled-Integer Range Analysis for Optimizing FPGA Dataflow Neural\n  Network Accelerators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIRA: Scaled-Integer Range Analysis for Optimizing FPGA Dataflow Neural\n  Network Accelerators"
                },
                "summary": "While neural network quantization effectively reduces the cost of matrix\nmultiplications, aggressive quantization can expose non-matrix-multiply\noperations as significant performance and resource bottlenecks on embedded\nsystems. Addressing such bottlenecks requires a comprehensive approach to\ntailoring the precision across operations in the inference computation. To this\nend, we introduce scaled-integer range analysis (SIRA), a static analysis\ntechnique employing interval arithmetic to determine the range, scale, and bias\nfor tensors in quantized neural networks. We show how this information can be\nexploited to reduce the resource footprint of FPGA dataflow neural network\naccelerators via tailored bitwidth adaptation for accumulators and downstream\noperations, aggregation of scales and biases, and conversion of consecutive\nelementwise operations to thresholding operations. We integrate SIRA-driven\noptimizations into the open-source FINN framework, then evaluate their\neffectiveness across a range of quantized neural network workloads and compare\nimplementation alternatives for non-matrix-multiply operations. We demonstrate\nan average reduction of 17% for LUTs, 66% for DSPs, and 22% for accumulator\nbitwidths with SIRA optimizations, providing detailed benchmark analysis and\nanalytical models to guide the implementation style for non-matrix layers.\nFinally, we open-source SIRA to facilitate community exploration of its\nbenefits across various applications and hardware platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While neural network quantization effectively reduces the cost of matrix\nmultiplications, aggressive quantization can expose non-matrix-multiply\noperations as significant performance and resource bottlenecks on embedded\nsystems. Addressing such bottlenecks requires a comprehensive approach to\ntailoring the precision across operations in the inference computation. To this\nend, we introduce scaled-integer range analysis (SIRA), a static analysis\ntechnique employing interval arithmetic to determine the range, scale, and bias\nfor tensors in quantized neural networks. We show how this information can be\nexploited to reduce the resource footprint of FPGA dataflow neural network\naccelerators via tailored bitwidth adaptation for accumulators and downstream\noperations, aggregation of scales and biases, and conversion of consecutive\nelementwise operations to thresholding operations. We integrate SIRA-driven\noptimizations into the open-source FINN framework, then evaluate their\neffectiveness across a range of quantized neural network workloads and compare\nimplementation alternatives for non-matrix-multiply operations. We demonstrate\nan average reduction of 17% for LUTs, 66% for DSPs, and 22% for accumulator\nbitwidths with SIRA optimizations, providing detailed benchmark analysis and\nanalytical models to guide the implementation style for non-matrix layers.\nFinally, we open-source SIRA to facilitate community exploration of its\nbenefits across various applications and hardware platforms."
                },
                "authors": [
                    {
                        "name": "Yaman Umuroglu"
                    },
                    {
                        "name": "Christoph Berganski"
                    },
                    {
                        "name": "Felix Jentzsch"
                    },
                    {
                        "name": "Michal Danilowicz"
                    },
                    {
                        "name": "Tomasz Kryjak"
                    },
                    {
                        "name": "Charalampos Bezaitis"
                    },
                    {
                        "name": "Magnus Sjalander"
                    },
                    {
                        "name": "Ian Colbert"
                    },
                    {
                        "name": "Thomas Preusser"
                    },
                    {
                        "name": "Jakoba Petri-Koenig"
                    },
                    {
                        "name": "Michaela Blott"
                    }
                ],
                "author_detail": {
                    "name": "Michaela Blott"
                },
                "author": "Michaela Blott",
                "arxiv_comment": "Submitted to ACM TRETS Special Issue on Open-Source Tools for\n  Reconfigurable Devices and Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21491v1",
                "updated": "2025-08-29T10:16:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    16,
                    37,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T10:16:37Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    16,
                    37,
                    4,
                    241,
                    0
                ],
                "title": "Geospatial Question Answering on Historical Maps Using Spatio-Temporal\n  Knowledge Graphs and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geospatial Question Answering on Historical Maps Using Spatio-Temporal\n  Knowledge Graphs and Large Language Models"
                },
                "summary": "Recent advances have enabled the extraction of vectorized features from\ndigital historical maps. To fully leverage this information, however, the\nextracted features must be organized in a structured and meaningful way that\nsupports efficient access and use. One promising approach is question answering\n(QA), which allows users -- especially those unfamiliar with database query\nlanguages -- to retrieve knowledge in a natural and intuitive manner. In this\nproject, we developed a GeoQA system by integrating a spatio-temporal knowledge\ngraph (KG) constructed from historical map data with large language models\n(LLMs). Specifically, we have defined the ontology to guide the construction of\nthe spatio-temporal KG and investigated workflows of two different types of\nGeoQA: factual and descriptive. Additional data sources, such as historical map\nimages and internet search results, are incorporated into our framework to\nprovide extra context for descriptive GeoQA. Evaluation results demonstrate\nthat the system can generate answers with a high delivery rate and a high\nsemantic accuracy. To make the framework accessible, we further developed a web\napplication that supports interactive querying and visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have enabled the extraction of vectorized features from\ndigital historical maps. To fully leverage this information, however, the\nextracted features must be organized in a structured and meaningful way that\nsupports efficient access and use. One promising approach is question answering\n(QA), which allows users -- especially those unfamiliar with database query\nlanguages -- to retrieve knowledge in a natural and intuitive manner. In this\nproject, we developed a GeoQA system by integrating a spatio-temporal knowledge\ngraph (KG) constructed from historical map data with large language models\n(LLMs). Specifically, we have defined the ontology to guide the construction of\nthe spatio-temporal KG and investigated workflows of two different types of\nGeoQA: factual and descriptive. Additional data sources, such as historical map\nimages and internet search results, are incorporated into our framework to\nprovide extra context for descriptive GeoQA. Evaluation results demonstrate\nthat the system can generate answers with a high delivery rate and a high\nsemantic accuracy. To make the framework accessible, we further developed a web\napplication that supports interactive querying and visualization."
                },
                "authors": [
                    {
                        "name": "Ziyi Liu"
                    },
                    {
                        "name": "Sidi Wu"
                    },
                    {
                        "name": "Lorenz Hurni"
                    }
                ],
                "author_detail": {
                    "name": "Lorenz Hurni"
                },
                "author": "Lorenz Hurni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2106.11640v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2106.11640v4",
                "updated": "2025-08-29T10:13:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    13,
                    47,
                    4,
                    241,
                    0
                ],
                "published": "2021-06-22T09:47:28Z",
                "published_parsed": [
                    2021,
                    6,
                    22,
                    9,
                    47,
                    28,
                    1,
                    173,
                    0
                ],
                "title": "Discovering Heterogeneous Treatment Effects in Regression Discontinuity\n  Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering Heterogeneous Treatment Effects in Regression Discontinuity\n  Designs"
                },
                "summary": "The paper proposes a causal supervised machine learning algorithm to uncover\ntreatment effect heterogeneity in sharp and fuzzy regression discontinuity (RD)\ndesigns. We develop a criterion for building an honest ``regression\ndiscontinuity tree'', where each leaf contains the RD estimate of a treatment\nconditional on the values of some pre-treatment covariates. It is a priori\nunknown which covariates are relevant for capturing treatment effect\nheterogeneity, and it is the task of the algorithm to discover them, without\ninvalidating inference, while employing a nonparametric estimator with expected\nMSE optimal bandwidth. We study the performance of the method through Monte\nCarlo simulations and apply it to uncover various sources of heterogeneity in\nthe impact of attending a better secondary school in Romania.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper proposes a causal supervised machine learning algorithm to uncover\ntreatment effect heterogeneity in sharp and fuzzy regression discontinuity (RD)\ndesigns. We develop a criterion for building an honest ``regression\ndiscontinuity tree'', where each leaf contains the RD estimate of a treatment\nconditional on the values of some pre-treatment covariates. It is a priori\nunknown which covariates are relevant for capturing treatment effect\nheterogeneity, and it is the task of the algorithm to discover them, without\ninvalidating inference, while employing a nonparametric estimator with expected\nMSE optimal bandwidth. We study the performance of the method through Monte\nCarlo simulations and apply it to uncover various sources of heterogeneity in\nthe impact of attending a better secondary school in Romania."
                },
                "authors": [
                    {
                        "name": "Ágoston Reguly"
                    }
                ],
                "author_detail": {
                    "name": "Ágoston Reguly"
                },
                "author": "Ágoston Reguly",
                "arxiv_comment": "39 pages, 8 tables, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2106.11640v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2106.11640v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21484v1",
                "updated": "2025-08-29T10:10:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    10,
                    2,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T10:10:02Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    10,
                    2,
                    4,
                    241,
                    0
                ],
                "title": "Data-driven Discovery of Digital Twins in Biomedical Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-driven Discovery of Digital Twins in Biomedical Research"
                },
                "summary": "Recent technological advances have expanded the availability of\nhigh-throughput biological datasets, enabling the reliable design of digital\ntwins of biomedical systems or patients. Such computational tools represent key\nreaction networks driving perturbation or drug response and can guide drug\ndiscovery and personalized therapeutics. Yet, their development still relies on\nlaborious data integration by the human modeler, so that automated approaches\nare critically needed. The success of data-driven system discovery in Physics,\nrooted in clean datasets and well-defined governing laws, has fueled interest\nin applying similar techniques in Biology, which presents unique challenges.\nHere, we reviewed methodologies for automatically inferring digital twins from\nbiological time series, which mostly involve symbolic or sparse regression. We\nevaluate algorithms according to eight biological and methodological\nchallenges, associated to noisy/incomplete data, multiple conditions, prior\nknowledge integration, latent variables, high dimensionality, unobserved\nvariable derivatives, candidate library design, and uncertainty quantification.\nUpon these criteria, sparse regression generally outperformed symbolic\nregression, particularly when using Bayesian frameworks. We further highlight\nthe emerging role of deep learning and large language models, which enable\ninnovative prior knowledge integration, though the reliability and consistency\nof such approaches must be improved. While no single method addresses all\nchallenges, we argue that progress in learning digital twins will come from\nhybrid and modular frameworks combining chemical reaction network-based\nmechanistic grounding, Bayesian uncertainty quantification, and the generative\nand knowledge integration capacities of deep learning. To support their\ndevelopment, we further propose a benchmarking framework to evaluate methods\nacross all challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent technological advances have expanded the availability of\nhigh-throughput biological datasets, enabling the reliable design of digital\ntwins of biomedical systems or patients. Such computational tools represent key\nreaction networks driving perturbation or drug response and can guide drug\ndiscovery and personalized therapeutics. Yet, their development still relies on\nlaborious data integration by the human modeler, so that automated approaches\nare critically needed. The success of data-driven system discovery in Physics,\nrooted in clean datasets and well-defined governing laws, has fueled interest\nin applying similar techniques in Biology, which presents unique challenges.\nHere, we reviewed methodologies for automatically inferring digital twins from\nbiological time series, which mostly involve symbolic or sparse regression. We\nevaluate algorithms according to eight biological and methodological\nchallenges, associated to noisy/incomplete data, multiple conditions, prior\nknowledge integration, latent variables, high dimensionality, unobserved\nvariable derivatives, candidate library design, and uncertainty quantification.\nUpon these criteria, sparse regression generally outperformed symbolic\nregression, particularly when using Bayesian frameworks. We further highlight\nthe emerging role of deep learning and large language models, which enable\ninnovative prior knowledge integration, though the reliability and consistency\nof such approaches must be improved. While no single method addresses all\nchallenges, we argue that progress in learning digital twins will come from\nhybrid and modular frameworks combining chemical reaction network-based\nmechanistic grounding, Bayesian uncertainty quantification, and the generative\nand knowledge integration capacities of deep learning. To support their\ndevelopment, we further propose a benchmarking framework to evaluate methods\nacross all challenges."
                },
                "authors": [
                    {
                        "name": "Clémence Métayer"
                    },
                    {
                        "name": "Annabelle Ballesta"
                    },
                    {
                        "name": "Julien Martinelli"
                    }
                ],
                "author_detail": {
                    "name": "Julien Martinelli"
                },
                "author": "Julien Martinelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21481v1",
                "updated": "2025-08-29T10:06:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    6,
                    3,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T10:06:03Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    6,
                    3,
                    4,
                    241,
                    0
                ],
                "title": "Mixed Dark Matter and Galaxy Clustering: The Importance of Relative\n  Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixed Dark Matter and Galaxy Clustering: The Importance of Relative\n  Perturbations"
                },
                "summary": "We develop a perturbative model to describe large-scale structure in\ncosmologies where dark matter consists of a mixture of cold (CDM) and warm\n(WDM) components. In such mixed dark matter (MDM) scenarios, even a subdominant\nwarm component can introduce distinctive signatures via its free-streaming\neffects, altering the evolution of density and velocity perturbations. We\npresent linear-order solutions for both total and relative perturbations in the\ntwo-fluid system, identifying novel contributions to galaxy bias caused by the\nrelative density and velocity modes between the components. Incorporating these\neffects into the galaxy bias expansion, we compute the linear galaxy power\nspectrum in both real and redshift space. Using Fisher matrix forecasts, we\nassess the sensitivity of upcoming surveys such as DESI and PFS to MDM\nscenarios. Our results demonstrate that neglecting relative perturbations can\nlead to significant biases in inferred constraints on the warm dark matter\nfraction, particularly for lighter WDM masses ($\\lesssim 150~\\mathrm{eV}$ and\n$\\lesssim 80~\\mathrm{eV}$) for PFS and DESI, respectively. This framework\nprovides a consistent and generalizable approach for incorporating\nmulti-component dark matter dynamics into galaxy clustering analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop a perturbative model to describe large-scale structure in\ncosmologies where dark matter consists of a mixture of cold (CDM) and warm\n(WDM) components. In such mixed dark matter (MDM) scenarios, even a subdominant\nwarm component can introduce distinctive signatures via its free-streaming\neffects, altering the evolution of density and velocity perturbations. We\npresent linear-order solutions for both total and relative perturbations in the\ntwo-fluid system, identifying novel contributions to galaxy bias caused by the\nrelative density and velocity modes between the components. Incorporating these\neffects into the galaxy bias expansion, we compute the linear galaxy power\nspectrum in both real and redshift space. Using Fisher matrix forecasts, we\nassess the sensitivity of upcoming surveys such as DESI and PFS to MDM\nscenarios. Our results demonstrate that neglecting relative perturbations can\nlead to significant biases in inferred constraints on the warm dark matter\nfraction, particularly for lighter WDM masses ($\\lesssim 150~\\mathrm{eV}$ and\n$\\lesssim 80~\\mathrm{eV}$) for PFS and DESI, respectively. This framework\nprovides a consistent and generalizable approach for incorporating\nmulti-component dark matter dynamics into galaxy clustering analyses."
                },
                "authors": [
                    {
                        "name": "Şafak Çelik"
                    },
                    {
                        "name": "Fabian Schmidt"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Schmidt"
                },
                "author": "Fabian Schmidt",
                "arxiv_comment": "29 pages, 11 figures; comments are welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12800v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12800v3",
                "updated": "2025-08-29T10:05:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    5,
                    13,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-18T10:23:10Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    10,
                    23,
                    10,
                    0,
                    230,
                    0
                ],
                "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic\n  Thought Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic\n  Thought Reward"
                },
                "summary": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns."
                },
                "authors": [
                    {
                        "name": "Yong Deng"
                    },
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Zhenzhe Ying"
                    },
                    {
                        "name": "Xiaofeng Wu"
                    },
                    {
                        "name": "Jinzhen Lin"
                    },
                    {
                        "name": "Wenwen Xiong"
                    },
                    {
                        "name": "Yuqin Dai"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Zhanwei Zhang"
                    },
                    {
                        "name": "Qiwen Wang"
                    },
                    {
                        "name": "Yang Qin"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Quanxing Zha"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Changhua Meng"
                    }
                ],
                "author_detail": {
                    "name": "Changhua Meng"
                },
                "author": "Changhua Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12800v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12800v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21476v1",
                "updated": "2025-08-29T10:00:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    0,
                    55,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T10:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    0,
                    55,
                    4,
                    241,
                    0
                ],
                "title": "Igniting Creative Writing in Small Language Models: LLM-as-a-Judge\n  versus Multi-Agent Refined Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Igniting Creative Writing in Small Language Models: LLM-as-a-Judge\n  versus Multi-Agent Refined Rewards"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable creative writing\ncapabilities, yet their substantial computational demands hinder widespread\nuse. Enhancing Small Language Models (SLMs) offers a promising alternative, but\ncurrent methods like Supervised Fine-Tuning (SFT) struggle with novelty, and\nReinforcement Learning from Human Feedback (RLHF) is costly. This paper\nexplores two distinct AI-driven reward strategies within a Reinforcement\nLearning from AI Feedback (RLAIF) framework to ignite the creative writing of a\n7B-parameter SLM, specifically for generating Chinese greetings. The first\nstrategy employs a RM trained on high-quality preference data curated by a\nnovel multi-agent rejection sampling framework designed for creative tasks. The\nsecond, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose\nreward function is optimized via an adversarial training scheme with a\nreflection mechanism, to directly provide reward signals. Comprehensive\nexperiments reveal that while both approaches significantly enhance creative\noutput over baselines, the principle-guided LLM-as-a-Judge demonstrably yields\nsuperior generation quality. Furthermore, it offers notable advantages in\ntraining efficiency and reduced dependency on human-annotated data, presenting\na more scalable and effective path towards creative SLMs. Our automated\nevaluation methods also exhibit strong alignment with human judgments. Our code\nand data are publicly available at\nhttps://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable creative writing\ncapabilities, yet their substantial computational demands hinder widespread\nuse. Enhancing Small Language Models (SLMs) offers a promising alternative, but\ncurrent methods like Supervised Fine-Tuning (SFT) struggle with novelty, and\nReinforcement Learning from Human Feedback (RLHF) is costly. This paper\nexplores two distinct AI-driven reward strategies within a Reinforcement\nLearning from AI Feedback (RLAIF) framework to ignite the creative writing of a\n7B-parameter SLM, specifically for generating Chinese greetings. The first\nstrategy employs a RM trained on high-quality preference data curated by a\nnovel multi-agent rejection sampling framework designed for creative tasks. The\nsecond, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose\nreward function is optimized via an adversarial training scheme with a\nreflection mechanism, to directly provide reward signals. Comprehensive\nexperiments reveal that while both approaches significantly enhance creative\noutput over baselines, the principle-guided LLM-as-a-Judge demonstrably yields\nsuperior generation quality. Furthermore, it offers notable advantages in\ntraining efficiency and reduced dependency on human-annotated data, presenting\na more scalable and effective path towards creative SLMs. Our automated\nevaluation methods also exhibit strong alignment with human judgments. Our code\nand data are publicly available at\nhttps://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models."
                },
                "authors": [
                    {
                        "name": "Xiaolong Wei"
                    },
                    {
                        "name": "Bo Lu"
                    },
                    {
                        "name": "Xingyu Zhang"
                    },
                    {
                        "name": "Zhejun Zhao"
                    },
                    {
                        "name": "Dongdong Shen"
                    },
                    {
                        "name": "Long Xia"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10187v2",
                "updated": "2025-08-29T09:58:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    42,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-14T12:40:39Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    40,
                    39,
                    0,
                    104,
                    0
                ],
                "title": "DeepTrans: Deep Reasoning Translation via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepTrans: Deep Reasoning Translation via Reinforcement Learning"
                },
                "summary": "Recently, deep reasoning LLMs (e.g., OpenAI o1 and DeepSeek-R1) have shown\npromising performance in various downstream tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation. However, the task is still under-explored in\ndeep reasoning LLMs. In this paper, we introduce DeepTrans, a deep reasoning\ntranslation model that learns free translation via reinforcement learning (RL).\nSpecifically, we carefully build a reward model with pre-defined scoring\ncriteria on both the translation results and the thought processes. The reward\nmodel teaches DeepTrans how to think and free-translate the given sentences\nduring RL. Besides, our RL training does not need any labeled translations,\navoiding the human-intensive annotation or resource-intensive data synthesis.\nExperimental results show the effectiveness of DeepTrans. Using Qwen2.5-7B as\nthe backbone, DeepTrans improves performance by 16.3% in literature\ntranslation, and outperforms strong deep reasoning LLMs. Moreover, we summarize\nthe failures and interesting findings during our RL exploration. We hope this\nwork could inspire other researchers in free translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, deep reasoning LLMs (e.g., OpenAI o1 and DeepSeek-R1) have shown\npromising performance in various downstream tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation. However, the task is still under-explored in\ndeep reasoning LLMs. In this paper, we introduce DeepTrans, a deep reasoning\ntranslation model that learns free translation via reinforcement learning (RL).\nSpecifically, we carefully build a reward model with pre-defined scoring\ncriteria on both the translation results and the thought processes. The reward\nmodel teaches DeepTrans how to think and free-translate the given sentences\nduring RL. Besides, our RL training does not need any labeled translations,\navoiding the human-intensive annotation or resource-intensive data synthesis.\nExperimental results show the effectiveness of DeepTrans. Using Qwen2.5-7B as\nthe backbone, DeepTrans improves performance by 16.3% in literature\ntranslation, and outperforms strong deep reasoning LLMs. Moreover, we summarize\nthe failures and interesting findings during our RL exploration. We hope this\nwork could inspire other researchers in free translation."
                },
                "authors": [
                    {
                        "name": "Jiaan Wang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "arxiv_comment": "Accepted by Transactions of the Association for Computational\n  Linguistics (TACL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19238v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19238v3",
                "updated": "2025-08-29T09:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    18,
                    4,
                    241,
                    0
                ],
                "published": "2024-06-27T15:01:53Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    15,
                    1,
                    53,
                    3,
                    179,
                    0
                ],
                "title": "Revealing Fine-Grained Values and Opinions in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Fine-Grained Values and Opinions in Large Language Models"
                },
                "summary": "Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances."
                },
                "authors": [
                    {
                        "name": "Dustin Wright"
                    },
                    {
                        "name": "Arnav Arora"
                    },
                    {
                        "name": "Nadav Borenstein"
                    },
                    {
                        "name": "Srishti Yadav"
                    },
                    {
                        "name": "Serge Belongie"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "Findings of EMNLP 2024; 28 pages, 20 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19238v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19238v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05930v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05930v2",
                "updated": "2025-08-29T09:58:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-06T09:55:59Z",
                "published_parsed": [
                    2025,
                    6,
                    6,
                    9,
                    55,
                    59,
                    4,
                    157,
                    0
                ],
                "title": "Neural Visibility Cache for Real-Time Light Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Visibility Cache for Real-Time Light Sampling"
                },
                "summary": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct illumination with many lights is an inherent component of\nphysically-based rendering, remaining challenging, especially in real-time\nscenarios. We propose an online-trained neural cache that stores visibility\nbetween lights and 3D positions. We feed light visibility to weighted reservoir\nsampling (WRS) to sample a light source. The cache is implemented as a\nfully-fused multilayer perceptron (MLP) with multi-resolution hash-grid\nencoding, enabling online training and efficient inference on modern GPUs in\nreal-time frame rates. The cache can be seamlessly integrated into existing\nrendering frameworks and can be used in combination with other real-time\ntechniques such as spatiotemporal reservoir sampling (ReSTIR)."
                },
                "authors": [
                    {
                        "name": "Jakub Bokšanský"
                    },
                    {
                        "name": "Daniel Meister"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Meister"
                },
                "author": "Daniel Meister",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05930v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05930v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21457v1",
                "updated": "2025-08-29T09:39:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    39,
                    46,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T09:39:46Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    39,
                    46,
                    4,
                    241,
                    0
                ],
                "title": "SoK: Large Language Model-Generated Textual Phishing Campaigns\n  End-to-End Analysis of Generation, Characteristics, and Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Large Language Model-Generated Textual Phishing Campaigns\n  End-to-End Analysis of Generation, Characteristics, and Detection"
                },
                "summary": "Phishing is a pervasive form of social engineering in which attackers\nimpersonate trusted entities to steal information or induce harmful actions.\nText-based phishing dominates for its low cost, scalability, and\nconcealability, advantages recently amplified by large language models (LLMs)\nthat enable ``Phishing-as-a-Service'' attacks at scale within minutes. Despite\nthe growing research into LLM-facilitated phishing attacks, consolidated\nsystematic research on the phishing attack life cycle remains scarce. In this\nwork, we present the first systematization of knowledge (SoK) on LLM-generated\nphishing, offering an end-to-end analysis that spans generation techniques,\nattack features, and mitigation strategies. We introduce\nGeneration-Characterization-Defense (GenCharDef), which systematizes the ways\nin which LLM-generated phishing differs from traditional phishing across\nmethodologies, security perspectives, data dependencies, and evaluation\npractices. This framework highlights unique challenges of LLM-driven phishing,\nproviding a coherent foundation for understanding the evolving threat landscape\nand guiding the design of more resilient defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a pervasive form of social engineering in which attackers\nimpersonate trusted entities to steal information or induce harmful actions.\nText-based phishing dominates for its low cost, scalability, and\nconcealability, advantages recently amplified by large language models (LLMs)\nthat enable ``Phishing-as-a-Service'' attacks at scale within minutes. Despite\nthe growing research into LLM-facilitated phishing attacks, consolidated\nsystematic research on the phishing attack life cycle remains scarce. In this\nwork, we present the first systematization of knowledge (SoK) on LLM-generated\nphishing, offering an end-to-end analysis that spans generation techniques,\nattack features, and mitigation strategies. We introduce\nGeneration-Characterization-Defense (GenCharDef), which systematizes the ways\nin which LLM-generated phishing differs from traditional phishing across\nmethodologies, security perspectives, data dependencies, and evaluation\npractices. This framework highlights unique challenges of LLM-driven phishing,\nproviding a coherent foundation for understanding the evolving threat landscape\nand guiding the design of more resilient defenses."
                },
                "authors": [
                    {
                        "name": "Fengchao Chen"
                    },
                    {
                        "name": "Tingmin Wu"
                    },
                    {
                        "name": "Van Nguyen"
                    },
                    {
                        "name": "Carsten Rudolph"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Rudolph"
                },
                "author": "Carsten Rudolph",
                "arxiv_comment": "13 pages, 3 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04342v2",
                "updated": "2025-08-29T09:38:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    38,
                    13,
                    4,
                    241,
                    0
                ],
                "published": "2024-12-05T17:00:32Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    0,
                    32,
                    3,
                    340,
                    0
                ],
                "title": "Retrieval-Augmented Machine Translation with Unstructured Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Machine Translation with Unstructured Knowledge"
                },
                "summary": "Retrieval-augmented generation (RAG) introduces additional information to\nenhance large language models (LLMs). In machine translation (MT), previous\nwork typically retrieves in-context examples from paired MT corpora, or\ndomain-specific knowledge from knowledge graphs, to enhance MT models. However,\na large amount of world knowledge is organized in unstructured documents, and\nmight not be fully paired across different languages. In this paper, we study\nretrieval-augmented MT using unstructured documents. Specifically, we build\nRAGtrans, the first benchmark to train and evaluate LLMs' retrieval-augmented\nMT ability. RAGtrans contains 169K MT samples collected via GPT-4o and human\ntranslators. Besides, documents from various languages are also provided to\nsupply the knowledge to these samples. Based on RAGtrans, we further propose a\nmulti-task training method to teach LLMs how to use information from\nmultilingual documents during their translation. The method uses existing\nmultilingual corpora to create auxiliary training objectives without additional\nlabeling requirements. Extensive experiments show that the method improves LLMs\nby 1.6-3.1 BLEU and 1.0-2.0 COMET scores in En-Zh, and 1.7-2.9 BLEU and 2.1-2.7\nCOMET scores in En-De. We also conclude the critical difficulties that current\nLLMs face with this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) introduces additional information to\nenhance large language models (LLMs). In machine translation (MT), previous\nwork typically retrieves in-context examples from paired MT corpora, or\ndomain-specific knowledge from knowledge graphs, to enhance MT models. However,\na large amount of world knowledge is organized in unstructured documents, and\nmight not be fully paired across different languages. In this paper, we study\nretrieval-augmented MT using unstructured documents. Specifically, we build\nRAGtrans, the first benchmark to train and evaluate LLMs' retrieval-augmented\nMT ability. RAGtrans contains 169K MT samples collected via GPT-4o and human\ntranslators. Besides, documents from various languages are also provided to\nsupply the knowledge to these samples. Based on RAGtrans, we further propose a\nmulti-task training method to teach LLMs how to use information from\nmultilingual documents during their translation. The method uses existing\nmultilingual corpora to create auxiliary training objectives without additional\nlabeling requirements. Extensive experiments show that the method improves LLMs\nby 1.6-3.1 BLEU and 1.0-2.0 COMET scores in En-Zh, and 1.7-2.9 BLEU and 2.1-2.7\nCOMET scores in En-De. We also conclude the critical difficulties that current\nLLMs face with this task."
                },
                "authors": [
                    {
                        "name": "Jiaan Wang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20863v2",
                "updated": "2025-08-29T09:37:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    37,
                    59,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-28T14:57:04Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    57,
                    4,
                    3,
                    240,
                    0
                ],
                "title": "Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review"
                },
                "summary": "Large Language Models (LLMs) are increasingly being integrated into the\nscientific peer-review process, raising new questions about their reliability\nand resilience to manipulation. In this work, we investigate the potential for\nhidden prompt injection attacks, where authors embed adversarial text within a\npaper's PDF to influence the LLM-generated review. We begin by formalising\nthree distinct threat models that envision attackers with different motivations\n-- not all of which implying malicious intent. For each threat model, we design\nadversarial prompts that remain invisible to human readers yet can steer an\nLLM's output toward the author's desired outcome. Using a user study with\ndomain scholars, we derive four representative reviewing prompts used to elicit\npeer reviews from LLMs. We then evaluate the robustness of our adversarial\nprompts across (i) different reviewing prompts, (ii) different commercial\nLLM-based systems, and (iii) different peer-reviewed papers. Our results show\nthat adversarial prompts can reliably mislead the LLM, sometimes in ways that\nadversely affect a \"honest-but-lazy\" reviewer. Finally, we propose and\nempirically assess methods to reduce detectability of adversarial prompts under\nautomated content checks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being integrated into the\nscientific peer-review process, raising new questions about their reliability\nand resilience to manipulation. In this work, we investigate the potential for\nhidden prompt injection attacks, where authors embed adversarial text within a\npaper's PDF to influence the LLM-generated review. We begin by formalising\nthree distinct threat models that envision attackers with different motivations\n-- not all of which implying malicious intent. For each threat model, we design\nadversarial prompts that remain invisible to human readers yet can steer an\nLLM's output toward the author's desired outcome. Using a user study with\ndomain scholars, we derive four representative reviewing prompts used to elicit\npeer reviews from LLMs. We then evaluate the robustness of our adversarial\nprompts across (i) different reviewing prompts, (ii) different commercial\nLLM-based systems, and (iii) different peer-reviewed papers. Our results show\nthat adversarial prompts can reliably mislead the LLM, sometimes in ways that\nadversely affect a \"honest-but-lazy\" reviewer. Finally, we propose and\nempirically assess methods to reduce detectability of adversarial prompts under\nautomated content checks."
                },
                "authors": [
                    {
                        "name": "Matteo Gioele Collu"
                    },
                    {
                        "name": "Umberto Salviati"
                    },
                    {
                        "name": "Roberto Confalonieri"
                    },
                    {
                        "name": "Mauro Conti"
                    },
                    {
                        "name": "Giovanni Apruzzese"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Apruzzese"
                },
                "author": "Giovanni Apruzzese",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21454v1",
                "updated": "2025-08-29T09:37:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    37,
                    42,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T09:37:42Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    37,
                    42,
                    4,
                    241,
                    0
                ],
                "title": "Enhancing Semantic Understanding in Pointer Analysis using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Semantic Understanding in Pointer Analysis using Large\n  Language Models"
                },
                "summary": "Pointer analysis has been studied for over four decades. However, existing\nframeworks continue to suffer from the propagation of incorrect facts. A major\nlimitation stems from their insufficient semantic understanding of code,\nresulting in overly conservative treatment of user-defined functions. Recent\nadvances in large language models (LLMs) present new opportunities to bridge\nthis gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a\nvision that integrates LLMs into pointer analysis to enhance both precision and\nscalability. LMPA identifies user-defined functions that resemble system APIs\nand models them accordingly, thereby mitigating erroneous cross-calling-context\npropagation. Furthermore, it enhances summary-based analysis by inferring\ninitial points-to sets and introducing a novel summary strategy augmented with\nnatural language. Finally, we discuss the key challenges involved in realizing\nthis vision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pointer analysis has been studied for over four decades. However, existing\nframeworks continue to suffer from the propagation of incorrect facts. A major\nlimitation stems from their insufficient semantic understanding of code,\nresulting in overly conservative treatment of user-defined functions. Recent\nadvances in large language models (LLMs) present new opportunities to bridge\nthis gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a\nvision that integrates LLMs into pointer analysis to enhance both precision and\nscalability. LMPA identifies user-defined functions that resemble system APIs\nand models them accordingly, thereby mitigating erroneous cross-calling-context\npropagation. Furthermore, it enhances summary-based analysis by inferring\ninitial points-to sets and introducing a novel summary strategy augmented with\nnatural language. Finally, we discuss the key challenges involved in realizing\nthis vision."
                },
                "authors": [
                    {
                        "name": "Baijun Cheng"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Yao Guo"
                    },
                    {
                        "name": "Ding Li"
                    },
                    {
                        "name": "Xiangqun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiangqun Chen"
                },
                "author": "Xiangqun Chen",
                "arxiv_comment": "Accepted by LMPL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21452v1",
                "updated": "2025-08-29T09:36:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    36,
                    54,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T09:36:54Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    36,
                    54,
                    4,
                    241,
                    0
                ],
                "title": "From Canonical to Complex: Benchmarking LLM Capabilities in\n  Undergraduate Thermodynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Canonical to Complex: Benchmarking LLM Capabilities in\n  Undergraduate Thermodynamics"
                },
                "summary": "Large language models (LLMs) are increasingly considered as tutoring aids in\nscience education. Yet their readiness for unsupervised use in undergraduate\ninstruction remains uncertain, as reliable teaching requires more than fluent\nrecall: it demands consistent, principle-grounded reasoning. Thermodynamics,\nwith its compact laws and subtle distinctions between state and path functions,\nreversibility, and entropy, provides an ideal testbed for evaluating such\ncapabilities. Here we present UTQA, a 50-item undergraduate thermodynamics\nquestion answering benchmark, covering ideal-gas processes, reversibility, and\ndiagram interpretation. No leading 2025-era model exceeded our 95\\% competence\nthreshold: the best LLMs achieved 82\\% accuracy, with text-only items\nperforming better than image reasoning tasks, which often fell to chance\nlevels. Prompt phrasing and syntactic complexity showed modest to little\ncorrelation with performance. The gap concentrates in finite-rate/irreversible\nscenarios and in binding visual features to thermodynamic meaning, indicating\nthat current LLMs are not yet suitable for unsupervised tutoring in this\ndomain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly considered as tutoring aids in\nscience education. Yet their readiness for unsupervised use in undergraduate\ninstruction remains uncertain, as reliable teaching requires more than fluent\nrecall: it demands consistent, principle-grounded reasoning. Thermodynamics,\nwith its compact laws and subtle distinctions between state and path functions,\nreversibility, and entropy, provides an ideal testbed for evaluating such\ncapabilities. Here we present UTQA, a 50-item undergraduate thermodynamics\nquestion answering benchmark, covering ideal-gas processes, reversibility, and\ndiagram interpretation. No leading 2025-era model exceeded our 95\\% competence\nthreshold: the best LLMs achieved 82\\% accuracy, with text-only items\nperforming better than image reasoning tasks, which often fell to chance\nlevels. Prompt phrasing and syntactic complexity showed modest to little\ncorrelation with performance. The gap concentrates in finite-rate/irreversible\nscenarios and in binding visual features to thermodynamic meaning, indicating\nthat current LLMs are not yet suitable for unsupervised tutoring in this\ndomain."
                },
                "authors": [
                    {
                        "name": "Anna Geißler"
                    },
                    {
                        "name": "Luca-Sophie Bien"
                    },
                    {
                        "name": "Friedrich Schöppler"
                    },
                    {
                        "name": "Tobias Hertel"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Hertel"
                },
                "author": "Tobias Hertel",
                "arxiv_comment": "Benchmark downloadable at\n  https://huggingface.co/datasets/herteltm/UTQA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21448v1",
                "updated": "2025-08-29T09:27:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    27,
                    1,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T09:27:01Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    27,
                    1,
                    4,
                    241,
                    0
                ],
                "title": "Beyond the Surface: Probing the Ideological Depth of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Surface: Probing the Ideological Depth of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated pronounced ideological\nleanings, yet the stability and depth of these positions remain poorly\nunderstood. Surface-level responses can often be manipulated through simple\nprompt engineering, calling into question whether they reflect a coherent\nunderlying ideology. This paper investigates the concept of \"ideological depth\"\nin LLMs, defined as the robustness and complexity of their internal political\nrepresentations. We employ a dual approach: first, we measure the\n\"steerability\" of two well-known open-source LLMs using instruction prompting\nand activation steering. We find that while some models can easily switch\nbetween liberal and conservative viewpoints, others exhibit resistance or an\nincreased rate of refusal, suggesting a more entrenched ideological structure.\nSecond, we probe the internal mechanisms of these models using Sparse\nAutoencoders (SAEs). Preliminary analysis reveals that models with lower\nsteerability possess more distinct and abstract ideological features. Our\nevaluations reveal that one model can contain 7.3x more political features than\nanother model of similar size. This allows targeted ablation of a core\npolitical feature in an ideologically \"deep\" model, leading to consistent,\nlogical shifts in its reasoning across related topics, whereas the same\nintervention in a \"shallow\" model results in an increase in refusal outputs.\nOur findings suggest that ideological depth is a quantifiable property of LLMs\nand that steerability serves as a valuable window into their latent political\narchitecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated pronounced ideological\nleanings, yet the stability and depth of these positions remain poorly\nunderstood. Surface-level responses can often be manipulated through simple\nprompt engineering, calling into question whether they reflect a coherent\nunderlying ideology. This paper investigates the concept of \"ideological depth\"\nin LLMs, defined as the robustness and complexity of their internal political\nrepresentations. We employ a dual approach: first, we measure the\n\"steerability\" of two well-known open-source LLMs using instruction prompting\nand activation steering. We find that while some models can easily switch\nbetween liberal and conservative viewpoints, others exhibit resistance or an\nincreased rate of refusal, suggesting a more entrenched ideological structure.\nSecond, we probe the internal mechanisms of these models using Sparse\nAutoencoders (SAEs). Preliminary analysis reveals that models with lower\nsteerability possess more distinct and abstract ideological features. Our\nevaluations reveal that one model can contain 7.3x more political features than\nanother model of similar size. This allows targeted ablation of a core\npolitical feature in an ideologically \"deep\" model, leading to consistent,\nlogical shifts in its reasoning across related topics, whereas the same\nintervention in a \"shallow\" model results in an increase in refusal outputs.\nOur findings suggest that ideological depth is a quantifiable property of LLMs\nand that steerability serves as a valuable window into their latent political\narchitecture."
                },
                "authors": [
                    {
                        "name": "Shariar Kabir"
                    },
                    {
                        "name": "Kevin Esterling"
                    },
                    {
                        "name": "Yue Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yue Dong"
                },
                "author": "Yue Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12072v2",
                "updated": "2025-08-29T09:25:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    25,
                    55,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-05T01:48:09Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    1,
                    48,
                    9,
                    3,
                    156,
                    0
                ],
                "title": "TrueGL: A Truthful, Reliable, and Unified Engine for Grounded Learning\n  in Full-Stack Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrueGL: A Truthful, Reliable, and Unified Engine for Grounded Learning\n  in Full-Stack Search"
                },
                "summary": "In the age of open and free information, a concerning trend of reliance on AI\nis emerging. However, existing AI tools struggle to evaluate the credibility of\ninformation and to justify their assessments. Hence, there is a growing need\nfor systems that can help users evaluate the trustworthiness of online\ninformation. Although major search engines incorporate AI features, they often\nlack clear reliability indicators. We present TrueGL, a model that makes\ntrustworthy search results more accessible. The model is a fine-tuned version\nof IBM's Granite-1B, trained on the custom dataset and integrated into a search\nengine with a reliability scoring system. We evaluate the system using prompt\nengineering and assigning each statement a continuous reliability score from\n0.1 to 1, then instructing the model to return a textual explanation alongside\nthe score. Each model's predicted scores are measured against real scores using\nstandard evaluation metrics. TrueGL consistently outperforms other small-scale\nLLMs and rule-based approaches across all experiments on key evaluation\nmetrics, including MAE, RMSE, and R2. The model's high accuracy, broad content\ncoverage, and ease of use make trustworthy information more accessible and help\nreduce the spread of false or misleading content online. Our code is publicly\navailable at https://github.com/AlgazinovAleksandr/TrueGL, and our model is\npublicly released at https://huggingface.co/JoydeepC/trueGL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the age of open and free information, a concerning trend of reliance on AI\nis emerging. However, existing AI tools struggle to evaluate the credibility of\ninformation and to justify their assessments. Hence, there is a growing need\nfor systems that can help users evaluate the trustworthiness of online\ninformation. Although major search engines incorporate AI features, they often\nlack clear reliability indicators. We present TrueGL, a model that makes\ntrustworthy search results more accessible. The model is a fine-tuned version\nof IBM's Granite-1B, trained on the custom dataset and integrated into a search\nengine with a reliability scoring system. We evaluate the system using prompt\nengineering and assigning each statement a continuous reliability score from\n0.1 to 1, then instructing the model to return a textual explanation alongside\nthe score. Each model's predicted scores are measured against real scores using\nstandard evaluation metrics. TrueGL consistently outperforms other small-scale\nLLMs and rule-based approaches across all experiments on key evaluation\nmetrics, including MAE, RMSE, and R2. The model's high accuracy, broad content\ncoverage, and ease of use make trustworthy information more accessible and help\nreduce the spread of false or misleading content online. Our code is publicly\navailable at https://github.com/AlgazinovAleksandr/TrueGL, and our model is\npublicly released at https://huggingface.co/JoydeepC/trueGL."
                },
                "authors": [
                    {
                        "name": "Joydeep Chandra"
                    },
                    {
                        "name": "Aleksandr Algazinov"
                    },
                    {
                        "name": "Satyam Kumar Navneet"
                    },
                    {
                        "name": "Rim El Filali"
                    },
                    {
                        "name": "Matt Laing"
                    },
                    {
                        "name": "Andrew Hanna"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Hanna"
                },
                "author": "Andrew Hanna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14179v2",
                "updated": "2025-08-29T09:14:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    14,
                    59,
                    4,
                    241,
                    0
                ],
                "published": "2025-01-24T02:12:08Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    2,
                    12,
                    8,
                    4,
                    24,
                    0
                ],
                "title": "AI Chatbots as Professional Service Agents: Developing a Professional\n  Identity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Chatbots as Professional Service Agents: Developing a Professional\n  Identity"
                },
                "summary": "With the rapid expansion of large language model (LLM) applications, there is\nan emerging shift in the role of LLM-based AI chatbots from serving merely as\ngeneral inquiry tools to acting as professional service agents. However,\ncurrent studies often overlook a critical aspect of professional service\nagents: the act of communicating in a manner consistent with their professional\nidentities. This is of particular importance in the healthcare sector, where\neffective communication with patients is essential for achieving professional\ngoals, such as promoting patient well-being by encouraging healthy behaviors.\nTo bridge this gap, we propose LAPI (LLM-based Agent with a Professional\nIdentity), a novel framework for designing professional service agent tailored\nfor medical question-and-answer (Q\\&A) services, ensuring alignment with a\nspecific professional identity. Our method includes a theory-guided task\nplanning process that decomposes complex professional tasks into manageable\nsubtasks aligned with professional objectives and a pragmatic entropy method\ndesigned to generate professional and ethical responses with low uncertainty.\nExperiments on various LLMs show that the proposed approach outperforms\nbaseline methods, including few-shot prompting, chain-of-thought prompting,\nacross key metrics such as fluency, naturalness, empathy, patient-centricity,\nand ROUGE-L scores. Additionally, the ablation study underscores the\ncontribution of each component to the overall effectiveness of the approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid expansion of large language model (LLM) applications, there is\nan emerging shift in the role of LLM-based AI chatbots from serving merely as\ngeneral inquiry tools to acting as professional service agents. However,\ncurrent studies often overlook a critical aspect of professional service\nagents: the act of communicating in a manner consistent with their professional\nidentities. This is of particular importance in the healthcare sector, where\neffective communication with patients is essential for achieving professional\ngoals, such as promoting patient well-being by encouraging healthy behaviors.\nTo bridge this gap, we propose LAPI (LLM-based Agent with a Professional\nIdentity), a novel framework for designing professional service agent tailored\nfor medical question-and-answer (Q\\&A) services, ensuring alignment with a\nspecific professional identity. Our method includes a theory-guided task\nplanning process that decomposes complex professional tasks into manageable\nsubtasks aligned with professional objectives and a pragmatic entropy method\ndesigned to generate professional and ethical responses with low uncertainty.\nExperiments on various LLMs show that the proposed approach outperforms\nbaseline methods, including few-shot prompting, chain-of-thought prompting,\nacross key metrics such as fluency, naturalness, empathy, patient-centricity,\nand ROUGE-L scores. Additionally, the ablation study underscores the\ncontribution of each component to the overall effectiveness of the approach."
                },
                "authors": [
                    {
                        "name": "Wenwen Li"
                    },
                    {
                        "name": "Kangwei Shi"
                    },
                    {
                        "name": "Yidong Chai"
                    }
                ],
                "author_detail": {
                    "name": "Yidong Chai"
                },
                "author": "Yidong Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04104v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04104v2",
                "updated": "2025-08-29T09:07:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    7,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-05T08:31:10Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    8,
                    31,
                    10,
                    5,
                    95,
                    0
                ],
                "title": "SpecPipe: Accelerating Pipeline Parallelism-based LLM Inference with\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecPipe: Accelerating Pipeline Parallelism-based LLM Inference with\n  Speculative Decoding"
                },
                "summary": "The demand for large language model inference is rapidly increasing. Pipeline\nparallelism offers a cost-effective deployment strategy for distributed\ninference but suffers from high service latency. While incorporating\nspeculative decoding to pipeline parallelism improves performance, it still\nfaces challenges of low hardware utilization and narrow speculative window.\nInspired by branch prediction in instruction pipelining, we introduce SpecPipe,\nwhich fills the pipeline with speculative tokens of a request step-by-step. By\nmaximizing the hardware utilization, SpecPipe decodes one token per pipeline\nstep ideally. Specifically, SpecPipe comprises a dynamic speculative token tree\nand a pipelined inference framework. The tree dynamically accepts tokens from a\nspeculative token source and outputs the tokens to the inference pipeline.\nSince the speculative window relaxed in our framework, a high-accuracy draft\nmodel is integrated without fine-tuning. The pipeline inference framework\nfollows node-wise computation, pruning propagation, and inter-node\ncommunication stages. We implement SpecPipe and a variant SpecPipe-DB with\ndynamic batching for single- and multi-request inference, respectively. On an\n8-stage pipeline, SpecPipe improves time between tokens on diverse\nsingle-request workloads by $4.19\\times$-$5.53\\times$ over standard pipeline\nparallelism and by $2.08\\times$-$2.38\\times$ over prior tree-based speculative\ndecoding methods. For multi-request workloads, SpecPipe-DB achieves\n$1.64\\times$-$2.08\\times$ higher throughput and $1.61\\times$-$2.06\\times$ lower\ntime between tokens than vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for large language model inference is rapidly increasing. Pipeline\nparallelism offers a cost-effective deployment strategy for distributed\ninference but suffers from high service latency. While incorporating\nspeculative decoding to pipeline parallelism improves performance, it still\nfaces challenges of low hardware utilization and narrow speculative window.\nInspired by branch prediction in instruction pipelining, we introduce SpecPipe,\nwhich fills the pipeline with speculative tokens of a request step-by-step. By\nmaximizing the hardware utilization, SpecPipe decodes one token per pipeline\nstep ideally. Specifically, SpecPipe comprises a dynamic speculative token tree\nand a pipelined inference framework. The tree dynamically accepts tokens from a\nspeculative token source and outputs the tokens to the inference pipeline.\nSince the speculative window relaxed in our framework, a high-accuracy draft\nmodel is integrated without fine-tuning. The pipeline inference framework\nfollows node-wise computation, pruning propagation, and inter-node\ncommunication stages. We implement SpecPipe and a variant SpecPipe-DB with\ndynamic batching for single- and multi-request inference, respectively. On an\n8-stage pipeline, SpecPipe improves time between tokens on diverse\nsingle-request workloads by $4.19\\times$-$5.53\\times$ over standard pipeline\nparallelism and by $2.08\\times$-$2.38\\times$ over prior tree-based speculative\ndecoding methods. For multi-request workloads, SpecPipe-DB achieves\n$1.64\\times$-$2.08\\times$ higher throughput and $1.61\\times$-$2.06\\times$ lower\ntime between tokens than vLLM."
                },
                "authors": [
                    {
                        "name": "Haofei Yin"
                    },
                    {
                        "name": "Mengbai Xiao"
                    },
                    {
                        "name": "Tinghong Li"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Dongxiao Yu"
                    },
                    {
                        "name": "Guanghui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Guanghui Zhang"
                },
                "author": "Guanghui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04104v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21435v1",
                "updated": "2025-08-29T09:04:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    4,
                    11,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T09:04:11Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    4,
                    11,
                    4,
                    241,
                    0
                ],
                "title": "MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedShift: Implicit Conditional Transport for X-Ray Domain Adaptation"
                },
                "summary": "Synthetic medical data offers a scalable solution for training robust models,\nbut significant domain gaps limit its generalizability to real-world clinical\nsettings. This paper addresses the challenge of cross-domain translation\nbetween synthetic and real X-ray images of the head, focusing on bridging\ndiscrepancies in attenuation behavior, noise characteristics, and soft tissue\nrepresentation. We propose MedShift, a unified class-conditional generative\nmodel based on Flow Matching and Schrodinger Bridges, which enables\nhigh-fidelity, unpaired image translation across multiple domains. Unlike prior\napproaches that require domain-specific training or rely on paired data,\nMedShift learns a shared domain-agnostic latent space and supports seamless\ntranslation between any pair of domains seen during training. We introduce\nX-DigiSkull, a new dataset comprising aligned synthetic and real skull X-rays\nunder varying radiation doses, to benchmark domain translation models.\nExperimental results demonstrate that, despite its smaller model size compared\nto diffusion-based approaches, MedShift offers strong performance and remains\nflexible at inference time, as it can be tuned to prioritize either perceptual\nfidelity or structural consistency, making it a scalable and generalizable\nsolution for domain adaptation in medical imaging. The code and dataset are\navailable at https://caetas.github.io/medshift.html",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic medical data offers a scalable solution for training robust models,\nbut significant domain gaps limit its generalizability to real-world clinical\nsettings. This paper addresses the challenge of cross-domain translation\nbetween synthetic and real X-ray images of the head, focusing on bridging\ndiscrepancies in attenuation behavior, noise characteristics, and soft tissue\nrepresentation. We propose MedShift, a unified class-conditional generative\nmodel based on Flow Matching and Schrodinger Bridges, which enables\nhigh-fidelity, unpaired image translation across multiple domains. Unlike prior\napproaches that require domain-specific training or rely on paired data,\nMedShift learns a shared domain-agnostic latent space and supports seamless\ntranslation between any pair of domains seen during training. We introduce\nX-DigiSkull, a new dataset comprising aligned synthetic and real skull X-rays\nunder varying radiation doses, to benchmark domain translation models.\nExperimental results demonstrate that, despite its smaller model size compared\nto diffusion-based approaches, MedShift offers strong performance and remains\nflexible at inference time, as it can be tuned to prioritize either perceptual\nfidelity or structural consistency, making it a scalable and generalizable\nsolution for domain adaptation in medical imaging. The code and dataset are\navailable at https://caetas.github.io/medshift.html"
                },
                "authors": [
                    {
                        "name": "Francisco Caetano"
                    },
                    {
                        "name": "Christiaan Viviers"
                    },
                    {
                        "name": "Peter H. H. de With"
                    },
                    {
                        "name": "Fons van der Sommen"
                    }
                ],
                "author_detail": {
                    "name": "Fons van der Sommen"
                },
                "author": "Fons van der Sommen",
                "arxiv_comment": "Accepted at the ICCV 2025 AIM Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21433v1",
                "updated": "2025-08-29T09:02:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    2,
                    35,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T09:02:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    2,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM\n  Summarization for Agent Context Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM\n  Summarization for Agent Context Management"
                },
                "summary": "Large Language Model (LLM)-based agents solve complex tasks through iterative\nreasoning, exploration, and tool-use, a process that can result in long,\nexpensive context histories. While state-of-the-art Software Engineering ( SE)\nagents like OpenHands or Cursor use LLM-based summarization to tackle this\nissue, it is unclear whether the increased complexity offers tangible\nperformance benefits compared to simply omitting older observations. We present\na systematic comparison of these strategies within SWE-agent on SWE-bench\nVerified across five diverse model configurations. We find that a simple\nobservation-masking strategy halves cost relative to a raw agent while\nmatching, and sometimes slightly exceeding, the solve rate of LLM\nsummarization. For example, with Qwen3-Coder 480B, masking improves solve rate\nfrom 53.8% (raw agent) to 54.8%, while remaining competitive with summarization\nat a lower cost. These results suggest that, at least within SWE-agent on\nSWE-bench Verified, the most effective and efficient context management can be\nthe simplest. We release code and data for reproducibility",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents solve complex tasks through iterative\nreasoning, exploration, and tool-use, a process that can result in long,\nexpensive context histories. While state-of-the-art Software Engineering ( SE)\nagents like OpenHands or Cursor use LLM-based summarization to tackle this\nissue, it is unclear whether the increased complexity offers tangible\nperformance benefits compared to simply omitting older observations. We present\na systematic comparison of these strategies within SWE-agent on SWE-bench\nVerified across five diverse model configurations. We find that a simple\nobservation-masking strategy halves cost relative to a raw agent while\nmatching, and sometimes slightly exceeding, the solve rate of LLM\nsummarization. For example, with Qwen3-Coder 480B, masking improves solve rate\nfrom 53.8% (raw agent) to 54.8%, while remaining competitive with summarization\nat a lower cost. These results suggest that, at least within SWE-agent on\nSWE-bench Verified, the most effective and efficient context management can be\nthe simplest. We release code and data for reproducibility"
                },
                "authors": [
                    {
                        "name": "Tobias Lindenbauer"
                    },
                    {
                        "name": "Igor Slinko"
                    },
                    {
                        "name": "Ludwig Felder"
                    },
                    {
                        "name": "Egor Bogomolov"
                    },
                    {
                        "name": "Yaroslav Zharov"
                    }
                ],
                "author_detail": {
                    "name": "Yaroslav Zharov"
                },
                "author": "Yaroslav Zharov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21432v1",
                "updated": "2025-08-29T09:01:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    1,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T09:01:34Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    1,
                    34,
                    4,
                    241,
                    0
                ],
                "title": "RepoMark: A Code Usage Auditing Framework for Code Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoMark: A Code Usage Auditing Framework for Code Large Language Models"
                },
                "summary": "The rapid development of Large Language Models (LLMs) for code generation has\ntransformed software development by automating coding tasks with unprecedented\nefficiency.\n  However, the training of these models on open-source code repositories (e.g.,\nfrom GitHub) raises critical ethical and legal concerns, particularly regarding\ndata authorization and open-source license compliance. Developers are\nincreasingly questioning whether model trainers have obtained proper\nauthorization before using repositories for training, especially given the lack\nof transparency in data collection.\n  To address these concerns, we propose a novel data marking framework RepoMark\nto audit the data usage of code LLMs. Our method enables repository owners to\nverify whether their code has been used in training, while ensuring semantic\npreservation, imperceptibility, and theoretical false detection rate (FDR)\nguarantees. By generating multiple semantically equivalent code variants,\nRepoMark introduces data marks into the code files, and during detection,\nRepoMark leverages a novel ranking-based hypothesis test to detect memorization\nwithin the model. Compared to prior data auditing approaches, RepoMark\nsignificantly enhances sample efficiency, allowing effective auditing even when\nthe user's repository possesses only a small number of code files.\n  Experiments demonstrate that RepoMark achieves a detection success rate over\n90\\% on small code repositories under a strict FDR guarantee of 5\\%. This\nrepresents a significant advancement over existing data marking techniques, all\nof which only achieve accuracy below 55\\% under identical settings. This\nfurther validates RepoMark as a robust, theoretically sound, and promising\nsolution for enhancing transparency in code LLM training, which can safeguard\nthe rights of repository owners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) for code generation has\ntransformed software development by automating coding tasks with unprecedented\nefficiency.\n  However, the training of these models on open-source code repositories (e.g.,\nfrom GitHub) raises critical ethical and legal concerns, particularly regarding\ndata authorization and open-source license compliance. Developers are\nincreasingly questioning whether model trainers have obtained proper\nauthorization before using repositories for training, especially given the lack\nof transparency in data collection.\n  To address these concerns, we propose a novel data marking framework RepoMark\nto audit the data usage of code LLMs. Our method enables repository owners to\nverify whether their code has been used in training, while ensuring semantic\npreservation, imperceptibility, and theoretical false detection rate (FDR)\nguarantees. By generating multiple semantically equivalent code variants,\nRepoMark introduces data marks into the code files, and during detection,\nRepoMark leverages a novel ranking-based hypothesis test to detect memorization\nwithin the model. Compared to prior data auditing approaches, RepoMark\nsignificantly enhances sample efficiency, allowing effective auditing even when\nthe user's repository possesses only a small number of code files.\n  Experiments demonstrate that RepoMark achieves a detection success rate over\n90\\% on small code repositories under a strict FDR guarantee of 5\\%. This\nrepresents a significant advancement over existing data marking techniques, all\nof which only achieve accuracy below 55\\% under identical settings. This\nfurther validates RepoMark as a robust, theoretically sound, and promising\nsolution for enhancing transparency in code LLM training, which can safeguard\nthe rights of repository owners."
                },
                "authors": [
                    {
                        "name": "Wenjie Qu"
                    },
                    {
                        "name": "Yuguang Zhou"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Wengrui Zheng"
                    },
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Zhang"
                },
                "author": "Jiaheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21607v2",
                "updated": "2025-08-29T08:57:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    57,
                    16,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-29T09:04:16Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    9,
                    4,
                    16,
                    1,
                    210,
                    0
                ],
                "title": "Does DESI DR2 challenge $Λ$CDM paradigm ?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does DESI DR2 challenge $Λ$CDM paradigm ?"
                },
                "summary": "Although debate on DESI DR1 systematics remains, DESI DR2 is consistent with\nDR1 and strengthens its trends. In our analysis, the LRG1 point at\n$z_{\\mathrm{eff}}=0.510$ and the LRG3+ELG1 point at $z_{\\mathrm{eff}}=0.934$\nare in tension with the $\\Lambda$CDM-anchored $\\Omega_m$ inferred from Planck\nand SNe Ia (Pantheon$^{+}$, Union3, DES-SN5YR): for LRG1 the tensions are\n$2.42\\sigma$, $1.91\\sigma$, $2.19\\sigma$, and $2.99\\sigma$; for LRG3+ELG1 they\nare $2.60\\sigma$, $2.24\\sigma$, $2.51\\sigma$, and $2.96\\sigma$. Across redshift\nbins DR2 shows improved agreement relative to DR1, with the $\\Omega_m$ tension\ndropping from $2.20\\sigma$ to $1.84\\sigma$. Nevertheless, DR2 alone is not\ndecisive against $\\Lambda$CDM, and the apparent deviation is driven mainly by\nLRG1 and LRG2. In a $\\omega_0\\omega_a$CDM fit using all tracers we find a\nposterior mean with $w_0>-1$, consistent with dynamical dark energy and\nnominally challenging $\\Lambda$CDM. Removing LRG1 and/or LRG2 restores\n$\\Lambda$CDM concordance ($\\omega_0\\to-1$); moreover,\n$\\omega_0^{\\mathrm{(LRG2)}}>w_0^{\\mathrm{(LRG1)}}$, indicating that LRG2 drives\nthe trend more strongly. Model selection via the natural-log Bayes factor\n$\\ln\\mathrm{BF}\\equiv\\ln(Z_{\\Lambda\\mathrm{CDM}}/Z_{\\omega_0\\omega_a\\mathrm{CDM}})$\nyields weak evidence for $\\Lambda$CDM when LRG1, LRG2, or both are removed, and\nis inconclusive for the full sample. Hence the data do not require the extra\n$\\omega_a$ freedom, and the apparent $\\omega_0>-1$ preference should be\ninterpreted cautiously as a reflection of the $\\omega_0$$\\omega_a$ degeneracy\nwith limited per-tracer information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although debate on DESI DR1 systematics remains, DESI DR2 is consistent with\nDR1 and strengthens its trends. In our analysis, the LRG1 point at\n$z_{\\mathrm{eff}}=0.510$ and the LRG3+ELG1 point at $z_{\\mathrm{eff}}=0.934$\nare in tension with the $\\Lambda$CDM-anchored $\\Omega_m$ inferred from Planck\nand SNe Ia (Pantheon$^{+}$, Union3, DES-SN5YR): for LRG1 the tensions are\n$2.42\\sigma$, $1.91\\sigma$, $2.19\\sigma$, and $2.99\\sigma$; for LRG3+ELG1 they\nare $2.60\\sigma$, $2.24\\sigma$, $2.51\\sigma$, and $2.96\\sigma$. Across redshift\nbins DR2 shows improved agreement relative to DR1, with the $\\Omega_m$ tension\ndropping from $2.20\\sigma$ to $1.84\\sigma$. Nevertheless, DR2 alone is not\ndecisive against $\\Lambda$CDM, and the apparent deviation is driven mainly by\nLRG1 and LRG2. In a $\\omega_0\\omega_a$CDM fit using all tracers we find a\nposterior mean with $w_0>-1$, consistent with dynamical dark energy and\nnominally challenging $\\Lambda$CDM. Removing LRG1 and/or LRG2 restores\n$\\Lambda$CDM concordance ($\\omega_0\\to-1$); moreover,\n$\\omega_0^{\\mathrm{(LRG2)}}>w_0^{\\mathrm{(LRG1)}}$, indicating that LRG2 drives\nthe trend more strongly. Model selection via the natural-log Bayes factor\n$\\ln\\mathrm{BF}\\equiv\\ln(Z_{\\Lambda\\mathrm{CDM}}/Z_{\\omega_0\\omega_a\\mathrm{CDM}})$\nyields weak evidence for $\\Lambda$CDM when LRG1, LRG2, or both are removed, and\nis inconclusive for the full sample. Hence the data do not require the extra\n$\\omega_a$ freedom, and the apparent $\\omega_0>-1$ preference should be\ninterpreted cautiously as a reflection of the $\\omega_0$$\\omega_a$ degeneracy\nwith limited per-tracer information."
                },
                "authors": [
                    {
                        "name": "Himanshu Chaudhary"
                    },
                    {
                        "name": "Salvatore Capozziello"
                    },
                    {
                        "name": "Vipin Kumar Sharma"
                    },
                    {
                        "name": "Ghulam Mustafa"
                    }
                ],
                "author_detail": {
                    "name": "Ghulam Mustafa"
                },
                "author": "Ghulam Mustafa",
                "arxiv_comment": "13 pages, 6 figures, Accepted for publication in The Astrophysical\n  Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16936v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16936v2",
                "updated": "2025-08-29T08:56:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    56,
                    6,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-23T08:05:37Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    8,
                    5,
                    37,
                    5,
                    235,
                    0
                ],
                "title": "THEME: Enhancing Thematic Investing with Semantic Stock Representations\n  and Temporal Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THEME: Enhancing Thematic Investing with Semantic Stock Representations\n  and Temporal Dynamics"
                },
                "summary": "Thematic investing, which aims to construct portfolios aligned with\nstructural trends, remains a challenging endeavor due to overlapping sector\nboundaries and evolving market dynamics. A promising direction is to build\nsemantic representations of investment themes from textual data. However,\ndespite their power, general-purpose LLM embedding models are not well-suited\nto capture the nuanced characteristics of financial assets, since the semantic\nrepresentation of investment assets may differ fundamentally from that of\ngeneral financial text. To address this, we introduce THEME, a framework that\nfine-tunes embeddings using hierarchical contrastive learning. THEME aligns\nthemes and their constituent stocks using their hierarchical relationship, and\nsubsequently refines these embeddings by incorporating stock returns. This\nprocess yields representations effective for retrieving thematically aligned\nassets with strong return potential. Empirical results demonstrate that THEME\nexcels in two key areas. For thematic asset retrieval, it significantly\noutperforms leading large language models. Furthermore, its constructed\nportfolios demonstrate compelling performance. By jointly modeling thematic\nrelationships from text and market dynamics from returns, THEME generates stock\nembeddings specifically tailored for a wide range of practical investment\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thematic investing, which aims to construct portfolios aligned with\nstructural trends, remains a challenging endeavor due to overlapping sector\nboundaries and evolving market dynamics. A promising direction is to build\nsemantic representations of investment themes from textual data. However,\ndespite their power, general-purpose LLM embedding models are not well-suited\nto capture the nuanced characteristics of financial assets, since the semantic\nrepresentation of investment assets may differ fundamentally from that of\ngeneral financial text. To address this, we introduce THEME, a framework that\nfine-tunes embeddings using hierarchical contrastive learning. THEME aligns\nthemes and their constituent stocks using their hierarchical relationship, and\nsubsequently refines these embeddings by incorporating stock returns. This\nprocess yields representations effective for retrieving thematically aligned\nassets with strong return potential. Empirical results demonstrate that THEME\nexcels in two key areas. For thematic asset retrieval, it significantly\noutperforms leading large language models. Furthermore, its constructed\nportfolios demonstrate compelling performance. By jointly modeling thematic\nrelationships from text and market dynamics from returns, THEME generates stock\nembeddings specifically tailored for a wide range of practical investment\napplications."
                },
                "authors": [
                    {
                        "name": "Hoyoung Lee"
                    },
                    {
                        "name": "Wonbin Ahn"
                    },
                    {
                        "name": "Suhwan Park"
                    },
                    {
                        "name": "Jaehoon Lee"
                    },
                    {
                        "name": "Minjae Kim"
                    },
                    {
                        "name": "Sungdong Yoo"
                    },
                    {
                        "name": "Taeyoon Lim"
                    },
                    {
                        "name": "Woohyung Lim"
                    },
                    {
                        "name": "Yongjae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yongjae Lee"
                },
                "author": "Yongjae Lee",
                "arxiv_doi": "10.1145/3746252.3761517",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761517",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.16936v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16936v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at ACM International Conference on Information and Knowledge\n  Management (CIKM)",
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12683v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12683v4",
                "updated": "2025-08-29T08:56:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    56,
                    1,
                    4,
                    241,
                    0
                ],
                "published": "2024-02-20T03:14:47Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    3,
                    14,
                    47,
                    1,
                    51,
                    0
                ],
                "title": "TorchCP: A Python Library for Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TorchCP: A Python Library for Conformal Prediction"
                },
                "summary": "Conformal prediction (CP) is a powerful statistical framework that generates\nprediction intervals or sets with guaranteed coverage probability. While CP\nalgorithms have evolved beyond traditional classifiers and regressors to\nsophisticated deep learning models like deep neural networks (DNNs), graph\nneural networks (GNNs), and large language models (LLMs), existing CP libraries\noften lack the model support and scalability for large-scale DL scenarios. This\npaper introduces TorchCP, a PyTorch-native library designed to integrate\nstate-of-the-art CP algorithms into deep learning techniques, including\nDNN-based classifier/regressor, GNN, and LLM. Released under the LGPL-3.0\nlicense, TorchCP comprises about 16k lines of code, validated with 100% unit\ntest coverage and detailed documentation. Notably, TorchCP enables CP-specific\ntraining algorithms, online prediction, and GPU-accelerated batch processing,\nachieving up to 90% reduction in inference time on large datasets. With its\nlow-coupling design, comprehensive suite of advanced methods, and full GPU\nscalability, TorchCP empowers researchers and practitioners to enhance\nuncertainty quantification across cutting-edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal prediction (CP) is a powerful statistical framework that generates\nprediction intervals or sets with guaranteed coverage probability. While CP\nalgorithms have evolved beyond traditional classifiers and regressors to\nsophisticated deep learning models like deep neural networks (DNNs), graph\nneural networks (GNNs), and large language models (LLMs), existing CP libraries\noften lack the model support and scalability for large-scale DL scenarios. This\npaper introduces TorchCP, a PyTorch-native library designed to integrate\nstate-of-the-art CP algorithms into deep learning techniques, including\nDNN-based classifier/regressor, GNN, and LLM. Released under the LGPL-3.0\nlicense, TorchCP comprises about 16k lines of code, validated with 100% unit\ntest coverage and detailed documentation. Notably, TorchCP enables CP-specific\ntraining algorithms, online prediction, and GPU-accelerated batch processing,\nachieving up to 90% reduction in inference time on large datasets. With its\nlow-coupling design, comprehensive suite of advanced methods, and full GPU\nscalability, TorchCP empowers researchers and practitioners to enhance\nuncertainty quantification across cutting-edge applications."
                },
                "authors": [
                    {
                        "name": "Jianguo Huang"
                    },
                    {
                        "name": "Jianqing Song"
                    },
                    {
                        "name": "Xuanning Zhou"
                    },
                    {
                        "name": "Bingyi Jing"
                    },
                    {
                        "name": "Hongxin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hongxin Wei"
                },
                "author": "Hongxin Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12683v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12683v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21422v1",
                "updated": "2025-08-29T08:48:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    48,
                    0,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T08:48:00Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    48,
                    0,
                    4,
                    241,
                    0
                ],
                "title": "Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers:\n  A New Counterfactual Evaluation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers:\n  A New Counterfactual Evaluation Framework"
                },
                "summary": "Large Language Models (LLMs) have great potential to accelerate and support\nscholarly peer review and are increasingly used as fully automatic review\ngenerators (ARGs). However, potential biases and systematic errors may pose\nsignificant risks to scientific integrity; understanding the specific\ncapabilities and limitations of state-of-the-art ARGs is essential. We focus on\na core reviewing skill that underpins high-quality peer review: detecting\nfaulty research logic. This involves evaluating the internal consistency\nbetween a paper's results, interpretations, and claims. We present a fully\nautomated counterfactual evaluation framework that isolates and tests this\nskill under controlled conditions. Testing a range of ARG approaches, we find\nthat, contrary to expectation, flaws in research logic have no significant\neffect on their output reviews. Based on our findings, we derive three\nactionable recommendations for future work and release our counterfactual\ndataset and evaluation framework publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have great potential to accelerate and support\nscholarly peer review and are increasingly used as fully automatic review\ngenerators (ARGs). However, potential biases and systematic errors may pose\nsignificant risks to scientific integrity; understanding the specific\ncapabilities and limitations of state-of-the-art ARGs is essential. We focus on\na core reviewing skill that underpins high-quality peer review: detecting\nfaulty research logic. This involves evaluating the internal consistency\nbetween a paper's results, interpretations, and claims. We present a fully\nautomated counterfactual evaluation framework that isolates and tests this\nskill under controlled conditions. Testing a range of ARG approaches, we find\nthat, contrary to expectation, flaws in research logic have no significant\neffect on their output reviews. Based on our findings, we derive three\nactionable recommendations for future work and release our counterfactual\ndataset and evaluation framework publicly."
                },
                "authors": [
                    {
                        "name": "Nils Dycke"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.04034v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.04034v4",
                "updated": "2025-08-29T08:47:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    47,
                    51,
                    4,
                    241,
                    0
                ],
                "published": "2023-07-08T19:11:32Z",
                "published_parsed": [
                    2023,
                    7,
                    8,
                    19,
                    11,
                    32,
                    5,
                    189,
                    0
                ],
                "title": "Robust Universal Inference For Misspecified Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Universal Inference For Misspecified Models"
                },
                "summary": "In statistical inference, it is rarely realistic that the hypothesized\nstatistical model is well-specified, and consequently it is important to\nunderstand the effects of misspecification on inferential procedures. When the\nhypothesized statistical model is misspecified, the natural target of inference\nis a projection of the data generating distribution onto the model. We present\na general method for constructing valid confidence sets for such projections,\nunder weak regularity conditions, despite possible model misspecification. Our\nmethod builds upon the universal inference method and is based on inverting a\nfamily of split-sample tests of relative fit. We study settings in which our\nmethods yield either exact or approximate, finite-sample valid confidence sets\nfor various projection distributions. We study rates at which the resulting\nconfidence sets shrink around their target of inference and complement these\nresults with a simulation study and a study of causal discovery using a linear\ncausal model with the CausalEffectPairs dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In statistical inference, it is rarely realistic that the hypothesized\nstatistical model is well-specified, and consequently it is important to\nunderstand the effects of misspecification on inferential procedures. When the\nhypothesized statistical model is misspecified, the natural target of inference\nis a projection of the data generating distribution onto the model. We present\na general method for constructing valid confidence sets for such projections,\nunder weak regularity conditions, despite possible model misspecification. Our\nmethod builds upon the universal inference method and is based on inverting a\nfamily of split-sample tests of relative fit. We study settings in which our\nmethods yield either exact or approximate, finite-sample valid confidence sets\nfor various projection distributions. We study rates at which the resulting\nconfidence sets shrink around their target of inference and complement these\nresults with a simulation study and a study of causal discovery using a linear\ncausal model with the CausalEffectPairs dataset."
                },
                "authors": [
                    {
                        "name": "Beomjo Park"
                    },
                    {
                        "name": "Sivaraman Balakrishnan"
                    },
                    {
                        "name": "Larry Wasserman"
                    }
                ],
                "author_detail": {
                    "name": "Larry Wasserman"
                },
                "author": "Larry Wasserman",
                "arxiv_comment": "53 pages, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.04034v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.04034v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.21824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21824v1",
                "updated": "2025-08-29T17:59:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    59,
                    53,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:59:53Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    59,
                    53,
                    4,
                    241,
                    0
                ],
                "title": "DriveQA: Passing the Driving Knowledge Test",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriveQA: Passing the Driving Knowledge Test"
                },
                "summary": "If a Large Language Model (LLM) were to take a driving knowledge test today,\nwould it pass? Beyond standard spatial and visual question-answering (QA) tasks\non current autonomous driving benchmarks, driving knowledge tests require a\ncomplete understanding of all traffic rules, signage, and right-of-way\nprinciples. To pass this test, human drivers must discern various edge cases\nthat rarely appear in real-world datasets. In this work, we present DriveQA, an\nextensive open-source text and vision-based benchmark that exhaustively covers\ntraffic regulations and scenarios. Through our experiments using DriveQA, we\nshow that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on\nbasic traffic rules but exhibit significant weaknesses in numerical reasoning\nand complex right-of-way scenarios, traffic sign variations, and spatial\nlayouts, (2) fine-tuning on DriveQA improves accuracy across multiple\ncategories, particularly in regulatory sign recognition and intersection\ndecision-making, (3) controlled variations in DriveQA-V provide insights into\nmodel sensitivity to environmental factors such as lighting, perspective,\ndistance, and weather conditions, and (4) pretraining on DriveQA enhances\ndownstream driving task performance, leading to improved results on real-world\ndatasets such as nuScenes and BDD, while also demonstrating that models can\ninternalize text and synthetic traffic knowledge to generalize effectively\nacross downstream QA tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "If a Large Language Model (LLM) were to take a driving knowledge test today,\nwould it pass? Beyond standard spatial and visual question-answering (QA) tasks\non current autonomous driving benchmarks, driving knowledge tests require a\ncomplete understanding of all traffic rules, signage, and right-of-way\nprinciples. To pass this test, human drivers must discern various edge cases\nthat rarely appear in real-world datasets. In this work, we present DriveQA, an\nextensive open-source text and vision-based benchmark that exhaustively covers\ntraffic regulations and scenarios. Through our experiments using DriveQA, we\nshow that (1) state-of-the-art LLMs and Multimodal LLMs (MLLMs) perform well on\nbasic traffic rules but exhibit significant weaknesses in numerical reasoning\nand complex right-of-way scenarios, traffic sign variations, and spatial\nlayouts, (2) fine-tuning on DriveQA improves accuracy across multiple\ncategories, particularly in regulatory sign recognition and intersection\ndecision-making, (3) controlled variations in DriveQA-V provide insights into\nmodel sensitivity to environmental factors such as lighting, perspective,\ndistance, and weather conditions, and (4) pretraining on DriveQA enhances\ndownstream driving task performance, leading to improved results on real-world\ndatasets such as nuScenes and BDD, while also demonstrating that models can\ninternalize text and synthetic traffic knowledge to generalize effectively\nacross downstream QA tasks."
                },
                "authors": [
                    {
                        "name": "Maolin Wei"
                    },
                    {
                        "name": "Wanzhou Liu"
                    },
                    {
                        "name": "Eshed Ohn-Bar"
                    }
                ],
                "author_detail": {
                    "name": "Eshed Ohn-Bar"
                },
                "author": "Eshed Ohn-Bar",
                "arxiv_comment": "Accepted by ICCV 2025. Project page: https://driveqaiccv.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21034v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21034v2",
                "updated": "2025-08-29T17:59:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    59,
                    50,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-27T23:10:00Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    23,
                    10,
                    0,
                    6,
                    117,
                    0
                ],
                "title": "SAGA: A Security Architecture for Governing AI Agentic Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGA: A Security Architecture for Governing AI Agentic Systems"
                },
                "summary": "Large Language Model (LLM)-based agents increasingly interact, collaborate,\nand delegate tasks to one another autonomously with minimal human interaction.\nIndustry guidelines for agentic system governance emphasize the need for users\nto maintain comprehensive control over their agents, mitigating potential\ndamage from malicious agents. Several proposed agentic system designs address\nagent identity, authorization, and delegation, but remain purely theoretical,\nwithout concrete implementation and evaluation. Most importantly, they do not\nprovide user-controlled agent management.\n  To address this gap, we propose SAGA, a scalable Security Architecture for\nGoverning Agentic systems, that offers user oversight over their agents'\nlifecycle. In our design, users register their agents with a central entity,\nthe Provider, that maintains agent contact information, user-defined access\ncontrol policies, and helps agents enforce these policies on inter-agent\ncommunication. We introduce a cryptographic mechanism for deriving access\ncontrol tokens, that offers fine-grained control over an agent's interaction\nwith other agents, providing formal security guarantees. We evaluate SAGA on\nseveral agentic tasks, using agents in different geolocations, and multiple\non-device and cloud LLMs, demonstrating minimal performance overhead with no\nimpact on underlying task utility in a wide range of conditions. Our\narchitecture enables secure and trustworthy deployment of autonomous agents,\naccelerating the responsible adoption of this technology in sensitive\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents increasingly interact, collaborate,\nand delegate tasks to one another autonomously with minimal human interaction.\nIndustry guidelines for agentic system governance emphasize the need for users\nto maintain comprehensive control over their agents, mitigating potential\ndamage from malicious agents. Several proposed agentic system designs address\nagent identity, authorization, and delegation, but remain purely theoretical,\nwithout concrete implementation and evaluation. Most importantly, they do not\nprovide user-controlled agent management.\n  To address this gap, we propose SAGA, a scalable Security Architecture for\nGoverning Agentic systems, that offers user oversight over their agents'\nlifecycle. In our design, users register their agents with a central entity,\nthe Provider, that maintains agent contact information, user-defined access\ncontrol policies, and helps agents enforce these policies on inter-agent\ncommunication. We introduce a cryptographic mechanism for deriving access\ncontrol tokens, that offers fine-grained control over an agent's interaction\nwith other agents, providing formal security guarantees. We evaluate SAGA on\nseveral agentic tasks, using agents in different geolocations, and multiple\non-device and cloud LLMs, demonstrating minimal performance overhead with no\nimpact on underlying task utility in a wide range of conditions. Our\narchitecture enables secure and trustworthy deployment of autonomous agents,\naccelerating the responsible adoption of this technology in sensitive\nenvironments."
                },
                "authors": [
                    {
                        "name": "Georgios Syros"
                    },
                    {
                        "name": "Anshuman Suri"
                    },
                    {
                        "name": "Jacob Ginesin"
                    },
                    {
                        "name": "Cristina Nita-Rotaru"
                    },
                    {
                        "name": "Alina Oprea"
                    }
                ],
                "author_detail": {
                    "name": "Alina Oprea"
                },
                "author": "Alina Oprea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21034v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21034v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00631v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00631v2",
                "updated": "2025-08-29T17:58:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    58,
                    46,
                    4,
                    241,
                    0
                ],
                "published": "2024-12-01T01:01:09Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    1,
                    1,
                    9,
                    6,
                    336,
                    0
                ],
                "title": "ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific\n  Instruction Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific\n  Instruction Tuning"
                },
                "summary": "Instruction tuning has underscored the significant potential of large\nlanguage models (LLMs) in producing more human controllable and effective\noutputs in various domains. In this work, we focus on the data selection\nproblem for task-specific instruction tuning of LLMs. Prevailing methods\nprimarily rely on the crafted similarity metrics to select training data that\naligns with the test data distribution. The goal is to minimize instruction\ntuning loss on the test data, ultimately improving performance on the target\ntask. However, it has been widely observed that instruction tuning loss (i.e.,\ncross-entropy loss for next token prediction) in LLMs often fails to exhibit a\nmonotonic relationship with actual task performance. This misalignment\nundermines the effectiveness of current data selection methods for\ntask-specific instruction tuning. To address this issue, we introduce ROSE, a\nnovel Reward-Oriented inStruction data sElection method which leverages\npairwise preference loss as a reward signal to optimize data selection for\ntask-specific instruction tuning. Specifically, ROSE adapts an influence\nformulation to approximate the influence of training data points relative to a\nfew-shot preference validation set to select the most task-related training\ndata points. Experimental results show that by selecting just 5\\% of the\ntraining data using ROSE, our approach can achieve competitive results compared\nto fine-tuning with the full training dataset, and it surpasses other\nstate-of-the-art data selection methods for task-specific instruction tuning.\nOur qualitative analysis further confirms the robust generalizability of our\nmethod across multiple benchmark datasets and diverse model architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction tuning has underscored the significant potential of large\nlanguage models (LLMs) in producing more human controllable and effective\noutputs in various domains. In this work, we focus on the data selection\nproblem for task-specific instruction tuning of LLMs. Prevailing methods\nprimarily rely on the crafted similarity metrics to select training data that\naligns with the test data distribution. The goal is to minimize instruction\ntuning loss on the test data, ultimately improving performance on the target\ntask. However, it has been widely observed that instruction tuning loss (i.e.,\ncross-entropy loss for next token prediction) in LLMs often fails to exhibit a\nmonotonic relationship with actual task performance. This misalignment\nundermines the effectiveness of current data selection methods for\ntask-specific instruction tuning. To address this issue, we introduce ROSE, a\nnovel Reward-Oriented inStruction data sElection method which leverages\npairwise preference loss as a reward signal to optimize data selection for\ntask-specific instruction tuning. Specifically, ROSE adapts an influence\nformulation to approximate the influence of training data points relative to a\nfew-shot preference validation set to select the most task-related training\ndata points. Experimental results show that by selecting just 5\\% of the\ntraining data using ROSE, our approach can achieve competitive results compared\nto fine-tuning with the full training dataset, and it surpasses other\nstate-of-the-art data selection methods for task-specific instruction tuning.\nOur qualitative analysis further confirms the robust generalizability of our\nmethod across multiple benchmark datasets and diverse model architectures."
                },
                "authors": [
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Huayi Zhang"
                    },
                    {
                        "name": "Yizheng Jiao"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    },
                    {
                        "name": "Jinhong Yu"
                    },
                    {
                        "name": "Dongyu Zhang"
                    },
                    {
                        "name": "Dezhi Yu"
                    },
                    {
                        "name": "Wei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xu"
                },
                "author": "Wei Xu",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00631v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00631v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21811v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21811v1",
                "updated": "2025-08-29T17:49:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    49,
                    54,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:49:54Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    49,
                    54,
                    4,
                    241,
                    0
                ],
                "title": "The Integration of Agile Methodologies in DevOps Practices within the\n  Information Technology Industry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Integration of Agile Methodologies in DevOps Practices within the\n  Information Technology Industry"
                },
                "summary": "The demand for rapid software delivery in the Information Technology (IT)\nindustry has significantly intensified, emphasising the need for faster\nsoftware products and service releases with enhanced features to meet customer\nexpectations. Agile methodologies are replacing traditional approaches such as\nWaterfall, where flexibility, iterative development and adaptation to change\nare favoured over rigid planning and execution. DevOps, a subsequent evolution\nfrom Agile, emphasises collaborative efforts in development and operations\nteams, focusing on continuous integration and deployment to deliver resilient\nand high-quality software products and services. This study aims to critically\nassess both Agile and DevOps practices in the IT industry to identify the\nfeasibility and applicability of Agile methods in DevOps practices. Eleven\nsemi-structured interviews were conducted with Agile and DevOps practitioners\nin varying capacities across several sectors within the IT industry. Through\nthematic analysis, 51 unique codes were extracted and synthesised into 19\nthemes that reported on each phase of the DevOps lifecycle, specifically\nregarding the integration and implementation of Agile methods into DevOps\npractices. Based on the findings, a new understanding detailing the\ninterrelationship of Agile methods in DevOps practices was discussed that met\nthe research objectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for rapid software delivery in the Information Technology (IT)\nindustry has significantly intensified, emphasising the need for faster\nsoftware products and service releases with enhanced features to meet customer\nexpectations. Agile methodologies are replacing traditional approaches such as\nWaterfall, where flexibility, iterative development and adaptation to change\nare favoured over rigid planning and execution. DevOps, a subsequent evolution\nfrom Agile, emphasises collaborative efforts in development and operations\nteams, focusing on continuous integration and deployment to deliver resilient\nand high-quality software products and services. This study aims to critically\nassess both Agile and DevOps practices in the IT industry to identify the\nfeasibility and applicability of Agile methods in DevOps practices. Eleven\nsemi-structured interviews were conducted with Agile and DevOps practitioners\nin varying capacities across several sectors within the IT industry. Through\nthematic analysis, 51 unique codes were extracted and synthesised into 19\nthemes that reported on each phase of the DevOps lifecycle, specifically\nregarding the integration and implementation of Agile methods into DevOps\npractices. Based on the findings, a new understanding detailing the\ninterrelationship of Agile methods in DevOps practices was discussed that met\nthe research objectives."
                },
                "authors": [
                    {
                        "name": "Ashley Hourigan"
                    },
                    {
                        "name": "Ridewaan Hanslo"
                    }
                ],
                "author_detail": {
                    "name": "Ridewaan Hanslo"
                },
                "author": "Ridewaan Hanslo",
                "arxiv_comment": "10 pages, 2 figures, conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21811v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21811v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21810v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21810v1",
                "updated": "2025-08-29T17:47:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    47,
                    27,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:47:27Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    47,
                    27,
                    4,
                    241,
                    0
                ],
                "title": "QR-LoRA: QR-Based Low-Rank Adaptation for Efficient Fine-Tuning of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QR-LoRA: QR-Based Low-Rank Adaptation for Efficient Fine-Tuning of Large\n  Language Models"
                },
                "summary": "The growing scale of Large Language Models (LLMs) has necessitated the\ndevelopment of parameter-efficient fine-tuning techniques. Low-Rank Adaptation\n(LoRA) has emerged as a promising approach, reducing the number of trainable\nparameters by applying low-rank updates to pretrained weights. While standard\nLoRA learns both update factors directly, several recent variants first\ninitialize those matrices via an SVD of the pretrained weights -- an operation\nthat can be expensive on large models and yields singular vectors that are not\nalways easy to interpret. In this work, we extract an orthonormal basis from\nthe pretrained weight matrix using QR decomposition with column pivoting, and\nthen express the LoRA update as a linear combination of these basis vectors --\ntraining only the scalar coefficients, which imposes clear structure on\nadaptation and drastically reduces parameter count. Experiments across GLUE\ntasks show that QR-LoRA matches or exceeds the performance of full fine-tuning,\nstandard LoRA, and SVD-LoRA (LoRA with update matrices initialized via singular\nvalue decomposition) with as few as 601 parameters -- a reduction of over 1000x\ncompared to full fine-tuning and 77x fewer than typical LoRA setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing scale of Large Language Models (LLMs) has necessitated the\ndevelopment of parameter-efficient fine-tuning techniques. Low-Rank Adaptation\n(LoRA) has emerged as a promising approach, reducing the number of trainable\nparameters by applying low-rank updates to pretrained weights. While standard\nLoRA learns both update factors directly, several recent variants first\ninitialize those matrices via an SVD of the pretrained weights -- an operation\nthat can be expensive on large models and yields singular vectors that are not\nalways easy to interpret. In this work, we extract an orthonormal basis from\nthe pretrained weight matrix using QR decomposition with column pivoting, and\nthen express the LoRA update as a linear combination of these basis vectors --\ntraining only the scalar coefficients, which imposes clear structure on\nadaptation and drastically reduces parameter count. Experiments across GLUE\ntasks show that QR-LoRA matches or exceeds the performance of full fine-tuning,\nstandard LoRA, and SVD-LoRA (LoRA with update matrices initialized via singular\nvalue decomposition) with as few as 601 parameters -- a reduction of over 1000x\ncompared to full fine-tuning and 77x fewer than typical LoRA setups."
                },
                "authors": [
                    {
                        "name": "Jessica Liang"
                    },
                    {
                        "name": "Anirudh Bharadwaj"
                    }
                ],
                "author_detail": {
                    "name": "Anirudh Bharadwaj"
                },
                "author": "Anirudh Bharadwaj",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21810v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21810v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17202v2",
                "updated": "2025-08-29T17:46:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    46,
                    43,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-24T03:34:40Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    3,
                    34,
                    40,
                    6,
                    236,
                    0
                ],
                "title": "Active Domain Knowledge Acquisition with 100-Dollar Budget: Enhancing\n  LLMs via Cost-Efficient, Expert-Involved Interaction in Sensitive Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active Domain Knowledge Acquisition with 100-Dollar Budget: Enhancing\n  LLMs via Cost-Efficient, Expert-Involved Interaction in Sensitive Domains"
                },
                "summary": "Large Language Models (LLMs) have demonstrated an impressive level of general\nknowledge. However, they often struggle in highly specialized and\ncost-sensitive domains such as drug discovery and rare disease research due to\nthe lack of expert knowledge. In this paper, we propose a novel framework\n(PU-ADKA) designed to efficiently enhance domain-specific LLMs by actively\nengaging domain experts within a fixed budget. Unlike traditional fine-tuning\napproaches, PU-ADKA selectively identifies and queries the most appropriate\nexpert from a team, taking into account each expert's availability, knowledge\nboundaries, and consultation costs. We train PU-ADKA using simulations on\nPubMed data and validate it through both controlled expert interactions and\nreal-world deployment with a drug development team, demonstrating its\neffectiveness in enhancing LLM performance in specialized domains under strict\nbudget constraints. In addition to outlining our methodological innovations and\nexperimental results, we introduce a new benchmark dataset, CKAD, for\ncost-effective LLM domain knowledge acquisition to foster further research in\nthis challenging area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated an impressive level of general\nknowledge. However, they often struggle in highly specialized and\ncost-sensitive domains such as drug discovery and rare disease research due to\nthe lack of expert knowledge. In this paper, we propose a novel framework\n(PU-ADKA) designed to efficiently enhance domain-specific LLMs by actively\nengaging domain experts within a fixed budget. Unlike traditional fine-tuning\napproaches, PU-ADKA selectively identifies and queries the most appropriate\nexpert from a team, taking into account each expert's availability, knowledge\nboundaries, and consultation costs. We train PU-ADKA using simulations on\nPubMed data and validate it through both controlled expert interactions and\nreal-world deployment with a drug development team, demonstrating its\neffectiveness in enhancing LLM performance in specialized domains under strict\nbudget constraints. In addition to outlining our methodological innovations and\nexperimental results, we introduce a new benchmark dataset, CKAD, for\ncost-effective LLM domain knowledge acquisition to foster further research in\nthis challenging area."
                },
                "authors": [
                    {
                        "name": "Yang Wu"
                    },
                    {
                        "name": "Raha Moraffah"
                    },
                    {
                        "name": "Rujing Yao"
                    },
                    {
                        "name": "Jinhong Yu"
                    },
                    {
                        "name": "Zhimin Tao"
                    },
                    {
                        "name": "Xiaozhong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaozhong Liu"
                },
                "author": "Xiaozhong Liu",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21803v1",
                "updated": "2025-08-29T17:31:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    31,
                    24,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:31:24Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    31,
                    24,
                    4,
                    241,
                    0
                ],
                "title": "Automated Clinical Problem Detection from SOAP Notes using a\n  Collaborative Multi-Agent LLM Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Clinical Problem Detection from SOAP Notes using a\n  Collaborative Multi-Agent LLM Architecture"
                },
                "summary": "Accurate interpretation of clinical narratives is critical for patient care,\nbut the complexity of these notes makes automation challenging. While Large\nLanguage Models (LLMs) show promise, single-model approaches can lack the\nrobustness required for high-stakes clinical tasks. We introduce a\ncollaborative multi-agent system (MAS) that models a clinical consultation team\nto address this gap. The system is tasked with identifying clinical problems by\nanalyzing only the Subjective (S) and Objective (O) sections of SOAP notes,\nsimulating the diagnostic reasoning process of synthesizing raw data into an\nassessment. A Manager agent orchestrates a dynamically assigned team of\nspecialist agents who engage in a hierarchical, iterative debate to reach a\nconsensus. We evaluated our MAS against a single-agent baseline on a curated\ndataset of 420 MIMIC-III notes. The dynamic multi-agent configuration\ndemonstrated consistently improved performance in identifying congestive heart\nfailure, acute kidney injury, and sepsis. Qualitative analysis of the agent\ndebates reveals that this structure effectively surfaces and weighs conflicting\nevidence, though it can occasionally be susceptible to groupthink. By modeling\na clinical team's reasoning process, our system offers a promising path toward\nmore accurate, robust, and interpretable clinical decision support tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate interpretation of clinical narratives is critical for patient care,\nbut the complexity of these notes makes automation challenging. While Large\nLanguage Models (LLMs) show promise, single-model approaches can lack the\nrobustness required for high-stakes clinical tasks. We introduce a\ncollaborative multi-agent system (MAS) that models a clinical consultation team\nto address this gap. The system is tasked with identifying clinical problems by\nanalyzing only the Subjective (S) and Objective (O) sections of SOAP notes,\nsimulating the diagnostic reasoning process of synthesizing raw data into an\nassessment. A Manager agent orchestrates a dynamically assigned team of\nspecialist agents who engage in a hierarchical, iterative debate to reach a\nconsensus. We evaluated our MAS against a single-agent baseline on a curated\ndataset of 420 MIMIC-III notes. The dynamic multi-agent configuration\ndemonstrated consistently improved performance in identifying congestive heart\nfailure, acute kidney injury, and sepsis. Qualitative analysis of the agent\ndebates reveals that this structure effectively surfaces and weighs conflicting\nevidence, though it can occasionally be susceptible to groupthink. By modeling\na clinical team's reasoning process, our system offers a promising path toward\nmore accurate, robust, and interpretable clinical decision support tools."
                },
                "authors": [
                    {
                        "name": "Yeawon Lee"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Christopher C. Yang"
                    }
                ],
                "author_detail": {
                    "name": "Christopher C. Yang"
                },
                "author": "Christopher C. Yang",
                "arxiv_comment": "Accepted to The 16th ACM Conference on Bioinformatics, Computational\n  Biology, and Health Informatics (ACM-BCB 2025)(Poster Paper)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21801v1",
                "updated": "2025-08-29T17:28:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    28,
                    7,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:28:07Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    28,
                    7,
                    4,
                    241,
                    0
                ],
                "title": "DMGIN: How Multimodal LLMs Enhance Large Recommendation Models for\n  Lifelong User Post-click Behaviors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DMGIN: How Multimodal LLMs Enhance Large Recommendation Models for\n  Lifelong User Post-click Behaviors"
                },
                "summary": "Modeling user interest based on lifelong user behavior sequences is crucial\nfor enhancing Click-Through Rate (CTR) prediction. However, long post-click\nbehavior sequences themselves pose severe performance issues: the sheer volume\nof data leads to high computational costs and inefficiencies in model training\nand inference. Traditional methods address this by introducing two-stage\napproaches, but this compromises model effectiveness due to incomplete\nutilization of the full sequence context. More importantly, integrating\nmultimodal embeddings into existing large recommendation models (LRM) presents\nsignificant challenges: These embeddings often exacerbate computational burdens\nand mismatch with LRM architectures. To address these issues and enhance the\nmodel's efficiency and accuracy, we introduce Deep Multimodal Group Interest\nNetwork (DMGIN). Given the observation that user post-click behavior sequences\ncontain a large number of repeated items with varying behaviors and timestamps,\nDMGIN employs Multimodal LLMs(MLLM) for grouping to reorganize complete\nlifelong post-click behavior sequences more effectively, with almost no\nadditional computational overhead, as opposed to directly introducing\nmultimodal embeddings. To mitigate the potential information loss from\ngrouping, we have implemented two key strategies. First, we analyze behaviors\nwithin each group using both interest statistics and intra-group transformers\nto capture group traits. Second, apply inter-group transformers to temporally\nordered groups to capture the evolution of user group interests. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMGIN. The A/B test in our LBS advertising system shows that\nDMGIN improves CTR by 4.7% and Revenue per Mile by 2.3%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling user interest based on lifelong user behavior sequences is crucial\nfor enhancing Click-Through Rate (CTR) prediction. However, long post-click\nbehavior sequences themselves pose severe performance issues: the sheer volume\nof data leads to high computational costs and inefficiencies in model training\nand inference. Traditional methods address this by introducing two-stage\napproaches, but this compromises model effectiveness due to incomplete\nutilization of the full sequence context. More importantly, integrating\nmultimodal embeddings into existing large recommendation models (LRM) presents\nsignificant challenges: These embeddings often exacerbate computational burdens\nand mismatch with LRM architectures. To address these issues and enhance the\nmodel's efficiency and accuracy, we introduce Deep Multimodal Group Interest\nNetwork (DMGIN). Given the observation that user post-click behavior sequences\ncontain a large number of repeated items with varying behaviors and timestamps,\nDMGIN employs Multimodal LLMs(MLLM) for grouping to reorganize complete\nlifelong post-click behavior sequences more effectively, with almost no\nadditional computational overhead, as opposed to directly introducing\nmultimodal embeddings. To mitigate the potential information loss from\ngrouping, we have implemented two key strategies. First, we analyze behaviors\nwithin each group using both interest statistics and intra-group transformers\nto capture group traits. Second, apply inter-group transformers to temporally\nordered groups to capture the evolution of user group interests. Our extensive\nexperiments on both industrial and public datasets confirm the effectiveness\nand efficiency of DMGIN. The A/B test in our LBS advertising system shows that\nDMGIN improves CTR by 4.7% and Revenue per Mile by 2.3%."
                },
                "authors": [
                    {
                        "name": "Zhuoxing Wei"
                    },
                    {
                        "name": "Qingchen Xie"
                    },
                    {
                        "name": "Qi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qi Liu"
                },
                "author": "Qi Liu",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20489v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20489v3",
                "updated": "2025-08-29T17:23:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    23,
                    50,
                    4,
                    241,
                    0
                ],
                "published": "2025-02-27T19:53:59Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    19,
                    53,
                    59,
                    3,
                    58,
                    0
                ],
                "title": "Do Sell-side Analyst Reports Have Investment Value?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Sell-side Analyst Reports Have Investment Value?"
                },
                "summary": "This paper documents novel investment value in analyst report text. Using 1.2\nmillion reports from 2000-2023, I embed narratives with large language models\n(LLMs) and fit machine learning (ML) forecasts of future long-term returns.\nPortfolios formed on the report narrative forecasts earn sizable and\nsignificant performance that is incremental to analysts' numerical outputs and\nto a broad set of established factors and characteristic-based predictors. The\neffect is stronger after adverse news and is amplified for growth stocks with\naggressive investment. To open the black box, I apply a Shapley decomposition\nthat attributes portfolio performance to distinct topics. Analysts' strategic\noutlook contributes the most to portfolio performance, especially\nforward-looking fundamental assessments. Beyond providing direct evidence that\nanalyst narratives contain value-relevant assessments that diffuse into price\nover time, this study illustrates how interpretable LLM-plus-ML pipelines can\nscale and augment human judgment in investment decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper documents novel investment value in analyst report text. Using 1.2\nmillion reports from 2000-2023, I embed narratives with large language models\n(LLMs) and fit machine learning (ML) forecasts of future long-term returns.\nPortfolios formed on the report narrative forecasts earn sizable and\nsignificant performance that is incremental to analysts' numerical outputs and\nto a broad set of established factors and characteristic-based predictors. The\neffect is stronger after adverse news and is amplified for growth stocks with\naggressive investment. To open the black box, I apply a Shapley decomposition\nthat attributes portfolio performance to distinct topics. Analysts' strategic\noutlook contributes the most to portfolio performance, especially\nforward-looking fundamental assessments. Beyond providing direct evidence that\nanalyst narratives contain value-relevant assessments that diffuse into price\nover time, this study illustrates how interpretable LLM-plus-ML pipelines can\nscale and augment human judgment in investment decisions."
                },
                "authors": [
                    {
                        "name": "Linying Lv"
                    }
                ],
                "author_detail": {
                    "name": "Linying Lv"
                },
                "author": "Linying Lv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20489v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20489v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21793v1",
                "updated": "2025-08-29T17:17:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    17,
                    11,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:17:11Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    17,
                    11,
                    4,
                    241,
                    0
                ],
                "title": "MoE-Health: A Mixture of Experts Framework for Robust Multimodal\n  Healthcare Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoE-Health: A Mixture of Experts Framework for Robust Multimodal\n  Healthcare Prediction"
                },
                "summary": "Healthcare systems generate diverse multimodal data, including Electronic\nHealth Records (EHR), clinical notes, and medical images. Effectively\nleveraging this data for clinical prediction is challenging, particularly as\nreal-world samples often present with varied or incomplete modalities. Existing\napproaches typically require complete modality data or rely on manual selection\nstrategies, limiting their applicability in real-world clinical settings where\ndata availability varies across patients and institutions. To address these\nlimitations, we propose MoE-Health, a novel Mixture of Experts framework\ndesigned for robust multimodal fusion in healthcare prediction. MoE-Health\narchitecture is specifically developed to handle samples with differing\nmodalities and improve performance on critical clinical tasks. By leveraging\nspecialized expert networks and a dynamic gating mechanism, our approach\ndynamically selects and combines relevant experts based on available data\nmodalities, enabling flexible adaptation to varying data availability\nscenarios. We evaluate MoE-Health on the MIMIC-IV dataset across three critical\nclinical prediction tasks: in-hospital mortality prediction, long length of\nstay, and hospital readmission prediction. Experimental results demonstrate\nthat MoE-Health achieves superior performance compared to existing multimodal\nfusion methods while maintaining robustness across different modality\navailability patterns. The framework effectively integrates multimodal\ninformation, offering improved predictive performance and robustness in\nhandling heterogeneous and incomplete healthcare data, making it particularly\nsuitable for deployment in diverse healthcare environments with heterogeneous\ndata availability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Healthcare systems generate diverse multimodal data, including Electronic\nHealth Records (EHR), clinical notes, and medical images. Effectively\nleveraging this data for clinical prediction is challenging, particularly as\nreal-world samples often present with varied or incomplete modalities. Existing\napproaches typically require complete modality data or rely on manual selection\nstrategies, limiting their applicability in real-world clinical settings where\ndata availability varies across patients and institutions. To address these\nlimitations, we propose MoE-Health, a novel Mixture of Experts framework\ndesigned for robust multimodal fusion in healthcare prediction. MoE-Health\narchitecture is specifically developed to handle samples with differing\nmodalities and improve performance on critical clinical tasks. By leveraging\nspecialized expert networks and a dynamic gating mechanism, our approach\ndynamically selects and combines relevant experts based on available data\nmodalities, enabling flexible adaptation to varying data availability\nscenarios. We evaluate MoE-Health on the MIMIC-IV dataset across three critical\nclinical prediction tasks: in-hospital mortality prediction, long length of\nstay, and hospital readmission prediction. Experimental results demonstrate\nthat MoE-Health achieves superior performance compared to existing multimodal\nfusion methods while maintaining robustness across different modality\navailability patterns. The framework effectively integrates multimodal\ninformation, offering improved predictive performance and robustness in\nhandling heterogeneous and incomplete healthcare data, making it particularly\nsuitable for deployment in diverse healthcare environments with heterogeneous\ndata availability."
                },
                "authors": [
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Christopher C. Yang"
                    }
                ],
                "author_detail": {
                    "name": "Christopher C. Yang"
                },
                "author": "Christopher C. Yang",
                "arxiv_comment": "Accepted to The 16th ACM Conference on Bioinformatics, Computational\n  Biology, and Health Informatics (ACM-BCB 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21788v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21788v1",
                "updated": "2025-08-29T17:04:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    4,
                    20,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:04:20Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    4,
                    20,
                    4,
                    241,
                    0
                ],
                "title": "Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing\n  Fine Web for Problematic Content Search and Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Going over Fine Web with a Fine-Tooth Comb: Technical Report of Indexing\n  Fine Web for Problematic Content Search and Retrieval"
                },
                "summary": "Large language models (LLMs) rely heavily on web-scale datasets like Common\nCrawl, which provides over 80\\% of training data for some modern models.\nHowever, the indiscriminate nature of web crawling raises challenges in data\nquality, safety, and ethics. Despite the critical importance of training data\nquality, prior research on harmful content has been limited to small samples\ndue to computational constraints. This project presents a framework for\nindexing and analyzing LLM training datasets using an ElasticSearch-based\npipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),\nachieving fast query performance--most searches in milliseconds, all under 2\nseconds. Our work demonstrates real-time dataset analysis, offering practical\ntools for safer, more accountable AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely heavily on web-scale datasets like Common\nCrawl, which provides over 80\\% of training data for some modern models.\nHowever, the indiscriminate nature of web crawling raises challenges in data\nquality, safety, and ethics. Despite the critical importance of training data\nquality, prior research on harmful content has been limited to small samples\ndue to computational constraints. This project presents a framework for\nindexing and analyzing LLM training datasets using an ElasticSearch-based\npipeline. We apply it to SwissAI's FineWeb-2 corpus (1.5TB, four languages),\nachieving fast query performance--most searches in milliseconds, all under 2\nseconds. Our work demonstrates real-time dataset analysis, offering practical\ntools for safer, more accountable AI systems."
                },
                "authors": [
                    {
                        "name": "Inés Altemir Marinas"
                    },
                    {
                        "name": "Anastasiia Kucherenko"
                    },
                    {
                        "name": "Andrei Kucharavy"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Kucharavy"
                },
                "author": "Andrei Kucharavy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21788v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21788v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21787v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21787v1",
                "updated": "2025-08-29T17:03:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    3,
                    47,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T17:03:47Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    17,
                    3,
                    47,
                    4,
                    241,
                    0
                ],
                "title": "PiCSAR: Probabilistic Confidence Selection And Ranking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PiCSAR: Probabilistic Confidence Selection And Ranking"
                },
                "summary": "Best-of-n sampling improves the accuracy of large language models (LLMs) and\nlarge reasoning models (LRMs) by generating multiple candidate solutions and\nselecting the one with the highest reward. The key challenge for reasoning\ntasks is designing a scoring function that can identify correct reasoning\nchains without access to ground-truth answers. We propose Probabilistic\nConfidence Selection And Ranking (PiCSAR): a simple, training-free method that\nscores each candidate generation using the joint log-likelihood of the\nreasoning and final answer. The joint log-likelihood of the reasoning and final\nanswer naturally decomposes into reasoning confidence and answer confidence.\nPiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,\n+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in\n16 out of 20 comparisons. Our analysis reveals that correct reasoning chains\nexhibit significantly higher reasoning and answer confidence, justifying the\neffectiveness of PiCSAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best-of-n sampling improves the accuracy of large language models (LLMs) and\nlarge reasoning models (LRMs) by generating multiple candidate solutions and\nselecting the one with the highest reward. The key challenge for reasoning\ntasks is designing a scoring function that can identify correct reasoning\nchains without access to ground-truth answers. We propose Probabilistic\nConfidence Selection And Ranking (PiCSAR): a simple, training-free method that\nscores each candidate generation using the joint log-likelihood of the\nreasoning and final answer. The joint log-likelihood of the reasoning and final\nanswer naturally decomposes into reasoning confidence and answer confidence.\nPiCSAR achieves substantial gains across diverse benchmarks (+10.18 on MATH500,\n+9.81 on AIME2025), outperforming baselines with at least 2x fewer samples in\n16 out of 20 comparisons. Our analysis reveals that correct reasoning chains\nexhibit significantly higher reasoning and answer confidence, justifying the\neffectiveness of PiCSAR."
                },
                "authors": [
                    {
                        "name": "Joshua Ong Jun Leang"
                    },
                    {
                        "name": "Zheng Zhao"
                    },
                    {
                        "name": "Aryo Pradipta Gema"
                    },
                    {
                        "name": "Sohee Yang"
                    },
                    {
                        "name": "Wai-Chung Kwan"
                    },
                    {
                        "name": "Xuanli He"
                    },
                    {
                        "name": "Wenda Li"
                    },
                    {
                        "name": "Pasquale Minervini"
                    },
                    {
                        "name": "Eleonora Giunchiglia"
                    },
                    {
                        "name": "Shay B. Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Shay B. Cohen"
                },
                "author": "Shay B. Cohen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21787v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21787v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21783v1",
                "updated": "2025-08-29T16:59:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    59,
                    21,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T16:59:21Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    59,
                    21,
                    4,
                    241,
                    0
                ],
                "title": "QoS-Aware Proportional Fairness Scheduling for Multi-Flow 5G UEs: A\n  Smart Factory Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QoS-Aware Proportional Fairness Scheduling for Multi-Flow 5G UEs: A\n  Smart Factory Perspective"
                },
                "summary": "Private 5G networks are emerging as key enablers for smart factories, where a\nsingle device often handles multiple concurrent traffic flows with distinct\nQuality of Service (QoS) requirements. Existing simulation frameworks, however,\nlack the fidelity to model such multi-flow behavior at the QoS Flow Identifier\n(QFI) level. This paper addresses this gap by extending Simu5G to support\nper-QFI modeling and by introducing a novel QoS-aware Proportional Fairness\n(QoS-PF) scheduler. The scheduler dynamically balances delay, Guaranteed Bit\nRate (GBR), and priority metrics to optimize resource allocation across\nheterogeneous flows. We evaluate the proposed approach in a realistic smart\nfactory scenario featuring edge-hosted machine vision, real-time control loops,\nand bulk data transfer. Results show that QoS-PF improves deadline adherence\nand fairness without compromising throughput. All extensions are implemented in\na modular and open-source manner to support future research. Our work provides\nboth a methodological and architectural foundation for simulating and analyzing\nadvanced QoS policies in industrial 5G deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private 5G networks are emerging as key enablers for smart factories, where a\nsingle device often handles multiple concurrent traffic flows with distinct\nQuality of Service (QoS) requirements. Existing simulation frameworks, however,\nlack the fidelity to model such multi-flow behavior at the QoS Flow Identifier\n(QFI) level. This paper addresses this gap by extending Simu5G to support\nper-QFI modeling and by introducing a novel QoS-aware Proportional Fairness\n(QoS-PF) scheduler. The scheduler dynamically balances delay, Guaranteed Bit\nRate (GBR), and priority metrics to optimize resource allocation across\nheterogeneous flows. We evaluate the proposed approach in a realistic smart\nfactory scenario featuring edge-hosted machine vision, real-time control loops,\nand bulk data transfer. Results show that QoS-PF improves deadline adherence\nand fairness without compromising throughput. All extensions are implemented in\na modular and open-source manner to support future research. Our work provides\nboth a methodological and architectural foundation for simulating and analyzing\nadvanced QoS policies in industrial 5G deployments."
                },
                "authors": [
                    {
                        "name": "Mohamed Seliem"
                    },
                    {
                        "name": "Utz Roedig"
                    },
                    {
                        "name": "Cormac Sreenan"
                    },
                    {
                        "name": "Dirk Pesch"
                    }
                ],
                "author_detail": {
                    "name": "Dirk Pesch"
                },
                "author": "Dirk Pesch",
                "arxiv_comment": "(c) 2025 IEEE. This is the author's version of a paper accepted for\n  presentation at the IEEE MSWiM 2025 conference. The final version will appear\n  in the conference proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21777v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21777v1",
                "updated": "2025-08-29T16:55:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    55,
                    25,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T16:55:25Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    55,
                    25,
                    4,
                    241,
                    0
                ],
                "title": "Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but\n  Persistent Need for Expert Oversight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking GPT-5 in Radiation Oncology: Measurable Gains, but\n  Persistent Need for Expert Oversight"
                },
                "summary": "Introduction: Large language models (LLM) have shown great potential in\nclinical decision support. GPT-5 is a novel LLM system that has been\nspecifically marketed towards oncology use.\n  Methods: Performance was assessed using two complementary benchmarks: (i) the\nACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300\nmultiple-choice items, and (ii) a curated set of 60 authentic radiation\noncologic vignettes representing diverse disease sites and treatment\nindications. For the vignette evaluation, GPT-5 was instructed to generate\nconcise therapeutic plans. Four board-certified radiation oncologists rated\ncorrectness, comprehensiveness, and hallucinations. Inter-rater reliability was\nquantified using Fleiss' \\k{appa}.\n  Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%,\noutperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were\nmost pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5's\ntreatment recommendations were rated highly for correctness (mean 3.24/4, 95%\nCI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69).\nHallucinations were rare with no case reaching majority consensus for their\npresence. Inter-rater agreement was low (Fleiss' \\k{appa} 0.083 for\ncorrectness), reflecting inherent variability in clinical judgment. Errors\nclustered in complex scenarios requiring precise trial knowledge or detailed\nclinical adaptation.\n  Discussion: GPT-5 clearly outperformed prior model variants on the radiation\noncology multiple-choice benchmark. Although GPT-5 exhibited favorable\nperformance in generating real-world radiation oncology treatment\nrecommendations, correctness ratings indicate room for further improvement.\nWhile hallucinations were infrequent, the presence of substantive errors\nunderscores that GPT-5-generated recommendations require rigorous expert\noversight before clinical implementation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: Large language models (LLM) have shown great potential in\nclinical decision support. GPT-5 is a novel LLM system that has been\nspecifically marketed towards oncology use.\n  Methods: Performance was assessed using two complementary benchmarks: (i) the\nACR Radiation Oncology In-Training Examination (TXIT, 2021), comprising 300\nmultiple-choice items, and (ii) a curated set of 60 authentic radiation\noncologic vignettes representing diverse disease sites and treatment\nindications. For the vignette evaluation, GPT-5 was instructed to generate\nconcise therapeutic plans. Four board-certified radiation oncologists rated\ncorrectness, comprehensiveness, and hallucinations. Inter-rater reliability was\nquantified using Fleiss' \\k{appa}.\n  Results: On the TXIT benchmark, GPT-5 achieved a mean accuracy of 92.8%,\noutperforming GPT-4 (78.8%) and GPT-3.5 (62.1%). Domain-specific gains were\nmost pronounced in Dose and Diagnosis. In the vignette evaluation, GPT-5's\ntreatment recommendations were rated highly for correctness (mean 3.24/4, 95%\nCI: 3.11-3.38) and comprehensiveness (3.59/4, 95% CI: 3.49-3.69).\nHallucinations were rare with no case reaching majority consensus for their\npresence. Inter-rater agreement was low (Fleiss' \\k{appa} 0.083 for\ncorrectness), reflecting inherent variability in clinical judgment. Errors\nclustered in complex scenarios requiring precise trial knowledge or detailed\nclinical adaptation.\n  Discussion: GPT-5 clearly outperformed prior model variants on the radiation\noncology multiple-choice benchmark. Although GPT-5 exhibited favorable\nperformance in generating real-world radiation oncology treatment\nrecommendations, correctness ratings indicate room for further improvement.\nWhile hallucinations were infrequent, the presence of substantive errors\nunderscores that GPT-5-generated recommendations require rigorous expert\noversight before clinical implementation."
                },
                "authors": [
                    {
                        "name": "Ugur Dinc"
                    },
                    {
                        "name": "Jibak Sarkar"
                    },
                    {
                        "name": "Philipp Schubert"
                    },
                    {
                        "name": "Sabine Semrau"
                    },
                    {
                        "name": "Thomas Weissmann"
                    },
                    {
                        "name": "Andre Karius"
                    },
                    {
                        "name": "Johann Brand"
                    },
                    {
                        "name": "Bernd-Niklas Axer"
                    },
                    {
                        "name": "Ahmed Gomaa"
                    },
                    {
                        "name": "Pluvio Stephan"
                    },
                    {
                        "name": "Ishita Sheth"
                    },
                    {
                        "name": "Sogand Beirami"
                    },
                    {
                        "name": "Annette Schwarz"
                    },
                    {
                        "name": "Udo Gaipl"
                    },
                    {
                        "name": "Benjamin Frey"
                    },
                    {
                        "name": "Christoph Bert"
                    },
                    {
                        "name": "Stefanie Corradini"
                    },
                    {
                        "name": "Rainer Fietkau"
                    },
                    {
                        "name": "Florian Putz"
                    }
                ],
                "author_detail": {
                    "name": "Florian Putz"
                },
                "author": "Florian Putz",
                "arxiv_comment": "Under review in Frontiers in Artificial Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21777v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21777v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21766v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21766v1",
                "updated": "2025-08-29T16:40:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    40,
                    26,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T16:40:26Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    40,
                    26,
                    4,
                    241,
                    0
                ],
                "title": "Magnetism Enhanced Surface Bonding of O$_{2}$ on CoPt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetism Enhanced Surface Bonding of O$_{2}$ on CoPt"
                },
                "summary": "For large-scale deployment and use of polymer electrolyte fuel cells,\nhigh-performance electrocatalysts with low platinum consumption are desirable.\nOne promising strategy to meet this demand is to explore alternative materials\nthat retain catalytic efficiency while introducing new mechanisms for\nperformance enhacement. In this study, we investigate a ferromagnetic CoPt as a\ncandidate material to accelerate oxygen reduction reactions. By using density\nfunctional theory calculations, we find the spin-polarized Co-$d$ states to\nenhance O$_2$ surface bonding due to local exchange splitting of Co-$d$\ncarriers at the Fermi level. Furthermore, O and O$_2$ adsorption and\ndissociation energies are found to be tuned by varying the thickness of the Pt\nlayers. Our study gives insight into the role magnetism plays in the oxygen\nreduction reaction process and how magnetic ions may aid in the design of new\nadvanced catalysts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For large-scale deployment and use of polymer electrolyte fuel cells,\nhigh-performance electrocatalysts with low platinum consumption are desirable.\nOne promising strategy to meet this demand is to explore alternative materials\nthat retain catalytic efficiency while introducing new mechanisms for\nperformance enhacement. In this study, we investigate a ferromagnetic CoPt as a\ncandidate material to accelerate oxygen reduction reactions. By using density\nfunctional theory calculations, we find the spin-polarized Co-$d$ states to\nenhance O$_2$ surface bonding due to local exchange splitting of Co-$d$\ncarriers at the Fermi level. Furthermore, O and O$_2$ adsorption and\ndissociation energies are found to be tuned by varying the thickness of the Pt\nlayers. Our study gives insight into the role magnetism plays in the oxygen\nreduction reaction process and how magnetic ions may aid in the design of new\nadvanced catalysts."
                },
                "authors": [
                    {
                        "name": "Kevin Allen"
                    },
                    {
                        "name": "Christopher Lane"
                    },
                    {
                        "name": "Emilia Morosan"
                    },
                    {
                        "name": "Jian-Xin Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jian-Xin Zhu"
                },
                "author": "Jian-Xin Zhu",
                "arxiv_comment": "7 pages, 3 figures, and 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21766v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21766v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14119v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14119v4",
                "updated": "2025-08-29T16:38:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    38,
                    48,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-18T14:24:27Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    14,
                    24,
                    27,
                    0,
                    230,
                    0
                ],
                "title": "Documenting Deployment with Fabric: A Repository of Real-World AI\n  Governance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Documenting Deployment with Fabric: A Repository of Real-World AI\n  Governance"
                },
                "summary": "Artificial intelligence (AI) is increasingly integrated into society, from\nfinancial services and traffic management to creative writing. Academic\nliterature on the deployment of AI has mostly focused on the risks and harms\nthat result from the use of AI. We introduce Fabric, a publicly available\nrepository of deployed AI use cases to outline their governance mechanisms.\nThrough semi-structured interviews with practitioners, we collect an initial\nset of 20 AI use cases. In addition, we co-design diagrams of the AI workflow\nwith the practitioners. We discuss the oversight mechanisms and guardrails used\nin practice to safeguard AI use. The Fabric repository includes visual diagrams\nof AI use cases and descriptions of the deployed systems. Using the repository,\nwe surface gaps in governance and find common patterns in human oversight of\ndeployed AI systems. We intend for Fabric to serve as an extendable, evolving\ntool for researchers to study the effectiveness of AI governance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) is increasingly integrated into society, from\nfinancial services and traffic management to creative writing. Academic\nliterature on the deployment of AI has mostly focused on the risks and harms\nthat result from the use of AI. We introduce Fabric, a publicly available\nrepository of deployed AI use cases to outline their governance mechanisms.\nThrough semi-structured interviews with practitioners, we collect an initial\nset of 20 AI use cases. In addition, we co-design diagrams of the AI workflow\nwith the practitioners. We discuss the oversight mechanisms and guardrails used\nin practice to safeguard AI use. The Fabric repository includes visual diagrams\nof AI use cases and descriptions of the deployed systems. Using the repository,\nwe surface gaps in governance and find common patterns in human oversight of\ndeployed AI systems. We intend for Fabric to serve as an extendable, evolving\ntool for researchers to study the effectiveness of AI governance."
                },
                "authors": [
                    {
                        "name": "Mackenzie Jorgensen"
                    },
                    {
                        "name": "Kendall Brogle"
                    },
                    {
                        "name": "Katherine M. Collins"
                    },
                    {
                        "name": "Lujain Ibrahim"
                    },
                    {
                        "name": "Arina Shah"
                    },
                    {
                        "name": "Petra Ivanovic"
                    },
                    {
                        "name": "Noah Broestl"
                    },
                    {
                        "name": "Gabriel Piles"
                    },
                    {
                        "name": "Paul Dongha"
                    },
                    {
                        "name": "Hatim Abdulhussein"
                    },
                    {
                        "name": "Adrian Weller"
                    },
                    {
                        "name": "Jillian Powers"
                    },
                    {
                        "name": "Umang Bhatt"
                    }
                ],
                "author_detail": {
                    "name": "Umang Bhatt"
                },
                "author": "Umang Bhatt",
                "arxiv_comment": "AIES 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14119v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14119v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21763v1",
                "updated": "2025-08-29T16:38:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    38,
                    18,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T16:38:18Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    38,
                    18,
                    4,
                    241,
                    0
                ],
                "title": "On the Implementation Security of Twin-Field Quantum Key Distribution\n  using Optical Injection Locking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Implementation Security of Twin-Field Quantum Key Distribution\n  using Optical Injection Locking"
                },
                "summary": "Twin-Field Quantum Key Distribution (TF-QKD) has emerged as a leading quantum\ncommunication protocol, enabling secure key distribution over unprecedented\ndistances by utilising coherent interference of quantum states. Optical\nInjection Locking (OIL) architectures have been used to simplify the precise\nphase and frequency stabilisation required by TF-QKD. In this work, we\nsystematically analyse potential side-channels in OIL-based TF-QKD that can be\nintroduced through the various optical degrees of freedom of the externally\ninjected reference laser. We experimentally demonstrate two realistic attack\nscenarios: rapid intensity modulation of the reference laser and Trojan-horse\nsignals exploiting wavelengths undetectable by conventional monitoring\ntechniques. To counter these vulnerabilities, we propose straightforward and\nhighly effective countermeasures including high-speed photodiodes for real-time\npower monitoring and targeted spectral filtering to detect and suppress\nout-of-band signals. Our experimental results confirm that these practical\nsolutions substantially reinforce the security of TF-QKD systems without\nsignificant additional complexity or performance degradation. More broadly, our\nanalysis highlights the critical importance of comprehensive optical monitoring\nto ensure the robust practical deployment of TF-QKD in real-world quantum\ncommunication networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Twin-Field Quantum Key Distribution (TF-QKD) has emerged as a leading quantum\ncommunication protocol, enabling secure key distribution over unprecedented\ndistances by utilising coherent interference of quantum states. Optical\nInjection Locking (OIL) architectures have been used to simplify the precise\nphase and frequency stabilisation required by TF-QKD. In this work, we\nsystematically analyse potential side-channels in OIL-based TF-QKD that can be\nintroduced through the various optical degrees of freedom of the externally\ninjected reference laser. We experimentally demonstrate two realistic attack\nscenarios: rapid intensity modulation of the reference laser and Trojan-horse\nsignals exploiting wavelengths undetectable by conventional monitoring\ntechniques. To counter these vulnerabilities, we propose straightforward and\nhighly effective countermeasures including high-speed photodiodes for real-time\npower monitoring and targeted spectral filtering to detect and suppress\nout-of-band signals. Our experimental results confirm that these practical\nsolutions substantially reinforce the security of TF-QKD systems without\nsignificant additional complexity or performance degradation. More broadly, our\nanalysis highlights the critical importance of comprehensive optical monitoring\nto ensure the robust practical deployment of TF-QKD in real-world quantum\ncommunication networks."
                },
                "authors": [
                    {
                        "name": "Sergio Juárez"
                    },
                    {
                        "name": "Alessandro Marcomini"
                    },
                    {
                        "name": "Mikhail Petrov"
                    },
                    {
                        "name": "Robert I. Woodward"
                    },
                    {
                        "name": "Toby J. Dowling"
                    },
                    {
                        "name": "R. Mark Stevenson"
                    },
                    {
                        "name": "Marcos Curty"
                    },
                    {
                        "name": "Davide Rusca"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rusca"
                },
                "author": "Davide Rusca",
                "arxiv_comment": "14 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05163v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05163v2",
                "updated": "2025-08-29T16:38:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    38,
                    16,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-07T15:08:03Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    15,
                    8,
                    3,
                    0,
                    97,
                    0
                ],
                "title": "Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods\n  under Knowledge Incompleteness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Knowledge Graph Based Retrieval Augmented Generation Methods\n  under Knowledge Incompleteness"
                },
                "summary": "Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) is a technique\nthat enhances Large Language Model (LLM) inference in tasks like Question\nAnswering (QA) by retrieving relevant information from knowledge graphs (KGs).\nHowever, real-world KGs are often incomplete, meaning that essential\ninformation for answering questions may be missing. Existing benchmarks do not\nadequately capture the impact of KG incompleteness on KG-RAG performance. In\nthis paper, we systematically evaluate KG-RAG methods under incomplete KGs by\nremoving triples using different methods and analyzing the resulting effects.\nWe demonstrate that KG-RAG methods are sensitive to KG incompleteness,\nhighlighting the need for more robust approaches in realistic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graph based Retrieval-Augmented Generation (KG-RAG) is a technique\nthat enhances Large Language Model (LLM) inference in tasks like Question\nAnswering (QA) by retrieving relevant information from knowledge graphs (KGs).\nHowever, real-world KGs are often incomplete, meaning that essential\ninformation for answering questions may be missing. Existing benchmarks do not\nadequately capture the impact of KG incompleteness on KG-RAG performance. In\nthis paper, we systematically evaluate KG-RAG methods under incomplete KGs by\nremoving triples using different methods and analyzing the resulting effects.\nWe demonstrate that KG-RAG methods are sensitive to KG incompleteness,\nhighlighting the need for more robust approaches in realistic settings."
                },
                "authors": [
                    {
                        "name": "Dongzhuoran Zhou"
                    },
                    {
                        "name": "Yuqicheng Zhu"
                    },
                    {
                        "name": "Xiaxia Wang"
                    },
                    {
                        "name": "Yuan He"
                    },
                    {
                        "name": "Jiaoyan Chen"
                    },
                    {
                        "name": "Steffen Staab"
                    },
                    {
                        "name": "Evgeny Kharlamov"
                    }
                ],
                "author_detail": {
                    "name": "Evgeny Kharlamov"
                },
                "author": "Evgeny Kharlamov",
                "arxiv_comment": "IRISAI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05163v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05163v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21762v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21762v1",
                "updated": "2025-08-29T16:37:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    37,
                    42,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T16:37:42Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    37,
                    42,
                    4,
                    241,
                    0
                ],
                "title": "Reasoning-Intensive Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-Intensive Regression"
                },
                "summary": "AI researchers and practitioners increasingly apply large language models\n(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing\nsubtle numerical properties from text. Unlike standard language regression\ntasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc\nproblems like rubric-based scoring or domain-specific retrieval, where much\ndeeper analysis of text is required while only limited task-specific training\ndata and computation are available. We cast three realistic problems as RiR\ntasks to establish an initial benchmark, and use that to test our hypothesis\nthat prompting frozen LLMs and finetuning Transformer encoders via gradient\ndescent will both often struggle in RiR. We then propose MENTAT, a simple and\nlightweight method that combines batch-reflective prompt optimization with\nneural ensemble learning. MENTAT achieves up to 65% improvement over both\nbaselines, though substantial room remains for future advances in RiR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI researchers and practitioners increasingly apply large language models\n(LLMs) to what we call reasoning-intensive regression (RiR), i.e. deducing\nsubtle numerical properties from text. Unlike standard language regression\ntasks, e.g. for sentiment or similarity, RiR often appears instead in ad-hoc\nproblems like rubric-based scoring or domain-specific retrieval, where much\ndeeper analysis of text is required while only limited task-specific training\ndata and computation are available. We cast three realistic problems as RiR\ntasks to establish an initial benchmark, and use that to test our hypothesis\nthat prompting frozen LLMs and finetuning Transformer encoders via gradient\ndescent will both often struggle in RiR. We then propose MENTAT, a simple and\nlightweight method that combines batch-reflective prompt optimization with\nneural ensemble learning. MENTAT achieves up to 65% improvement over both\nbaselines, though substantial room remains for future advances in RiR."
                },
                "authors": [
                    {
                        "name": "Diane Tchuindjo"
                    },
                    {
                        "name": "Omar Khattab"
                    }
                ],
                "author_detail": {
                    "name": "Omar Khattab"
                },
                "author": "Omar Khattab",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21762v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21762v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19028v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19028v4",
                "updated": "2025-08-29T16:20:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    20,
                    31,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-23T18:31:22Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    18,
                    31,
                    22,
                    0,
                    174,
                    0
                ],
                "title": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical\n  Perspective"
                },
                "summary": "Large Language Models (LLMs) often generate responses with inherent biases,\nundermining their reliability in real-world applications. Existing evaluation\nmethods often overlook biases in long-form responses and the intrinsic\nvariability of LLM outputs. To address these challenges, we propose FiSCo\n(Fine-grained Semantic Comparison), a novel statistical framework to evaluate\ngroup-level fairness in LLMs by detecting subtle semantic differences in\nlong-form responses across demographic groups. Unlike prior work focusing on\nsentiment or token-level comparisons, FiSCo goes beyond surface-level analysis\nby operating at the claim level, leveraging entailment checks to assess the\nconsistency of meaning across responses. We decompose model outputs into\nsemantically distinct claims and apply statistical hypothesis testing to\ncompare inter- and intra-group similarities, enabling robust detection of\nsubtle biases. We formalize a new group counterfactual fairness definition and\nvalidate FiSCo on both synthetic and human-annotated datasets spanning gender,\nrace, and age. Experiments show that FiSCo more reliably identifies nuanced\nbiases while reducing the impact of stochastic LLM variability, outperforming\nvarious evaluation metrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often generate responses with inherent biases,\nundermining their reliability in real-world applications. Existing evaluation\nmethods often overlook biases in long-form responses and the intrinsic\nvariability of LLM outputs. To address these challenges, we propose FiSCo\n(Fine-grained Semantic Comparison), a novel statistical framework to evaluate\ngroup-level fairness in LLMs by detecting subtle semantic differences in\nlong-form responses across demographic groups. Unlike prior work focusing on\nsentiment or token-level comparisons, FiSCo goes beyond surface-level analysis\nby operating at the claim level, leveraging entailment checks to assess the\nconsistency of meaning across responses. We decompose model outputs into\nsemantically distinct claims and apply statistical hypothesis testing to\ncompare inter- and intra-group similarities, enabling robust detection of\nsubtle biases. We formalize a new group counterfactual fairness definition and\nvalidate FiSCo on both synthetic and human-annotated datasets spanning gender,\nrace, and age. Experiments show that FiSCo more reliably identifies nuanced\nbiases while reducing the impact of stochastic LLM variability, outperforming\nvarious evaluation metrics."
                },
                "authors": [
                    {
                        "name": "Weijie Xu"
                    },
                    {
                        "name": "Yiwen Wang"
                    },
                    {
                        "name": "Chi Xue"
                    },
                    {
                        "name": "Xiangkun Hu"
                    },
                    {
                        "name": "Xi Fang"
                    },
                    {
                        "name": "Guimin Dong"
                    },
                    {
                        "name": "Chandan K. Reddy"
                    }
                ],
                "author_detail": {
                    "name": "Chandan K. Reddy"
                },
                "author": "Chandan K. Reddy",
                "arxiv_comment": "29 pages, 9 figures, 15 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19028v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19028v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15008v2",
                "updated": "2025-08-29T16:17:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    17,
                    21,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-20T18:56:26Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    18,
                    56,
                    26,
                    2,
                    232,
                    0
                ],
                "title": "Quantized Neural Networks for Microcontrollers: A Comprehensive Review\n  of Methods, Platforms, and Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized Neural Networks for Microcontrollers: A Comprehensive Review\n  of Methods, Platforms, and Applications"
                },
                "summary": "The deployment of Quantized Neural Networks (QNNs) on resource-constrained\ndevices, such as microcontrollers, has introduced significant challenges in\nbalancing model performance, computational complexity, and memory constraints.\nTiny Machine Learning (TinyML) addresses these issues by integrating\nadvancements across machine learning algorithms, hardware acceleration, and\nsoftware optimization to efficiently run deep neural networks on embedded\nsystems. This survey presents a hardware-centric introduction to quantization,\nsystematically reviewing essential quantization techniques employed to\naccelerate deep learning models for embedded applications. In particular,\nfurther emphasis is placed on the critical trade-offs between model performance\nand hardware capabilities. The survey further evaluates existing software\nframeworks and hardware platforms designed specifically for supporting QNN\nexecution on microcontrollers. Moreover, we provide an analysis of the current\nchallenges and an outline of promising future directions in the rapidly\nevolving domain of QNN deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Quantized Neural Networks (QNNs) on resource-constrained\ndevices, such as microcontrollers, has introduced significant challenges in\nbalancing model performance, computational complexity, and memory constraints.\nTiny Machine Learning (TinyML) addresses these issues by integrating\nadvancements across machine learning algorithms, hardware acceleration, and\nsoftware optimization to efficiently run deep neural networks on embedded\nsystems. This survey presents a hardware-centric introduction to quantization,\nsystematically reviewing essential quantization techniques employed to\naccelerate deep learning models for embedded applications. In particular,\nfurther emphasis is placed on the critical trade-offs between model performance\nand hardware capabilities. The survey further evaluates existing software\nframeworks and hardware platforms designed specifically for supporting QNN\nexecution on microcontrollers. Moreover, we provide an analysis of the current\nchallenges and an outline of promising future directions in the rapidly\nevolving domain of QNN deployment."
                },
                "authors": [
                    {
                        "name": "Hamza A. Abushahla"
                    },
                    {
                        "name": "Dara Varam"
                    },
                    {
                        "name": "Ariel J. N. Panopio"
                    },
                    {
                        "name": "Mohamed I. AlHajri"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed I. AlHajri"
                },
                "author": "Mohamed I. AlHajri",
                "arxiv_comment": "39 pages, 16 figures, 8 Tables, submitted to the Proceedings of the\n  IEEE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21741v1",
                "updated": "2025-08-29T16:07:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    7,
                    33,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T16:07:33Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    7,
                    33,
                    4,
                    241,
                    0
                ],
                "title": "Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning\n  Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Parameters Are Created Equal: Smart Isolation Boosts Fine-Tuning\n  Performance"
                },
                "summary": "Supervised fine-tuning (SFT) is a pivotal approach to adapting large language\nmodels (LLMs) for downstream tasks; however, performance often suffers from the\n``seesaw phenomenon'', where indiscriminate parameter updates yield progress on\ncertain tasks at the expense of others. To address this challenge, we propose a\nnovel \\emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.\nSpecifically, we first independently fine-tune the LLM on each task to identify\nits core parameter regions by quantifying parameter update magnitudes. Tasks\nwith similar core regions are then grouped based on region overlap, forming\nclusters for joint modeling. We further introduce a parameter fusion technique:\nfor each task, core parameters from its individually fine-tuned model are\ndirectly transplanted into a unified backbone, while non-core parameters from\ndifferent tasks are smoothly integrated via Spherical Linear Interpolation\n(SLERP), mitigating destructive interference. A lightweight, pipelined SFT\ntraining phase using mixed-task data is subsequently employed, while freezing\ncore regions from prior tasks to prevent catastrophic forgetting. Extensive\nexperiments on multiple public benchmarks demonstrate that our approach\nsignificantly alleviates task interference and forgetting, consistently\noutperforming vanilla multi-task and multi-stage fine-tuning baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised fine-tuning (SFT) is a pivotal approach to adapting large language\nmodels (LLMs) for downstream tasks; however, performance often suffers from the\n``seesaw phenomenon'', where indiscriminate parameter updates yield progress on\ncertain tasks at the expense of others. To address this challenge, we propose a\nnovel \\emph{Core Parameter Isolation Fine-Tuning} (CPI-FT) framework.\nSpecifically, we first independently fine-tune the LLM on each task to identify\nits core parameter regions by quantifying parameter update magnitudes. Tasks\nwith similar core regions are then grouped based on region overlap, forming\nclusters for joint modeling. We further introduce a parameter fusion technique:\nfor each task, core parameters from its individually fine-tuned model are\ndirectly transplanted into a unified backbone, while non-core parameters from\ndifferent tasks are smoothly integrated via Spherical Linear Interpolation\n(SLERP), mitigating destructive interference. A lightweight, pipelined SFT\ntraining phase using mixed-task data is subsequently employed, while freezing\ncore regions from prior tasks to prevent catastrophic forgetting. Extensive\nexperiments on multiple public benchmarks demonstrate that our approach\nsignificantly alleviates task interference and forgetting, consistently\noutperforming vanilla multi-task and multi-stage fine-tuning baselines."
                },
                "authors": [
                    {
                        "name": "Yao Wang"
                    },
                    {
                        "name": "Di Liang"
                    },
                    {
                        "name": "Minlong Peng"
                    }
                ],
                "author_detail": {
                    "name": "Minlong Peng"
                },
                "author": "Minlong Peng",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21740v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21740v1",
                "updated": "2025-08-29T16:06:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    6,
                    27,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T16:06:27Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    16,
                    6,
                    27,
                    4,
                    241,
                    0
                ],
                "title": "Operational Validation of Large-Language-Model Agent Social Simulation:\n  Evidence from Voat v/technology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operational Validation of Large-Language-Model Agent Social Simulation:\n  Evidence from Voat v/technology"
                },
                "summary": "Large Language Models (LLMs) enable generative social simulations that can\ncapture culturally informed, norm-guided interaction on online social\nplatforms. We build a technology community simulation modeled on Voat, a\nReddit-like alt-right news aggregator and discussion platform active from 2014\nto 2020. Using the YSocial framework, we seed the simulation with a fixed\ncatalog of technology links sampled from Voat's shared URLs (covering 30+\ndomains) and calibrate parameters to Voat's v/technology using samples from the\nMADOC dataset. Agents use a base, uncensored model (Dolphin 3.0, based on Llama\n3.1 8B) and concise personas (demographics, political leaning, interests,\neducation, toxicity propensity) to generate posts, replies, and reactions under\nplatform rules for link and text submissions, threaded replies and daily\nactivity cycles. We run a 30-day simulation and evaluate operational validity\nby comparing distributions and structures with matched Voat data: activity\npatterns, interaction networks, toxicity, and topic coverage. Results indicate\nfamiliar online regularities: similar activity rhythms, heavy-tailed\nparticipation, sparse low-clustering interaction networks, core-periphery\nstructure, topical alignment with Voat, and elevated toxicity. Limitations of\nthe current study include the stateless agent design and evaluation based on a\nsingle 30-day run, which constrains external validity and variance estimates.\nThe simulation generates realistic discussions, often featuring toxic language,\nprimarily centered on technology topics such as Big Tech and AI. This approach\noffers a valuable method for examining toxicity dynamics and testing moderation\nstrategies within a controlled environment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) enable generative social simulations that can\ncapture culturally informed, norm-guided interaction on online social\nplatforms. We build a technology community simulation modeled on Voat, a\nReddit-like alt-right news aggregator and discussion platform active from 2014\nto 2020. Using the YSocial framework, we seed the simulation with a fixed\ncatalog of technology links sampled from Voat's shared URLs (covering 30+\ndomains) and calibrate parameters to Voat's v/technology using samples from the\nMADOC dataset. Agents use a base, uncensored model (Dolphin 3.0, based on Llama\n3.1 8B) and concise personas (demographics, political leaning, interests,\neducation, toxicity propensity) to generate posts, replies, and reactions under\nplatform rules for link and text submissions, threaded replies and daily\nactivity cycles. We run a 30-day simulation and evaluate operational validity\nby comparing distributions and structures with matched Voat data: activity\npatterns, interaction networks, toxicity, and topic coverage. Results indicate\nfamiliar online regularities: similar activity rhythms, heavy-tailed\nparticipation, sparse low-clustering interaction networks, core-periphery\nstructure, topical alignment with Voat, and elevated toxicity. Limitations of\nthe current study include the stateless agent design and evaluation based on a\nsingle 30-day run, which constrains external validity and variance estimates.\nThe simulation generates realistic discussions, often featuring toxic language,\nprimarily centered on technology topics such as Big Tech and AI. This approach\noffers a valuable method for examining toxicity dynamics and testing moderation\nstrategies within a controlled environment."
                },
                "authors": [
                    {
                        "name": "Aleksandar Tomašević"
                    },
                    {
                        "name": "Darja Cvetković"
                    },
                    {
                        "name": "Sara Major"
                    },
                    {
                        "name": "Slobodan Maletić"
                    },
                    {
                        "name": "Miroslav Anđelković"
                    },
                    {
                        "name": "Ana Vranić"
                    },
                    {
                        "name": "Boris Stupovski"
                    },
                    {
                        "name": "Dušan Vudragović"
                    },
                    {
                        "name": "Aleksandar Bogojević"
                    },
                    {
                        "name": "Marija Mitrović Dankulov"
                    }
                ],
                "author_detail": {
                    "name": "Marija Mitrović Dankulov"
                },
                "author": "Marija Mitrović Dankulov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21740v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21740v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21715v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21715v1",
                "updated": "2025-08-29T15:33:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    15,
                    33,
                    45,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T15:33:45Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    15,
                    33,
                    45,
                    4,
                    241,
                    0
                ],
                "title": "Entropy-Based Non-Invasive Reliability Monitoring of Convolutional\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entropy-Based Non-Invasive Reliability Monitoring of Convolutional\n  Neural Networks"
                },
                "summary": "Convolutional Neural Networks (CNNs) have become the foundation of modern\ncomputer vision, achieving unprecedented accuracy across diverse image\nrecognition tasks. While these networks excel on in-distribution data, they\nremain vulnerable to adversarial perturbations imperceptible input\nmodifications that cause misclassification with high confidence. However,\nexisting detection methods either require expensive retraining, modify network\narchitecture, or degrade performance on clean inputs. Here we show that\nadversarial perturbations create immediate, detectable entropy signatures in\nCNN activations that can be monitored without any model modification. Using\nparallel entropy monitoring on VGG-16, we demonstrate that adversarial inputs\nconsistently shift activation entropy by 7% in early convolutional layers,\nenabling 90% detection accuracy with false positives and false negative rates\nbelow 20%. The complete separation between clean and adversarial entropy\ndistributions reveals that CNNs inherently encode distribution shifts in their\nactivation patterns. This work establishes that CNN reliability can be assessed\nthrough activation entropy alone, enabling practical deployment of\nself-diagnostic vision systems that detect adversarial inputs in real-time\nwithout compromising original model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Convolutional Neural Networks (CNNs) have become the foundation of modern\ncomputer vision, achieving unprecedented accuracy across diverse image\nrecognition tasks. While these networks excel on in-distribution data, they\nremain vulnerable to adversarial perturbations imperceptible input\nmodifications that cause misclassification with high confidence. However,\nexisting detection methods either require expensive retraining, modify network\narchitecture, or degrade performance on clean inputs. Here we show that\nadversarial perturbations create immediate, detectable entropy signatures in\nCNN activations that can be monitored without any model modification. Using\nparallel entropy monitoring on VGG-16, we demonstrate that adversarial inputs\nconsistently shift activation entropy by 7% in early convolutional layers,\nenabling 90% detection accuracy with false positives and false negative rates\nbelow 20%. The complete separation between clean and adversarial entropy\ndistributions reveals that CNNs inherently encode distribution shifts in their\nactivation patterns. This work establishes that CNN reliability can be assessed\nthrough activation entropy alone, enabling practical deployment of\nself-diagnostic vision systems that detect adversarial inputs in real-time\nwithout compromising original model performance."
                },
                "authors": [
                    {
                        "name": "Amirhossein Nazeri"
                    },
                    {
                        "name": "Wael Hafez"
                    }
                ],
                "author_detail": {
                    "name": "Wael Hafez"
                },
                "author": "Wael Hafez",
                "arxiv_comment": "8 pages, 3 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21715v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21715v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05137v2",
                "updated": "2025-08-29T15:28:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    15,
                    28,
                    0,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-07T15:49:23Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    15,
                    49,
                    23,
                    0,
                    188,
                    0
                ],
                "title": "Interpretable Mnemonic Generation for Kanji Learning via\n  Expectation-Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpretable Mnemonic Generation for Kanji Learning via\n  Expectation-Maximization"
                },
                "summary": "Learning Japanese vocabulary is a challenge for learners from Roman alphabet\nbackgrounds due to script differences. Japanese combines syllabaries like\nhiragana with kanji, which are logographic characters of Chinese origin. Kanji\nare also complicated due to their complexity and volume. Keyword mnemonics are\na common strategy to aid memorization, often using the compositional structure\nof kanji to form vivid associations. Despite recent efforts to use large\nlanguage models (LLMs) to assist learners, existing methods for LLM-based\nkeyword mnemonic generation function as a black box, offering limited\ninterpretability. We propose a generative framework that explicitly models the\nmnemonic construction process as driven by a set of common rules, and learn\nthem using a novel Expectation-Maximization-type algorithm. Trained on\nlearner-authored mnemonics from an online platform, our method learns latent\nstructures and compositional rules, enabling interpretable and systematic\nmnemonics generation. Experiments show that our method performs well in the\ncold-start setting for new learners while providing insight into the mechanisms\nbehind effective mnemonic creation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Japanese vocabulary is a challenge for learners from Roman alphabet\nbackgrounds due to script differences. Japanese combines syllabaries like\nhiragana with kanji, which are logographic characters of Chinese origin. Kanji\nare also complicated due to their complexity and volume. Keyword mnemonics are\na common strategy to aid memorization, often using the compositional structure\nof kanji to form vivid associations. Despite recent efforts to use large\nlanguage models (LLMs) to assist learners, existing methods for LLM-based\nkeyword mnemonic generation function as a black box, offering limited\ninterpretability. We propose a generative framework that explicitly models the\nmnemonic construction process as driven by a set of common rules, and learn\nthem using a novel Expectation-Maximization-type algorithm. Trained on\nlearner-authored mnemonics from an online platform, our method learns latent\nstructures and compositional rules, enabling interpretable and systematic\nmnemonics generation. Experiments show that our method performs well in the\ncold-start setting for new learners while providing insight into the mechanisms\nbehind effective mnemonic creation."
                },
                "authors": [
                    {
                        "name": "Jaewook Lee"
                    },
                    {
                        "name": "Alexander Scarlatos"
                    },
                    {
                        "name": "Andrew Lan"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Lan"
                },
                "author": "Andrew Lan",
                "arxiv_comment": "The Conference on Empirical Methods in Natural Language Processing\n  (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21696v1",
                "updated": "2025-08-29T15:06:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    15,
                    6,
                    25,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T15:06:25Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    15,
                    6,
                    25,
                    4,
                    241,
                    0
                ],
                "title": "Demonstration of an optical microwave rectification by a superconducting\n  diode with near 100% efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demonstration of an optical microwave rectification by a superconducting\n  diode with near 100% efficiency"
                },
                "summary": "Superconducting electronics offer significant advantages in speed and power\nefficiency for next-generation computing and communication systems. However,\ntheir practical deployment is limited by the absence of simple, efficient, and\nscalable superconducting counterparts to key semiconductor components. In this\nwork, we investigate diodes based on planar Josephson junctions fabricated from\na conventional niobium superconductor. The nonreciprocity in these diodes\narises from the self-field effect induced by the geometrical asymmetry of the\njunction. By deliberate tuning of the junction parameters, we achieved\neffectively infinite nonreciprocity (within experimental resolution),\ncharacterized by a complete suppression of the superconducting critical current\nin one direction while maintaining a significant current in the opposite\ndirection. The key novelty of this work lies in the demonstration of the\noptical diode effect. We observed threshold-free rectification of 75 GHz\nmicrowave radiation, indicating that these diodes exhibit near-ideal optical\nnonreciprocity. Our results open new avenues for ultrafast superconducting\nelectronics and lay the groundwork for wireless sub-THz signal processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superconducting electronics offer significant advantages in speed and power\nefficiency for next-generation computing and communication systems. However,\ntheir practical deployment is limited by the absence of simple, efficient, and\nscalable superconducting counterparts to key semiconductor components. In this\nwork, we investigate diodes based on planar Josephson junctions fabricated from\na conventional niobium superconductor. The nonreciprocity in these diodes\narises from the self-field effect induced by the geometrical asymmetry of the\njunction. By deliberate tuning of the junction parameters, we achieved\neffectively infinite nonreciprocity (within experimental resolution),\ncharacterized by a complete suppression of the superconducting critical current\nin one direction while maintaining a significant current in the opposite\ndirection. The key novelty of this work lies in the demonstration of the\noptical diode effect. We observed threshold-free rectification of 75 GHz\nmicrowave radiation, indicating that these diodes exhibit near-ideal optical\nnonreciprocity. Our results open new avenues for ultrafast superconducting\nelectronics and lay the groundwork for wireless sub-THz signal processing."
                },
                "authors": [
                    {
                        "name": "Razmik A. Hovhannisyan"
                    },
                    {
                        "name": "Amirreza Lotfian"
                    },
                    {
                        "name": "Taras Golod"
                    },
                    {
                        "name": "Vladimir M. Krasnov"
                    }
                ],
                "author_detail": {
                    "name": "Vladimir M. Krasnov"
                },
                "author": "Vladimir M. Krasnov",
                "arxiv_comment": "6 pages, 2 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04619v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04619v2",
                "updated": "2025-08-29T14:54:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    54,
                    30,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-07T17:59:28Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    17,
                    59,
                    28,
                    2,
                    127,
                    0
                ],
                "title": "Merging and Disentangling Views in Visual Reinforcement Learning for\n  Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Merging and Disentangling Views in Visual Reinforcement Learning for\n  Robotic Manipulation"
                },
                "summary": "Vision is well-known for its use in manipulation, especially using visual\nservoing. Due to the 3D nature of the world, using multiple camera views and\nmerging them creates better representations for Q-learning and in turn, trains\nmore sample efficient policies. Nevertheless, these multi-view policies are\nsensitive to failing cameras and can be burdensome to deploy. To mitigate these\nissues, we introduce a Merge And Disentanglement (MAD) algorithm that\nefficiently merges views to increase sample efficiency while simultaneously\ndisentangling views by augmenting multi-view feature inputs with single-view\nfeatures. This produces robust policies and allows lightweight deployment. We\ndemonstrate the efficiency and robustness of our approach using Meta-World and\nManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision is well-known for its use in manipulation, especially using visual\nservoing. Due to the 3D nature of the world, using multiple camera views and\nmerging them creates better representations for Q-learning and in turn, trains\nmore sample efficient policies. Nevertheless, these multi-view policies are\nsensitive to failing cameras and can be burdensome to deploy. To mitigate these\nissues, we introduce a Merge And Disentanglement (MAD) algorithm that\nefficiently merges views to increase sample efficiency while simultaneously\ndisentangling views by augmenting multi-view feature inputs with single-view\nfeatures. This produces robust policies and allows lightweight deployment. We\ndemonstrate the efficiency and robustness of our approach using Meta-World and\nManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad"
                },
                "authors": [
                    {
                        "name": "Abdulaziz Almuzairee"
                    },
                    {
                        "name": "Rohan Patil"
                    },
                    {
                        "name": "Dwait Bhatt"
                    },
                    {
                        "name": "Henrik I. Christensen"
                    }
                ],
                "author_detail": {
                    "name": "Henrik I. Christensen"
                },
                "author": "Henrik I. Christensen",
                "arxiv_comment": "Accepted at CoRL 2025. For project website and code, see\n  https://aalmuzairee.github.io/mad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04619v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04619v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17196v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17196v2",
                "updated": "2025-08-29T14:42:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    42,
                    16,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-24T03:17:50Z",
                "published_parsed": [
                    2025,
                    8,
                    24,
                    3,
                    17,
                    50,
                    6,
                    236,
                    0
                ],
                "title": "BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BudgetThinker: Empowering Budget-aware LLM Reasoning with Control Tokens"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have leveraged increased\ntest-time computation to enhance reasoning capabilities, a strategy that, while\neffective, incurs significant latency and resource costs, limiting their\napplicability in real-world time-constrained or cost-sensitive scenarios. This\npaper introduces BudgetThinker, a novel framework designed to empower LLMs with\nbudget-aware reasoning, enabling precise control over the length of their\nthought processes. We propose a methodology that periodically inserts special\ncontrol tokens during inference to continuously inform the model of its\nremaining token budget. This approach is coupled with a comprehensive two-stage\ntraining pipeline, beginning with Supervised Fine-Tuning (SFT) to familiarize\nthe model with budget constraints, followed by a curriculum-based Reinforcement\nLearning (RL) phase that utilizes a length-aware reward function to optimize\nfor both accuracy and budget adherence. We demonstrate that BudgetThinker\nsignificantly surpasses strong baselines in maintaining performance across a\nvariety of reasoning budgets on challenging mathematical benchmarks. Our method\nprovides a scalable and effective solution for developing efficient and\ncontrollable LLM reasoning, making advanced models more practical for\ndeployment in resource-constrained and real-time environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have leveraged increased\ntest-time computation to enhance reasoning capabilities, a strategy that, while\neffective, incurs significant latency and resource costs, limiting their\napplicability in real-world time-constrained or cost-sensitive scenarios. This\npaper introduces BudgetThinker, a novel framework designed to empower LLMs with\nbudget-aware reasoning, enabling precise control over the length of their\nthought processes. We propose a methodology that periodically inserts special\ncontrol tokens during inference to continuously inform the model of its\nremaining token budget. This approach is coupled with a comprehensive two-stage\ntraining pipeline, beginning with Supervised Fine-Tuning (SFT) to familiarize\nthe model with budget constraints, followed by a curriculum-based Reinforcement\nLearning (RL) phase that utilizes a length-aware reward function to optimize\nfor both accuracy and budget adherence. We demonstrate that BudgetThinker\nsignificantly surpasses strong baselines in maintaining performance across a\nvariety of reasoning budgets on challenging mathematical benchmarks. Our method\nprovides a scalable and effective solution for developing efficient and\ncontrollable LLM reasoning, making advanced models more practical for\ndeployment in resource-constrained and real-time environments."
                },
                "authors": [
                    {
                        "name": "Hao Wen"
                    },
                    {
                        "name": "Xinrui Wu"
                    },
                    {
                        "name": "Yi Sun"
                    },
                    {
                        "name": "Feifei Zhang"
                    },
                    {
                        "name": "Liye Chen"
                    },
                    {
                        "name": "Jie Wang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Yunhao Liu"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Yuanchun Li"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchun Li"
                },
                "author": "Yuanchun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17196v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17196v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15066v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15066v3",
                "updated": "2025-08-29T14:41:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    41,
                    56,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-20T18:02:50Z",
                "published_parsed": [
                    2025,
                    7,
                    20,
                    18,
                    2,
                    50,
                    6,
                    201,
                    0
                ],
                "title": "Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback"
                },
                "summary": "Time series anomaly detection is critical across various domains, yet current\napproaches often limit analysis to mere binary anomaly classification without\ndetailed categorization or further explanatory reasoning. To address these\nlimitations, we propose a novel task, Time-series Reasoning for Anomaly\n(Time-RA) that transforms classical time series anomaly detection from a\ndiscriminative into a generative, reasoning-intensive task leveraging Large\nLanguage Models (LLMs). Also, we introduce the first real-world multimodal\nbenchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,\ncomprising approximately 40,000 samples across 10 real-world domains. Each\nsample includes numeric time series data, contextual text information, and\nvisual representations, each annotated with fine-grained categories (14 types\nfor univariate anomalies and 6 for multivariate anomalies) and structured\nexplanatory reasoning. We develop a sophisticated annotation framework\nutilizing ensemble-generated labels refined through GPT-4-driven feedback,\nensuring accuracy and interpretability. Extensive benchmarking of LLMs and\nmultimodal LLMs demonstrates the capabilities and limitations of current\nmodels, highlighting the critical role of supervised fine-tuning. Our dataset\nand task pave the way for significant advancements in interpretable time series\nanomaly detection and reasoning. The code\n(https://github.com/yyysjz1997/Time-RA) and dataset\n(https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourced\nto support and accelerate future research in this area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time series anomaly detection is critical across various domains, yet current\napproaches often limit analysis to mere binary anomaly classification without\ndetailed categorization or further explanatory reasoning. To address these\nlimitations, we propose a novel task, Time-series Reasoning for Anomaly\n(Time-RA) that transforms classical time series anomaly detection from a\ndiscriminative into a generative, reasoning-intensive task leveraging Large\nLanguage Models (LLMs). Also, we introduce the first real-world multimodal\nbenchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,\ncomprising approximately 40,000 samples across 10 real-world domains. Each\nsample includes numeric time series data, contextual text information, and\nvisual representations, each annotated with fine-grained categories (14 types\nfor univariate anomalies and 6 for multivariate anomalies) and structured\nexplanatory reasoning. We develop a sophisticated annotation framework\nutilizing ensemble-generated labels refined through GPT-4-driven feedback,\nensuring accuracy and interpretability. Extensive benchmarking of LLMs and\nmultimodal LLMs demonstrates the capabilities and limitations of current\nmodels, highlighting the critical role of supervised fine-tuning. Our dataset\nand task pave the way for significant advancements in interpretable time series\nanomaly detection and reasoning. The code\n(https://github.com/yyysjz1997/Time-RA) and dataset\n(https://huggingface.co/datasets/Time-RA/RATs40K) have been fully open-sourced\nto support and accelerate future research in this area."
                },
                "authors": [
                    {
                        "name": "Yiyuan Yang"
                    },
                    {
                        "name": "Zichuan Liu"
                    },
                    {
                        "name": "Lei Song"
                    },
                    {
                        "name": "Kai Ying"
                    },
                    {
                        "name": "Zhiguang Wang"
                    },
                    {
                        "name": "Tom Bamford"
                    },
                    {
                        "name": "Svitlana Vyetrenko"
                    },
                    {
                        "name": "Jiang Bian"
                    },
                    {
                        "name": "Qingsong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Qingsong Wen"
                },
                "author": "Qingsong Wen",
                "arxiv_comment": "Under review. 19 pages, 8 figures, 12 tables. Code and dataset are\n  publicly available",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15066v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15066v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21669v1",
                "updated": "2025-08-29T14:32:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    32,
                    48,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T14:32:48Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    32,
                    48,
                    4,
                    241,
                    0
                ],
                "title": "Cybersecurity AI: Hacking the AI Hackers via Prompt Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cybersecurity AI: Hacking the AI Hackers via Prompt Injection"
                },
                "summary": "We demonstrate how AI-powered cybersecurity tools can be turned against\nthemselves through prompt injection attacks. Prompt injection is reminiscent of\ncross-site scripting (XSS): malicious text is hidden within seemingly trusted\ncontent, and when the system processes it, that text is transformed into\nunintended instructions. When AI agents designed to find and exploit\nvulnerabilities interact with malicious web servers, carefully crafted reponses\ncan hijack their execution flow, potentially granting attackers system access.\nWe present proof-of-concept exploits against the Cybersecurity AI (CAI)\nframework and its CLI tool, and detail our mitigations against such attacks in\na multi-layered defense implementation. Our findings indicate that prompt\ninjection is a recurring and systemic issue in LLM-based architectures, one\nthat will require dedicated work to address, much as the security community has\nhad to do with XSS in traditional web applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate how AI-powered cybersecurity tools can be turned against\nthemselves through prompt injection attacks. Prompt injection is reminiscent of\ncross-site scripting (XSS): malicious text is hidden within seemingly trusted\ncontent, and when the system processes it, that text is transformed into\nunintended instructions. When AI agents designed to find and exploit\nvulnerabilities interact with malicious web servers, carefully crafted reponses\ncan hijack their execution flow, potentially granting attackers system access.\nWe present proof-of-concept exploits against the Cybersecurity AI (CAI)\nframework and its CLI tool, and detail our mitigations against such attacks in\na multi-layered defense implementation. Our findings indicate that prompt\ninjection is a recurring and systemic issue in LLM-based architectures, one\nthat will require dedicated work to address, much as the security community has\nhad to do with XSS in traditional web applications."
                },
                "authors": [
                    {
                        "name": "Víctor Mayoral-Vilches"
                    },
                    {
                        "name": "Per Mannermaa Rynning"
                    }
                ],
                "author_detail": {
                    "name": "Per Mannermaa Rynning"
                },
                "author": "Per Mannermaa Rynning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17625v3",
                "updated": "2025-08-29T14:31:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    31,
                    3,
                    4,
                    241,
                    0
                ],
                "published": "2024-04-26T15:19:58Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    15,
                    19,
                    58,
                    4,
                    117,
                    0
                ],
                "title": "Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of\n  the Land",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Alice's Adventures in a Differentiable Wonderland -- Volume I, A Tour of\n  the Land"
                },
                "summary": "Neural networks surround us, in the form of large language models, speech\ntranscription systems, molecular discovery algorithms, robotics, and much more.\nStripped of anything else, neural networks are compositions of differentiable\nprimitives, and studying them means learning how to program and how to interact\nwith these models, a particular example of what is called differentiable\nprogramming.\n  This primer is an introduction to this fascinating field imagined for\nsomeone, like Alice, who has just ventured into this strange differentiable\nwonderland. I overview the basics of optimizing a function via automatic\ndifferentiation, and a selection of the most common designs for handling\nsequences, graphs, texts, and audios. The focus is on a intuitive,\nself-contained introduction to the most important design techniques, including\nconvolutional, attentional, and recurrent blocks, hoping to bridge the gap\nbetween theory and code (PyTorch and JAX) and leaving the reader capable of\nunderstanding some of the most advanced models out there, such as large\nlanguage models (LLMs) and multimodal architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural networks surround us, in the form of large language models, speech\ntranscription systems, molecular discovery algorithms, robotics, and much more.\nStripped of anything else, neural networks are compositions of differentiable\nprimitives, and studying them means learning how to program and how to interact\nwith these models, a particular example of what is called differentiable\nprogramming.\n  This primer is an introduction to this fascinating field imagined for\nsomeone, like Alice, who has just ventured into this strange differentiable\nwonderland. I overview the basics of optimizing a function via automatic\ndifferentiation, and a selection of the most common designs for handling\nsequences, graphs, texts, and audios. The focus is on a intuitive,\nself-contained introduction to the most important design techniques, including\nconvolutional, attentional, and recurrent blocks, hoping to bridge the gap\nbetween theory and code (PyTorch and JAX) and leaving the reader capable of\nunderstanding some of the most advanced models out there, such as large\nlanguage models (LLMs) and multimodal architectures."
                },
                "authors": [
                    {
                        "name": "Simone Scardapane"
                    }
                ],
                "author_detail": {
                    "name": "Simone Scardapane"
                },
                "author": "Simone Scardapane",
                "arxiv_comment": "Companion website for additional chapters:\n  https://www.sscardapane.it/alice-book",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18124v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18124v3",
                "updated": "2025-08-29T14:28:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    28,
                    32,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-25T15:32:22Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    15,
                    32,
                    22,
                    0,
                    237,
                    0
                ],
                "title": "CMPhysBench: A Benchmark for Evaluating Large Language Models in\n  Condensed Matter Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CMPhysBench: A Benchmark for Evaluating Large Language Models in\n  Condensed Matter Physics"
                },
                "summary": "We introduce CMPhysBench, designed to assess the proficiency of Large\nLanguage Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.\nCMPhysBench is composed of more than 520 graduate-level meticulously curated\nquestions covering both representative subfields and foundational theoretical\nframeworks of condensed matter physics, such as magnetism, superconductivity,\nstrongly correlated systems, etc. To ensure a deep understanding of the\nproblem-solving process,we focus exclusively on calculation problems, requiring\nLLMs to independently generate comprehensive solutions. Meanwhile, leveraging\ntree-based representations of expressions, we introduce the Scalable Expression\nEdit Distance (SEED) score, which provides fine-grained (non-binary) partial\ncredit and yields a more accurate assessment of similarity between prediction\nand ground-truth. Our results show that even the best models, Grok-4, reach\nonly 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a\nsignificant capability gap, especially for this practical and frontier domain\nrelative to traditional physics. The code anddataset are publicly available at\nhttps://github.com/CMPhysBench/CMPhysBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce CMPhysBench, designed to assess the proficiency of Large\nLanguage Models (LLMs) in Condensed Matter Physics, as a novel Benchmark.\nCMPhysBench is composed of more than 520 graduate-level meticulously curated\nquestions covering both representative subfields and foundational theoretical\nframeworks of condensed matter physics, such as magnetism, superconductivity,\nstrongly correlated systems, etc. To ensure a deep understanding of the\nproblem-solving process,we focus exclusively on calculation problems, requiring\nLLMs to independently generate comprehensive solutions. Meanwhile, leveraging\ntree-based representations of expressions, we introduce the Scalable Expression\nEdit Distance (SEED) score, which provides fine-grained (non-binary) partial\ncredit and yields a more accurate assessment of similarity between prediction\nand ground-truth. Our results show that even the best models, Grok-4, reach\nonly 36 average SEED score and 28% accuracy on CMPhysBench, underscoring a\nsignificant capability gap, especially for this practical and frontier domain\nrelative to traditional physics. The code anddataset are publicly available at\nhttps://github.com/CMPhysBench/CMPhysBench."
                },
                "authors": [
                    {
                        "name": "Weida Wang"
                    },
                    {
                        "name": "Dongchen Huang"
                    },
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Tengchao Yang"
                    },
                    {
                        "name": "Ziyang Zheng"
                    },
                    {
                        "name": "Di Zhang"
                    },
                    {
                        "name": "Dong Han"
                    },
                    {
                        "name": "Benteng Chen"
                    },
                    {
                        "name": "Binzhao Luo"
                    },
                    {
                        "name": "Zhiyu Liu"
                    },
                    {
                        "name": "Kunling Liu"
                    },
                    {
                        "name": "Zhiyuan Gao"
                    },
                    {
                        "name": "Shiqi Geng"
                    },
                    {
                        "name": "Wei Ma"
                    },
                    {
                        "name": "Jiaming Su"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Shuchen Pu"
                    },
                    {
                        "name": "Yuhan Shui"
                    },
                    {
                        "name": "Qianjia Cheng"
                    },
                    {
                        "name": "Zhihao Dou"
                    },
                    {
                        "name": "Dongfei Cui"
                    },
                    {
                        "name": "Changyong He"
                    },
                    {
                        "name": "Jin Zeng"
                    },
                    {
                        "name": "Zeke Xie"
                    },
                    {
                        "name": "Mao Su"
                    },
                    {
                        "name": "Dongzhan Zhou"
                    },
                    {
                        "name": "Yuqiang Li"
                    },
                    {
                        "name": "Wanli Ouyang"
                    },
                    {
                        "name": "Yunqi Cai"
                    },
                    {
                        "name": "Xi Dai"
                    },
                    {
                        "name": "Shufei Zhang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Jinguang Cheng"
                    },
                    {
                        "name": "Zhong Fang"
                    },
                    {
                        "name": "Hongming Weng"
                    }
                ],
                "author_detail": {
                    "name": "Hongming Weng"
                },
                "author": "Hongming Weng",
                "arxiv_comment": "29 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18124v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18124v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07210v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07210v2",
                "updated": "2025-08-29T14:23:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    23,
                    2,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-10T07:10:59Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    7,
                    10,
                    59,
                    6,
                    222,
                    0
                ],
                "title": "Uncertainty-Aware Semantic Decoding for LLM-Based Sequential\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware Semantic Decoding for LLM-Based Sequential\n  Recommendation"
                },
                "summary": "Large language models have been widely applied to sequential recommendation\ntasks, yet during inference, they continue to rely on decoding strategies\ndeveloped for natural language processing. This creates a mismatch between\ntext-generation objectives and recommendation next item selection objectives.\nThis paper addresses this limitation by proposing an Uncertainty-aware Semantic\nDecoding (USD) framework that combines logit-based clustering with adaptive\nscoring to improve next-item predictions. Our approach clusters items with\nsimilar logit vectors into semantic equivalence groups, then redistributes\nprobability mass within these clusters and computes entropy across them to\ncontrol item scoring and sampling temperature during recommendation inference.\nExperiments on Amazon Product datasets (six domains) gains of 18.5\\% in HR@3,\n11.9\\% in NDCG@3, and 10.8\\% in MRR@3 compared to state-of-the-art baselines.\nHyperparameter analysis confirms the optimal parameters among various settings,\nand experiments on H\\&M, and Netflix datasets indicate that the framework can\nadapt to differing recommendation domains. The experimental results confirm\nthat integrating semantic clustering and uncertainty assessment yields more\nreliable and accurate recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been widely applied to sequential recommendation\ntasks, yet during inference, they continue to rely on decoding strategies\ndeveloped for natural language processing. This creates a mismatch between\ntext-generation objectives and recommendation next item selection objectives.\nThis paper addresses this limitation by proposing an Uncertainty-aware Semantic\nDecoding (USD) framework that combines logit-based clustering with adaptive\nscoring to improve next-item predictions. Our approach clusters items with\nsimilar logit vectors into semantic equivalence groups, then redistributes\nprobability mass within these clusters and computes entropy across them to\ncontrol item scoring and sampling temperature during recommendation inference.\nExperiments on Amazon Product datasets (six domains) gains of 18.5\\% in HR@3,\n11.9\\% in NDCG@3, and 10.8\\% in MRR@3 compared to state-of-the-art baselines.\nHyperparameter analysis confirms the optimal parameters among various settings,\nand experiments on H\\&M, and Netflix datasets indicate that the framework can\nadapt to differing recommendation domains. The experimental results confirm\nthat integrating semantic clustering and uncertainty assessment yields more\nreliable and accurate recommendations."
                },
                "authors": [
                    {
                        "name": "Chenke Yin"
                    },
                    {
                        "name": "Li Fan"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Dongxiao Hu"
                    },
                    {
                        "name": "Haichao Zhang"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yang Xiang"
                    }
                ],
                "author_detail": {
                    "name": "Yang Xiang"
                },
                "author": "Yang Xiang",
                "arxiv_comment": "Accepted by APWeb 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07210v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07210v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21640v1",
                "updated": "2025-08-29T14:03:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    3,
                    1,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T14:03:01Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    14,
                    3,
                    1,
                    4,
                    241,
                    0
                ],
                "title": "On the Deployment of Multiple Radio Stripes for Large-Scale Near-Field\n  RF Wireless Power Transfer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Deployment of Multiple Radio Stripes for Large-Scale Near-Field\n  RF Wireless Power Transfer"
                },
                "summary": "This paper investigates the deployment of radio stripe systems for indoor\nradio-frequency (RF) wireless power transfer (WPT) in line-of-sight near-field\nscenarios. The focus is on environments where energy demand is concentrated in\nspecific areas, referred to as 'hotspots', spatial zones with higher user\ndensity or consistent energy requirements. We formulate a joint clustering and\nradio stripe deployment problem that aims to maximize the minimum received\npower across all hotspots. To address the complexity, we decouple the problem\ninto two stages: i) clustering for assigning radio stripes to hotspots based on\ntheir spatial positions and near-field propagation characteristics, and ii)\nantenna element placement optimization. In particular, we propose four radio\nstripe deployment algorithms. Two are based on general successive convex\napproximation (SCA) and signomial programming (SGP) methods. The other two are\nshape-constrained solutions where antenna elements are arranged along either\nstraight lines or regular polygons, enabling simpler deployment. Numerical\nresults show that the proposed clustering method converges effectively, with\nChebyshev initialization significantly outperforming random initialization. The\noptimized deployments consistently outperform baseline benchmarks across a wide\nrange of frequencies and radio stripe lengths, while the polygon-shaped\ndeployment achieves better performance compared to other approaches. Meanwhile,\nthe line-shaped deployment demonstrates an advantage under high boresight gain\nsettings, benefiting from increased spatial diversity and broader angular\ncoverage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the deployment of radio stripe systems for indoor\nradio-frequency (RF) wireless power transfer (WPT) in line-of-sight near-field\nscenarios. The focus is on environments where energy demand is concentrated in\nspecific areas, referred to as 'hotspots', spatial zones with higher user\ndensity or consistent energy requirements. We formulate a joint clustering and\nradio stripe deployment problem that aims to maximize the minimum received\npower across all hotspots. To address the complexity, we decouple the problem\ninto two stages: i) clustering for assigning radio stripes to hotspots based on\ntheir spatial positions and near-field propagation characteristics, and ii)\nantenna element placement optimization. In particular, we propose four radio\nstripe deployment algorithms. Two are based on general successive convex\napproximation (SCA) and signomial programming (SGP) methods. The other two are\nshape-constrained solutions where antenna elements are arranged along either\nstraight lines or regular polygons, enabling simpler deployment. Numerical\nresults show that the proposed clustering method converges effectively, with\nChebyshev initialization significantly outperforming random initialization. The\noptimized deployments consistently outperform baseline benchmarks across a wide\nrange of frequencies and radio stripe lengths, while the polygon-shaped\ndeployment achieves better performance compared to other approaches. Meanwhile,\nthe line-shaped deployment demonstrates an advantage under high boresight gain\nsettings, benefiting from increased spatial diversity and broader angular\ncoverage."
                },
                "authors": [
                    {
                        "name": "Amirhossein Azarbahram"
                    },
                    {
                        "name": "Onel L. A. López"
                    },
                    {
                        "name": "Petar Popovski"
                    },
                    {
                        "name": "Matti Latva-aho"
                    }
                ],
                "author_detail": {
                    "name": "Matti Latva-aho"
                },
                "author": "Matti Latva-aho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21634v1",
                "updated": "2025-08-29T13:51:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    51,
                    28,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T13:51:28Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    51,
                    28,
                    4,
                    241,
                    0
                ],
                "title": "Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects,\n  Vulnerabilities, and Complexity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Written vs. AI-Generated Code: A Large-Scale Study of Defects,\n  Vulnerabilities, and Complexity"
                },
                "summary": "As AI code assistants become increasingly integrated into software\ndevelopment workflows, understanding how their code compares to human-written\nprograms is critical for ensuring reliability, maintainability, and security.\nIn this paper, we present a large-scale comparison of code authored by human\ndevelopers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and\nQwen-Coder, on multiple dimensions of software quality: code defects, security\nvulnerabilities, and structural complexity. Our evaluation spans over 500k code\nsamples in two widely used languages, Python and Java, classifying defects via\nOrthogonal Defect Classification and security vulnerabilities using the Common\nWeakness Enumeration. We find that AI-generated code is generally simpler and\nmore repetitive, yet more prone to unused constructs and hardcoded debugging,\nwhile human-written code exhibits greater structural complexity and a higher\nconcentration of maintainability issues. Notably, AI-generated code also\ncontains more high-risk security vulnerabilities. These findings highlight the\ndistinct defect profiles of AI- and human-authored code and underscore the need\nfor specialized quality assurance practices in AI-assisted programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As AI code assistants become increasingly integrated into software\ndevelopment workflows, understanding how their code compares to human-written\nprograms is critical for ensuring reliability, maintainability, and security.\nIn this paper, we present a large-scale comparison of code authored by human\ndevelopers and three state-of-the-art LLMs, i.e., ChatGPT, DeepSeek-Coder, and\nQwen-Coder, on multiple dimensions of software quality: code defects, security\nvulnerabilities, and structural complexity. Our evaluation spans over 500k code\nsamples in two widely used languages, Python and Java, classifying defects via\nOrthogonal Defect Classification and security vulnerabilities using the Common\nWeakness Enumeration. We find that AI-generated code is generally simpler and\nmore repetitive, yet more prone to unused constructs and hardcoded debugging,\nwhile human-written code exhibits greater structural complexity and a higher\nconcentration of maintainability issues. Notably, AI-generated code also\ncontains more high-risk security vulnerabilities. These findings highlight the\ndistinct defect profiles of AI- and human-authored code and underscore the need\nfor specialized quality assurance practices in AI-assisted programming."
                },
                "authors": [
                    {
                        "name": "Domenico Cotroneo"
                    },
                    {
                        "name": "Cristina Improta"
                    },
                    {
                        "name": "Pietro Liguori"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Liguori"
                },
                "author": "Pietro Liguori",
                "arxiv_comment": "Accepted to the 36th IEEE International Symposium on Software\n  Reliability Engineering (ISSRE, 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21632v1",
                "updated": "2025-08-29T13:47:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    47,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T13:47:22Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    47,
                    22,
                    4,
                    241,
                    0
                ],
                "title": "QZhou-Embedding Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QZhou-Embedding Technical Report"
                },
                "summary": "We present QZhou-Embedding, a general-purpose contextual text embedding model\nwith exceptional text representation capabilities. Built upon the\nQwen2.5-7B-Instruct foundation model, we designed a unified multi-task\nframework comprising specialized data transformation and training strategies.\nThe data transformation scheme enables the incorporation of more diverse\ntextual training datasets, while the task-specific training strategies enhance\nmodel learning efficiency. We developed a data synthesis pipeline leveraging\nLLM API, incorporating techniques such as paraphrasing, augmentation, and hard\nnegative example generation to improve the semantic richness and sample\ndifficulty of the training set. Additionally, we employ a two-stage training\nstrategy, comprising initial retrieval-focused pretraining followed by\nfull-task fine-tuning, enabling the embedding model to extend its capabilities\nbased on robust retrieval performance. Our model achieves state-of-the-art\nresults on the MTEB and CMTEB benchmarks, ranking first on both leaderboards\n(August 27 2025), and simultaneously achieves state-of-the-art performance on\ntasks including reranking, clustering, etc. Our findings demonstrate that\nhigher-quality, more diverse data is crucial for advancing retrieval model\nperformance, and that leveraging LLMs generative capabilities can further\noptimize data quality for embedding model breakthroughs. Our model weights are\nreleased on HuggingFace under Apache 2.0 license. For reproducibility, we\nprovide evaluation code and instructions on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present QZhou-Embedding, a general-purpose contextual text embedding model\nwith exceptional text representation capabilities. Built upon the\nQwen2.5-7B-Instruct foundation model, we designed a unified multi-task\nframework comprising specialized data transformation and training strategies.\nThe data transformation scheme enables the incorporation of more diverse\ntextual training datasets, while the task-specific training strategies enhance\nmodel learning efficiency. We developed a data synthesis pipeline leveraging\nLLM API, incorporating techniques such as paraphrasing, augmentation, and hard\nnegative example generation to improve the semantic richness and sample\ndifficulty of the training set. Additionally, we employ a two-stage training\nstrategy, comprising initial retrieval-focused pretraining followed by\nfull-task fine-tuning, enabling the embedding model to extend its capabilities\nbased on robust retrieval performance. Our model achieves state-of-the-art\nresults on the MTEB and CMTEB benchmarks, ranking first on both leaderboards\n(August 27 2025), and simultaneously achieves state-of-the-art performance on\ntasks including reranking, clustering, etc. Our findings demonstrate that\nhigher-quality, more diverse data is crucial for advancing retrieval model\nperformance, and that leveraging LLMs generative capabilities can further\noptimize data quality for embedding model breakthroughs. Our model weights are\nreleased on HuggingFace under Apache 2.0 license. For reproducibility, we\nprovide evaluation code and instructions on GitHub."
                },
                "authors": [
                    {
                        "name": "Peng Yu"
                    },
                    {
                        "name": "En Xu"
                    },
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Haibiao Chen"
                    },
                    {
                        "name": "Yinfei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yinfei Xu"
                },
                "author": "Yinfei Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19563v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19563v2",
                "updated": "2025-08-29T13:46:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    46,
                    29,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-27T04:46:05Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    4,
                    46,
                    5,
                    2,
                    239,
                    0
                ],
                "title": "Robustness is Important: Limitations of LLMs for Data Fitting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustness is Important: Limitations of LLMs for Data Fitting"
                },
                "summary": "Large Language Models (LLMs) are being applied in a wide array of settings,\nwell beyond the typical language-oriented use cases. In particular, LLMs are\nincreasingly used as a plug-and-play method for fitting data and generating\npredictions. Prior work has shown that LLMs, via in-context learning or\nsupervised fine-tuning, can perform competitively with many tabular supervised\nlearning techniques in terms of predictive performance. However, we identify a\ncritical vulnerability of using LLMs for data fitting -- making changes to data\nrepresentation that are completely irrelevant to the underlying learning task\ncan drastically alter LLMs' predictions on the same data. For example, simply\nchanging variable names can sway the size of prediction error by as much as 82%\nin certain settings. Such prediction sensitivity with respect to\ntask-irrelevant variations manifests under both in-context learning and\nsupervised fine-tuning, for both close-weight and open-weight general-purpose\nLLMs. Moreover, by examining the attention scores of an open-weight LLM, we\ndiscover a non-uniform attention pattern: training examples and variable\nnames/values which happen to occupy certain positions in the prompt receive\nmore attention when output tokens are generated, even though different\npositions are expected to receive roughly the same attention. This partially\nexplains the sensitivity in the presence of task-irrelevant variations. We also\nconsider a state-of-the-art tabular foundation model (TabPFN) trained\nspecifically for data fitting. Despite being explicitly designed to achieve\nprediction robustness, TabPFN is still not immune to task-irrelevant\nvariations. Overall, despite LLMs' impressive predictive capabilities,\ncurrently they lack even the basic level of robustness to be used as a\nprincipled data-fitting tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are being applied in a wide array of settings,\nwell beyond the typical language-oriented use cases. In particular, LLMs are\nincreasingly used as a plug-and-play method for fitting data and generating\npredictions. Prior work has shown that LLMs, via in-context learning or\nsupervised fine-tuning, can perform competitively with many tabular supervised\nlearning techniques in terms of predictive performance. However, we identify a\ncritical vulnerability of using LLMs for data fitting -- making changes to data\nrepresentation that are completely irrelevant to the underlying learning task\ncan drastically alter LLMs' predictions on the same data. For example, simply\nchanging variable names can sway the size of prediction error by as much as 82%\nin certain settings. Such prediction sensitivity with respect to\ntask-irrelevant variations manifests under both in-context learning and\nsupervised fine-tuning, for both close-weight and open-weight general-purpose\nLLMs. Moreover, by examining the attention scores of an open-weight LLM, we\ndiscover a non-uniform attention pattern: training examples and variable\nnames/values which happen to occupy certain positions in the prompt receive\nmore attention when output tokens are generated, even though different\npositions are expected to receive roughly the same attention. This partially\nexplains the sensitivity in the presence of task-irrelevant variations. We also\nconsider a state-of-the-art tabular foundation model (TabPFN) trained\nspecifically for data fitting. Despite being explicitly designed to achieve\nprediction robustness, TabPFN is still not immune to task-irrelevant\nvariations. Overall, despite LLMs' impressive predictive capabilities,\ncurrently they lack even the basic level of robustness to be used as a\nprincipled data-fitting tool."
                },
                "authors": [
                    {
                        "name": "Hejia Liu"
                    },
                    {
                        "name": "Mochen Yang"
                    },
                    {
                        "name": "Gediminas Adomavicius"
                    }
                ],
                "author_detail": {
                    "name": "Gediminas Adomavicius"
                },
                "author": "Gediminas Adomavicius",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19563v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19563v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21628v1",
                "updated": "2025-08-29T13:42:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    42,
                    26,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T13:42:26Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    42,
                    26,
                    4,
                    241,
                    0
                ],
                "title": "Personality Matters: User Traits Predict LLM Preferences in Multi-Turn\n  Collaborative Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personality Matters: User Traits Predict LLM Preferences in Multi-Turn\n  Collaborative Tasks"
                },
                "summary": "As Large Language Models (LLMs) increasingly integrate into everyday\nworkflows, where users shape outcomes through multi-turn collaboration, a\ncritical question emerges: do users with different personality traits\nsystematically prefer certain LLMs over others? We conducted a study with 32\nparticipants evenly distributed across four Keirsey personality types,\nevaluating their interactions with GPT-4 and Claude 3.5 across four\ncollaborative tasks: data analysis, creative writing, information retrieval,\nand writing assistance. Results revealed significant personality-driven\npreferences: Rationals strongly preferred GPT-4, particularly for goal-oriented\ntasks, while idealists favored Claude 3.5, especially for creative and\nanalytical tasks. Other personality types showed task-dependent preferences.\nSentiment analysis of qualitative feedback confirmed these patterns. Notably,\naggregate helpfulness ratings were similar across models, showing how\npersonality-based analysis reveals LLM differences that traditional evaluations\nmiss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) increasingly integrate into everyday\nworkflows, where users shape outcomes through multi-turn collaboration, a\ncritical question emerges: do users with different personality traits\nsystematically prefer certain LLMs over others? We conducted a study with 32\nparticipants evenly distributed across four Keirsey personality types,\nevaluating their interactions with GPT-4 and Claude 3.5 across four\ncollaborative tasks: data analysis, creative writing, information retrieval,\nand writing assistance. Results revealed significant personality-driven\npreferences: Rationals strongly preferred GPT-4, particularly for goal-oriented\ntasks, while idealists favored Claude 3.5, especially for creative and\nanalytical tasks. Other personality types showed task-dependent preferences.\nSentiment analysis of qualitative feedback confirmed these patterns. Notably,\naggregate helpfulness ratings were similar across models, showing how\npersonality-based analysis reveals LLM differences that traditional evaluations\nmiss."
                },
                "authors": [
                    {
                        "name": "Sarfaroz Yunusov"
                    },
                    {
                        "name": "Kaige Chen"
                    },
                    {
                        "name": "Kazi Nishat Anwar"
                    },
                    {
                        "name": "Ali Emami"
                    }
                ],
                "author_detail": {
                    "name": "Ali Emami"
                },
                "author": "Ali Emami",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21622v1",
                "updated": "2025-08-29T13:34:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    34,
                    55,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T13:34:55Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    34,
                    55,
                    4,
                    241,
                    0
                ],
                "title": "Integrating Large Language Models with Network Optimization for\n  Interactive and Explainable Supply Chain Planning: A Real-World Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models with Network Optimization for\n  Interactive and Explainable Supply Chain Planning: A Real-World Case Study"
                },
                "summary": "This paper presents an integrated framework that combines traditional network\noptimization models with large language models (LLMs) to deliver interactive,\nexplainable, and role-aware decision support for supply chain planning. The\nproposed system bridges the gap between complex operations research outputs and\nbusiness stakeholder understanding by generating natural language summaries,\ncontextual visualizations, and tailored key performance indicators (KPIs). The\ncore optimization model addresses tactical inventory redistribution across a\nnetwork of distribution centers for multi-period and multi-item, using a\nmixed-integer formulation. The technical architecture incorporates AI agents,\nRESTful APIs, and a dynamic user interface to support real-time interaction,\nconfiguration updates, and simulation-based insights. A case study demonstrates\nhow the system improves planning outcomes by preventing stockouts, reducing\ncosts, and maintaining service levels. Future extensions include integrating\nprivate LLMs, transfer learning, reinforcement learning, and Bayesian neural\nnetworks to enhance explainability, adaptability, and real-time\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an integrated framework that combines traditional network\noptimization models with large language models (LLMs) to deliver interactive,\nexplainable, and role-aware decision support for supply chain planning. The\nproposed system bridges the gap between complex operations research outputs and\nbusiness stakeholder understanding by generating natural language summaries,\ncontextual visualizations, and tailored key performance indicators (KPIs). The\ncore optimization model addresses tactical inventory redistribution across a\nnetwork of distribution centers for multi-period and multi-item, using a\nmixed-integer formulation. The technical architecture incorporates AI agents,\nRESTful APIs, and a dynamic user interface to support real-time interaction,\nconfiguration updates, and simulation-based insights. A case study demonstrates\nhow the system improves planning outcomes by preventing stockouts, reducing\ncosts, and maintaining service levels. Future extensions include integrating\nprivate LLMs, transfer learning, reinforcement learning, and Bayesian neural\nnetworks to enhance explainability, adaptability, and real-time\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Saravanan Venkatachalam"
                    }
                ],
                "author_detail": {
                    "name": "Saravanan Venkatachalam"
                },
                "author": "Saravanan Venkatachalam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17464v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17464v3",
                "updated": "2025-08-29T13:34:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    34,
                    45,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-23T04:45:37Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    4,
                    45,
                    37,
                    4,
                    143,
                    0
                ],
                "title": "Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydra: Structured Cross-Source Enhanced Large Language Model Reasoning"
                },
                "summary": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. Current hybrid RAG system retrieves evidence\nfrom both knowledge graphs (KGs) and text documents to support LLM reasoning.\nHowever, it faces challenges like handling multi-hop reasoning, multi-entity\nquestions, multi-source verification, and effective graph utilization. To\naddress these limitations, we present Hydra, a training-free framework that\nunifies graph topology, document semantics, and source reliability to support\ndeep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity\nproblems through agent-driven exploration that combines structured and\nunstructured retrieval, increasing both diversity and precision of evidence. To\ntackle multi-source verification, Hydra uses a tri-factor cross-source\nverification (source trustworthiness assessment, cross-source corroboration,\nand entity-path alignment), to balance topic relevance with cross-modal\nagreement. By leveraging graph structure, Hydra fuses heterogeneous sources,\nguides efficient exploration, and prunes noise early. Comprehensive experiments\non seven benchmark datasets show that Hydra achieves overall state-of-the-art\nresults on all benchmarks with GPT-3.5, outperforming the strong hybrid\nbaseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra\nenables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance\ncomparable to that of GPT-4-Turbo. The source code is available on\nhttps://stevetantan.github.io/Hydra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nincorporating external knowledge. Current hybrid RAG system retrieves evidence\nfrom both knowledge graphs (KGs) and text documents to support LLM reasoning.\nHowever, it faces challenges like handling multi-hop reasoning, multi-entity\nquestions, multi-source verification, and effective graph utilization. To\naddress these limitations, we present Hydra, a training-free framework that\nunifies graph topology, document semantics, and source reliability to support\ndeep, faithful reasoning in LLMs. Hydra handles multi-hop and multi-entity\nproblems through agent-driven exploration that combines structured and\nunstructured retrieval, increasing both diversity and precision of evidence. To\ntackle multi-source verification, Hydra uses a tri-factor cross-source\nverification (source trustworthiness assessment, cross-source corroboration,\nand entity-path alignment), to balance topic relevance with cross-modal\nagreement. By leveraging graph structure, Hydra fuses heterogeneous sources,\nguides efficient exploration, and prunes noise early. Comprehensive experiments\non seven benchmark datasets show that Hydra achieves overall state-of-the-art\nresults on all benchmarks with GPT-3.5, outperforming the strong hybrid\nbaseline ToG-2 by an average of 20.3% and up to 30.1%. Furthermore, Hydra\nenables smaller models (e.g., Llama-3.1-8B) to achieve reasoning performance\ncomparable to that of GPT-4-Turbo. The source code is available on\nhttps://stevetantan.github.io/Hydra/."
                },
                "authors": [
                    {
                        "name": "Xingyu Tan"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Qing Liu"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Liming Zhu"
                    },
                    {
                        "name": "Wenjie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Zhang"
                },
                "author": "Wenjie Zhang",
                "arxiv_comment": "Accepted by EMNLP2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17464v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17464v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21606v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21606v1",
                "updated": "2025-08-29T13:13:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    13,
                    43,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T13:13:43Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    13,
                    13,
                    43,
                    4,
                    241,
                    0
                ],
                "title": "Hybrid Cryptographic Monitoring System for Side-Channel Attack Detection\n  on PYNQ SoCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Cryptographic Monitoring System for Side-Channel Attack Detection\n  on PYNQ SoCs"
                },
                "summary": "AES-128 encryption is theoretically secure but vulnerable in practical\ndeployments due to timing and fault injection attacks on embedded systems. This\nwork presents a lightweight dual-detection framework combining statistical\nthresholding and machine learning (ML) for real-time anomaly detection. By\nsimulating anomalies via delays and ciphertext corruption, we collect timing\nand data features to evaluate two strategies: (1) a statistical threshold\nmethod based on execution time and (2) a Random Forest classifier trained on\nblock-level anomalies. Implemented on CPU and FPGA (PYNQ-Z1), our results show\nthat the ML approach outperforms static thresholds in accuracy, while\nmaintaining real-time feasibility on embedded platforms. The framework operates\nwithout modifying AES internals or relying on hardware performance counters.\nThis makes it especially suitable for low-power, resource-constrained systems\nwhere detection accuracy and computational efficiency must be balanced.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AES-128 encryption is theoretically secure but vulnerable in practical\ndeployments due to timing and fault injection attacks on embedded systems. This\nwork presents a lightweight dual-detection framework combining statistical\nthresholding and machine learning (ML) for real-time anomaly detection. By\nsimulating anomalies via delays and ciphertext corruption, we collect timing\nand data features to evaluate two strategies: (1) a statistical threshold\nmethod based on execution time and (2) a Random Forest classifier trained on\nblock-level anomalies. Implemented on CPU and FPGA (PYNQ-Z1), our results show\nthat the ML approach outperforms static thresholds in accuracy, while\nmaintaining real-time feasibility on embedded platforms. The framework operates\nwithout modifying AES internals or relying on hardware performance counters.\nThis makes it especially suitable for low-power, resource-constrained systems\nwhere detection accuracy and computational efficiency must be balanced."
                },
                "authors": [
                    {
                        "name": "Nishant Chinnasami"
                    },
                    {
                        "name": "Rasha Karakchi"
                    }
                ],
                "author_detail": {
                    "name": "Rasha Karakchi"
                },
                "author": "Rasha Karakchi",
                "arxiv_comment": "This paper is submitted at Supercomputing (SC'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21606v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21606v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12321v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12321v2",
                "updated": "2025-08-29T12:47:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    47,
                    49,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-14T03:02:42Z",
                "published_parsed": [
                    2025,
                    6,
                    14,
                    3,
                    2,
                    42,
                    5,
                    165,
                    0
                ],
                "title": "Beyond Frequency: The Role of Redundancy in Large Language Model\n  Memorization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Frequency: The Role of Redundancy in Large Language Model\n  Memorization"
                },
                "summary": "Memorization in large language models poses critical risks for privacy and\nfairness as these systems scale to billions of parameters. While previous\nstudies established correlations between memorization and factors like token\nfrequency and repetition patterns, we revealed distinct response patterns:\nfrequency increases minimally impact memorized samples (e.g. 0.09) while\nsubstantially affecting non-memorized samples (e.g., 0.25), with consistency\nobserved across model scales. Through counterfactual analysis by perturbing\nsample prefixes and quantifying perturbation strength through token positional\nchanges, we demonstrate that redundancy correlates with memorization patterns.\nOur findings establish that: about 79% of memorized samples are low-redundancy,\nthese low-redundancy samples exhibit 2-fold higher vulnerability than\nhigh-redundancy ones, and consequently memorized samples drop by 0.6 under\nperturbation while non-memorized samples drop by only 0.01, indicating that\nmore redundant content becomes both more memorable and more fragile. These\nfindings suggest potential redundancy-guided approaches for data preprocessing,\nthereby reducing privacy risks and mitigating bias to ensure fairness in model\ndeployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memorization in large language models poses critical risks for privacy and\nfairness as these systems scale to billions of parameters. While previous\nstudies established correlations between memorization and factors like token\nfrequency and repetition patterns, we revealed distinct response patterns:\nfrequency increases minimally impact memorized samples (e.g. 0.09) while\nsubstantially affecting non-memorized samples (e.g., 0.25), with consistency\nobserved across model scales. Through counterfactual analysis by perturbing\nsample prefixes and quantifying perturbation strength through token positional\nchanges, we demonstrate that redundancy correlates with memorization patterns.\nOur findings establish that: about 79% of memorized samples are low-redundancy,\nthese low-redundancy samples exhibit 2-fold higher vulnerability than\nhigh-redundancy ones, and consequently memorized samples drop by 0.6 under\nperturbation while non-memorized samples drop by only 0.01, indicating that\nmore redundant content becomes both more memorable and more fragile. These\nfindings suggest potential redundancy-guided approaches for data preprocessing,\nthereby reducing privacy risks and mitigating bias to ensure fairness in model\ndeployments."
                },
                "authors": [
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Qinghua Zhao"
                    },
                    {
                        "name": "Chi-ho Lin"
                    },
                    {
                        "name": "Zhongfeng Kang"
                    },
                    {
                        "name": "Lei Li"
                    }
                ],
                "author_detail": {
                    "name": "Lei Li"
                },
                "author": "Lei Li",
                "arxiv_comment": "8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12321v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12321v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21589v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21589v1",
                "updated": "2025-08-29T12:47:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    47,
                    27,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T12:47:27Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    47,
                    27,
                    4,
                    241,
                    0
                ],
                "title": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM\n  Fine-Tuning via Closed-Loop Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM\n  Fine-Tuning via Closed-Loop Learning"
                },
                "summary": "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our \\method consistently enhances the quality of\nseed data and boosts LLM's performance with improving accuracy by 7.15% on\naverage while maintaining the original dataset scale. This work establishes a\nnew paradigm for sustainable LLM training through dynamic human-AI co-evolution\nof data and models. Our datasets, models, and code are coming soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Supervised Fine-Tuning (SFT) Large Language Models (LLM) fundamentally rely\non high-quality training data. While data selection and data synthesis are two\ncommon strategies to improve data quality, existing approaches often face\nlimitations in static dataset curation that fail to adapt to evolving model\ncapabilities. In this paper, we introduce Middo, a self-evolving Model-informed\ndynamic data optimization framework that uses model-aware data selection and\ncontext-preserving data refinement. Unlike conventional one-off\nfiltering/synthesis methods, our framework establishes a closed-loop\noptimization system: (1) A self-referential diagnostic module proactively\nidentifies suboptimal samples through tri-axial model signals - loss patterns\n(complexity), embedding cluster dynamics (diversity), and self-alignment scores\n(quality); (2) An adaptive optimization engine then transforms suboptimal\nsamples into pedagogically valuable training points while preserving semantic\nintegrity; (3) This optimization process continuously evolves with model\ncapability through dynamic learning principles. Experiments on multiple\nbenchmarks demonstrate that our \\method consistently enhances the quality of\nseed data and boosts LLM's performance with improving accuracy by 7.15% on\naverage while maintaining the original dataset scale. This work establishes a\nnew paradigm for sustainable LLM training through dynamic human-AI co-evolution\nof data and models. Our datasets, models, and code are coming soon."
                },
                "authors": [
                    {
                        "name": "Zinan Tang"
                    },
                    {
                        "name": "Xin Gao"
                    },
                    {
                        "name": "Qizhi Pei"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Mengzhang Cai"
                    },
                    {
                        "name": "Jiang Wu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Lijun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Wu"
                },
                "author": "Lijun Wu",
                "arxiv_comment": "Accepted by EMNLP 2025 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21589v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21589v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21587v1",
                "updated": "2025-08-29T12:43:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    43,
                    6,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T12:43:06Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    43,
                    6,
                    4,
                    241,
                    0
                ],
                "title": "A Survey on Current Trends and Recent Advances in Text Anonymization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Current Trends and Recent Advances in Text Anonymization"
                },
                "summary": "The proliferation of textual data containing sensitive personal information\nacross various domains requires robust anonymization techniques to protect\nprivacy and comply with regulations, while preserving data usability for\ndiverse and crucial downstream tasks. This survey provides a comprehensive\noverview of current trends and recent advances in text anonymization\ntechniques. We begin by discussing foundational approaches, primarily centered\non Named Entity Recognition, before examining the transformative impact of\nLarge Language Models, detailing their dual role as sophisticated anonymizers\nand potent de-anonymization threats. The survey further explores\ndomain-specific challenges and tailored solutions in critical sectors such as\nhealthcare, law, finance, and education. We investigate advanced methodologies\nincorporating formal privacy models and risk-aware frameworks, and address the\nspecialized subfield of authorship anonymization. Additionally, we review\nevaluation frameworks, comprehensive metrics, benchmarks, and practical\ntoolkits for real-world deployment of anonymization solutions. This review\nconsolidates current knowledge, identifies emerging trends and persistent\nchallenges, including the evolving privacy-utility trade-off, the need to\naddress quasi-identifiers, and the implications of LLM capabilities, and aims\nto guide future research directions for both academics and practitioners in\nthis field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The proliferation of textual data containing sensitive personal information\nacross various domains requires robust anonymization techniques to protect\nprivacy and comply with regulations, while preserving data usability for\ndiverse and crucial downstream tasks. This survey provides a comprehensive\noverview of current trends and recent advances in text anonymization\ntechniques. We begin by discussing foundational approaches, primarily centered\non Named Entity Recognition, before examining the transformative impact of\nLarge Language Models, detailing their dual role as sophisticated anonymizers\nand potent de-anonymization threats. The survey further explores\ndomain-specific challenges and tailored solutions in critical sectors such as\nhealthcare, law, finance, and education. We investigate advanced methodologies\nincorporating formal privacy models and risk-aware frameworks, and address the\nspecialized subfield of authorship anonymization. Additionally, we review\nevaluation frameworks, comprehensive metrics, benchmarks, and practical\ntoolkits for real-world deployment of anonymization solutions. This review\nconsolidates current knowledge, identifies emerging trends and persistent\nchallenges, including the evolving privacy-utility trade-off, the need to\naddress quasi-identifiers, and the implications of LLM capabilities, and aims\nto guide future research directions for both academics and practitioners in\nthis field."
                },
                "authors": [
                    {
                        "name": "Tobias Deußer"
                    },
                    {
                        "name": "Lorenz Sparrenberg"
                    },
                    {
                        "name": "Armin Berger"
                    },
                    {
                        "name": "Max Hahnbück"
                    },
                    {
                        "name": "Christian Bauckhage"
                    },
                    {
                        "name": "Rafet Sifa"
                    }
                ],
                "author_detail": {
                    "name": "Rafet Sifa"
                },
                "author": "Rafet Sifa",
                "arxiv_comment": "Accepted at IEEE DSAA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21565v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21565v1",
                "updated": "2025-08-29T12:21:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    21,
                    57,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T12:21:57Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    21,
                    57,
                    4,
                    241,
                    0
                ],
                "title": "How Well Do Vision--Language Models Understand Cities? A Comparative\n  Study on Spatial Reasoning from Street-View Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Well Do Vision--Language Models Understand Cities? A Comparative\n  Study on Spatial Reasoning from Street-View Images"
                },
                "summary": "Effectively understanding urban scenes requires fine-grained spatial\nreasoning about objects, layouts, and depth cues. However, how well current\nvision-language models (VLMs), pretrained on general scenes, transfer these\nabilities to urban domain remains underexplored. To address this gap, we\nconduct a comparative study of three off-the-shelf VLMs-BLIP-2, InstructBLIP,\nand LLaVA-1.5-evaluating both zero-shot performance and the effects of\nfine-tuning with a synthetic VQA dataset specific to urban scenes. We construct\nsuch dataset from segmentation, depth, and object detection predictions of\nstreet-view images, pairing each question with LLM-generated Chain-of-Thought\n(CoT) answers for step-by-step reasoning supervision. Results show that while\nVLMs perform reasonably well in zero-shot settings, fine-tuning with our\nsynthetic CoT-supervised dataset substantially boosts performance, especially\nfor challenging question types such as negation and counterfactuals. This study\nintroduces urban spatial reasoning as a new challenge for VLMs and demonstrates\nsynthetic dataset construction as a practical path for adapting general-purpose\nmodels to specialized domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effectively understanding urban scenes requires fine-grained spatial\nreasoning about objects, layouts, and depth cues. However, how well current\nvision-language models (VLMs), pretrained on general scenes, transfer these\nabilities to urban domain remains underexplored. To address this gap, we\nconduct a comparative study of three off-the-shelf VLMs-BLIP-2, InstructBLIP,\nand LLaVA-1.5-evaluating both zero-shot performance and the effects of\nfine-tuning with a synthetic VQA dataset specific to urban scenes. We construct\nsuch dataset from segmentation, depth, and object detection predictions of\nstreet-view images, pairing each question with LLM-generated Chain-of-Thought\n(CoT) answers for step-by-step reasoning supervision. Results show that while\nVLMs perform reasonably well in zero-shot settings, fine-tuning with our\nsynthetic CoT-supervised dataset substantially boosts performance, especially\nfor challenging question types such as negation and counterfactuals. This study\nintroduces urban spatial reasoning as a new challenge for VLMs and demonstrates\nsynthetic dataset construction as a practical path for adapting general-purpose\nmodels to specialized domains."
                },
                "authors": [
                    {
                        "name": "Juneyoung Ro"
                    },
                    {
                        "name": "Namwoo Kim"
                    },
                    {
                        "name": "Yoonjin Yoon"
                    }
                ],
                "author_detail": {
                    "name": "Yoonjin Yoon"
                },
                "author": "Yoonjin Yoon",
                "arxiv_comment": "Accepted to ICCV Workshop 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21565v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21565v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21561v1",
                "updated": "2025-08-29T12:16:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    16,
                    24,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T12:16:24Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    16,
                    24,
                    4,
                    241,
                    0
                ],
                "title": "Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers\n  LLMs for Few-shot Tabular Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summarize-Exemplify-Reflect: Data-driven Insight Distillation Empowers\n  LLMs for Few-shot Tabular Classification"
                },
                "summary": "Recent studies show the promise of large language models (LLMs) for few-shot\ntabular classification but highlight challenges due to the variability in\nstructured data. To address this, we propose distilling data into actionable\ninsights to enable robust and effective classification by LLMs. Drawing\ninspiration from human learning processes, we introduce InsightTab, an insight\ndistillation framework guided by principles of divide-and-conquer, easy-first,\nand reflective learning. Our approach integrates rule summarization, strategic\nexemplification, and insight reflection through deep collaboration between LLMs\nand data modeling techniques. The obtained insights enable LLMs to better align\ntheir general knowledge and capabilities with the particular requirements of\nspecific tabular tasks. We extensively evaluate InsightTab on nine datasets.\nThe results demonstrate consistent improvement over state-of-the-art methods.\nAblation studies further validate the principle-guided distillation process,\nwhile analyses emphasize InsightTab's effectiveness in leveraging labeled data\nand managing bias.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies show the promise of large language models (LLMs) for few-shot\ntabular classification but highlight challenges due to the variability in\nstructured data. To address this, we propose distilling data into actionable\ninsights to enable robust and effective classification by LLMs. Drawing\ninspiration from human learning processes, we introduce InsightTab, an insight\ndistillation framework guided by principles of divide-and-conquer, easy-first,\nand reflective learning. Our approach integrates rule summarization, strategic\nexemplification, and insight reflection through deep collaboration between LLMs\nand data modeling techniques. The obtained insights enable LLMs to better align\ntheir general knowledge and capabilities with the particular requirements of\nspecific tabular tasks. We extensively evaluate InsightTab on nine datasets.\nThe results demonstrate consistent improvement over state-of-the-art methods.\nAblation studies further validate the principle-guided distillation process,\nwhile analyses emphasize InsightTab's effectiveness in leveraging labeled data\nand managing bias."
                },
                "authors": [
                    {
                        "name": "Yifei Yuan"
                    },
                    {
                        "name": "Jiatong Li"
                    },
                    {
                        "name": "Weijia Zhang"
                    },
                    {
                        "name": "Mohammad Aliannejadi"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    },
                    {
                        "name": "Renjun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Renjun Hu"
                },
                "author": "Renjun Hu",
                "arxiv_comment": "EMNLP 25 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21553v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21553v1",
                "updated": "2025-08-29T12:10:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    10,
                    5,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T12:10:05Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    10,
                    5,
                    4,
                    241,
                    0
                ],
                "title": "Reusable Test Suites for Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reusable Test Suites for Reinforcement Learning"
                },
                "summary": "Reinforcement learning (RL) agents show great promise in solving sequential\ndecision-making tasks. However, validating the reliability and performance of\nthe agent policies' behavior for deployment remains challenging. Most\nreinforcement learning policy testing methods produce test suites tailored to\nthe agent policy being tested, and their relevance to other policies is\nunclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel\nautomated test suite selection method for RL environments, designed to extract\ntest cases generated by any policy testing framework based on their\nsolvability, diversity, and general difficulty. MPTCS uses a set of policies to\nselect a diverse collection of reusable policy-agnostic test cases that reveal\ntypical flaws in the agents' behavior. The set of policies selects test cases\nfrom a candidate pool, which can be generated by any policy testing method,\nbased on a difficulty score. We assess the effectiveness of the difficulty\nscore and how the method's effectiveness and cost depend on the number of\npolicies in the set. Additionally, a method for promoting diversity in the test\nsuite, a discretized general test case descriptor surface inspired by\nquality-diversity algorithms, is examined to determine how it covers the state\nspace and which policies it triggers to produce faulty behaviors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) agents show great promise in solving sequential\ndecision-making tasks. However, validating the reliability and performance of\nthe agent policies' behavior for deployment remains challenging. Most\nreinforcement learning policy testing methods produce test suites tailored to\nthe agent policy being tested, and their relevance to other policies is\nunclear. This work presents Multi-Policy Test Case Selection (MPTCS), a novel\nautomated test suite selection method for RL environments, designed to extract\ntest cases generated by any policy testing framework based on their\nsolvability, diversity, and general difficulty. MPTCS uses a set of policies to\nselect a diverse collection of reusable policy-agnostic test cases that reveal\ntypical flaws in the agents' behavior. The set of policies selects test cases\nfrom a candidate pool, which can be generated by any policy testing method,\nbased on a difficulty score. We assess the effectiveness of the difficulty\nscore and how the method's effectiveness and cost depend on the number of\npolicies in the set. Additionally, a method for promoting diversity in the test\nsuite, a discretized general test case descriptor surface inspired by\nquality-diversity algorithms, is examined to determine how it covers the state\nspace and which policies it triggers to produce faulty behaviors."
                },
                "authors": [
                    {
                        "name": "Jørn Eirik Betten"
                    },
                    {
                        "name": "Quentin Mazouni"
                    },
                    {
                        "name": "Dennis Gross"
                    },
                    {
                        "name": "Pedro Lind"
                    },
                    {
                        "name": "Helge Spieker"
                    }
                ],
                "author_detail": {
                    "name": "Helge Spieker"
                },
                "author": "Helge Spieker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21553v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21553v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15689v2",
                "updated": "2025-08-29T12:03:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    12,
                    3,
                    45,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-26T14:22:21Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    14,
                    22,
                    21,
                    0,
                    146,
                    0
                ],
                "title": "BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BASE-Q: Bias and Asymmetric Scaling Enhanced Rotational Quantization for\n  Large Language Models"
                },
                "summary": "Rotations have become essential to state-of-the-art quantization pipelines\nfor large language models (LLMs) by effectively smoothing outliers in weights\nand activations. However, further optimizing the rotation parameters offers\nonly limited performance gains and introduces significant training overhead:\ndue to rotation parameter sharing, full-model must be loaded simultaneously to\nenable backpropagation, resulting in substantial memory consumption and limited\npractical utility. In this work, we identify two fundamental limitations of\ncurrent rotational quantization methods: (i) rotation fails to align channel\nmeans, resulting in wider quantization bounds and increased rounding errors;\nand (ii) rotation makes the activation distribution more Gaussian-like,\nincreasing energy loss caused by clipping errors. To address these issues, we\nintroduce \\textbf{BASE-Q}, a simple yet powerful approach that combines bias\ncorrection and asymmetric scaling to effectively reduce rounding and clipping\nerrors. Furthermore, BASE-Q enables blockwise optimization, eliminating the\nneed for memory-intensive full-model backpropagation. Extensive experiments on\nvarious LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing\nthe accuracy gap to full-precision models by 50.5\\%, 42.9\\%, and 29.2\\%\ncompared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be\nreleased soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rotations have become essential to state-of-the-art quantization pipelines\nfor large language models (LLMs) by effectively smoothing outliers in weights\nand activations. However, further optimizing the rotation parameters offers\nonly limited performance gains and introduces significant training overhead:\ndue to rotation parameter sharing, full-model must be loaded simultaneously to\nenable backpropagation, resulting in substantial memory consumption and limited\npractical utility. In this work, we identify two fundamental limitations of\ncurrent rotational quantization methods: (i) rotation fails to align channel\nmeans, resulting in wider quantization bounds and increased rounding errors;\nand (ii) rotation makes the activation distribution more Gaussian-like,\nincreasing energy loss caused by clipping errors. To address these issues, we\nintroduce \\textbf{BASE-Q}, a simple yet powerful approach that combines bias\ncorrection and asymmetric scaling to effectively reduce rounding and clipping\nerrors. Furthermore, BASE-Q enables blockwise optimization, eliminating the\nneed for memory-intensive full-model backpropagation. Extensive experiments on\nvarious LLMs and benchmarks demonstrate the effectiveness of BASE-Q, narrowing\nthe accuracy gap to full-precision models by 50.5\\%, 42.9\\%, and 29.2\\%\ncompared to QuaRot, SpinQuant, and OSTQuant, respectively. The code will be\nreleased soon."
                },
                "authors": [
                    {
                        "name": "Liulu He"
                    },
                    {
                        "name": "Shenli Zheng"
                    },
                    {
                        "name": "Karwei Sun"
                    },
                    {
                        "name": "Yijiang Liu"
                    },
                    {
                        "name": "Yufei Zhao"
                    },
                    {
                        "name": "Chongkang Tan"
                    },
                    {
                        "name": "Huanrui Yang"
                    },
                    {
                        "name": "Yuan Du"
                    },
                    {
                        "name": "Li Du"
                    }
                ],
                "author_detail": {
                    "name": "Li Du"
                },
                "author": "Li Du",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21540v1",
                "updated": "2025-08-29T11:53:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    53,
                    16,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T11:53:16Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    53,
                    16,
                    4,
                    241,
                    0
                ],
                "title": "HealthProcessAI: A Technical Framework and Proof-of-Concept for\n  LLM-Enhanced Healthcare Process Mining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HealthProcessAI: A Technical Framework and Proof-of-Concept for\n  LLM-Enhanced Healthcare Process Mining"
                },
                "summary": "Process mining has emerged as a powerful analytical technique for\nunderstanding complex healthcare workflows. However, its application faces\nsignificant barriers, including technical complexity, a lack of standardized\napproaches, and limited access to practical training resources. We introduce\nHealthProcessAI, a GenAI framework designed to simplify process mining\napplications in healthcare and epidemiology by providing a comprehensive\nwrapper around existing Python (PM4PY) and R (bupaR) libraries. To address\nunfamiliarity and improve accessibility, the framework integrates multiple\nLarge Language Models (LLMs) for automated process map interpretation and\nreport generation, helping translate technical analyses into outputs that\ndiverse users can readily understand. We validated the framework using sepsis\nprogression data as a proof-of-concept example and compared the outputs of five\nstate-of-the-art LLM models through the OpenRouter platform. To test its\nfunctionality, the framework successfully processed sepsis data across four\nproof-of-concept scenarios, demonstrating robust technical performance and its\ncapability to generate reports through automated LLM analysis. LLM evaluation\nusing five independent LLMs as automated evaluators revealed distinct model\nstrengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency\nscores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By\nintegrating multiple Large Language Models (LLMs) for automated interpretation\nand report generation, the framework addresses widespread unfamiliarity with\nprocess mining outputs, making them more accessible to clinicians, data\nscientists, and researchers. This structured analytics and AI-driven\ninterpretation combination represents a novel methodological advance in\ntranslating complex process mining results into potentially actionable insights\nfor healthcare applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Process mining has emerged as a powerful analytical technique for\nunderstanding complex healthcare workflows. However, its application faces\nsignificant barriers, including technical complexity, a lack of standardized\napproaches, and limited access to practical training resources. We introduce\nHealthProcessAI, a GenAI framework designed to simplify process mining\napplications in healthcare and epidemiology by providing a comprehensive\nwrapper around existing Python (PM4PY) and R (bupaR) libraries. To address\nunfamiliarity and improve accessibility, the framework integrates multiple\nLarge Language Models (LLMs) for automated process map interpretation and\nreport generation, helping translate technical analyses into outputs that\ndiverse users can readily understand. We validated the framework using sepsis\nprogression data as a proof-of-concept example and compared the outputs of five\nstate-of-the-art LLM models through the OpenRouter platform. To test its\nfunctionality, the framework successfully processed sepsis data across four\nproof-of-concept scenarios, demonstrating robust technical performance and its\ncapability to generate reports through automated LLM analysis. LLM evaluation\nusing five independent LLMs as automated evaluators revealed distinct model\nstrengths: Claude Sonnet-4 and Gemini 2.5-Pro achieved the highest consistency\nscores (3.79/4.0 and 3.65/4.0) when evaluated by automated LLM assessors. By\nintegrating multiple Large Language Models (LLMs) for automated interpretation\nand report generation, the framework addresses widespread unfamiliarity with\nprocess mining outputs, making them more accessible to clinicians, data\nscientists, and researchers. This structured analytics and AI-driven\ninterpretation combination represents a novel methodological advance in\ntranslating complex process mining results into potentially actionable insights\nfor healthcare applications."
                },
                "authors": [
                    {
                        "name": "Eduardo Illueca-Fernandez"
                    },
                    {
                        "name": "Kaile Chen"
                    },
                    {
                        "name": "Fernando Seoane"
                    },
                    {
                        "name": "Farhad Abtahi"
                    }
                ],
                "author_detail": {
                    "name": "Farhad Abtahi"
                },
                "author": "Farhad Abtahi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00863v2",
                "updated": "2025-08-29T11:51:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    51,
                    24,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-01T07:01:34Z",
                "published_parsed": [
                    2025,
                    6,
                    1,
                    7,
                    1,
                    34,
                    6,
                    152,
                    0
                ],
                "title": "L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with\n  Synthetic Annotations using CoTR prompting and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3Cube-MahaEmotions: A Marathi Emotion Recognition Dataset with\n  Synthetic Annotations using CoTR prompting and Large Language Models"
                },
                "summary": "Emotion recognition in low-resource languages like Marathi remains\nchallenging due to limited annotated data. We present L3Cube-MahaEmotions, a\nhigh-quality Marathi emotion recognition dataset with 11 fine-grained emotion\nlabels. The training data is synthetically annotated using large language\nmodels (LLMs), while the validation and test sets are manually labeled to serve\nas a reliable gold-standard benchmark. Building on the MahaSent dataset, we\napply the Chain-of-Translation (CoTR) prompting technique, where Marathi\nsentences are translated into English and emotion labeled via a single prompt.\nGPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data\nannotation due to superior label quality. We evaluate model performance using\nstandard metrics and explore label aggregation strategies (e.g., Union,\nIntersection). While GPT-4 predictions outperform fine-tuned BERT models,\nBERT-based models trained on synthetic labels fail to surpass GPT-4. This\nhighlights both the importance of high-quality human-labeled data and the\ninherent complexity of emotion recognition. An important finding of this work\nis that generic LLMs like GPT-4 and Llama3-405B generalize better than\nfine-tuned BERT for complex low-resource emotion recognition tasks. The dataset\nand model are shared publicly at https://github.com/l3cube-pune/MarathiNLP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emotion recognition in low-resource languages like Marathi remains\nchallenging due to limited annotated data. We present L3Cube-MahaEmotions, a\nhigh-quality Marathi emotion recognition dataset with 11 fine-grained emotion\nlabels. The training data is synthetically annotated using large language\nmodels (LLMs), while the validation and test sets are manually labeled to serve\nas a reliable gold-standard benchmark. Building on the MahaSent dataset, we\napply the Chain-of-Translation (CoTR) prompting technique, where Marathi\nsentences are translated into English and emotion labeled via a single prompt.\nGPT-4 and Llama3-405B were evaluated, with GPT-4 selected for training data\nannotation due to superior label quality. We evaluate model performance using\nstandard metrics and explore label aggregation strategies (e.g., Union,\nIntersection). While GPT-4 predictions outperform fine-tuned BERT models,\nBERT-based models trained on synthetic labels fail to surpass GPT-4. This\nhighlights both the importance of high-quality human-labeled data and the\ninherent complexity of emotion recognition. An important finding of this work\nis that generic LLMs like GPT-4 and Llama3-405B generalize better than\nfine-tuned BERT for complex low-resource emotion recognition tasks. The dataset\nand model are shared publicly at https://github.com/l3cube-pune/MarathiNLP"
                },
                "authors": [
                    {
                        "name": "Nidhi Kowtal"
                    },
                    {
                        "name": "Raviraj Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Raviraj Joshi"
                },
                "author": "Raviraj Joshi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20417v2",
                "updated": "2025-08-29T11:37:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    37,
                    41,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-28T04:37:15Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    4,
                    37,
                    15,
                    3,
                    240,
                    0
                ],
                "title": "KG-CQR: Leveraging Structured Relation Representations in Knowledge\n  Graphs for Contextual Query Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG-CQR: Leveraging Structured Relation Representations in Knowledge\n  Graphs for Contextual Query Retrieval"
                },
                "summary": "The integration of knowledge graphs (KGs) with large language models (LLMs)\noffers significant potential to improve the retrieval phase of\nretrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,\na novel framework for Contextual Query Retrieval (CQR) that enhances the\nretrieval phase by enriching the contextual representation of complex input\nqueries using a corpus-centric KG. Unlike existing methods that primarily\naddress corpus-level context loss, KG-CQR focuses on query enrichment through\nstructured relation representations, extracting and completing relevant KG\nsubgraphs to generate semantically rich query contexts. Comprising subgraph\nextraction, completion, and contextual generation modules, KG-CQR operates as a\nmodel-agnostic pipeline, ensuring scalability across LLMs of varying sizes\nwithout additional training. Experimental results on RAGBench and MultiHop-RAG\ndatasets demonstrate KG-CQR's superior performance, achieving a 4-6%\nimprovement in mAP and a 2-3% improvement in Recall@25 over strong baseline\nmodels. Furthermore, evaluations on challenging RAG tasks such as multi-hop\nquestion answering show that, by incorporating KG-CQR, the performance\nconsistently outperforms the existing baseline in terms of retrieval\neffectiveness",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of knowledge graphs (KGs) with large language models (LLMs)\noffers significant potential to improve the retrieval phase of\nretrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR,\na novel framework for Contextual Query Retrieval (CQR) that enhances the\nretrieval phase by enriching the contextual representation of complex input\nqueries using a corpus-centric KG. Unlike existing methods that primarily\naddress corpus-level context loss, KG-CQR focuses on query enrichment through\nstructured relation representations, extracting and completing relevant KG\nsubgraphs to generate semantically rich query contexts. Comprising subgraph\nextraction, completion, and contextual generation modules, KG-CQR operates as a\nmodel-agnostic pipeline, ensuring scalability across LLMs of varying sizes\nwithout additional training. Experimental results on RAGBench and MultiHop-RAG\ndatasets demonstrate KG-CQR's superior performance, achieving a 4-6%\nimprovement in mAP and a 2-3% improvement in Recall@25 over strong baseline\nmodels. Furthermore, evaluations on challenging RAG tasks such as multi-hop\nquestion answering show that, by incorporating KG-CQR, the performance\nconsistently outperforms the existing baseline in terms of retrieval\neffectiveness"
                },
                "authors": [
                    {
                        "name": "Chi Minh Bui"
                    },
                    {
                        "name": "Ngoc Mai Thieu"
                    },
                    {
                        "name": "Van Vinh Nguyen"
                    },
                    {
                        "name": "Jason J. Jung"
                    },
                    {
                        "name": "Khac-Hoai Nam Bui"
                    }
                ],
                "author_detail": {
                    "name": "Khac-Hoai Nam Bui"
                },
                "author": "Khac-Hoai Nam Bui",
                "arxiv_comment": "Accepted at Main EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20661v2",
                "updated": "2025-08-29T11:00:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    11,
                    0,
                    18,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-28T11:09:19Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    11,
                    9,
                    19,
                    3,
                    240,
                    0
                ],
                "title": "Traversing the Narrow Path: A Two-Stage Reinforcement Learning Framework\n  for Humanoid Beam Walking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traversing the Narrow Path: A Two-Stage Reinforcement Learning Framework\n  for Humanoid Beam Walking"
                },
                "summary": "Traversing narrow beams is challenging for humanoids due to sparse,\nsafety-critical contacts and the fragility of purely learned policies. We\npropose a physically grounded, two-stage framework that couples an XCoM/LIPM\nfootstep template with a lightweight residual planner and a simple low-level\ntracker. Stage-1 is trained on flat ground: the tracker learns to robustly\nfollow footstep targets by adding small random perturbations to heuristic\nfootsteps, without any hand-crafted centerline locking, so it acquires stable\ncontact scheduling and strong target-tracking robustness. Stage-2 is trained in\nsimulation on a beam: a high-level planner predicts a body-frame residual\n(Delta x, Delta y, Delta psi) for the swing foot only, refining the template\nstep to prioritize safe, precise placement under narrow support while\npreserving interpretability. To ease deployment, sensing is kept minimal and\nconsistent between simulation and hardware: the planner consumes compact,\nforward-facing elevation cues together with onboard IMU and joint signals. On a\nUnitree G1, our system reliably traverses a 0.2 m-wide, 3 m-long beam. Across\nsimulation and real-world studies, residual refinement consistently outperforms\ntemplate-only and monolithic baselines in success rate, centerline adherence,\nand safety margins, while the structured footstep interface enables transparent\nanalysis and low-friction sim-to-real transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traversing narrow beams is challenging for humanoids due to sparse,\nsafety-critical contacts and the fragility of purely learned policies. We\npropose a physically grounded, two-stage framework that couples an XCoM/LIPM\nfootstep template with a lightweight residual planner and a simple low-level\ntracker. Stage-1 is trained on flat ground: the tracker learns to robustly\nfollow footstep targets by adding small random perturbations to heuristic\nfootsteps, without any hand-crafted centerline locking, so it acquires stable\ncontact scheduling and strong target-tracking robustness. Stage-2 is trained in\nsimulation on a beam: a high-level planner predicts a body-frame residual\n(Delta x, Delta y, Delta psi) for the swing foot only, refining the template\nstep to prioritize safe, precise placement under narrow support while\npreserving interpretability. To ease deployment, sensing is kept minimal and\nconsistent between simulation and hardware: the planner consumes compact,\nforward-facing elevation cues together with onboard IMU and joint signals. On a\nUnitree G1, our system reliably traverses a 0.2 m-wide, 3 m-long beam. Across\nsimulation and real-world studies, residual refinement consistently outperforms\ntemplate-only and monolithic baselines in success rate, centerline adherence,\nand safety margins, while the structured footstep interface enables transparent\nanalysis and low-friction sim-to-real transfer."
                },
                "authors": [
                    {
                        "name": "TianChen Huang"
                    },
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Runchen Xu"
                    },
                    {
                        "name": "Shiwu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shiwu Zhang"
                },
                "author": "Shiwu Zhang",
                "arxiv_comment": "Project website:\n  https://huangtc233.github.io/Traversing-the-Narrow-Path/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21512v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21512v1",
                "updated": "2025-08-29T10:51:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    51,
                    41,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T10:51:41Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    51,
                    41,
                    4,
                    241,
                    0
                ],
                "title": "Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval\n  across Table-to-Text Serialization Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accept or Deny? Evaluating LLM Fairness and Performance in Loan Approval\n  across Table-to-Text Serialization Approaches"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in high-stakes\ndecision-making tasks, such as loan approvals. While their applications expand\nacross domains, LLMs struggle to process tabular data, ensuring fairness and\ndelivering reliable predictions. In this work, we assess the performance and\nfairness of LLMs on serialized loan approval datasets from three geographically\ndistinct regions: Ghana, Germany, and the United States. Our evaluation focuses\non the model's zero-shot and in-context learning (ICL) capabilities. Our\nresults reveal that the choice of serialization (Serialization refers to the\nprocess of converting tabular data into text formats suitable for processing by\nLLMs.) format significantly affects both performance and fairness in LLMs, with\ncertain formats such as GReat and LIFT yielding higher F1 scores but\nexacerbating fairness disparities. Notably, while ICL improved model\nperformance by 4.9-59.6% relative to zero-shot baselines, its effect on\nfairness varied considerably across datasets. Our work underscores the\nimportance of effective tabular data representation methods and fairness-aware\nmodels to improve the reliability of LLMs in financial decision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in high-stakes\ndecision-making tasks, such as loan approvals. While their applications expand\nacross domains, LLMs struggle to process tabular data, ensuring fairness and\ndelivering reliable predictions. In this work, we assess the performance and\nfairness of LLMs on serialized loan approval datasets from three geographically\ndistinct regions: Ghana, Germany, and the United States. Our evaluation focuses\non the model's zero-shot and in-context learning (ICL) capabilities. Our\nresults reveal that the choice of serialization (Serialization refers to the\nprocess of converting tabular data into text formats suitable for processing by\nLLMs.) format significantly affects both performance and fairness in LLMs, with\ncertain formats such as GReat and LIFT yielding higher F1 scores but\nexacerbating fairness disparities. Notably, while ICL improved model\nperformance by 4.9-59.6% relative to zero-shot baselines, its effect on\nfairness varied considerably across datasets. Our work underscores the\nimportance of effective tabular data representation methods and fairness-aware\nmodels to improve the reliability of LLMs in financial decision-making."
                },
                "authors": [
                    {
                        "name": "Israel Abebe Azime"
                    },
                    {
                        "name": "Deborah D. Kanubala"
                    },
                    {
                        "name": "Tejumade Afonja"
                    },
                    {
                        "name": "Mario Fritz"
                    },
                    {
                        "name": "Isabel Valera"
                    },
                    {
                        "name": "Dietrich Klakow"
                    },
                    {
                        "name": "Philipp Slusallek"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Slusallek"
                },
                "author": "Philipp Slusallek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21512v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21512v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17052v2",
                "updated": "2025-08-29T10:47:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    47,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-23T19:00:39Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    19,
                    0,
                    39,
                    2,
                    113,
                    0
                ],
                "title": "Testing Conviction: An Argumentative Framework for Measuring LLM\n  Political Stability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Conviction: An Argumentative Framework for Measuring LLM\n  Political Stability"
                },
                "summary": "Large Language Models (LLMs) increasingly shape political discourse, yet\nexhibit inconsistent responses when challenged. While prior research\ncategorizes LLMs as left- or right-leaning based on single-prompt responses, a\ncritical question remains: Do these classifications reflect stable ideologies\nor superficial mimicry? Existing methods cannot distinguish between genuine\nideological alignment and performative text generation. To address this, we\npropose a framework for evaluating ideological depth through (1) argumentative\nconsistency and (2) uncertainty quantification. Testing 12 LLMs on 19 economic\npolicies from the Political Compass Test, we classify responses as stable or\nperformative ideological positioning. Results show 95% of left-leaning models\nand 89% of right-leaning models demonstrate behavior consistent with our\nclassifications across different experimental conditions. Furthermore, semantic\nentropy strongly validates our classifications (AUROC=0.78), revealing\nuncertainty's relationship to ideological consistency. Our findings demonstrate\nthat ideological stability is topic-dependent and challenge the notion of\nmonolithic LLM ideologies, and offer a robust way to distinguish genuine\nalignment from performative behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly shape political discourse, yet\nexhibit inconsistent responses when challenged. While prior research\ncategorizes LLMs as left- or right-leaning based on single-prompt responses, a\ncritical question remains: Do these classifications reflect stable ideologies\nor superficial mimicry? Existing methods cannot distinguish between genuine\nideological alignment and performative text generation. To address this, we\npropose a framework for evaluating ideological depth through (1) argumentative\nconsistency and (2) uncertainty quantification. Testing 12 LLMs on 19 economic\npolicies from the Political Compass Test, we classify responses as stable or\nperformative ideological positioning. Results show 95% of left-leaning models\nand 89% of right-leaning models demonstrate behavior consistent with our\nclassifications across different experimental conditions. Furthermore, semantic\nentropy strongly validates our classifications (AUROC=0.78), revealing\nuncertainty's relationship to ideological consistency. Our findings demonstrate\nthat ideological stability is topic-dependent and challenge the notion of\nmonolithic LLM ideologies, and offer a robust way to distinguish genuine\nalignment from performative behavior."
                },
                "authors": [
                    {
                        "name": "Shariar Kabir"
                    },
                    {
                        "name": "Kevin Esterling"
                    },
                    {
                        "name": "Yue Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yue Dong"
                },
                "author": "Yue Dong",
                "arxiv_comment": "14 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21491v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21491v1",
                "updated": "2025-08-29T10:16:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    16,
                    37,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T10:16:37Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    16,
                    37,
                    4,
                    241,
                    0
                ],
                "title": "Geospatial Question Answering on Historical Maps Using Spatio-Temporal\n  Knowledge Graphs and Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geospatial Question Answering on Historical Maps Using Spatio-Temporal\n  Knowledge Graphs and Large Language Models"
                },
                "summary": "Recent advances have enabled the extraction of vectorized features from\ndigital historical maps. To fully leverage this information, however, the\nextracted features must be organized in a structured and meaningful way that\nsupports efficient access and use. One promising approach is question answering\n(QA), which allows users -- especially those unfamiliar with database query\nlanguages -- to retrieve knowledge in a natural and intuitive manner. In this\nproject, we developed a GeoQA system by integrating a spatio-temporal knowledge\ngraph (KG) constructed from historical map data with large language models\n(LLMs). Specifically, we have defined the ontology to guide the construction of\nthe spatio-temporal KG and investigated workflows of two different types of\nGeoQA: factual and descriptive. Additional data sources, such as historical map\nimages and internet search results, are incorporated into our framework to\nprovide extra context for descriptive GeoQA. Evaluation results demonstrate\nthat the system can generate answers with a high delivery rate and a high\nsemantic accuracy. To make the framework accessible, we further developed a web\napplication that supports interactive querying and visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have enabled the extraction of vectorized features from\ndigital historical maps. To fully leverage this information, however, the\nextracted features must be organized in a structured and meaningful way that\nsupports efficient access and use. One promising approach is question answering\n(QA), which allows users -- especially those unfamiliar with database query\nlanguages -- to retrieve knowledge in a natural and intuitive manner. In this\nproject, we developed a GeoQA system by integrating a spatio-temporal knowledge\ngraph (KG) constructed from historical map data with large language models\n(LLMs). Specifically, we have defined the ontology to guide the construction of\nthe spatio-temporal KG and investigated workflows of two different types of\nGeoQA: factual and descriptive. Additional data sources, such as historical map\nimages and internet search results, are incorporated into our framework to\nprovide extra context for descriptive GeoQA. Evaluation results demonstrate\nthat the system can generate answers with a high delivery rate and a high\nsemantic accuracy. To make the framework accessible, we further developed a web\napplication that supports interactive querying and visualization."
                },
                "authors": [
                    {
                        "name": "Ziyi Liu"
                    },
                    {
                        "name": "Sidi Wu"
                    },
                    {
                        "name": "Lorenz Hurni"
                    }
                ],
                "author_detail": {
                    "name": "Lorenz Hurni"
                },
                "author": "Lorenz Hurni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21491v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21491v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21480v1",
                "updated": "2025-08-29T10:05:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    5,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T10:05:22Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    5,
                    22,
                    4,
                    241,
                    0
                ],
                "title": "Towards a Decentralized IoT Onboarding for Smart Homes Using Consortium\n  Blockchain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards a Decentralized IoT Onboarding for Smart Homes Using Consortium\n  Blockchain"
                },
                "summary": "The increasing adoption of smart home devices and IoT-based security systems\npresents significant opportunities to enhance convenience, safety, and risk\nmanagement for homeowners and service providers. However, secure\nonboarding-provisioning credentials and establishing trust with cloud\nplatforms-remains a considerable challenge. Traditional onboarding methods\noften rely on centralized Public Key Infrastructure (PKI) models and\nmanufacturer-controlled keys, which introduce security risks and limit the\nuser's digital sovereignty. These limitations hinder the widespread deployment\nof scalable IoT solutions. This paper presents a novel onboarding framework\nthat builds upon existing network-layer onboarding techniques and extends them\nto the application layer to address these challenges. By integrating consortium\nblockchain technology, we propose a decentralized onboarding mechanism that\nenhances transparency, security, and monitoring for smart home architectures.\nThe architecture supports device registration, key revocation, access control\nmanagement, and risk detection through event-driven alerts across dedicated\nblockchain channels and smart contracts. To evaluate the framework, we formally\nmodel the protocol using the Tamarin Prover under the Dolev-Yao adversary\nmodel. The analysis focuses on authentication, token integrity, key\nconfidentiality, and resilience over public channels. A prototype\nimplementation demonstrates the system's viability in smart home settings, with\nverification completing in 0.34 seconds, highlighting its scalability and\nsuitability for constrained devices and diverse stakeholders. Additionally,\nperformance evaluation shows that the blockchain-based approach effectively\nhandles varying workloads, maintains high throughput and low latency, and\nsupports near real-time IoT data processing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing adoption of smart home devices and IoT-based security systems\npresents significant opportunities to enhance convenience, safety, and risk\nmanagement for homeowners and service providers. However, secure\nonboarding-provisioning credentials and establishing trust with cloud\nplatforms-remains a considerable challenge. Traditional onboarding methods\noften rely on centralized Public Key Infrastructure (PKI) models and\nmanufacturer-controlled keys, which introduce security risks and limit the\nuser's digital sovereignty. These limitations hinder the widespread deployment\nof scalable IoT solutions. This paper presents a novel onboarding framework\nthat builds upon existing network-layer onboarding techniques and extends them\nto the application layer to address these challenges. By integrating consortium\nblockchain technology, we propose a decentralized onboarding mechanism that\nenhances transparency, security, and monitoring for smart home architectures.\nThe architecture supports device registration, key revocation, access control\nmanagement, and risk detection through event-driven alerts across dedicated\nblockchain channels and smart contracts. To evaluate the framework, we formally\nmodel the protocol using the Tamarin Prover under the Dolev-Yao adversary\nmodel. The analysis focuses on authentication, token integrity, key\nconfidentiality, and resilience over public channels. A prototype\nimplementation demonstrates the system's viability in smart home settings, with\nverification completing in 0.34 seconds, highlighting its scalability and\nsuitability for constrained devices and diverse stakeholders. Additionally,\nperformance evaluation shows that the blockchain-based approach effectively\nhandles varying workloads, maintains high throughput and low latency, and\nsupports near real-time IoT data processing."
                },
                "authors": [
                    {
                        "name": "Narges Dadkhah"
                    },
                    {
                        "name": "Khan Reaz"
                    },
                    {
                        "name": "Gerhard Wunder"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Wunder"
                },
                "author": "Gerhard Wunder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.12800v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.12800v3",
                "updated": "2025-08-29T10:05:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    5,
                    13,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-18T10:23:10Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    10,
                    23,
                    10,
                    0,
                    230,
                    0
                ],
                "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic\n  Thought Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic\n  Thought Reward"
                },
                "summary": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns."
                },
                "authors": [
                    {
                        "name": "Yong Deng"
                    },
                    {
                        "name": "Guoqing Wang"
                    },
                    {
                        "name": "Zhenzhe Ying"
                    },
                    {
                        "name": "Xiaofeng Wu"
                    },
                    {
                        "name": "Jinzhen Lin"
                    },
                    {
                        "name": "Wenwen Xiong"
                    },
                    {
                        "name": "Yuqin Dai"
                    },
                    {
                        "name": "Shuo Yang"
                    },
                    {
                        "name": "Zhanwei Zhang"
                    },
                    {
                        "name": "Qiwen Wang"
                    },
                    {
                        "name": "Yang Qin"
                    },
                    {
                        "name": "Yuan Wang"
                    },
                    {
                        "name": "Quanxing Zha"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Changhua Meng"
                    }
                ],
                "author_detail": {
                    "name": "Changhua Meng"
                },
                "author": "Changhua Meng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.12800v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.12800v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21479v1",
                "updated": "2025-08-29T10:02:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    2,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T10:02:22Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    2,
                    22,
                    4,
                    241,
                    0
                ],
                "title": "Realization of an untrusted intermediate relay architecture using a\n  quantum dot single-photon source",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Realization of an untrusted intermediate relay architecture using a\n  quantum dot single-photon source"
                },
                "summary": "To fully exploit the potential of quantum technologies, quantum networks are\nneeded to link different systems, significantly enhancing applications in\ncomputing, cryptography, and metrology. Central to these networks are quantum\nrelays that can facilitate long-distance entanglement distribution and quantum\ncommunication. In this work, we present a modular and scalable quantum relay\narchitecture using a high-quality single-photon source. The proposed network\nincorporates three untrusted intermediate nodes and is capable of a repetition\nrate of 304.52 MHz. We use a measurement-device-independent protocol to\ndemonstrate secure key establishment over fibers covering up to 300 kilometers.\nThis study highlights the potential of single-photon sources in quantum relays\nto enhance information transmission, expand network coverage, and improve\ndeployment flexibility, with promising applications in future quantum networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To fully exploit the potential of quantum technologies, quantum networks are\nneeded to link different systems, significantly enhancing applications in\ncomputing, cryptography, and metrology. Central to these networks are quantum\nrelays that can facilitate long-distance entanglement distribution and quantum\ncommunication. In this work, we present a modular and scalable quantum relay\narchitecture using a high-quality single-photon source. The proposed network\nincorporates three untrusted intermediate nodes and is capable of a repetition\nrate of 304.52 MHz. We use a measurement-device-independent protocol to\ndemonstrate secure key establishment over fibers covering up to 300 kilometers.\nThis study highlights the potential of single-photon sources in quantum relays\nto enhance information transmission, expand network coverage, and improve\ndeployment flexibility, with promising applications in future quantum networks."
                },
                "authors": [
                    {
                        "name": "Mi Zou"
                    },
                    {
                        "name": "Yu-Ming He"
                    },
                    {
                        "name": "Yizhi Huang"
                    },
                    {
                        "name": "Jun-Yi Zhao"
                    },
                    {
                        "name": "Bin-Chen Li"
                    },
                    {
                        "name": "Yong-Peng Guo"
                    },
                    {
                        "name": "Xing Ding"
                    },
                    {
                        "name": "Mo-Chi Xu"
                    },
                    {
                        "name": "Run-Ze Liu"
                    },
                    {
                        "name": "Geng-Yan Zou"
                    },
                    {
                        "name": "Zhen Ning"
                    },
                    {
                        "name": "Xiang You"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Wen-Xin Pan"
                    },
                    {
                        "name": "Hao-Tao Zhu"
                    },
                    {
                        "name": "Ming-Yang Zheng"
                    },
                    {
                        "name": "Xiu-Ping Xie"
                    },
                    {
                        "name": "Dandan Qin"
                    },
                    {
                        "name": "Xiao Jiang"
                    },
                    {
                        "name": "Yong-Heng Huo"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Chao-Yang Lu"
                    },
                    {
                        "name": "Xiongfeng Ma"
                    },
                    {
                        "name": "Teng-Yun Chen"
                    },
                    {
                        "name": "Jian-Wei Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jian-Wei Pan"
                },
                "author": "Jian-Wei Pan",
                "arxiv_doi": "10.1038/s41567-025-03005-5",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41567-025-03005-5",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.21479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "29 pages,17 figures, 2 tables",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21476v1",
                "updated": "2025-08-29T10:00:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    0,
                    55,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T10:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    10,
                    0,
                    55,
                    4,
                    241,
                    0
                ],
                "title": "Igniting Creative Writing in Small Language Models: LLM-as-a-Judge\n  versus Multi-Agent Refined Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Igniting Creative Writing in Small Language Models: LLM-as-a-Judge\n  versus Multi-Agent Refined Rewards"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable creative writing\ncapabilities, yet their substantial computational demands hinder widespread\nuse. Enhancing Small Language Models (SLMs) offers a promising alternative, but\ncurrent methods like Supervised Fine-Tuning (SFT) struggle with novelty, and\nReinforcement Learning from Human Feedback (RLHF) is costly. This paper\nexplores two distinct AI-driven reward strategies within a Reinforcement\nLearning from AI Feedback (RLAIF) framework to ignite the creative writing of a\n7B-parameter SLM, specifically for generating Chinese greetings. The first\nstrategy employs a RM trained on high-quality preference data curated by a\nnovel multi-agent rejection sampling framework designed for creative tasks. The\nsecond, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose\nreward function is optimized via an adversarial training scheme with a\nreflection mechanism, to directly provide reward signals. Comprehensive\nexperiments reveal that while both approaches significantly enhance creative\noutput over baselines, the principle-guided LLM-as-a-Judge demonstrably yields\nsuperior generation quality. Furthermore, it offers notable advantages in\ntraining efficiency and reduced dependency on human-annotated data, presenting\na more scalable and effective path towards creative SLMs. Our automated\nevaluation methods also exhibit strong alignment with human judgments. Our code\nand data are publicly available at\nhttps://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable creative writing\ncapabilities, yet their substantial computational demands hinder widespread\nuse. Enhancing Small Language Models (SLMs) offers a promising alternative, but\ncurrent methods like Supervised Fine-Tuning (SFT) struggle with novelty, and\nReinforcement Learning from Human Feedback (RLHF) is costly. This paper\nexplores two distinct AI-driven reward strategies within a Reinforcement\nLearning from AI Feedback (RLAIF) framework to ignite the creative writing of a\n7B-parameter SLM, specifically for generating Chinese greetings. The first\nstrategy employs a RM trained on high-quality preference data curated by a\nnovel multi-agent rejection sampling framework designed for creative tasks. The\nsecond, more novel strategy utilizes a principle-guided LLM-as-a-Judge, whose\nreward function is optimized via an adversarial training scheme with a\nreflection mechanism, to directly provide reward signals. Comprehensive\nexperiments reveal that while both approaches significantly enhance creative\noutput over baselines, the principle-guided LLM-as-a-Judge demonstrably yields\nsuperior generation quality. Furthermore, it offers notable advantages in\ntraining efficiency and reduced dependency on human-annotated data, presenting\na more scalable and effective path towards creative SLMs. Our automated\nevaluation methods also exhibit strong alignment with human judgments. Our code\nand data are publicly available at\nhttps://github.com/weixiaolong94-hub/Igniting-Creative-Writing-in-Small-Language-Models."
                },
                "authors": [
                    {
                        "name": "Xiaolong Wei"
                    },
                    {
                        "name": "Bo Lu"
                    },
                    {
                        "name": "Xingyu Zhang"
                    },
                    {
                        "name": "Zhejun Zhao"
                    },
                    {
                        "name": "Dongdong Shen"
                    },
                    {
                        "name": "Long Xia"
                    },
                    {
                        "name": "Dawei Yin"
                    }
                ],
                "author_detail": {
                    "name": "Dawei Yin"
                },
                "author": "Dawei Yin",
                "arxiv_comment": "EMNLP 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.10187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.10187v2",
                "updated": "2025-08-29T09:58:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    42,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-14T12:40:39Z",
                "published_parsed": [
                    2025,
                    4,
                    14,
                    12,
                    40,
                    39,
                    0,
                    104,
                    0
                ],
                "title": "DeepTrans: Deep Reasoning Translation via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepTrans: Deep Reasoning Translation via Reinforcement Learning"
                },
                "summary": "Recently, deep reasoning LLMs (e.g., OpenAI o1 and DeepSeek-R1) have shown\npromising performance in various downstream tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation. However, the task is still under-explored in\ndeep reasoning LLMs. In this paper, we introduce DeepTrans, a deep reasoning\ntranslation model that learns free translation via reinforcement learning (RL).\nSpecifically, we carefully build a reward model with pre-defined scoring\ncriteria on both the translation results and the thought processes. The reward\nmodel teaches DeepTrans how to think and free-translate the given sentences\nduring RL. Besides, our RL training does not need any labeled translations,\navoiding the human-intensive annotation or resource-intensive data synthesis.\nExperimental results show the effectiveness of DeepTrans. Using Qwen2.5-7B as\nthe backbone, DeepTrans improves performance by 16.3% in literature\ntranslation, and outperforms strong deep reasoning LLMs. Moreover, we summarize\nthe failures and interesting findings during our RL exploration. We hope this\nwork could inspire other researchers in free translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, deep reasoning LLMs (e.g., OpenAI o1 and DeepSeek-R1) have shown\npromising performance in various downstream tasks. Free translation is an\nimportant and interesting task in the multilingual world, which requires going\nbeyond word-for-word translation. However, the task is still under-explored in\ndeep reasoning LLMs. In this paper, we introduce DeepTrans, a deep reasoning\ntranslation model that learns free translation via reinforcement learning (RL).\nSpecifically, we carefully build a reward model with pre-defined scoring\ncriteria on both the translation results and the thought processes. The reward\nmodel teaches DeepTrans how to think and free-translate the given sentences\nduring RL. Besides, our RL training does not need any labeled translations,\navoiding the human-intensive annotation or resource-intensive data synthesis.\nExperimental results show the effectiveness of DeepTrans. Using Qwen2.5-7B as\nthe backbone, DeepTrans improves performance by 16.3% in literature\ntranslation, and outperforms strong deep reasoning LLMs. Moreover, we summarize\nthe failures and interesting findings during our RL exploration. We hope this\nwork could inspire other researchers in free translation."
                },
                "authors": [
                    {
                        "name": "Jiaan Wang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "arxiv_comment": "Accepted by Transactions of the Association for Computational\n  Linguistics (TACL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.10187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.10187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.19238v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.19238v3",
                "updated": "2025-08-29T09:58:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    58,
                    18,
                    4,
                    241,
                    0
                ],
                "published": "2024-06-27T15:01:53Z",
                "published_parsed": [
                    2024,
                    6,
                    27,
                    15,
                    1,
                    53,
                    3,
                    179,
                    0
                ],
                "title": "Revealing Fine-Grained Values and Opinions in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revealing Fine-Grained Values and Opinions in Large Language Models"
                },
                "summary": "Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncovering latent values and opinions embedded in large language models\n(LLMs) can help identify biases and mitigate potential harm. Recently, this has\nbeen approached by prompting LLMs with survey questions and quantifying the\nstances in the outputs towards morally and politically charged statements.\nHowever, the stances generated by LLMs can vary greatly depending on how they\nare prompted, and there are many ways to argue for or against a given position.\nIn this work, we propose to address this by analysing a large and robust\ndataset of 156k LLM responses to the 62 propositions of the Political Compass\nTest (PCT) generated by 6 LLMs using 420 prompt variations. We perform\ncoarse-grained analysis of their generated stances and fine-grained analysis of\nthe plain text justifications for those stances. For fine-grained analysis, we\npropose to identify tropes in the responses: semantically similar phrases that\nare recurrent and consistent across different prompts, revealing natural\npatterns in the text that a given LLM is prone to produce. We find that\ndemographic features added to prompts significantly affect outcomes on the PCT,\nreflecting bias, as well as disparities between the results of tests when\neliciting closed-form vs. open domain responses. Additionally, patterns in the\nplain text rationales via tropes show that similar justifications are\nrepeatedly generated across models and prompts even with disparate stances."
                },
                "authors": [
                    {
                        "name": "Dustin Wright"
                    },
                    {
                        "name": "Arnav Arora"
                    },
                    {
                        "name": "Nadav Borenstein"
                    },
                    {
                        "name": "Srishti Yadav"
                    },
                    {
                        "name": "Serge Belongie"
                    },
                    {
                        "name": "Isabelle Augenstein"
                    }
                ],
                "author_detail": {
                    "name": "Isabelle Augenstein"
                },
                "author": "Isabelle Augenstein",
                "arxiv_comment": "Findings of EMNLP 2024; 28 pages, 20 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.19238v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.19238v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04969v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04969v3",
                "updated": "2025-08-29T09:49:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    49,
                    10,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-07T13:12:28Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    13,
                    12,
                    28,
                    0,
                    188,
                    0
                ],
                "title": "Silent Failures in Stateless Systems: Rethinking Anomaly Detection for\n  Serverless Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Silent Failures in Stateless Systems: Rethinking Anomaly Detection for\n  Serverless Computing"
                },
                "summary": "Serverless computing has redefined cloud application deployment by\nabstracting infrastructure and enabling on-demand, event-driven execution,\nthereby enhancing developer agility and scalability. However, maintaining\nconsistent application performance in serverless environments remains a\nsignificant challenge. The dynamic and transient nature of serverless functions\nmakes it difficult to distinguish between benign and anomalous behavior, which\nin turn undermines the effectiveness of traditional anomaly detection methods.\nThese conventional approaches, designed for stateful and long-running services,\nstruggle in serverless settings where executions are short-lived, functions are\nisolated, and observability is limited.\n  In this first comprehensive vision paper on anomaly detection for serverless\nsystems, we systematically explore the unique challenges posed by this\nparadigm, including the absence of persistent state, inconsistent monitoring\ngranularity, and the difficulty of correlating behaviors across distributed\nfunctions. We further examine a range of threats that manifest as anomalies,\nfrom classical Denial-of-Service (DoS) attacks to serverless-specific threats\nsuch as Denial-of-Wallet (DoW) and cold start amplification. Building on these\nobservations, we articulate a research agenda for next-generation detection\nframeworks that address the need for context-aware, multi-source data fusion,\nreal-time, lightweight, privacy-preserving, and edge-cloud adaptive\ncapabilities.\n  Through the identification of key research directions and design principles,\nwe aim to lay the foundation for the next generation of anomaly detection in\ncloud-native, serverless ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing has redefined cloud application deployment by\nabstracting infrastructure and enabling on-demand, event-driven execution,\nthereby enhancing developer agility and scalability. However, maintaining\nconsistent application performance in serverless environments remains a\nsignificant challenge. The dynamic and transient nature of serverless functions\nmakes it difficult to distinguish between benign and anomalous behavior, which\nin turn undermines the effectiveness of traditional anomaly detection methods.\nThese conventional approaches, designed for stateful and long-running services,\nstruggle in serverless settings where executions are short-lived, functions are\nisolated, and observability is limited.\n  In this first comprehensive vision paper on anomaly detection for serverless\nsystems, we systematically explore the unique challenges posed by this\nparadigm, including the absence of persistent state, inconsistent monitoring\ngranularity, and the difficulty of correlating behaviors across distributed\nfunctions. We further examine a range of threats that manifest as anomalies,\nfrom classical Denial-of-Service (DoS) attacks to serverless-specific threats\nsuch as Denial-of-Wallet (DoW) and cold start amplification. Building on these\nobservations, we articulate a research agenda for next-generation detection\nframeworks that address the need for context-aware, multi-source data fusion,\nreal-time, lightweight, privacy-preserving, and edge-cloud adaptive\ncapabilities.\n  Through the identification of key research directions and design principles,\nwe aim to lay the foundation for the next generation of anomaly detection in\ncloud-native, serverless ecosystems."
                },
                "authors": [
                    {
                        "name": "Chanh Nguyen"
                    },
                    {
                        "name": "Erik Elmroth"
                    },
                    {
                        "name": "Monowar Bhuyan"
                    }
                ],
                "author_detail": {
                    "name": "Monowar Bhuyan"
                },
                "author": "Monowar Bhuyan",
                "arxiv_doi": "10.1109/SOSE67019.2025.00006",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SOSE67019.2025.00006",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.04969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04969v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 6 figures, Preprint accepted at 2025 IEEE International\n  Conference on Service-Oriented System Engineering (SOSE)",
                "arxiv_journal_ref": "2025 IEEE International Conference on Service-Oriented System\n  Engineering (SOSE), Tucson, AZ, USA, 2025, pp. 8-19",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21457v1",
                "updated": "2025-08-29T09:39:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    39,
                    46,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T09:39:46Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    39,
                    46,
                    4,
                    241,
                    0
                ],
                "title": "SoK: Large Language Model-Generated Textual Phishing Campaigns\n  End-to-End Analysis of Generation, Characteristics, and Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoK: Large Language Model-Generated Textual Phishing Campaigns\n  End-to-End Analysis of Generation, Characteristics, and Detection"
                },
                "summary": "Phishing is a pervasive form of social engineering in which attackers\nimpersonate trusted entities to steal information or induce harmful actions.\nText-based phishing dominates for its low cost, scalability, and\nconcealability, advantages recently amplified by large language models (LLMs)\nthat enable ``Phishing-as-a-Service'' attacks at scale within minutes. Despite\nthe growing research into LLM-facilitated phishing attacks, consolidated\nsystematic research on the phishing attack life cycle remains scarce. In this\nwork, we present the first systematization of knowledge (SoK) on LLM-generated\nphishing, offering an end-to-end analysis that spans generation techniques,\nattack features, and mitigation strategies. We introduce\nGeneration-Characterization-Defense (GenCharDef), which systematizes the ways\nin which LLM-generated phishing differs from traditional phishing across\nmethodologies, security perspectives, data dependencies, and evaluation\npractices. This framework highlights unique challenges of LLM-driven phishing,\nproviding a coherent foundation for understanding the evolving threat landscape\nand guiding the design of more resilient defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a pervasive form of social engineering in which attackers\nimpersonate trusted entities to steal information or induce harmful actions.\nText-based phishing dominates for its low cost, scalability, and\nconcealability, advantages recently amplified by large language models (LLMs)\nthat enable ``Phishing-as-a-Service'' attacks at scale within minutes. Despite\nthe growing research into LLM-facilitated phishing attacks, consolidated\nsystematic research on the phishing attack life cycle remains scarce. In this\nwork, we present the first systematization of knowledge (SoK) on LLM-generated\nphishing, offering an end-to-end analysis that spans generation techniques,\nattack features, and mitigation strategies. We introduce\nGeneration-Characterization-Defense (GenCharDef), which systematizes the ways\nin which LLM-generated phishing differs from traditional phishing across\nmethodologies, security perspectives, data dependencies, and evaluation\npractices. This framework highlights unique challenges of LLM-driven phishing,\nproviding a coherent foundation for understanding the evolving threat landscape\nand guiding the design of more resilient defenses."
                },
                "authors": [
                    {
                        "name": "Fengchao Chen"
                    },
                    {
                        "name": "Tingmin Wu"
                    },
                    {
                        "name": "Van Nguyen"
                    },
                    {
                        "name": "Carsten Rudolph"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Rudolph"
                },
                "author": "Carsten Rudolph",
                "arxiv_comment": "13 pages, 3 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04342v2",
                "updated": "2025-08-29T09:38:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    38,
                    13,
                    4,
                    241,
                    0
                ],
                "published": "2024-12-05T17:00:32Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    17,
                    0,
                    32,
                    3,
                    340,
                    0
                ],
                "title": "Retrieval-Augmented Machine Translation with Unstructured Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Machine Translation with Unstructured Knowledge"
                },
                "summary": "Retrieval-augmented generation (RAG) introduces additional information to\nenhance large language models (LLMs). In machine translation (MT), previous\nwork typically retrieves in-context examples from paired MT corpora, or\ndomain-specific knowledge from knowledge graphs, to enhance MT models. However,\na large amount of world knowledge is organized in unstructured documents, and\nmight not be fully paired across different languages. In this paper, we study\nretrieval-augmented MT using unstructured documents. Specifically, we build\nRAGtrans, the first benchmark to train and evaluate LLMs' retrieval-augmented\nMT ability. RAGtrans contains 169K MT samples collected via GPT-4o and human\ntranslators. Besides, documents from various languages are also provided to\nsupply the knowledge to these samples. Based on RAGtrans, we further propose a\nmulti-task training method to teach LLMs how to use information from\nmultilingual documents during their translation. The method uses existing\nmultilingual corpora to create auxiliary training objectives without additional\nlabeling requirements. Extensive experiments show that the method improves LLMs\nby 1.6-3.1 BLEU and 1.0-2.0 COMET scores in En-Zh, and 1.7-2.9 BLEU and 2.1-2.7\nCOMET scores in En-De. We also conclude the critical difficulties that current\nLLMs face with this task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) introduces additional information to\nenhance large language models (LLMs). In machine translation (MT), previous\nwork typically retrieves in-context examples from paired MT corpora, or\ndomain-specific knowledge from knowledge graphs, to enhance MT models. However,\na large amount of world knowledge is organized in unstructured documents, and\nmight not be fully paired across different languages. In this paper, we study\nretrieval-augmented MT using unstructured documents. Specifically, we build\nRAGtrans, the first benchmark to train and evaluate LLMs' retrieval-augmented\nMT ability. RAGtrans contains 169K MT samples collected via GPT-4o and human\ntranslators. Besides, documents from various languages are also provided to\nsupply the knowledge to these samples. Based on RAGtrans, we further propose a\nmulti-task training method to teach LLMs how to use information from\nmultilingual documents during their translation. The method uses existing\nmultilingual corpora to create auxiliary training objectives without additional\nlabeling requirements. Extensive experiments show that the method improves LLMs\nby 1.6-3.1 BLEU and 1.0-2.0 COMET scores in En-Zh, and 1.7-2.9 BLEU and 2.1-2.7\nCOMET scores in En-De. We also conclude the critical difficulties that current\nLLMs face with this task."
                },
                "authors": [
                    {
                        "name": "Jiaan Wang"
                    },
                    {
                        "name": "Fandong Meng"
                    },
                    {
                        "name": "Yingxue Zhang"
                    },
                    {
                        "name": "Jie Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhou"
                },
                "author": "Jie Zhou",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.20863v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.20863v2",
                "updated": "2025-08-29T09:37:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    37,
                    59,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-28T14:57:04Z",
                "published_parsed": [
                    2025,
                    8,
                    28,
                    14,
                    57,
                    4,
                    3,
                    240,
                    0
                ],
                "title": "Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Publish to Perish: Prompt Injection Attacks on LLM-Assisted Peer Review"
                },
                "summary": "Large Language Models (LLMs) are increasingly being integrated into the\nscientific peer-review process, raising new questions about their reliability\nand resilience to manipulation. In this work, we investigate the potential for\nhidden prompt injection attacks, where authors embed adversarial text within a\npaper's PDF to influence the LLM-generated review. We begin by formalising\nthree distinct threat models that envision attackers with different motivations\n-- not all of which implying malicious intent. For each threat model, we design\nadversarial prompts that remain invisible to human readers yet can steer an\nLLM's output toward the author's desired outcome. Using a user study with\ndomain scholars, we derive four representative reviewing prompts used to elicit\npeer reviews from LLMs. We then evaluate the robustness of our adversarial\nprompts across (i) different reviewing prompts, (ii) different commercial\nLLM-based systems, and (iii) different peer-reviewed papers. Our results show\nthat adversarial prompts can reliably mislead the LLM, sometimes in ways that\nadversely affect a \"honest-but-lazy\" reviewer. Finally, we propose and\nempirically assess methods to reduce detectability of adversarial prompts under\nautomated content checks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being integrated into the\nscientific peer-review process, raising new questions about their reliability\nand resilience to manipulation. In this work, we investigate the potential for\nhidden prompt injection attacks, where authors embed adversarial text within a\npaper's PDF to influence the LLM-generated review. We begin by formalising\nthree distinct threat models that envision attackers with different motivations\n-- not all of which implying malicious intent. For each threat model, we design\nadversarial prompts that remain invisible to human readers yet can steer an\nLLM's output toward the author's desired outcome. Using a user study with\ndomain scholars, we derive four representative reviewing prompts used to elicit\npeer reviews from LLMs. We then evaluate the robustness of our adversarial\nprompts across (i) different reviewing prompts, (ii) different commercial\nLLM-based systems, and (iii) different peer-reviewed papers. Our results show\nthat adversarial prompts can reliably mislead the LLM, sometimes in ways that\nadversely affect a \"honest-but-lazy\" reviewer. Finally, we propose and\nempirically assess methods to reduce detectability of adversarial prompts under\nautomated content checks."
                },
                "authors": [
                    {
                        "name": "Matteo Gioele Collu"
                    },
                    {
                        "name": "Umberto Salviati"
                    },
                    {
                        "name": "Roberto Confalonieri"
                    },
                    {
                        "name": "Mauro Conti"
                    },
                    {
                        "name": "Giovanni Apruzzese"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Apruzzese"
                },
                "author": "Giovanni Apruzzese",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.20863v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.20863v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21454v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21454v1",
                "updated": "2025-08-29T09:37:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    37,
                    42,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T09:37:42Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    37,
                    42,
                    4,
                    241,
                    0
                ],
                "title": "Enhancing Semantic Understanding in Pointer Analysis using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Semantic Understanding in Pointer Analysis using Large\n  Language Models"
                },
                "summary": "Pointer analysis has been studied for over four decades. However, existing\nframeworks continue to suffer from the propagation of incorrect facts. A major\nlimitation stems from their insufficient semantic understanding of code,\nresulting in overly conservative treatment of user-defined functions. Recent\nadvances in large language models (LLMs) present new opportunities to bridge\nthis gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a\nvision that integrates LLMs into pointer analysis to enhance both precision and\nscalability. LMPA identifies user-defined functions that resemble system APIs\nand models them accordingly, thereby mitigating erroneous cross-calling-context\npropagation. Furthermore, it enhances summary-based analysis by inferring\ninitial points-to sets and introducing a novel summary strategy augmented with\nnatural language. Finally, we discuss the key challenges involved in realizing\nthis vision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pointer analysis has been studied for over four decades. However, existing\nframeworks continue to suffer from the propagation of incorrect facts. A major\nlimitation stems from their insufficient semantic understanding of code,\nresulting in overly conservative treatment of user-defined functions. Recent\nadvances in large language models (LLMs) present new opportunities to bridge\nthis gap. In this paper, we propose LMPA (LLM-enhanced Pointer Analysis), a\nvision that integrates LLMs into pointer analysis to enhance both precision and\nscalability. LMPA identifies user-defined functions that resemble system APIs\nand models them accordingly, thereby mitigating erroneous cross-calling-context\npropagation. Furthermore, it enhances summary-based analysis by inferring\ninitial points-to sets and introducing a novel summary strategy augmented with\nnatural language. Finally, we discuss the key challenges involved in realizing\nthis vision."
                },
                "authors": [
                    {
                        "name": "Baijun Cheng"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Yao Guo"
                    },
                    {
                        "name": "Ding Li"
                    },
                    {
                        "name": "Xiangqun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiangqun Chen"
                },
                "author": "Xiangqun Chen",
                "arxiv_comment": "Accepted by LMPL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21454v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21454v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21452v1",
                "updated": "2025-08-29T09:36:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    36,
                    54,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T09:36:54Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    36,
                    54,
                    4,
                    241,
                    0
                ],
                "title": "From Canonical to Complex: Benchmarking LLM Capabilities in\n  Undergraduate Thermodynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Canonical to Complex: Benchmarking LLM Capabilities in\n  Undergraduate Thermodynamics"
                },
                "summary": "Large language models (LLMs) are increasingly considered as tutoring aids in\nscience education. Yet their readiness for unsupervised use in undergraduate\ninstruction remains uncertain, as reliable teaching requires more than fluent\nrecall: it demands consistent, principle-grounded reasoning. Thermodynamics,\nwith its compact laws and subtle distinctions between state and path functions,\nreversibility, and entropy, provides an ideal testbed for evaluating such\ncapabilities. Here we present UTQA, a 50-item undergraduate thermodynamics\nquestion answering benchmark, covering ideal-gas processes, reversibility, and\ndiagram interpretation. No leading 2025-era model exceeded our 95\\% competence\nthreshold: the best LLMs achieved 82\\% accuracy, with text-only items\nperforming better than image reasoning tasks, which often fell to chance\nlevels. Prompt phrasing and syntactic complexity showed modest to little\ncorrelation with performance. The gap concentrates in finite-rate/irreversible\nscenarios and in binding visual features to thermodynamic meaning, indicating\nthat current LLMs are not yet suitable for unsupervised tutoring in this\ndomain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly considered as tutoring aids in\nscience education. Yet their readiness for unsupervised use in undergraduate\ninstruction remains uncertain, as reliable teaching requires more than fluent\nrecall: it demands consistent, principle-grounded reasoning. Thermodynamics,\nwith its compact laws and subtle distinctions between state and path functions,\nreversibility, and entropy, provides an ideal testbed for evaluating such\ncapabilities. Here we present UTQA, a 50-item undergraduate thermodynamics\nquestion answering benchmark, covering ideal-gas processes, reversibility, and\ndiagram interpretation. No leading 2025-era model exceeded our 95\\% competence\nthreshold: the best LLMs achieved 82\\% accuracy, with text-only items\nperforming better than image reasoning tasks, which often fell to chance\nlevels. Prompt phrasing and syntactic complexity showed modest to little\ncorrelation with performance. The gap concentrates in finite-rate/irreversible\nscenarios and in binding visual features to thermodynamic meaning, indicating\nthat current LLMs are not yet suitable for unsupervised tutoring in this\ndomain."
                },
                "authors": [
                    {
                        "name": "Anna Geißler"
                    },
                    {
                        "name": "Luca-Sophie Bien"
                    },
                    {
                        "name": "Friedrich Schöppler"
                    },
                    {
                        "name": "Tobias Hertel"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Hertel"
                },
                "author": "Tobias Hertel",
                "arxiv_comment": "Benchmark downloadable at\n  https://huggingface.co/datasets/herteltm/UTQA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21448v1",
                "updated": "2025-08-29T09:27:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    27,
                    1,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T09:27:01Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    27,
                    1,
                    4,
                    241,
                    0
                ],
                "title": "Beyond the Surface: Probing the Ideological Depth of Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond the Surface: Probing the Ideological Depth of Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated pronounced ideological\nleanings, yet the stability and depth of these positions remain poorly\nunderstood. Surface-level responses can often be manipulated through simple\nprompt engineering, calling into question whether they reflect a coherent\nunderlying ideology. This paper investigates the concept of \"ideological depth\"\nin LLMs, defined as the robustness and complexity of their internal political\nrepresentations. We employ a dual approach: first, we measure the\n\"steerability\" of two well-known open-source LLMs using instruction prompting\nand activation steering. We find that while some models can easily switch\nbetween liberal and conservative viewpoints, others exhibit resistance or an\nincreased rate of refusal, suggesting a more entrenched ideological structure.\nSecond, we probe the internal mechanisms of these models using Sparse\nAutoencoders (SAEs). Preliminary analysis reveals that models with lower\nsteerability possess more distinct and abstract ideological features. Our\nevaluations reveal that one model can contain 7.3x more political features than\nanother model of similar size. This allows targeted ablation of a core\npolitical feature in an ideologically \"deep\" model, leading to consistent,\nlogical shifts in its reasoning across related topics, whereas the same\nintervention in a \"shallow\" model results in an increase in refusal outputs.\nOur findings suggest that ideological depth is a quantifiable property of LLMs\nand that steerability serves as a valuable window into their latent political\narchitecture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated pronounced ideological\nleanings, yet the stability and depth of these positions remain poorly\nunderstood. Surface-level responses can often be manipulated through simple\nprompt engineering, calling into question whether they reflect a coherent\nunderlying ideology. This paper investigates the concept of \"ideological depth\"\nin LLMs, defined as the robustness and complexity of their internal political\nrepresentations. We employ a dual approach: first, we measure the\n\"steerability\" of two well-known open-source LLMs using instruction prompting\nand activation steering. We find that while some models can easily switch\nbetween liberal and conservative viewpoints, others exhibit resistance or an\nincreased rate of refusal, suggesting a more entrenched ideological structure.\nSecond, we probe the internal mechanisms of these models using Sparse\nAutoencoders (SAEs). Preliminary analysis reveals that models with lower\nsteerability possess more distinct and abstract ideological features. Our\nevaluations reveal that one model can contain 7.3x more political features than\nanother model of similar size. This allows targeted ablation of a core\npolitical feature in an ideologically \"deep\" model, leading to consistent,\nlogical shifts in its reasoning across related topics, whereas the same\nintervention in a \"shallow\" model results in an increase in refusal outputs.\nOur findings suggest that ideological depth is a quantifiable property of LLMs\nand that steerability serves as a valuable window into their latent political\narchitecture."
                },
                "authors": [
                    {
                        "name": "Shariar Kabir"
                    },
                    {
                        "name": "Kevin Esterling"
                    },
                    {
                        "name": "Yue Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yue Dong"
                },
                "author": "Yue Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.12072v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.12072v2",
                "updated": "2025-08-29T09:25:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    25,
                    55,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-05T01:48:09Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    1,
                    48,
                    9,
                    3,
                    156,
                    0
                ],
                "title": "TrueGL: A Truthful, Reliable, and Unified Engine for Grounded Learning\n  in Full-Stack Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrueGL: A Truthful, Reliable, and Unified Engine for Grounded Learning\n  in Full-Stack Search"
                },
                "summary": "In the age of open and free information, a concerning trend of reliance on AI\nis emerging. However, existing AI tools struggle to evaluate the credibility of\ninformation and to justify their assessments. Hence, there is a growing need\nfor systems that can help users evaluate the trustworthiness of online\ninformation. Although major search engines incorporate AI features, they often\nlack clear reliability indicators. We present TrueGL, a model that makes\ntrustworthy search results more accessible. The model is a fine-tuned version\nof IBM's Granite-1B, trained on the custom dataset and integrated into a search\nengine with a reliability scoring system. We evaluate the system using prompt\nengineering and assigning each statement a continuous reliability score from\n0.1 to 1, then instructing the model to return a textual explanation alongside\nthe score. Each model's predicted scores are measured against real scores using\nstandard evaluation metrics. TrueGL consistently outperforms other small-scale\nLLMs and rule-based approaches across all experiments on key evaluation\nmetrics, including MAE, RMSE, and R2. The model's high accuracy, broad content\ncoverage, and ease of use make trustworthy information more accessible and help\nreduce the spread of false or misleading content online. Our code is publicly\navailable at https://github.com/AlgazinovAleksandr/TrueGL, and our model is\npublicly released at https://huggingface.co/JoydeepC/trueGL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the age of open and free information, a concerning trend of reliance on AI\nis emerging. However, existing AI tools struggle to evaluate the credibility of\ninformation and to justify their assessments. Hence, there is a growing need\nfor systems that can help users evaluate the trustworthiness of online\ninformation. Although major search engines incorporate AI features, they often\nlack clear reliability indicators. We present TrueGL, a model that makes\ntrustworthy search results more accessible. The model is a fine-tuned version\nof IBM's Granite-1B, trained on the custom dataset and integrated into a search\nengine with a reliability scoring system. We evaluate the system using prompt\nengineering and assigning each statement a continuous reliability score from\n0.1 to 1, then instructing the model to return a textual explanation alongside\nthe score. Each model's predicted scores are measured against real scores using\nstandard evaluation metrics. TrueGL consistently outperforms other small-scale\nLLMs and rule-based approaches across all experiments on key evaluation\nmetrics, including MAE, RMSE, and R2. The model's high accuracy, broad content\ncoverage, and ease of use make trustworthy information more accessible and help\nreduce the spread of false or misleading content online. Our code is publicly\navailable at https://github.com/AlgazinovAleksandr/TrueGL, and our model is\npublicly released at https://huggingface.co/JoydeepC/trueGL."
                },
                "authors": [
                    {
                        "name": "Joydeep Chandra"
                    },
                    {
                        "name": "Aleksandr Algazinov"
                    },
                    {
                        "name": "Satyam Kumar Navneet"
                    },
                    {
                        "name": "Rim El Filali"
                    },
                    {
                        "name": "Matt Laing"
                    },
                    {
                        "name": "Andrew Hanna"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Hanna"
                },
                "author": "Andrew Hanna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.12072v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.12072v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14179v2",
                "updated": "2025-08-29T09:14:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    14,
                    59,
                    4,
                    241,
                    0
                ],
                "published": "2025-01-24T02:12:08Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    2,
                    12,
                    8,
                    4,
                    24,
                    0
                ],
                "title": "AI Chatbots as Professional Service Agents: Developing a Professional\n  Identity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Chatbots as Professional Service Agents: Developing a Professional\n  Identity"
                },
                "summary": "With the rapid expansion of large language model (LLM) applications, there is\nan emerging shift in the role of LLM-based AI chatbots from serving merely as\ngeneral inquiry tools to acting as professional service agents. However,\ncurrent studies often overlook a critical aspect of professional service\nagents: the act of communicating in a manner consistent with their professional\nidentities. This is of particular importance in the healthcare sector, where\neffective communication with patients is essential for achieving professional\ngoals, such as promoting patient well-being by encouraging healthy behaviors.\nTo bridge this gap, we propose LAPI (LLM-based Agent with a Professional\nIdentity), a novel framework for designing professional service agent tailored\nfor medical question-and-answer (Q\\&A) services, ensuring alignment with a\nspecific professional identity. Our method includes a theory-guided task\nplanning process that decomposes complex professional tasks into manageable\nsubtasks aligned with professional objectives and a pragmatic entropy method\ndesigned to generate professional and ethical responses with low uncertainty.\nExperiments on various LLMs show that the proposed approach outperforms\nbaseline methods, including few-shot prompting, chain-of-thought prompting,\nacross key metrics such as fluency, naturalness, empathy, patient-centricity,\nand ROUGE-L scores. Additionally, the ablation study underscores the\ncontribution of each component to the overall effectiveness of the approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid expansion of large language model (LLM) applications, there is\nan emerging shift in the role of LLM-based AI chatbots from serving merely as\ngeneral inquiry tools to acting as professional service agents. However,\ncurrent studies often overlook a critical aspect of professional service\nagents: the act of communicating in a manner consistent with their professional\nidentities. This is of particular importance in the healthcare sector, where\neffective communication with patients is essential for achieving professional\ngoals, such as promoting patient well-being by encouraging healthy behaviors.\nTo bridge this gap, we propose LAPI (LLM-based Agent with a Professional\nIdentity), a novel framework for designing professional service agent tailored\nfor medical question-and-answer (Q\\&A) services, ensuring alignment with a\nspecific professional identity. Our method includes a theory-guided task\nplanning process that decomposes complex professional tasks into manageable\nsubtasks aligned with professional objectives and a pragmatic entropy method\ndesigned to generate professional and ethical responses with low uncertainty.\nExperiments on various LLMs show that the proposed approach outperforms\nbaseline methods, including few-shot prompting, chain-of-thought prompting,\nacross key metrics such as fluency, naturalness, empathy, patient-centricity,\nand ROUGE-L scores. Additionally, the ablation study underscores the\ncontribution of each component to the overall effectiveness of the approach."
                },
                "authors": [
                    {
                        "name": "Wenwen Li"
                    },
                    {
                        "name": "Kangwei Shi"
                    },
                    {
                        "name": "Yidong Chai"
                    }
                ],
                "author_detail": {
                    "name": "Yidong Chai"
                },
                "author": "Yidong Chai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04104v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04104v2",
                "updated": "2025-08-29T09:07:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    7,
                    22,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-05T08:31:10Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    8,
                    31,
                    10,
                    5,
                    95,
                    0
                ],
                "title": "SpecPipe: Accelerating Pipeline Parallelism-based LLM Inference with\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecPipe: Accelerating Pipeline Parallelism-based LLM Inference with\n  Speculative Decoding"
                },
                "summary": "The demand for large language model inference is rapidly increasing. Pipeline\nparallelism offers a cost-effective deployment strategy for distributed\ninference but suffers from high service latency. While incorporating\nspeculative decoding to pipeline parallelism improves performance, it still\nfaces challenges of low hardware utilization and narrow speculative window.\nInspired by branch prediction in instruction pipelining, we introduce SpecPipe,\nwhich fills the pipeline with speculative tokens of a request step-by-step. By\nmaximizing the hardware utilization, SpecPipe decodes one token per pipeline\nstep ideally. Specifically, SpecPipe comprises a dynamic speculative token tree\nand a pipelined inference framework. The tree dynamically accepts tokens from a\nspeculative token source and outputs the tokens to the inference pipeline.\nSince the speculative window relaxed in our framework, a high-accuracy draft\nmodel is integrated without fine-tuning. The pipeline inference framework\nfollows node-wise computation, pruning propagation, and inter-node\ncommunication stages. We implement SpecPipe and a variant SpecPipe-DB with\ndynamic batching for single- and multi-request inference, respectively. On an\n8-stage pipeline, SpecPipe improves time between tokens on diverse\nsingle-request workloads by $4.19\\times$-$5.53\\times$ over standard pipeline\nparallelism and by $2.08\\times$-$2.38\\times$ over prior tree-based speculative\ndecoding methods. For multi-request workloads, SpecPipe-DB achieves\n$1.64\\times$-$2.08\\times$ higher throughput and $1.61\\times$-$2.06\\times$ lower\ntime between tokens than vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The demand for large language model inference is rapidly increasing. Pipeline\nparallelism offers a cost-effective deployment strategy for distributed\ninference but suffers from high service latency. While incorporating\nspeculative decoding to pipeline parallelism improves performance, it still\nfaces challenges of low hardware utilization and narrow speculative window.\nInspired by branch prediction in instruction pipelining, we introduce SpecPipe,\nwhich fills the pipeline with speculative tokens of a request step-by-step. By\nmaximizing the hardware utilization, SpecPipe decodes one token per pipeline\nstep ideally. Specifically, SpecPipe comprises a dynamic speculative token tree\nand a pipelined inference framework. The tree dynamically accepts tokens from a\nspeculative token source and outputs the tokens to the inference pipeline.\nSince the speculative window relaxed in our framework, a high-accuracy draft\nmodel is integrated without fine-tuning. The pipeline inference framework\nfollows node-wise computation, pruning propagation, and inter-node\ncommunication stages. We implement SpecPipe and a variant SpecPipe-DB with\ndynamic batching for single- and multi-request inference, respectively. On an\n8-stage pipeline, SpecPipe improves time between tokens on diverse\nsingle-request workloads by $4.19\\times$-$5.53\\times$ over standard pipeline\nparallelism and by $2.08\\times$-$2.38\\times$ over prior tree-based speculative\ndecoding methods. For multi-request workloads, SpecPipe-DB achieves\n$1.64\\times$-$2.08\\times$ higher throughput and $1.61\\times$-$2.06\\times$ lower\ntime between tokens than vLLM."
                },
                "authors": [
                    {
                        "name": "Haofei Yin"
                    },
                    {
                        "name": "Mengbai Xiao"
                    },
                    {
                        "name": "Tinghong Li"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Dongxiao Yu"
                    },
                    {
                        "name": "Guanghui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Guanghui Zhang"
                },
                "author": "Guanghui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04104v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04104v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21433v1",
                "updated": "2025-08-29T09:02:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    2,
                    35,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T09:02:35Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    2,
                    35,
                    4,
                    241,
                    0
                ],
                "title": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM\n  Summarization for Agent Context Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Complexity Trap: Simple Observation Masking Is as Efficient as LLM\n  Summarization for Agent Context Management"
                },
                "summary": "Large Language Model (LLM)-based agents solve complex tasks through iterative\nreasoning, exploration, and tool-use, a process that can result in long,\nexpensive context histories. While state-of-the-art Software Engineering ( SE)\nagents like OpenHands or Cursor use LLM-based summarization to tackle this\nissue, it is unclear whether the increased complexity offers tangible\nperformance benefits compared to simply omitting older observations. We present\na systematic comparison of these strategies within SWE-agent on SWE-bench\nVerified across five diverse model configurations. We find that a simple\nobservation-masking strategy halves cost relative to a raw agent while\nmatching, and sometimes slightly exceeding, the solve rate of LLM\nsummarization. For example, with Qwen3-Coder 480B, masking improves solve rate\nfrom 53.8% (raw agent) to 54.8%, while remaining competitive with summarization\nat a lower cost. These results suggest that, at least within SWE-agent on\nSWE-bench Verified, the most effective and efficient context management can be\nthe simplest. We release code and data for reproducibility",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based agents solve complex tasks through iterative\nreasoning, exploration, and tool-use, a process that can result in long,\nexpensive context histories. While state-of-the-art Software Engineering ( SE)\nagents like OpenHands or Cursor use LLM-based summarization to tackle this\nissue, it is unclear whether the increased complexity offers tangible\nperformance benefits compared to simply omitting older observations. We present\na systematic comparison of these strategies within SWE-agent on SWE-bench\nVerified across five diverse model configurations. We find that a simple\nobservation-masking strategy halves cost relative to a raw agent while\nmatching, and sometimes slightly exceeding, the solve rate of LLM\nsummarization. For example, with Qwen3-Coder 480B, masking improves solve rate\nfrom 53.8% (raw agent) to 54.8%, while remaining competitive with summarization\nat a lower cost. These results suggest that, at least within SWE-agent on\nSWE-bench Verified, the most effective and efficient context management can be\nthe simplest. We release code and data for reproducibility"
                },
                "authors": [
                    {
                        "name": "Tobias Lindenbauer"
                    },
                    {
                        "name": "Igor Slinko"
                    },
                    {
                        "name": "Ludwig Felder"
                    },
                    {
                        "name": "Egor Bogomolov"
                    },
                    {
                        "name": "Yaroslav Zharov"
                    }
                ],
                "author_detail": {
                    "name": "Yaroslav Zharov"
                },
                "author": "Yaroslav Zharov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21432v1",
                "updated": "2025-08-29T09:01:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    1,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T09:01:34Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    9,
                    1,
                    34,
                    4,
                    241,
                    0
                ],
                "title": "RepoMark: A Code Usage Auditing Framework for Code Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RepoMark: A Code Usage Auditing Framework for Code Large Language Models"
                },
                "summary": "The rapid development of Large Language Models (LLMs) for code generation has\ntransformed software development by automating coding tasks with unprecedented\nefficiency.\n  However, the training of these models on open-source code repositories (e.g.,\nfrom GitHub) raises critical ethical and legal concerns, particularly regarding\ndata authorization and open-source license compliance. Developers are\nincreasingly questioning whether model trainers have obtained proper\nauthorization before using repositories for training, especially given the lack\nof transparency in data collection.\n  To address these concerns, we propose a novel data marking framework RepoMark\nto audit the data usage of code LLMs. Our method enables repository owners to\nverify whether their code has been used in training, while ensuring semantic\npreservation, imperceptibility, and theoretical false detection rate (FDR)\nguarantees. By generating multiple semantically equivalent code variants,\nRepoMark introduces data marks into the code files, and during detection,\nRepoMark leverages a novel ranking-based hypothesis test to detect memorization\nwithin the model. Compared to prior data auditing approaches, RepoMark\nsignificantly enhances sample efficiency, allowing effective auditing even when\nthe user's repository possesses only a small number of code files.\n  Experiments demonstrate that RepoMark achieves a detection success rate over\n90\\% on small code repositories under a strict FDR guarantee of 5\\%. This\nrepresents a significant advancement over existing data marking techniques, all\nof which only achieve accuracy below 55\\% under identical settings. This\nfurther validates RepoMark as a robust, theoretically sound, and promising\nsolution for enhancing transparency in code LLM training, which can safeguard\nthe rights of repository owners.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) for code generation has\ntransformed software development by automating coding tasks with unprecedented\nefficiency.\n  However, the training of these models on open-source code repositories (e.g.,\nfrom GitHub) raises critical ethical and legal concerns, particularly regarding\ndata authorization and open-source license compliance. Developers are\nincreasingly questioning whether model trainers have obtained proper\nauthorization before using repositories for training, especially given the lack\nof transparency in data collection.\n  To address these concerns, we propose a novel data marking framework RepoMark\nto audit the data usage of code LLMs. Our method enables repository owners to\nverify whether their code has been used in training, while ensuring semantic\npreservation, imperceptibility, and theoretical false detection rate (FDR)\nguarantees. By generating multiple semantically equivalent code variants,\nRepoMark introduces data marks into the code files, and during detection,\nRepoMark leverages a novel ranking-based hypothesis test to detect memorization\nwithin the model. Compared to prior data auditing approaches, RepoMark\nsignificantly enhances sample efficiency, allowing effective auditing even when\nthe user's repository possesses only a small number of code files.\n  Experiments demonstrate that RepoMark achieves a detection success rate over\n90\\% on small code repositories under a strict FDR guarantee of 5\\%. This\nrepresents a significant advancement over existing data marking techniques, all\nof which only achieve accuracy below 55\\% under identical settings. This\nfurther validates RepoMark as a robust, theoretically sound, and promising\nsolution for enhancing transparency in code LLM training, which can safeguard\nthe rights of repository owners."
                },
                "authors": [
                    {
                        "name": "Wenjie Qu"
                    },
                    {
                        "name": "Yuguang Zhou"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Wengrui Zheng"
                    },
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Jinyuan Jia"
                    },
                    {
                        "name": "Jiaheng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiaheng Zhang"
                },
                "author": "Jiaheng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.08420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.08420v2",
                "updated": "2025-08-29T08:56:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    56,
                    49,
                    4,
                    241,
                    0
                ],
                "published": "2024-03-13T11:11:59Z",
                "published_parsed": [
                    2024,
                    3,
                    13,
                    11,
                    11,
                    59,
                    2,
                    73,
                    0
                ],
                "title": "ALow-Cost Real-Time Framework for Industrial Action Recognition Using\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALow-Cost Real-Time Framework for Industrial Action Recognition Using\n  Foundation Models"
                },
                "summary": "Action recognition (AR) in industrial environments -- particularly for\nidentifying actions and operational gestures -- faces persistent challenges due\nto high deployment costs, poor cross-scenario generalization, and limited\nreal-time performance. To address these issues, we propose a low-cost real-time\nframework for industrial action recognition using foundation models, denoted as\nLRIAR, to enhance recognition accuracy and transferability while minimizing\nhuman annotation and computational overhead. The proposed framework constructs\nan automatically labeled dataset by coupling Grounding DINO with the pretrained\nBLIP-2 image encoder, enabling efficient and scalable action labeling.\nLeveraging the constructed dataset, we train YOLOv5 for real-time action\ndetection, and a Vision Transformer (ViT) classifier is deceloped via\nLoRA-based fine-tuning for action classification. Extensive experiments\nconducted in real-world industrial settings validate the effectiveness of\nLRIAR, demonstrating consistent improvements over state-of-the-art methods in\nrecognition accuracy, scenario generalization, and deployment efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Action recognition (AR) in industrial environments -- particularly for\nidentifying actions and operational gestures -- faces persistent challenges due\nto high deployment costs, poor cross-scenario generalization, and limited\nreal-time performance. To address these issues, we propose a low-cost real-time\nframework for industrial action recognition using foundation models, denoted as\nLRIAR, to enhance recognition accuracy and transferability while minimizing\nhuman annotation and computational overhead. The proposed framework constructs\nan automatically labeled dataset by coupling Grounding DINO with the pretrained\nBLIP-2 image encoder, enabling efficient and scalable action labeling.\nLeveraging the constructed dataset, we train YOLOv5 for real-time action\ndetection, and a Vision Transformer (ViT) classifier is deceloped via\nLoRA-based fine-tuning for action classification. Extensive experiments\nconducted in real-world industrial settings validate the effectiveness of\nLRIAR, demonstrating consistent improvements over state-of-the-art methods in\nrecognition accuracy, scenario generalization, and deployment efficiency."
                },
                "authors": [
                    {
                        "name": "Zhicheng Wang"
                    },
                    {
                        "name": "Wensheng Liang"
                    },
                    {
                        "name": "Ruiyan Zhuang"
                    },
                    {
                        "name": "Shuai Li"
                    },
                    {
                        "name": "Jianwei Tan"
                    },
                    {
                        "name": "Xiaoguang Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoguang Ma"
                },
                "author": "Xiaoguang Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.08420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.08420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16936v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16936v2",
                "updated": "2025-08-29T08:56:06Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    56,
                    6,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-23T08:05:37Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    8,
                    5,
                    37,
                    5,
                    235,
                    0
                ],
                "title": "THEME: Enhancing Thematic Investing with Semantic Stock Representations\n  and Temporal Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THEME: Enhancing Thematic Investing with Semantic Stock Representations\n  and Temporal Dynamics"
                },
                "summary": "Thematic investing, which aims to construct portfolios aligned with\nstructural trends, remains a challenging endeavor due to overlapping sector\nboundaries and evolving market dynamics. A promising direction is to build\nsemantic representations of investment themes from textual data. However,\ndespite their power, general-purpose LLM embedding models are not well-suited\nto capture the nuanced characteristics of financial assets, since the semantic\nrepresentation of investment assets may differ fundamentally from that of\ngeneral financial text. To address this, we introduce THEME, a framework that\nfine-tunes embeddings using hierarchical contrastive learning. THEME aligns\nthemes and their constituent stocks using their hierarchical relationship, and\nsubsequently refines these embeddings by incorporating stock returns. This\nprocess yields representations effective for retrieving thematically aligned\nassets with strong return potential. Empirical results demonstrate that THEME\nexcels in two key areas. For thematic asset retrieval, it significantly\noutperforms leading large language models. Furthermore, its constructed\nportfolios demonstrate compelling performance. By jointly modeling thematic\nrelationships from text and market dynamics from returns, THEME generates stock\nembeddings specifically tailored for a wide range of practical investment\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thematic investing, which aims to construct portfolios aligned with\nstructural trends, remains a challenging endeavor due to overlapping sector\nboundaries and evolving market dynamics. A promising direction is to build\nsemantic representations of investment themes from textual data. However,\ndespite their power, general-purpose LLM embedding models are not well-suited\nto capture the nuanced characteristics of financial assets, since the semantic\nrepresentation of investment assets may differ fundamentally from that of\ngeneral financial text. To address this, we introduce THEME, a framework that\nfine-tunes embeddings using hierarchical contrastive learning. THEME aligns\nthemes and their constituent stocks using their hierarchical relationship, and\nsubsequently refines these embeddings by incorporating stock returns. This\nprocess yields representations effective for retrieving thematically aligned\nassets with strong return potential. Empirical results demonstrate that THEME\nexcels in two key areas. For thematic asset retrieval, it significantly\noutperforms leading large language models. Furthermore, its constructed\nportfolios demonstrate compelling performance. By jointly modeling thematic\nrelationships from text and market dynamics from returns, THEME generates stock\nembeddings specifically tailored for a wide range of practical investment\napplications."
                },
                "authors": [
                    {
                        "name": "Hoyoung Lee"
                    },
                    {
                        "name": "Wonbin Ahn"
                    },
                    {
                        "name": "Suhwan Park"
                    },
                    {
                        "name": "Jaehoon Lee"
                    },
                    {
                        "name": "Minjae Kim"
                    },
                    {
                        "name": "Sungdong Yoo"
                    },
                    {
                        "name": "Taeyoon Lim"
                    },
                    {
                        "name": "Woohyung Lim"
                    },
                    {
                        "name": "Yongjae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yongjae Lee"
                },
                "author": "Yongjae Lee",
                "arxiv_doi": "10.1145/3746252.3761517",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761517",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.16936v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16936v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at ACM International Conference on Information and Knowledge\n  Management (CIKM)",
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.12683v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.12683v4",
                "updated": "2025-08-29T08:56:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    56,
                    1,
                    4,
                    241,
                    0
                ],
                "published": "2024-02-20T03:14:47Z",
                "published_parsed": [
                    2024,
                    2,
                    20,
                    3,
                    14,
                    47,
                    1,
                    51,
                    0
                ],
                "title": "TorchCP: A Python Library for Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TorchCP: A Python Library for Conformal Prediction"
                },
                "summary": "Conformal prediction (CP) is a powerful statistical framework that generates\nprediction intervals or sets with guaranteed coverage probability. While CP\nalgorithms have evolved beyond traditional classifiers and regressors to\nsophisticated deep learning models like deep neural networks (DNNs), graph\nneural networks (GNNs), and large language models (LLMs), existing CP libraries\noften lack the model support and scalability for large-scale DL scenarios. This\npaper introduces TorchCP, a PyTorch-native library designed to integrate\nstate-of-the-art CP algorithms into deep learning techniques, including\nDNN-based classifier/regressor, GNN, and LLM. Released under the LGPL-3.0\nlicense, TorchCP comprises about 16k lines of code, validated with 100% unit\ntest coverage and detailed documentation. Notably, TorchCP enables CP-specific\ntraining algorithms, online prediction, and GPU-accelerated batch processing,\nachieving up to 90% reduction in inference time on large datasets. With its\nlow-coupling design, comprehensive suite of advanced methods, and full GPU\nscalability, TorchCP empowers researchers and practitioners to enhance\nuncertainty quantification across cutting-edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conformal prediction (CP) is a powerful statistical framework that generates\nprediction intervals or sets with guaranteed coverage probability. While CP\nalgorithms have evolved beyond traditional classifiers and regressors to\nsophisticated deep learning models like deep neural networks (DNNs), graph\nneural networks (GNNs), and large language models (LLMs), existing CP libraries\noften lack the model support and scalability for large-scale DL scenarios. This\npaper introduces TorchCP, a PyTorch-native library designed to integrate\nstate-of-the-art CP algorithms into deep learning techniques, including\nDNN-based classifier/regressor, GNN, and LLM. Released under the LGPL-3.0\nlicense, TorchCP comprises about 16k lines of code, validated with 100% unit\ntest coverage and detailed documentation. Notably, TorchCP enables CP-specific\ntraining algorithms, online prediction, and GPU-accelerated batch processing,\nachieving up to 90% reduction in inference time on large datasets. With its\nlow-coupling design, comprehensive suite of advanced methods, and full GPU\nscalability, TorchCP empowers researchers and practitioners to enhance\nuncertainty quantification across cutting-edge applications."
                },
                "authors": [
                    {
                        "name": "Jianguo Huang"
                    },
                    {
                        "name": "Jianqing Song"
                    },
                    {
                        "name": "Xuanning Zhou"
                    },
                    {
                        "name": "Bingyi Jing"
                    },
                    {
                        "name": "Hongxin Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hongxin Wei"
                },
                "author": "Hongxin Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.12683v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.12683v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21422v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21422v1",
                "updated": "2025-08-29T08:48:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    48,
                    0,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T08:48:00Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    48,
                    0,
                    4,
                    241,
                    0
                ],
                "title": "Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers:\n  A New Counterfactual Evaluation Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Reviewers Fail to Detect Faulty Reasoning in Research Papers:\n  A New Counterfactual Evaluation Framework"
                },
                "summary": "Large Language Models (LLMs) have great potential to accelerate and support\nscholarly peer review and are increasingly used as fully automatic review\ngenerators (ARGs). However, potential biases and systematic errors may pose\nsignificant risks to scientific integrity; understanding the specific\ncapabilities and limitations of state-of-the-art ARGs is essential. We focus on\na core reviewing skill that underpins high-quality peer review: detecting\nfaulty research logic. This involves evaluating the internal consistency\nbetween a paper's results, interpretations, and claims. We present a fully\nautomated counterfactual evaluation framework that isolates and tests this\nskill under controlled conditions. Testing a range of ARG approaches, we find\nthat, contrary to expectation, flaws in research logic have no significant\neffect on their output reviews. Based on our findings, we derive three\nactionable recommendations for future work and release our counterfactual\ndataset and evaluation framework publicly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have great potential to accelerate and support\nscholarly peer review and are increasingly used as fully automatic review\ngenerators (ARGs). However, potential biases and systematic errors may pose\nsignificant risks to scientific integrity; understanding the specific\ncapabilities and limitations of state-of-the-art ARGs is essential. We focus on\na core reviewing skill that underpins high-quality peer review: detecting\nfaulty research logic. This involves evaluating the internal consistency\nbetween a paper's results, interpretations, and claims. We present a fully\nautomated counterfactual evaluation framework that isolates and tests this\nskill under controlled conditions. Testing a range of ARG approaches, we find\nthat, contrary to expectation, flaws in research logic have no significant\neffect on their output reviews. Based on our findings, we derive three\nactionable recommendations for future work and release our counterfactual\ndataset and evaluation framework publicly."
                },
                "authors": [
                    {
                        "name": "Nils Dycke"
                    },
                    {
                        "name": "Iryna Gurevych"
                    }
                ],
                "author_detail": {
                    "name": "Iryna Gurevych"
                },
                "author": "Iryna Gurevych",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21422v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21422v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21417v1",
                "updated": "2025-08-29T08:38:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    38,
                    58,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T08:38:58Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    38,
                    58,
                    4,
                    241,
                    0
                ],
                "title": "An Empirical Study of Vulnerable Package Dependencies in LLM\n  Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Vulnerable Package Dependencies in LLM\n  Repositories"
                },
                "summary": "Large language models (LLMs) have developed rapidly in recent years,\nrevolutionizing various fields. Despite their widespread success, LLMs heavily\nrely on external code dependencies from package management systems, creating a\ncomplex and interconnected LLM dependency supply chain. Vulnerabilities in\ndependencies can expose LLMs to security risks. While existing research\npredominantly focuses on model-level security threats, vulnerabilities within\nthe LLM dependency supply chain have been overlooked. To fill this gap, we\nconducted an empirical analysis of 52 open-source LLMs, examining their\nthird-party dependencies and associated vulnerabilities. We then explored\nactivities within the LLM repositories to understand how maintainers manage\nthird-party vulnerabilities in practice. Finally, we compared third-party\ndependency vulnerabilities in the LLM ecosystem to those in the Python\necosystem. Our results show that half of the vulnerabilities in the LLM\necosystem remain undisclosed for more than 56.2 months, significantly longer\nthan those in the Python ecosystem. Additionally, 75.8% of LLMs include\nvulnerable dependencies in their configuration files. This study advances the\nunderstanding of LLM supply chain risks, provides insights for practitioners,\nand highlights potential directions for improving the security of the LLM\nsupply chain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have developed rapidly in recent years,\nrevolutionizing various fields. Despite their widespread success, LLMs heavily\nrely on external code dependencies from package management systems, creating a\ncomplex and interconnected LLM dependency supply chain. Vulnerabilities in\ndependencies can expose LLMs to security risks. While existing research\npredominantly focuses on model-level security threats, vulnerabilities within\nthe LLM dependency supply chain have been overlooked. To fill this gap, we\nconducted an empirical analysis of 52 open-source LLMs, examining their\nthird-party dependencies and associated vulnerabilities. We then explored\nactivities within the LLM repositories to understand how maintainers manage\nthird-party vulnerabilities in practice. Finally, we compared third-party\ndependency vulnerabilities in the LLM ecosystem to those in the Python\necosystem. Our results show that half of the vulnerabilities in the LLM\necosystem remain undisclosed for more than 56.2 months, significantly longer\nthan those in the Python ecosystem. Additionally, 75.8% of LLMs include\nvulnerable dependencies in their configuration files. This study advances the\nunderstanding of LLM supply chain risks, provides insights for practitioners,\nand highlights potential directions for improving the security of the LLM\nsupply chain."
                },
                "authors": [
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Xin Xia"
                    },
                    {
                        "name": "David Lo"
                    },
                    {
                        "name": "Xiaohu Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaohu Yang"
                },
                "author": "Xiaohu Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.15780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.15780v2",
                "updated": "2025-08-29T08:38:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    38,
                    35,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-22T10:45:23Z",
                "published_parsed": [
                    2025,
                    4,
                    22,
                    10,
                    45,
                    23,
                    1,
                    112,
                    0
                ],
                "title": "TrustGeoGen: Formal-Verified Data Engine for Trustworthy Multi-modal\n  Geometric Problem Solving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustGeoGen: Formal-Verified Data Engine for Trustworthy Multi-modal\n  Geometric Problem Solving"
                },
                "summary": "Mathematical geometric problem solving (GPS) demands verifiable logical\ncoherence and multimodal reasoning capabilities. While large language models\n(LLMs) have shown rapid progress in GPS, their advancement is hindered by the\nlack of reliable benchmarks and systematic methodologies. A critical challenge\nis the inherent hallucination in LLMs, which leads to synthetic GPS datasets\nthat are often noisy, unverified, and self-contradictory. To address this, we\nintroduce TrustGeoGen, a data engine that generates formally verified geometric\nproblems to establish a principled and trustworthy benchmark. Our engine\nintegrates four key innovations: 1) Multimodal Alignment, which synchronizes\nthe generation of diagrams, text, and step-by-step solutions; 2) Formal\nVerification, ensuring all reasoning paths are rule-compliant; 3) Connection\nThinking, bridging formal deduction with human-like logical steps; and 4) our\n\\textit{GeoExplore} series algorithms, which produce diverse problem variants\nwith multiple solutions and self-reflective backtracking. Using this engine, we\ncreate the GeoTrust-200K dataset and the corresponding GeoTrust-test benchmark,\nboth with guaranteed cross-modal integrity. Experiments reveal that\nstate-of-the-art models achieve only 45.83\\% accuracy on GeoTrust-test,\nhighlighting its significant challenge. Furthermore, training on our\nsynthesized data substantially improves model performance on GPS tasks, with\nstrong generalization to out-of-domain (OOD) benchmarks. Our code and data are\navailable at https://github.com/Alpha-Innovator/TrustGeoGen",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical geometric problem solving (GPS) demands verifiable logical\ncoherence and multimodal reasoning capabilities. While large language models\n(LLMs) have shown rapid progress in GPS, their advancement is hindered by the\nlack of reliable benchmarks and systematic methodologies. A critical challenge\nis the inherent hallucination in LLMs, which leads to synthetic GPS datasets\nthat are often noisy, unverified, and self-contradictory. To address this, we\nintroduce TrustGeoGen, a data engine that generates formally verified geometric\nproblems to establish a principled and trustworthy benchmark. Our engine\nintegrates four key innovations: 1) Multimodal Alignment, which synchronizes\nthe generation of diagrams, text, and step-by-step solutions; 2) Formal\nVerification, ensuring all reasoning paths are rule-compliant; 3) Connection\nThinking, bridging formal deduction with human-like logical steps; and 4) our\n\\textit{GeoExplore} series algorithms, which produce diverse problem variants\nwith multiple solutions and self-reflective backtracking. Using this engine, we\ncreate the GeoTrust-200K dataset and the corresponding GeoTrust-test benchmark,\nboth with guaranteed cross-modal integrity. Experiments reveal that\nstate-of-the-art models achieve only 45.83\\% accuracy on GeoTrust-test,\nhighlighting its significant challenge. Furthermore, training on our\nsynthesized data substantially improves model performance on GPS tasks, with\nstrong generalization to out-of-domain (OOD) benchmarks. Our code and data are\navailable at https://github.com/Alpha-Innovator/TrustGeoGen"
                },
                "authors": [
                    {
                        "name": "Daocheng Fu"
                    },
                    {
                        "name": "Jianlong Chen"
                    },
                    {
                        "name": "Renqiu Xia"
                    },
                    {
                        "name": "Zijun Chen"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Hongbin Zhou"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Shiyang Feng"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Hongyuan Zha"
                    },
                    {
                        "name": "Junchi Yan"
                    },
                    {
                        "name": "Botian Shi"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Bo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhang"
                },
                "author": "Bo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.15780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.15780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21394v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21394v1",
                "updated": "2025-08-29T08:14:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    14,
                    45,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T08:14:45Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    14,
                    45,
                    4,
                    241,
                    0
                ],
                "title": "AI Compute Architecture and Evolution Trends",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Compute Architecture and Evolution Trends"
                },
                "summary": "The focus of AI development has shifted from academic research to practical\napplications. However, AI development faces numerous challenges at various\nlevels. This article will attempt to analyze the opportunities and challenges\nof AI from several different perspectives using a structured approach. This\narticle proposes a seven-layer model for AI compute architecture, including\nPhysical Layer, Link Layer, Neural Network Layer, Context Layer, Agent Layer,\nOrchestrator Layer, and Application Layer, from bottom to top. It also explains\nhow AI computing has evolved into this 7-layer architecture through the\nthree-stage evolution on large-scale language models (LLMs). For each layer, we\ndescribe the development trajectory and key technologies. In Layers 1 and 2 we\ndiscuss AI computing issues and the impact of Scale-Up and Scale-Out strategies\non computing architecture. In Layer 3 we explore two different development\npaths for LLMs. In Layer 4 we discuss the impact of contextual memory on LLMs\nand compares it to traditional processor memory. In Layers 5 to 7 we discuss\nthe trends of AI agents and explore the issues in evolution from a single AI\nagent to an AI-based ecosystem, and their impact on the AI industry.\nFurthermore, AI development involves not only technical challenges but also the\neconomic issues to build self-sustainable ecosystem. This article analyzes the\ninternet industry to provide predictions on the future trajectory of AI\ndevelopment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The focus of AI development has shifted from academic research to practical\napplications. However, AI development faces numerous challenges at various\nlevels. This article will attempt to analyze the opportunities and challenges\nof AI from several different perspectives using a structured approach. This\narticle proposes a seven-layer model for AI compute architecture, including\nPhysical Layer, Link Layer, Neural Network Layer, Context Layer, Agent Layer,\nOrchestrator Layer, and Application Layer, from bottom to top. It also explains\nhow AI computing has evolved into this 7-layer architecture through the\nthree-stage evolution on large-scale language models (LLMs). For each layer, we\ndescribe the development trajectory and key technologies. In Layers 1 and 2 we\ndiscuss AI computing issues and the impact of Scale-Up and Scale-Out strategies\non computing architecture. In Layer 3 we explore two different development\npaths for LLMs. In Layer 4 we discuss the impact of contextual memory on LLMs\nand compares it to traditional processor memory. In Layers 5 to 7 we discuss\nthe trends of AI agents and explore the issues in evolution from a single AI\nagent to an AI-based ecosystem, and their impact on the AI industry.\nFurthermore, AI development involves not only technical challenges but also the\neconomic issues to build self-sustainable ecosystem. This article analyzes the\ninternet industry to provide predictions on the future trajectory of AI\ndevelopment."
                },
                "authors": [
                    {
                        "name": "Bor-Sung Liang"
                    }
                ],
                "author_detail": {
                    "name": "Bor-Sung Liang"
                },
                "author": "Bor-Sung Liang",
                "arxiv_comment": "29 pages, 26 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21394v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21394v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21393v1",
                "updated": "2025-08-29T08:14:38Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    14,
                    38,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T08:14:38Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    14,
                    38,
                    4,
                    241,
                    0
                ],
                "title": "zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via\n  Zero-Knowledge Proofs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "zkLoRA: Fine-Tuning Large Language Models with Verifiable Security via\n  Zero-Knowledge Proofs"
                },
                "summary": "Fine-tuning large language models (LLMs) is crucial for adapting them to\nspecific tasks, yet it remains computationally demanding and raises concerns\nabout correctness and privacy, particularly in untrusted environments. Although\nparameter-efficient methods like Low-Rank Adaptation (LoRA) significantly\nreduce resource requirements, ensuring the security and verifiability of\nfine-tuning under zero-knowledge constraints remains an unresolved challenge.\nTo address this, we introduce zkLoRA, the first framework to integrate LoRA\nfine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and\ncorrectness. zkLoRA employs advanced cryptographic techniques -- such as lookup\narguments, sumcheck protocols, and polynomial commitments -- to verify both\narithmetic and non-arithmetic operations in Transformer-based architectures.\nThe framework provides end-to-end verifiability for forward propagation,\nbackward propagation, and parameter updates during LoRA fine-tuning, while\nsafeguarding the privacy of model parameters and training data. Leveraging\nGPU-based implementations, zkLoRA demonstrates practicality and efficiency\nthrough experimental validation on open-source LLMs like LLaMA, scaling up to\n13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,\nzkLoRA bridges a critical gap, enabling secure and trustworthy deployment of\nLLMs in sensitive or untrusted environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) is crucial for adapting them to\nspecific tasks, yet it remains computationally demanding and raises concerns\nabout correctness and privacy, particularly in untrusted environments. Although\nparameter-efficient methods like Low-Rank Adaptation (LoRA) significantly\nreduce resource requirements, ensuring the security and verifiability of\nfine-tuning under zero-knowledge constraints remains an unresolved challenge.\nTo address this, we introduce zkLoRA, the first framework to integrate LoRA\nfine-tuning with zero-knowledge proofs (ZKPs), achieving provable security and\ncorrectness. zkLoRA employs advanced cryptographic techniques -- such as lookup\narguments, sumcheck protocols, and polynomial commitments -- to verify both\narithmetic and non-arithmetic operations in Transformer-based architectures.\nThe framework provides end-to-end verifiability for forward propagation,\nbackward propagation, and parameter updates during LoRA fine-tuning, while\nsafeguarding the privacy of model parameters and training data. Leveraging\nGPU-based implementations, zkLoRA demonstrates practicality and efficiency\nthrough experimental validation on open-source LLMs like LLaMA, scaling up to\n13 billion parameters. By combining parameter-efficient fine-tuning with ZKPs,\nzkLoRA bridges a critical gap, enabling secure and trustworthy deployment of\nLLMs in sensitive or untrusted environments."
                },
                "authors": [
                    {
                        "name": "Guofu Liao"
                    },
                    {
                        "name": "Taotao Wang"
                    },
                    {
                        "name": "Shengli Zhang"
                    },
                    {
                        "name": "Jiqun Zhang"
                    },
                    {
                        "name": "Shi Long"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21389v1",
                "updated": "2025-08-29T08:05:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    5,
                    0,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T08:05:00Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    5,
                    0,
                    4,
                    241,
                    0
                ],
                "title": "AllSummedUp: un framework open-source pour comparer les metriques\n  d'evaluation de resume",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AllSummedUp: un framework open-source pour comparer les metriques\n  d'evaluation de resume"
                },
                "summary": "This paper investigates reproducibility challenges in automatic text\nsummarization evaluation. Based on experiments conducted across six\nrepresentative metrics ranging from classical approaches like ROUGE to recent\nLLM-based methods (G-Eval, SEval-Ex), we highlight significant discrepancies\nbetween reported performances in the literature and those observed in our\nexperimental setting. We introduce a unified, open-source framework, applied to\nthe SummEval dataset and designed to support fair and transparent comparison of\nevaluation metrics. Our results reveal a structural trade-off: metrics with the\nhighest alignment with human judgments tend to be computationally intensive and\nless stable across runs. Beyond comparative analysis, this study highlights key\nconcerns about relying on LLMs for evaluation, stressing their randomness,\ntechnical dependencies, and limited reproducibility. We advocate for more\nrobust evaluation protocols including exhaustive documentation and\nmethodological standardization to ensure greater reliability in automatic\nsummarization assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates reproducibility challenges in automatic text\nsummarization evaluation. Based on experiments conducted across six\nrepresentative metrics ranging from classical approaches like ROUGE to recent\nLLM-based methods (G-Eval, SEval-Ex), we highlight significant discrepancies\nbetween reported performances in the literature and those observed in our\nexperimental setting. We introduce a unified, open-source framework, applied to\nthe SummEval dataset and designed to support fair and transparent comparison of\nevaluation metrics. Our results reveal a structural trade-off: metrics with the\nhighest alignment with human judgments tend to be computationally intensive and\nless stable across runs. Beyond comparative analysis, this study highlights key\nconcerns about relying on LLMs for evaluation, stressing their randomness,\ntechnical dependencies, and limited reproducibility. We advocate for more\nrobust evaluation protocols including exhaustive documentation and\nmethodological standardization to ensure greater reliability in automatic\nsummarization assessment."
                },
                "authors": [
                    {
                        "name": "Tanguy Herserant"
                    },
                    {
                        "name": "Vincent Guigue"
                    }
                ],
                "author_detail": {
                    "name": "Vincent Guigue"
                },
                "author": "Vincent Guigue",
                "arxiv_comment": "in French language",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.11356v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.11356v2",
                "updated": "2025-08-29T08:04:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    8,
                    4,
                    20,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-15T09:49:14Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    9,
                    49,
                    14,
                    4,
                    227,
                    0
                ],
                "title": "ETTRL: Balancing Exploration and Exploitation in LLM Test-Time\n  Reinforcement Learning Via Entropy Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ETTRL: Balancing Exploration and Exploitation in LLM Test-Time\n  Reinforcement Learning Via Entropy Mechanism"
                },
                "summary": "Recent advancements in Large Language Models have yielded significant\nimprovements in complex reasoning tasks such as mathematics and programming.\nHowever, these models remain heavily dependent on annotated data and exhibit\nlimited adaptability in unsupervised scenarios. To address these limitations,\ntest-time reinforcement learning (TTRL) has been proposed, which enables\nself-optimization by leveraging model-generated pseudo-labels. Despite its\npromise, TTRL faces several key challenges, including high inference costs due\nto parallel rollouts and early-stage estimation bias that fosters\noverconfidence, reducing output diversity and causing performance plateaus. To\naddress these challenges, we introduce an entropy-based mechanism to enhance\nthe exploration-exploitation balance in test-time reinforcement learning\nthrough two strategies: Entropy-fork Tree Majority Rollout (ETMR) and\nEntropy-based Advantage Reshaping (EAR). Compared with the baseline, our\napproach enables Llama3.1-8B to achieve a 68 percent relative improvement in\nPass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of\nthe rollout tokens budget. This highlights our method's ability to effectively\noptimize the trade-off between inference efficiency, diversity, and estimation\nrobustness, thereby advancing unsupervised reinforcement learning for\nopen-domain reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models have yielded significant\nimprovements in complex reasoning tasks such as mathematics and programming.\nHowever, these models remain heavily dependent on annotated data and exhibit\nlimited adaptability in unsupervised scenarios. To address these limitations,\ntest-time reinforcement learning (TTRL) has been proposed, which enables\nself-optimization by leveraging model-generated pseudo-labels. Despite its\npromise, TTRL faces several key challenges, including high inference costs due\nto parallel rollouts and early-stage estimation bias that fosters\noverconfidence, reducing output diversity and causing performance plateaus. To\naddress these challenges, we introduce an entropy-based mechanism to enhance\nthe exploration-exploitation balance in test-time reinforcement learning\nthrough two strategies: Entropy-fork Tree Majority Rollout (ETMR) and\nEntropy-based Advantage Reshaping (EAR). Compared with the baseline, our\napproach enables Llama3.1-8B to achieve a 68 percent relative improvement in\nPass at 1 metric on the AIME 2024 benchmark, while consuming only 60 percent of\nthe rollout tokens budget. This highlights our method's ability to effectively\noptimize the trade-off between inference efficiency, diversity, and estimation\nrobustness, thereby advancing unsupervised reinforcement learning for\nopen-domain reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Jia Liu"
                    },
                    {
                        "name": "ChangYi He"
                    },
                    {
                        "name": "YingQiao Lin"
                    },
                    {
                        "name": "MingMin Yang"
                    },
                    {
                        "name": "FeiYang Shen"
                    },
                    {
                        "name": "ShaoGuo Liu"
                    }
                ],
                "author_detail": {
                    "name": "ShaoGuo Liu"
                },
                "author": "ShaoGuo Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.11356v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.11356v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21773v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21773v3",
                "updated": "2025-08-29T07:52:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    52,
                    14,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-30T16:17:53Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    16,
                    17,
                    53,
                    2,
                    120,
                    0
                ],
                "title": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced\n  Knowledge Boundary Awareness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced\n  Knowledge Boundary Awareness"
                },
                "summary": "The hallucination of non-existent facts by LLMs is an important problem given\nits widespread adoption across various applications. Previous research\naddresses this problem by analyzing the internal parameterized knowledge\nboundaries to estimate confidence. However, these studies focus on the\nsingle-problem setting and have not explored the more challenging multi-problem\nsetting, which requires accurately answering multiple questions simultaneously.\nWe introduce a novel method for the multi-problem setting, Multiple Answers and\nConfidence Stepwise Tuning (MAC-Tuning), that separates the learning of answer\nprediction and confidence estimation during fine-tuning on instruction data.\nExtensive experiments demonstrate that our method outperforms baselines by up\nto 25\\% in average precision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The hallucination of non-existent facts by LLMs is an important problem given\nits widespread adoption across various applications. Previous research\naddresses this problem by analyzing the internal parameterized knowledge\nboundaries to estimate confidence. However, these studies focus on the\nsingle-problem setting and have not explored the more challenging multi-problem\nsetting, which requires accurately answering multiple questions simultaneously.\nWe introduce a novel method for the multi-problem setting, Multiple Answers and\nConfidence Stepwise Tuning (MAC-Tuning), that separates the learning of answer\nprediction and confidence estimation during fine-tuning on instruction data.\nExtensive experiments demonstrate that our method outperforms baselines by up\nto 25\\% in average precision."
                },
                "authors": [
                    {
                        "name": "Junsheng Huang"
                    },
                    {
                        "name": "Zhitao He"
                    },
                    {
                        "name": "Yucheng Huang"
                    },
                    {
                        "name": "Sandeep Polisetty"
                    },
                    {
                        "name": "Qingyun Wang"
                    },
                    {
                        "name": "Yi. R"
                    },
                    {
                        "name": "Fung"
                    }
                ],
                "author_detail": {
                    "name": "Fung"
                },
                "arxiv_affiliation": "May",
                "author": "Fung",
                "arxiv_comment": "We release our code and resource at\n  https://github.com/no-touch-fish/Multi-QA-Tuning. The paper is accepted into\n  EMNLP 2025 main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21773v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21773v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21378v1",
                "updated": "2025-08-29T07:47:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    47,
                    17,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T07:47:17Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    47,
                    17,
                    4,
                    241,
                    0
                ],
                "title": "RoboInspector: Unveiling the Unreliability of Policy Code for\n  LLM-enabled Robotic Manipulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoboInspector: Unveiling the Unreliability of Policy Code for\n  LLM-enabled Robotic Manipulation"
                },
                "summary": "Large language models (LLMs) demonstrate remarkable capabilities in reasoning\nand code generation, enabling robotic manipulation to be initiated with just a\nsingle instruction. The LLM carries out various tasks by generating policy code\nrequired to control the robot. Despite advances in LLMs, achieving reliable\npolicy code generation remains a significant challenge due to the diverse\nrequirements of real-world tasks and the inherent complexity of user\ninstructions. In practice, different users may provide distinct instructions to\ndrive the robot for the same task, which may cause the unreliability of policy\ncode generation. To bridge this gap, we design RoboInspector, a pipeline to\nunveil and characterize the unreliability of the policy code for LLM-enabled\nrobotic manipulation from two perspectives: the complexity of the manipulation\ntask and the granularity of the instruction. We perform comprehensive\nexperiments with 168 distinct combinations of tasks, instructions, and LLMs in\ntwo prominent frameworks. The RoboInspector identifies four main unreliable\nbehaviors that lead to manipulation failure. We provide a detailed\ncharacterization of these behaviors and their underlying causes, giving insight\nfor practical development to reduce unreliability. Furthermore, we introduce a\nrefinement approach guided by failure policy code feedback that improves the\nreliability of policy code generation by up to 35% in LLM-enabled robotic\nmanipulation, evaluated in both simulation and real-world environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate remarkable capabilities in reasoning\nand code generation, enabling robotic manipulation to be initiated with just a\nsingle instruction. The LLM carries out various tasks by generating policy code\nrequired to control the robot. Despite advances in LLMs, achieving reliable\npolicy code generation remains a significant challenge due to the diverse\nrequirements of real-world tasks and the inherent complexity of user\ninstructions. In practice, different users may provide distinct instructions to\ndrive the robot for the same task, which may cause the unreliability of policy\ncode generation. To bridge this gap, we design RoboInspector, a pipeline to\nunveil and characterize the unreliability of the policy code for LLM-enabled\nrobotic manipulation from two perspectives: the complexity of the manipulation\ntask and the granularity of the instruction. We perform comprehensive\nexperiments with 168 distinct combinations of tasks, instructions, and LLMs in\ntwo prominent frameworks. The RoboInspector identifies four main unreliable\nbehaviors that lead to manipulation failure. We provide a detailed\ncharacterization of these behaviors and their underlying causes, giving insight\nfor practical development to reduce unreliability. Furthermore, we introduce a\nrefinement approach guided by failure policy code feedback that improves the\nreliability of policy code generation by up to 35% in LLM-enabled robotic\nmanipulation, evaluated in both simulation and real-world environments."
                },
                "authors": [
                    {
                        "name": "Chenduo Ying"
                    },
                    {
                        "name": "Linkang Du"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Yuanchao Shu"
                    }
                ],
                "author_detail": {
                    "name": "Yuanchao Shu"
                },
                "author": "Yuanchao Shu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21377v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21377v1",
                "updated": "2025-08-29T07:41:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    41,
                    4,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T07:41:04Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    41,
                    4,
                    4,
                    241,
                    0
                ],
                "title": "Challenges and Applications of Large Language Models: A Comparison of\n  GPT and DeepSeek family of models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Challenges and Applications of Large Language Models: A Comparison of\n  GPT and DeepSeek family of models"
                },
                "summary": "Large Language Models (LLMs) are transforming AI across industries, but their\ndevelopment and deployment remain complex. This survey reviews 16 key\nchallenges in building and using LLMs and examines how these challenges are\naddressed by two state-of-the-art models with unique approaches: OpenAI's\nclosed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a\nlarge open source Mixture-of-Experts model. Through this comparison, we\nshowcase the trade-offs between closed source models (robust safety, fine-tuned\nreliability) and open source models (efficiency, adaptability). We also explore\nLLM applications across different domains (from chatbots and coding tools to\nhealthcare and education), highlighting which model attributes are best suited\nfor each use case. This article aims to guide AI researchers, developers, and\ndecision-makers in understanding current LLM capabilities, limitations, and\nbest practices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are transforming AI across industries, but their\ndevelopment and deployment remain complex. This survey reviews 16 key\nchallenges in building and using LLMs and examines how these challenges are\naddressed by two state-of-the-art models with unique approaches: OpenAI's\nclosed source GPT-4o (May 2024 update) and DeepSeek-V3-0324 (March 2025), a\nlarge open source Mixture-of-Experts model. Through this comparison, we\nshowcase the trade-offs between closed source models (robust safety, fine-tuned\nreliability) and open source models (efficiency, adaptability). We also explore\nLLM applications across different domains (from chatbots and coding tools to\nhealthcare and education), highlighting which model attributes are best suited\nfor each use case. This article aims to guide AI researchers, developers, and\ndecision-makers in understanding current LLM capabilities, limitations, and\nbest practices."
                },
                "authors": [
                    {
                        "name": "Shubham Sharma"
                    },
                    {
                        "name": "Sneha Tuli"
                    },
                    {
                        "name": "Narendra Badam"
                    }
                ],
                "author_detail": {
                    "name": "Narendra Badam"
                },
                "author": "Narendra Badam",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21377v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21377v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15683v2",
                "updated": "2025-08-29T07:40:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    40,
                    34,
                    4,
                    241,
                    0
                ],
                "published": "2025-05-21T15:58:08Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    15,
                    58,
                    8,
                    2,
                    141,
                    0
                ],
                "title": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedSEA-LLaMA: A Secure, Efficient and Adaptive Federated Splitting\n  Framework for Large Language Models"
                },
                "summary": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private data holds promise for improving LLMs due to its high quality, but\nits scattered distribution across data silos and the high computational demands\nof LLMs limit their deployment in federated environments. To address this, the\ntransformer-based federated split models are proposed, which offload most model\nparameters to the server (or distributed clients) while retaining only a small\nportion on the client to ensure data privacy. Despite this design, they still\nface three challenges: 1) Peer-to-peer key encryption struggles to secure\ntransmitted vectors effectively; 2) The auto-regressive nature of LLMs means\nthat federated split learning can only train and infer sequentially, causing\nhigh communication overhead; 3) Fixed partition points lack adaptability to\ndownstream tasks. In this paper, we introduce FedSEA-LLaMA, a Secure,\nEfficient, and Adaptive Federated splitting framework based on LLaMA2. First,\nwe inject Gaussian noise into forward-pass hidden states to enable secure\nend-to-end vector transmission. Second, we employ attention-mask compression\nand KV cache collaboration to reduce communication costs, accelerating training\nand inference. Third, we allow users to dynamically adjust the partition points\nfor input/output blocks based on specific task requirements. Experiments on\nnatural language understanding, summarization, and conversational QA tasks show\nthat FedSEA-LLaMA maintains performance comparable to centralized LLaMA2 and\nachieves up to 8x speedups in training and inference. Further analysis of\nprivacy attacks and different partition points also demonstrates the\neffectiveness of FedSEA-LLaMA in security and adaptability."
                },
                "authors": [
                    {
                        "name": "Zishuai Zhang"
                    },
                    {
                        "name": "Hainan zhang"
                    },
                    {
                        "name": "Weihua Li"
                    },
                    {
                        "name": "Qinnan zhang"
                    },
                    {
                        "name": "jin Dong"
                    },
                    {
                        "name": "Yongxin Tong"
                    },
                    {
                        "name": "Zhiming Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zhiming Zheng"
                },
                "author": "Zhiming Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02161v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02161v3",
                "updated": "2025-08-29T07:34:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    34,
                    50,
                    4,
                    241,
                    0
                ],
                "published": "2025-02-04T09:37:20Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    9,
                    37,
                    20,
                    1,
                    35,
                    0
                ],
                "title": "A plug-and-play solution for characterizing two-way optical frequency\n  transfer over free-space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A plug-and-play solution for characterizing two-way optical frequency\n  transfer over free-space"
                },
                "summary": "Optical clock networks connected by phase-coherent links offer significant\npotential for advancing fundamental research and diverse scientific\napplications. Free-space optical frequency transfer extends fiber-based\nconnectivity to remote areas and holds the potential for global coverage via\nsatellite links. Here we present a compact and robust portable, rack-integrated\ntwo-way free-space link characterization system. Equipped with plug-and-play\ncapabilities, the system enables straightforward interfacing with various\noptical systems and facilitates quick deployment for field experiments. In this\nwork, we achieve a fractional frequency instability of $2.0 \\times 10^{-19}$\nfor an averaging time of 10 s over a 3.4 km horizontal fully folded intra-city\nfree-space link. Moreover, the system maintains an uptime of $94\\%$ over 15\nhours, illustrating its reliability and effectiveness for high-precision\noptical frequency comparisons over free-space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optical clock networks connected by phase-coherent links offer significant\npotential for advancing fundamental research and diverse scientific\napplications. Free-space optical frequency transfer extends fiber-based\nconnectivity to remote areas and holds the potential for global coverage via\nsatellite links. Here we present a compact and robust portable, rack-integrated\ntwo-way free-space link characterization system. Equipped with plug-and-play\ncapabilities, the system enables straightforward interfacing with various\noptical systems and facilitates quick deployment for field experiments. In this\nwork, we achieve a fractional frequency instability of $2.0 \\times 10^{-19}$\nfor an averaging time of 10 s over a 3.4 km horizontal fully folded intra-city\nfree-space link. Moreover, the system maintains an uptime of $94\\%$ over 15\nhours, illustrating its reliability and effectiveness for high-precision\noptical frequency comparisons over free-space."
                },
                "authors": [
                    {
                        "name": "Jingxian Ji"
                    },
                    {
                        "name": "Shambo Mukherjee"
                    },
                    {
                        "name": "Alexander Kuhl"
                    },
                    {
                        "name": "Sebastian Koke"
                    },
                    {
                        "name": "Markus Leipe"
                    },
                    {
                        "name": "Markus Rothe"
                    },
                    {
                        "name": "Fabian Steinlechner"
                    },
                    {
                        "name": "Jochen Kronjäger"
                    }
                ],
                "author_detail": {
                    "name": "Jochen Kronjäger"
                },
                "author": "Jochen Kronjäger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02161v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02161v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21368v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21368v1",
                "updated": "2025-08-29T07:17:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    17,
                    44,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T07:17:44Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    17,
                    44,
                    4,
                    241,
                    0
                ],
                "title": "EconAgentic in DePIN Markets: A Large Language Model Approach to the\n  Sharing Economy of Decentralized Physical Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EconAgentic in DePIN Markets: A Large Language Model Approach to the\n  Sharing Economy of Decentralized Physical Infrastructure"
                },
                "summary": "The Decentralized Physical Infrastructure (DePIN) market is revolutionizing\nthe sharing economy through token-based economics and smart contracts that\ngovern decentralized operations. By 2024, DePIN projects have exceeded \\$10\nbillion in market capitalization, underscoring their rapid growth. However, the\nunregulated nature of these markets, coupled with the autonomous deployment of\nAI agents in smart contracts, introduces risks such as inefficiencies and\npotential misalignment with human values. To address these concerns, we\nintroduce EconAgentic, a Large Language Model (LLM)-powered framework designed\nto mitigate these challenges. Our research focuses on three key areas: 1)\nmodeling the dynamic evolution of DePIN markets, 2) evaluating stakeholders'\nactions and their economic impacts, and 3) analyzing macroeconomic indicators\nto align market outcomes with societal goals. Through EconAgentic, we simulate\nhow AI agents respond to token incentives, invest in infrastructure, and adapt\nto market conditions, comparing AI-driven decisions with human heuristic\nbenchmarks. Our results show that EconAgentic provides valuable insights into\nthe efficiency, inclusion, and stability of DePIN markets, contributing to both\nacademic understanding and practical improvements in the design and governance\nof decentralized, tokenized economies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Decentralized Physical Infrastructure (DePIN) market is revolutionizing\nthe sharing economy through token-based economics and smart contracts that\ngovern decentralized operations. By 2024, DePIN projects have exceeded \\$10\nbillion in market capitalization, underscoring their rapid growth. However, the\nunregulated nature of these markets, coupled with the autonomous deployment of\nAI agents in smart contracts, introduces risks such as inefficiencies and\npotential misalignment with human values. To address these concerns, we\nintroduce EconAgentic, a Large Language Model (LLM)-powered framework designed\nto mitigate these challenges. Our research focuses on three key areas: 1)\nmodeling the dynamic evolution of DePIN markets, 2) evaluating stakeholders'\nactions and their economic impacts, and 3) analyzing macroeconomic indicators\nto align market outcomes with societal goals. Through EconAgentic, we simulate\nhow AI agents respond to token incentives, invest in infrastructure, and adapt\nto market conditions, comparing AI-driven decisions with human heuristic\nbenchmarks. Our results show that EconAgentic provides valuable insights into\nthe efficiency, inclusion, and stability of DePIN markets, contributing to both\nacademic understanding and practical improvements in the design and governance\nof decentralized, tokenized economies."
                },
                "authors": [
                    {
                        "name": "Yulin Liu"
                    },
                    {
                        "name": "Mocca Schweitzer"
                    }
                ],
                "author_detail": {
                    "name": "Mocca Schweitzer"
                },
                "author": "Mocca Schweitzer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21368v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21368v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21366v1",
                "updated": "2025-08-29T07:14:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    14,
                    20,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T07:14:20Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    14,
                    20,
                    4,
                    241,
                    0
                ],
                "title": "CircuitHunt: Automated Quantum Circuit Screening for Superior\n  Credit-Card Fraud Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CircuitHunt: Automated Quantum Circuit Screening for Superior\n  Credit-Card Fraud Detection"
                },
                "summary": "Designing effective quantum models for real-world tasks remains a key\nchallenge within Quantum Machine Learning (QML), particularly in applications\nsuch as credit card fraud detection, where extreme class imbalance and evolving\nattack patterns demand both accuracy and adaptability. Most existing approaches\nrely on either manually designed or randomly initialized circuits, leading to\nhigh failure rates and limited scalability. In this work, we introduce\nCircuitHunt, a fully automated quantum circuit screening framework that\nstreamlines the discovery of high-performing models. CircuitHunt filters\ncircuits from the KetGPT dataset using qubit and parameter constraints, embeds\neach candidate into a standardized hybrid QNN, and performs rapid training with\ncheckpointing based on macro-F1 scores to discard weak performers early. The\ntop-ranked circuit is then fully trained, achieving 97% test accuracy and a\nhigh macro-F1 score on a challenging fraud detection benchmark. By combining\nbudget-aware pruning, empirical evaluation, and end-to-end automation,\nCircuitHunt reduces architecture search time from days to hours while\nmaintaining performance. It thus provides a scalable and task-driven tool for\nQML deployment in critical financial applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing effective quantum models for real-world tasks remains a key\nchallenge within Quantum Machine Learning (QML), particularly in applications\nsuch as credit card fraud detection, where extreme class imbalance and evolving\nattack patterns demand both accuracy and adaptability. Most existing approaches\nrely on either manually designed or randomly initialized circuits, leading to\nhigh failure rates and limited scalability. In this work, we introduce\nCircuitHunt, a fully automated quantum circuit screening framework that\nstreamlines the discovery of high-performing models. CircuitHunt filters\ncircuits from the KetGPT dataset using qubit and parameter constraints, embeds\neach candidate into a standardized hybrid QNN, and performs rapid training with\ncheckpointing based on macro-F1 scores to discard weak performers early. The\ntop-ranked circuit is then fully trained, achieving 97% test accuracy and a\nhigh macro-F1 score on a challenging fraud detection benchmark. By combining\nbudget-aware pruning, empirical evaluation, and end-to-end automation,\nCircuitHunt reduces architecture search time from days to hours while\nmaintaining performance. It thus provides a scalable and task-driven tool for\nQML deployment in critical financial applications."
                },
                "authors": [
                    {
                        "name": "Nouhaila Innan"
                    },
                    {
                        "name": "Akshat Singh"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "arxiv_comment": "7 pages, 4 figures, 4 tables. Accepted at IEEE QAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.21365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.21365v1",
                "updated": "2025-08-29T07:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    13,
                    39,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-29T07:13:39Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    13,
                    39,
                    4,
                    241,
                    0
                ],
                "title": "Think in Games: Learning to Reason in Games via Reinforcement Learning\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think in Games: Learning to Reason in Games via Reinforcement Learning\n  with Large Language Models"
                },
                "summary": "Large language models (LLMs) excel at complex reasoning tasks such as\nmathematics and coding, yet they frequently struggle with simple interactive\ntasks that young children perform effortlessly. This discrepancy highlights a\ncritical gap between declarative knowledge (knowing about something) and\nprocedural knowledge (knowing how to do something). Although traditional\nreinforcement learning (RL) agents can acquire procedural knowledge through\nenvironmental interaction, they often operate as black boxes and require\nsubstantial training data. In contrast, LLMs possess extensive world knowledge\nand reasoning capabilities, but are unable to effectively convert this static\nknowledge into dynamic decision-making in interactive settings. To address this\nchallenge, we propose Think in Games (TiG), a novel framework that empowers\nLLMs to develop procedural understanding through direct interaction with game\nenvironments, while retaining their inherent reasoning and explanatory\nabilities. Specifically, TiG reformulates RL-based decision-making as a\nlanguage modeling task: LLMs generate language-guided policies, which are\nrefined iteratively through online reinforcement learning based on\nenvironmental feedback. Our experimental results show that TiG successfully\nbridges the gap between declarative and procedural knowledge, achieving\ncompetitive performance with dramatically lower data and computational demands\ncompared to conventional RL methods. Moreover, TiG provides step-by-step\nnatural language explanations for its decisions, greatly improving transparency\nand interpretability in complex interactive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at complex reasoning tasks such as\nmathematics and coding, yet they frequently struggle with simple interactive\ntasks that young children perform effortlessly. This discrepancy highlights a\ncritical gap between declarative knowledge (knowing about something) and\nprocedural knowledge (knowing how to do something). Although traditional\nreinforcement learning (RL) agents can acquire procedural knowledge through\nenvironmental interaction, they often operate as black boxes and require\nsubstantial training data. In contrast, LLMs possess extensive world knowledge\nand reasoning capabilities, but are unable to effectively convert this static\nknowledge into dynamic decision-making in interactive settings. To address this\nchallenge, we propose Think in Games (TiG), a novel framework that empowers\nLLMs to develop procedural understanding through direct interaction with game\nenvironments, while retaining their inherent reasoning and explanatory\nabilities. Specifically, TiG reformulates RL-based decision-making as a\nlanguage modeling task: LLMs generate language-guided policies, which are\nrefined iteratively through online reinforcement learning based on\nenvironmental feedback. Our experimental results show that TiG successfully\nbridges the gap between declarative and procedural knowledge, achieving\ncompetitive performance with dramatically lower data and computational demands\ncompared to conventional RL methods. Moreover, TiG provides step-by-step\nnatural language explanations for its decisions, greatly improving transparency\nand interpretability in complex interactive tasks."
                },
                "authors": [
                    {
                        "name": "Yi Liao"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Yuan Sui"
                    },
                    {
                        "name": "Zining Zhu"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Guohua Tang"
                    },
                    {
                        "name": "Zhongqian Sun"
                    },
                    {
                        "name": "Wei Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Yang"
                },
                "author": "Wei Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.21365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.21365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.20008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.20008v2",
                "updated": "2025-08-29T07:06:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    7,
                    6,
                    23,
                    4,
                    241,
                    0
                ],
                "published": "2025-06-24T20:54:56Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    20,
                    54,
                    56,
                    1,
                    175,
                    0
                ],
                "title": "QHackBench: Benchmarking Large Language Models for Quantum Code\n  Generation Using PennyLane Hackathon Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QHackBench: Benchmarking Large Language Models for Quantum Code\n  Generation Using PennyLane Hackathon Challenges"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have demonstrated strong\npotential in code generation, yet their effectiveness in quantum computing\nremains underexplored. This paper benchmarks LLMs for PennyLane-based quantum\ncode generation using real-world challenges from the Quantum Hackathon (QHack).\nWe introduce QHackBench, a novel benchmark dataset derived from QHack\ncompetitions, and evaluate model performance under vanilla prompting and\nRetrieval-Augmented Generation (RAG). Our structured evaluation framework\nassesses functional correctness, syntactic validity, and execution success\nacross varying challenge difficulties. Results indicate that RAG-enhanced\nmodels, supplemented with an augmented PennyLane dataset, approximately\ngenerate similar results as the standard prompting, particularly in complex\nquantum algorithms. Additionally, we introduce a multi-agent evaluation\npipeline that iteratively refines incorrect solutions, further enhancing\nexecution success rates. To foster further research, we commit to publicly\nreleasing QHackBench, along with our evaluation framework and experimental\nresults, enabling continued advancements in AI-assisted quantum programming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have demonstrated strong\npotential in code generation, yet their effectiveness in quantum computing\nremains underexplored. This paper benchmarks LLMs for PennyLane-based quantum\ncode generation using real-world challenges from the Quantum Hackathon (QHack).\nWe introduce QHackBench, a novel benchmark dataset derived from QHack\ncompetitions, and evaluate model performance under vanilla prompting and\nRetrieval-Augmented Generation (RAG). Our structured evaluation framework\nassesses functional correctness, syntactic validity, and execution success\nacross varying challenge difficulties. Results indicate that RAG-enhanced\nmodels, supplemented with an augmented PennyLane dataset, approximately\ngenerate similar results as the standard prompting, particularly in complex\nquantum algorithms. Additionally, we introduce a multi-agent evaluation\npipeline that iteratively refines incorrect solutions, further enhancing\nexecution success rates. To foster further research, we commit to publicly\nreleasing QHackBench, along with our evaluation framework and experimental\nresults, enabling continued advancements in AI-assisted quantum programming."
                },
                "authors": [
                    {
                        "name": "Abdul Basit"
                    },
                    {
                        "name": "Minghao Shao"
                    },
                    {
                        "name": "Muhammad Haider Asif"
                    },
                    {
                        "name": "Nouhaila Innan"
                    },
                    {
                        "name": "Muhammad Kashif"
                    },
                    {
                        "name": "Alberto Marchisio"
                    },
                    {
                        "name": "Muhammad Shafique"
                    }
                ],
                "author_detail": {
                    "name": "Muhammad Shafique"
                },
                "author": "Muhammad Shafique",
                "arxiv_comment": "To appear at the IEEE International Conference on Quantum Artificial\n  Intelligence (QAI), Naples, Italy, November 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.20008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.20008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 81P68, 68T07, 68T20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06679v2",
                "updated": "2025-08-29T06:56:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    6,
                    56,
                    2,
                    4,
                    241,
                    0
                ],
                "published": "2024-09-10T17:44:35Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    17,
                    44,
                    35,
                    1,
                    254,
                    0
                ],
                "title": "E2LLM: Encoder Elongated Large Language Models for Long-Context\n  Understanding and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E2LLM: Encoder Elongated Large Language Models for Long-Context\n  Understanding and Reasoning"
                },
                "summary": "Processing long contexts is increasingly important for Large Language Models\n(LLMs) in tasks like multi-turn dialogues, code generation, and document\nsummarization. This paper addresses the challenges of achieving high\nlong-context performance, low computational complexity, and compatibility with\npretrained models -- collectively termed the ``impossible triangle''. We\nintroduce E2LLM (Encoder Elongated Large Language Models), a novel approach\nthat effectively navigates this paradox. E2LLM divides long contexts into\nchunks, compresses each into soft prompts using a pretrained text encoder, and\naligns these representations with a decoder-only LLM via an adapter. To enhance\nthe LLM's reasoning with these soft prompts, we employ two training objectives:\nencoder output reconstruction and long-context instruction fine-tuning.\nExtensive experiments reveal that E2LLM not only outperforms 8 state-of-the-art\n(SOTA) methods in effectiveness and efficiency for document summarization and\nquestion answering, but also achieves the best performance on LongBench v2\namong models of comparable size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts is increasingly important for Large Language Models\n(LLMs) in tasks like multi-turn dialogues, code generation, and document\nsummarization. This paper addresses the challenges of achieving high\nlong-context performance, low computational complexity, and compatibility with\npretrained models -- collectively termed the ``impossible triangle''. We\nintroduce E2LLM (Encoder Elongated Large Language Models), a novel approach\nthat effectively navigates this paradox. E2LLM divides long contexts into\nchunks, compresses each into soft prompts using a pretrained text encoder, and\naligns these representations with a decoder-only LLM via an adapter. To enhance\nthe LLM's reasoning with these soft prompts, we employ two training objectives:\nencoder output reconstruction and long-context instruction fine-tuning.\nExtensive experiments reveal that E2LLM not only outperforms 8 state-of-the-art\n(SOTA) methods in effectiveness and efficiency for document summarization and\nquestion answering, but also achieves the best performance on LongBench v2\namong models of comparable size."
                },
                "authors": [
                    {
                        "name": "Zihan Liao"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Hang Yu"
                    },
                    {
                        "name": "Lingxiao Wei"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "arxiv_comment": "Accept by EMNLP'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17178v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17178v3",
                "updated": "2025-08-29T06:52:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    6,
                    52,
                    20,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-23T03:52:24Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    3,
                    52,
                    24,
                    2,
                    204,
                    0
                ],
                "title": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge\n  Understanding of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge\n  Understanding of LLMs"
                },
                "summary": "Although large language models (LLMs) have made significant progress in\nunderstanding Structured Knowledge (SK) like KG and Table, existing evaluations\nfor SK understanding are non-rigorous (i.e., lacking evaluations of specific\ncapabilities) and focus on a single type of SK. Therefore, we aim to propose a\nmore comprehensive and rigorous structured knowledge understanding benchmark to\ndiagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a\nStructured Knowledge Augmented QA Benchmark that encompasses four widely used\nstructured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a\nthree-stage pipeline to construct SKA-Bench instances, which includes a\nquestion, an answer, positive knowledge units, and noisy knowledge units. To\nevaluate the SK understanding capabilities of LLMs in a fine-grained manner, we\nexpand the instances into four fundamental ability testbeds: Noise Robustness,\nOrder Insensitivity, Information Integration, and Negative Rejection. Empirical\nevaluations on 8 representative LLMs, including the advanced DeepSeek-R1,\nindicate that existing LLMs still face significant challenges in understanding\nstructured knowledge, and their performance is influenced by factors such as\nthe amount of noise, the order of knowledge units, and hallucination\nphenomenon. Our dataset and code are available at\nhttps://github.com/zjukg/SKA-Bench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although large language models (LLMs) have made significant progress in\nunderstanding Structured Knowledge (SK) like KG and Table, existing evaluations\nfor SK understanding are non-rigorous (i.e., lacking evaluations of specific\ncapabilities) and focus on a single type of SK. Therefore, we aim to propose a\nmore comprehensive and rigorous structured knowledge understanding benchmark to\ndiagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a\nStructured Knowledge Augmented QA Benchmark that encompasses four widely used\nstructured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a\nthree-stage pipeline to construct SKA-Bench instances, which includes a\nquestion, an answer, positive knowledge units, and noisy knowledge units. To\nevaluate the SK understanding capabilities of LLMs in a fine-grained manner, we\nexpand the instances into four fundamental ability testbeds: Noise Robustness,\nOrder Insensitivity, Information Integration, and Negative Rejection. Empirical\nevaluations on 8 representative LLMs, including the advanced DeepSeek-R1,\nindicate that existing LLMs still face significant challenges in understanding\nstructured knowledge, and their performance is influenced by factors such as\nthe amount of noise, the order of knowledge units, and hallucination\nphenomenon. Our dataset and code are available at\nhttps://github.com/zjukg/SKA-Bench."
                },
                "authors": [
                    {
                        "name": "Zhiqiang Liu"
                    },
                    {
                        "name": "Enpei Niu"
                    },
                    {
                        "name": "Yin Hua"
                    },
                    {
                        "name": "Mengshu Sun"
                    },
                    {
                        "name": "Lei Liang"
                    },
                    {
                        "name": "Huajun Chen"
                    },
                    {
                        "name": "Wen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wen Zhang"
                },
                "author": "Wen Zhang",
                "arxiv_comment": "EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17178v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17178v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18956v2",
                "updated": "2025-08-29T06:48:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    6,
                    48,
                    45,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-26T15:38:14Z",
                "published_parsed": [
                    2025,
                    4,
                    26,
                    15,
                    38,
                    14,
                    5,
                    116,
                    0
                ],
                "title": "Towards Automated Detection of Inline Code Comment Smells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Automated Detection of Inline Code Comment Smells"
                },
                "summary": "Code comments are important in software development because they directly\ninfluence software maintainability and overall quality. Bad practices of code\ncomments lead to code comment smells, negatively impacting software\nmaintenance. Recent research has been conducted on classifying inline code\ncomment smells, yet automatically detecting these still remains a challenge. We\naim to automatically detect and classify inline code comment smells through\nmachine learning (ML) models and a large language model (LLM) to determine how\naccurately each smell type can be detected. We enhanced a previously labeled\ndataset, where comments are labeled according to a determined taxonomy, by\naugmenting it with additional code segments and their associated comments. GPT\n4, a large language model, was used to classify code comment smells on both the\noriginal and augmented datasets to evaluate its performance. In parallel, we\ntrained and tested seven different machine learning algorithms on the augmented\ndataset to compare their classification performance against GPT 4. The\nperformance of models, particularly Random Forest, which achieved an overall\naccuracy of 69 percent, along with Gradient Boosting and Logistic Regression,\neach achieving 66 percent and 65 percent, respectively, establishes a solid\nbaseline for future research in this domain. The Random Forest model\noutperformed all other ML models, by achieving the highest Matthews Correlation\nCoefficient (MCC) score of 0.44. The augmented dataset improved the overall\nclassification accuracy of the GPT 4 model predictions from 34 percent to 55\npercent. This study contributes to software maintainability by exploring the\nautomatic detection and classification of inline code comment smells. We have\nmade our augmented dataset and code artifacts available online, offering a\nvaluable resource for developing automated comment smell detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code comments are important in software development because they directly\ninfluence software maintainability and overall quality. Bad practices of code\ncomments lead to code comment smells, negatively impacting software\nmaintenance. Recent research has been conducted on classifying inline code\ncomment smells, yet automatically detecting these still remains a challenge. We\naim to automatically detect and classify inline code comment smells through\nmachine learning (ML) models and a large language model (LLM) to determine how\naccurately each smell type can be detected. We enhanced a previously labeled\ndataset, where comments are labeled according to a determined taxonomy, by\naugmenting it with additional code segments and their associated comments. GPT\n4, a large language model, was used to classify code comment smells on both the\noriginal and augmented datasets to evaluate its performance. In parallel, we\ntrained and tested seven different machine learning algorithms on the augmented\ndataset to compare their classification performance against GPT 4. The\nperformance of models, particularly Random Forest, which achieved an overall\naccuracy of 69 percent, along with Gradient Boosting and Logistic Regression,\neach achieving 66 percent and 65 percent, respectively, establishes a solid\nbaseline for future research in this domain. The Random Forest model\noutperformed all other ML models, by achieving the highest Matthews Correlation\nCoefficient (MCC) score of 0.44. The augmented dataset improved the overall\nclassification accuracy of the GPT 4 model predictions from 34 percent to 55\npercent. This study contributes to software maintainability by exploring the\nautomatic detection and classification of inline code comment smells. We have\nmade our augmented dataset and code artifacts available online, offering a\nvaluable resource for developing automated comment smell detection tools."
                },
                "authors": [
                    {
                        "name": "Ipek Oztas"
                    },
                    {
                        "name": "U Boran Torun"
                    },
                    {
                        "name": "Eray Tüzün"
                    }
                ],
                "author_detail": {
                    "name": "Eray Tüzün"
                },
                "author": "Eray Tüzün",
                "arxiv_journal_ref": "Presented at EASE 2025 29th International Conference on Evaluation\n  and Assessment in Software Engineering in AI Models / Data Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16665v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16665v2",
                "updated": "2025-08-29T06:44:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    6,
                    44,
                    16,
                    4,
                    241,
                    0
                ],
                "published": "2025-08-20T22:27:21Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    22,
                    27,
                    21,
                    2,
                    232,
                    0
                ],
                "title": "Trust but Verify! A Survey on Verification Design for Test-time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust but Verify! A Survey on Verification Design for Test-time Scaling"
                },
                "summary": "Test-time scaling (TTS) has emerged as a new frontier for scaling the\nperformance of Large Language Models. In test-time scaling, by using more\ncomputational resources during inference, LLMs can improve their reasoning\nprocess and task performance. Several approaches have emerged for TTS such as\ndistilling reasoning traces from another model or exploring the vast decoding\nsearch space by employing a verifier. The verifiers serve as reward models that\nhelp score the candidate outputs from the decoding process to diligently\nexplore the vast solution space and select the best outcome. This paradigm\ncommonly termed has emerged as a superior approach owing to parameter free\nscaling at inference time and high performance gains. The verifiers could be\nprompt-based, fine-tuned as a discriminative or generative model to verify\nprocess paths, outcomes or both. Despite their widespread adoption, there is no\ndetailed collection, clear categorization and discussion of diverse\nverification approaches and their training mechanisms. In this survey, we cover\nthe diverse approaches in the literature and present a unified view of verifier\ntraining, types and their utility in test-time scaling. Our repository can be\nfound at\nhttps://github.com/elixir-research-group/Verifierstesttimescaling.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) has emerged as a new frontier for scaling the\nperformance of Large Language Models. In test-time scaling, by using more\ncomputational resources during inference, LLMs can improve their reasoning\nprocess and task performance. Several approaches have emerged for TTS such as\ndistilling reasoning traces from another model or exploring the vast decoding\nsearch space by employing a verifier. The verifiers serve as reward models that\nhelp score the candidate outputs from the decoding process to diligently\nexplore the vast solution space and select the best outcome. This paradigm\ncommonly termed has emerged as a superior approach owing to parameter free\nscaling at inference time and high performance gains. The verifiers could be\nprompt-based, fine-tuned as a discriminative or generative model to verify\nprocess paths, outcomes or both. Despite their widespread adoption, there is no\ndetailed collection, clear categorization and discussion of diverse\nverification approaches and their training mechanisms. In this survey, we cover\nthe diverse approaches in the literature and present a unified view of verifier\ntraining, types and their utility in test-time scaling. Our repository can be\nfound at\nhttps://github.com/elixir-research-group/Verifierstesttimescaling.github.io."
                },
                "authors": [
                    {
                        "name": "V Venktesh"
                    },
                    {
                        "name": "Mandeep Rathee"
                    },
                    {
                        "name": "Avishek Anand"
                    }
                ],
                "author_detail": {
                    "name": "Avishek Anand"
                },
                "author": "Avishek Anand",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16665v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16665v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02617v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02617v2",
                "updated": "2025-08-29T06:33:46Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    6,
                    33,
                    46,
                    4,
                    241,
                    0
                ],
                "published": "2025-04-03T14:16:41Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    16,
                    41,
                    3,
                    93,
                    0
                ],
                "title": "PicoPose: Progressive Pixel-to-Pixel Correspondence Learning for Novel\n  Object Pose Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PicoPose: Progressive Pixel-to-Pixel Correspondence Learning for Novel\n  Object Pose Estimation"
                },
                "summary": "RGB-based novel object pose estimation is critical for rapid deployment in\nrobotic applications, yet zero-shot generalization remains a key challenge. In\nthis paper, we introduce PicoPose, a novel framework designed to tackle this\ntask using a three-stage pixel-to-pixel correspondence learning process.\nFirstly, PicoPose matches features from the RGB observation with those from\nrendered object templates, identifying the best-matched template and\nestablishing coarse correspondences. Secondly, PicoPose smooths the\ncorrespondences by globally regressing a 2D affine transformation, including\nin-plane rotation, scale, and 2D translation, from the coarse correspondence\nmap. Thirdly, PicoPose applies the affine transformation to the feature map of\nthe best-matched template and learns correspondence offsets within local\nregions to achieve fine-grained correspondences. By progressively refining the\ncorrespondences, PicoPose significantly improves the accuracy of object poses\ncomputed via PnP/RANSAC. PicoPose achieves state-of-the-art performance on the\nseven core datasets of the BOP benchmark, demonstrating exceptional\ngeneralization to novel objects. Code and trained models are available at\nhttps://github.com/foollh/PicoPose.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RGB-based novel object pose estimation is critical for rapid deployment in\nrobotic applications, yet zero-shot generalization remains a key challenge. In\nthis paper, we introduce PicoPose, a novel framework designed to tackle this\ntask using a three-stage pixel-to-pixel correspondence learning process.\nFirstly, PicoPose matches features from the RGB observation with those from\nrendered object templates, identifying the best-matched template and\nestablishing coarse correspondences. Secondly, PicoPose smooths the\ncorrespondences by globally regressing a 2D affine transformation, including\nin-plane rotation, scale, and 2D translation, from the coarse correspondence\nmap. Thirdly, PicoPose applies the affine transformation to the feature map of\nthe best-matched template and learns correspondence offsets within local\nregions to achieve fine-grained correspondences. By progressively refining the\ncorrespondences, PicoPose significantly improves the accuracy of object poses\ncomputed via PnP/RANSAC. PicoPose achieves state-of-the-art performance on the\nseven core datasets of the BOP benchmark, demonstrating exceptional\ngeneralization to novel objects. Code and trained models are available at\nhttps://github.com/foollh/PicoPose."
                },
                "authors": [
                    {
                        "name": "Lihua Liu"
                    },
                    {
                        "name": "Jiehong Lin"
                    },
                    {
                        "name": "Zhenxin Liu"
                    },
                    {
                        "name": "Kui Jia"
                    }
                ],
                "author_detail": {
                    "name": "Kui Jia"
                },
                "author": "Kui Jia",
                "arxiv_comment": "CoRL2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02617v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02617v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18948v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18948v5",
                "updated": "2025-08-29T06:03:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    6,
                    3,
                    42,
                    4,
                    241,
                    0
                ],
                "published": "2024-11-28T06:29:46Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    6,
                    29,
                    46,
                    3,
                    333,
                    0
                ],
                "title": "RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation\n  through LLM Activation Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RevPRAG: Revealing Poisoning Attacks in Retrieval-Augmented Generation\n  through LLM Activation Analysis"
                },
                "summary": "Retrieval-Augmented Generation (RAG) enriches the input to LLMs by retrieving\ninformation from the relevant knowledge database, enabling them to produce\nresponses that are more accurate and contextually appropriate. It is worth\nnoting that the knowledge database, being sourced from publicly available\nchannels such as Wikipedia, inevitably introduces a new attack surface. RAG\npoisoning involves injecting malicious texts into the knowledge database,\nultimately leading to the generation of the attacker's target response (also\ncalled poisoned response). However, there are currently limited methods\navailable for detecting such poisoning attacks. We aim to bridge the gap in\nthis work. Particularly, we introduce RevPRAG, a flexible and automated\ndetection pipeline that leverages the activations of LLMs for poisoned response\ndetection. Our investigation uncovers distinct patterns in LLMs' activations\nwhen generating correct responses versus poisoned responses. Our results on\nmultiple benchmark datasets and RAG architectures show our approach could\nachieve 98% true positive rate, while maintaining false positive rates close to\n1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) enriches the input to LLMs by retrieving\ninformation from the relevant knowledge database, enabling them to produce\nresponses that are more accurate and contextually appropriate. It is worth\nnoting that the knowledge database, being sourced from publicly available\nchannels such as Wikipedia, inevitably introduces a new attack surface. RAG\npoisoning involves injecting malicious texts into the knowledge database,\nultimately leading to the generation of the attacker's target response (also\ncalled poisoned response). However, there are currently limited methods\navailable for detecting such poisoning attacks. We aim to bridge the gap in\nthis work. Particularly, we introduce RevPRAG, a flexible and automated\ndetection pipeline that leverages the activations of LLMs for poisoned response\ndetection. Our investigation uncovers distinct patterns in LLMs' activations\nwhen generating correct responses versus poisoned responses. Our results on\nmultiple benchmark datasets and RAG architectures show our approach could\nachieve 98% true positive rate, while maintaining false positive rates close to\n1%."
                },
                "authors": [
                    {
                        "name": "Xue Tan"
                    },
                    {
                        "name": "Hao Luan"
                    },
                    {
                        "name": "Mingyu Luo"
                    },
                    {
                        "name": "Xiaoyan Sun"
                    },
                    {
                        "name": "Ping Chen"
                    },
                    {
                        "name": "Jun Dai"
                    }
                ],
                "author_detail": {
                    "name": "Jun Dai"
                },
                "author": "Jun Dai",
                "arxiv_comment": "Accepted to Findings of EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18948v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18948v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03468v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03468v3",
                "updated": "2025-08-29T06:01:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    6,
                    1,
                    31,
                    4,
                    241,
                    0
                ],
                "published": "2025-07-04T10:46:11Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    10,
                    46,
                    11,
                    4,
                    185,
                    0
                ],
                "title": "Robust Localization of Partially Fake Speech: Metrics and Out-of-Domain\n  Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Localization of Partially Fake Speech: Metrics and Out-of-Domain\n  Evaluation"
                },
                "summary": "Partial audio deepfake localization poses unique challenges and remain\nunderexplored compared to full-utterance spoofing detection. While recent\nmethods report strong in-domain performance, their real-world utility remains\nunclear. In this analysis, we critically examine the limitations of current\nevaluation practices, particularly the widespread use of Equal Error Rate\n(EER), which often obscures generalization and deployment readiness. We propose\nreframing the localization task as a sequential anomaly detection problem and\nadvocate for the use of threshold-dependent metrics such as accuracy,\nprecision, recall, and F1-score, which better reflect real-world behavior.\nSpecifically, we analyze the performance of the open-source Coarse-to-Fine\nProposal Refinement Framework (CFPRF), which achieves a 20-ms EER of 7.61% on\nthe in-domain PartialSpoof evaluation set, but 43.25% and 27.59% on the\nLlamaPartialSpoof and Half-Truth out-of-domain test sets. Interestingly, our\nreproduced version of the same model performs worse on in-domain data (9.84%)\nbut better on the out-of-domain sets (41.72% and 14.98%, respectively). This\nhighlights the risks of over-optimizing for in-domain EER, which can lead to\nmodels that perform poorly in real-world scenarios. It also suggests that while\ndeep learning models can be effective on in-domain data, they generalize poorly\nto out-of-domain scenarios, failing to detect novel synthetic samples and\nmisclassifying unfamiliar bona fide audio. Finally, we observe that adding more\nbona fide or fully synthetic utterances to the training data often degrades\nperformance, whereas adding partially fake utterances improves it.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partial audio deepfake localization poses unique challenges and remain\nunderexplored compared to full-utterance spoofing detection. While recent\nmethods report strong in-domain performance, their real-world utility remains\nunclear. In this analysis, we critically examine the limitations of current\nevaluation practices, particularly the widespread use of Equal Error Rate\n(EER), which often obscures generalization and deployment readiness. We propose\nreframing the localization task as a sequential anomaly detection problem and\nadvocate for the use of threshold-dependent metrics such as accuracy,\nprecision, recall, and F1-score, which better reflect real-world behavior.\nSpecifically, we analyze the performance of the open-source Coarse-to-Fine\nProposal Refinement Framework (CFPRF), which achieves a 20-ms EER of 7.61% on\nthe in-domain PartialSpoof evaluation set, but 43.25% and 27.59% on the\nLlamaPartialSpoof and Half-Truth out-of-domain test sets. Interestingly, our\nreproduced version of the same model performs worse on in-domain data (9.84%)\nbut better on the out-of-domain sets (41.72% and 14.98%, respectively). This\nhighlights the risks of over-optimizing for in-domain EER, which can lead to\nmodels that perform poorly in real-world scenarios. It also suggests that while\ndeep learning models can be effective on in-domain data, they generalize poorly\nto out-of-domain scenarios, failing to detect novel synthetic samples and\nmisclassifying unfamiliar bona fide audio. Finally, we observe that adding more\nbona fide or fully synthetic utterances to the training data often degrades\nperformance, whereas adding partially fake utterances improves it."
                },
                "authors": [
                    {
                        "name": "Hieu-Thi Luong"
                    },
                    {
                        "name": "Inbal Rimon"
                    },
                    {
                        "name": "Haim Permuter"
                    },
                    {
                        "name": "Kong Aik Lee"
                    },
                    {
                        "name": "Eng Siong Chng"
                    }
                ],
                "author_detail": {
                    "name": "Eng Siong Chng"
                },
                "author": "Eng Siong Chng",
                "arxiv_comment": "APSIPA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03468v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03468v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00209v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00209v3",
                "updated": "2025-08-29T05:39:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    29,
                    5,
                    39,
                    13,
                    4,
                    241,
                    0
                ],
                "published": "2024-05-31T21:46:23Z",
                "published_parsed": [
                    2024,
                    5,
                    31,
                    21,
                    46,
                    23,
                    4,
                    152,
                    0
                ],
                "title": "Mamba State-Space Models Are Lyapunov-Stable Learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba State-Space Models Are Lyapunov-Stable Learners"
                },
                "summary": "Mamba state-space models (SSMs) have recently outperformed state-of-the-art\n(SOTA) Transformer large language models (LLMs) in various tasks and been\nwidely adapted. However, a major concern for stable learning in recurrent-based\ndeep models (such as SSMs) is the sensitivity of their recurrent dynamics.\nDespite widespread adaptation, the sensitivity of Mamba's recurrent dynamics\nunder common fine-tuning methods-e.g., mixed-precision fine-tuning (MPFT) and\nparameter-efficient fine-tuning (PEFT)-remains unexplored. Empirically, we show\nthat Mamba LLMs are extremely stable to changes introduced by combinations of\nMPFT and PEFT, in stark contrast to Transformer LLMs, which we demonstrate may\ndrastically diverge from their respective full-precision counterparts under\ndifferent combinations of MPFT and PEFT (despite the near-ubiquitous adaptation\nof these fine-tuning frameworks for attention-based models). The demonstrated\nrobustness of Mamba LLMs are due to their recurrent dynamics, which we prove\nare guaranteed to be stable using dynamical systems theory (in particular,\nLyapunov stability). We conclude by using MPFT and PEFT to novelly study Mamba\nLLMs' in-context learning (ICL) abilities on natural language tasks, thus\nsupplementing other recent work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mamba state-space models (SSMs) have recently outperformed state-of-the-art\n(SOTA) Transformer large language models (LLMs) in various tasks and been\nwidely adapted. However, a major concern for stable learning in recurrent-based\ndeep models (such as SSMs) is the sensitivity of their recurrent dynamics.\nDespite widespread adaptation, the sensitivity of Mamba's recurrent dynamics\nunder common fine-tuning methods-e.g., mixed-precision fine-tuning (MPFT) and\nparameter-efficient fine-tuning (PEFT)-remains unexplored. Empirically, we show\nthat Mamba LLMs are extremely stable to changes introduced by combinations of\nMPFT and PEFT, in stark contrast to Transformer LLMs, which we demonstrate may\ndrastically diverge from their respective full-precision counterparts under\ndifferent combinations of MPFT and PEFT (despite the near-ubiquitous adaptation\nof these fine-tuning frameworks for attention-based models). The demonstrated\nrobustness of Mamba LLMs are due to their recurrent dynamics, which we prove\nare guaranteed to be stable using dynamical systems theory (in particular,\nLyapunov stability). We conclude by using MPFT and PEFT to novelly study Mamba\nLLMs' in-context learning (ICL) abilities on natural language tasks, thus\nsupplementing other recent work."
                },
                "authors": [
                    {
                        "name": "John T. Halloran"
                    },
                    {
                        "name": "Manbir Gulati"
                    },
                    {
                        "name": "Paul F. Roysdon"
                    }
                ],
                "author_detail": {
                    "name": "Paul F. Roysdon"
                },
                "author": "Paul F. Roysdon",
                "arxiv_comment": "TMLR, 27 pages, 12 figures, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00209v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00209v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]