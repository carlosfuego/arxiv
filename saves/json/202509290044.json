[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2503.10568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v2",
                "updated": "2025-09-25T13:55:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    55,
                    44,
                    3,
                    268,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel decoupled decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot inference tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only\n32 sampling steps, achieving over a 30 times speedup in inference and a 75\npercent reduction in memory consumption compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel decoupled decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot inference tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only\n32 sampling steps, achieving over a 30 times speedup in inference and a 75\npercent reduction in memory consumption compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21153v1",
                "updated": "2025-09-25T13:39:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    39,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:39:16Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    39,
                    16,
                    3,
                    268,
                    0
                ],
                "title": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP"
                },
                "summary": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings."
                },
                "authors": [
                    {
                        "name": "Moshe Kimhi"
                    },
                    {
                        "name": "Erez Koifman"
                    },
                    {
                        "name": "Ehud Rivlin"
                    },
                    {
                        "name": "Eli Schwartz"
                    },
                    {
                        "name": "Chaim Baskin"
                    }
                ],
                "author_detail": {
                    "name": "Chaim Baskin"
                },
                "author": "Chaim Baskin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22156v2",
                "updated": "2025-09-25T13:15:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    15,
                    45,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing"
                },
                "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "18 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.17138v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.17138v3",
                "updated": "2025-09-25T10:45:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    45,
                    1,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-22T06:12:42Z",
                "published_parsed": [
                    2025,
                    5,
                    22,
                    6,
                    12,
                    42,
                    3,
                    142,
                    0
                ],
                "title": "Runtime-Adaptive Pruning for LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runtime-Adaptive Pruning for LLM Inference"
                },
                "summary": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at language understanding and generation,\nbut their enormous computational and memory requirements hinder deployment.\nCompression offers a potential solution to mitigate these constraints. However,\nmost existing methods rely on fixed heuristics and thus fail to adapt to\nruntime memory variations or heterogeneous KV-cache demands arising from\ndiverse user requests. To address these limitations, we propose RAP, an elastic\npruning framework driven by reinforcement learning (RL) that dynamically\nadjusts compression strategies in a runtime-aware manner. Specifically, RAP\ndynamically tracks the evolving ratio between model parameters and KV-cache\nacross practical execution. Recognizing that FFNs house most parameters,\nwhereas parameter -light attention layers dominate KV-cache formation, the RL\nagent retains only those components that maximize utility within the current\nmemory budget, conditioned on instantaneous workload and device state.\nExtensive experiments results demonstrate that RAP outperforms state-of-the-art\nbaselines, marking the first time to jointly consider model weights and\nKV-cache on the fly."
                },
                "authors": [
                    {
                        "name": "Huanrong Liu"
                    },
                    {
                        "name": "Chunlin Tian"
                    },
                    {
                        "name": "Xuyang Wei"
                    },
                    {
                        "name": "Qingbiao Li"
                    },
                    {
                        "name": "Li Li"
                    }
                ],
                "author_detail": {
                    "name": "Li Li"
                },
                "author": "Li Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.17138v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.17138v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v2",
                "updated": "2025-09-25T10:24:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    24,
                    14,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Modern large language models (LLMs) extend context lengths to up to millions\nof tokens, enabling AI assistants to generate coherent and personalized\nresponses grounded in long conversational histories. This ability, however,\nhinges on Key-Value (KV) caching, whose memory grows linearly with dialogue\nlength and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache\ncompression, which seeks to limit cache size while preserving accuracy. Yet\nexisting methods face two major limitations: (i) evicting the KV cache after\nfull-context prefill causes unbounded peak memory, and (ii) query-dependent\neviction narrows the cache to a single query, leading to failure cases in\nmulti-turn conversations. We introduce EpiCache, a training-free KV cache\nmanagement framework for long conversational question answering (LongConvQA)\nunder fixed memory budgets. EpiCache bounds cache growth through block-wise\nprefill and preserves topic-relevant context via episodic KV compression, which\nclusters conversation history into coherent episodes and applies\nepisode-specific KV cache eviction. We further design an adaptive layer-wise\nbudget allocation strategy that measures each layer's sensitivity to eviction\nand distributes the memory budget across layers accordingly. Across three\nLongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent\nbaselines, sustains near-full KV accuracy under 4-6x compression, and reduces\nlatency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) extend context lengths to up to millions\nof tokens, enabling AI assistants to generate coherent and personalized\nresponses grounded in long conversational histories. This ability, however,\nhinges on Key-Value (KV) caching, whose memory grows linearly with dialogue\nlength and quickly becomes the bottleneck in resource-constrained environments.\nAn active line of research for reducing memory bottleneck is KV cache\ncompression, which seeks to limit cache size while preserving accuracy. Yet\nexisting methods face two major limitations: (i) evicting the KV cache after\nfull-context prefill causes unbounded peak memory, and (ii) query-dependent\neviction narrows the cache to a single query, leading to failure cases in\nmulti-turn conversations. We introduce EpiCache, a training-free KV cache\nmanagement framework for long conversational question answering (LongConvQA)\nunder fixed memory budgets. EpiCache bounds cache growth through block-wise\nprefill and preserves topic-relevant context via episodic KV compression, which\nclusters conversation history into coherent episodes and applies\nepisode-specific KV cache eviction. We further design an adaptive layer-wise\nbudget allocation strategy that measures each layer's sensitivity to eviction\nand distributes the memory budget across layers accordingly. Across three\nLongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent\nbaselines, sustains near-full KV accuracy under 4-6x compression, and reduces\nlatency and memory by up to 2.4x and 3.5x, thereby enabling efficient\nmulti-turn interaction under strict resource constraints."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20979v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20979v1",
                "updated": "2025-09-25T10:23:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    23,
                    50,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T10:23:50Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    10,
                    23,
                    50,
                    3,
                    268,
                    0
                ],
                "title": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Robust and Efficient ML-Based GPU Caching for Modern Inference"
                },
                "summary": "In modern GPU inference, cache efficiency remains a major bottleneck. In\nrecommendation models, embedding hit rates largely determine throughput, while\nin large language models, KV-cache misses substantially increase\ntime-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often\nstruggle under structured access patterns. Learning-based approaches are\npromising, but in practice face two major limitations: they degrade sharply\nwhen predictions are inaccurate, or they gain little even with accurate\npredictions due to conservative designs. Some also incur high overhead, further\nlimiting practicality.\n  We present \\textsc{LCR}, a practical framework for learning-based GPU caching\nthat delivers performance gains while ensuring robustness and efficiency. Its\ncore algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned\npredictions and dynamically adapts to prediction accuracy through online error\nestimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal\nperformance. With inaccurate predictions, it degrades gracefully to\nnear-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between\nempirical progress and theoretical advances in learning-based caching.\n  Experiments show that \\textsc{LCR} delivers consistent gains under realistic\nconditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\%\nand reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference\nsystems. Even under poor predictions, its performance remains stable,\ndemonstrating practical robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern GPU inference, cache efficiency remains a major bottleneck. In\nrecommendation models, embedding hit rates largely determine throughput, while\nin large language models, KV-cache misses substantially increase\ntime-to-first-token (TTFT). Heuristic policies such as \\textsc{LRU} often\nstruggle under structured access patterns. Learning-based approaches are\npromising, but in practice face two major limitations: they degrade sharply\nwhen predictions are inaccurate, or they gain little even with accurate\npredictions due to conservative designs. Some also incur high overhead, further\nlimiting practicality.\n  We present \\textsc{LCR}, a practical framework for learning-based GPU caching\nthat delivers performance gains while ensuring robustness and efficiency. Its\ncore algorithm, \\textsc{LARU}, enhances \\textsc{LRU} with machine-learned\npredictions and dynamically adapts to prediction accuracy through online error\nestimation. When predictions are accurate, \\textsc{LARU} achieves near-optimal\nperformance. With inaccurate predictions, it degrades gracefully to\nnear-\\textsc{LRU} performance. With \\textsc{LCR}, we bridge the gap between\nempirical progress and theoretical advances in learning-based caching.\n  Experiments show that \\textsc{LCR} delivers consistent gains under realistic\nconditions. In DLRM and LLM scenarios, it improves throughput by up to 24.2\\%\nand reduces P99 TTFT by up to 28.3\\%, outperforming widely used inference\nsystems. Even under poor predictions, its performance remains stable,\ndemonstrating practical robustness."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Yirong Zhang"
                    },
                    {
                        "name": "Jiahong Yu"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Jianping Zou"
                    },
                    {
                        "name": "Gang Xiong"
                    },
                    {
                        "name": "Kingsum Chow"
                    },
                    {
                        "name": "Shuibing He"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20979v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20979v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v5",
                "updated": "2025-09-25T09:49:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    9,
                    49,
                    59,
                    3,
                    268,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robustifying Learning-Augmented Caching Efficiently without Compromising\n  1-Consistency"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.17892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.17892v2",
                "updated": "2025-09-25T03:30:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    30,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-25T10:59:02Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    10,
                    59,
                    2,
                    0,
                    237,
                    0
                ],
                "title": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ILRe: Intermediate Layer Retrieval for Context Compression in Causal\n  Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to\na few tenths of that required for the full context, but also delivers\nperformance comparable to or superior to the full-context setup in long-context\nscenarios. Without additional post training or operator development, ILRe can\nprocess a single $1M$ tokens request in less than half a minute (speedup\n$\\approx 180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with\nmodel Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated success across many\nbenchmarks. However, they still exhibit limitations in long-context scenarios,\nprimarily due to their short effective context length, quadratic computational\ncomplexity, and high memory overhead when processing lengthy inputs. To\nmitigate these issues, we introduce a novel context compression pipeline,\ncalled Intermediate Layer Retrieval (ILRe), which determines one intermediate\ndecoder layer offline, encodes context by streaming chunked prefill only up to\nthat layer, and recalls tokens by the attention scores between the input query\nand full key cache in that specified layer. In particular, we propose a\nmulti-pooling kernels allocating strategy in the token recalling process to\nmaintain the completeness of semantics. Our approach not only reduces the\nprefilling complexity from $O(L^2)$ to $O(L)$ and trims the memory footprint to\na few tenths of that required for the full context, but also delivers\nperformance comparable to or superior to the full-context setup in long-context\nscenarios. Without additional post training or operator development, ILRe can\nprocess a single $1M$ tokens request in less than half a minute (speedup\n$\\approx 180\\times$) and scores RULER-$1M$ benchmark of $\\approx 79.8$ with\nmodel Llama-3.1-UltraLong-8B-1M-Instruct on a Huawei Ascend 910B NPU."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "Mandi Liu"
                    },
                    {
                        "name": "Jiangzhou Ji"
                    },
                    {
                        "name": "Huaijun Li"
                    },
                    {
                        "name": "Haobo Yang"
                    },
                    {
                        "name": "Yaohan He"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.17892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.17892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15919v2",
                "updated": "2025-09-25T03:00:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    0,
                    22,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-21T18:40:20Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    18,
                    40,
                    20,
                    3,
                    233,
                    0
                ],
                "title": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperFlexis: Joint Design of Algorithms and Systems for Multi-SLO\n  Serving and Fast Scaling"
                },
                "summary": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a\nunified LLM serving system that integrates algorithmic and system-level\ninnovations to jointly optimize scheduling and scaling under multiple SLOs. It\nfeatures a multi-SLO-aware scheduler that leverages budget estimation and\nrequest prioritization to ensure proactive SLO compliance for both new and\nongoing requests. The system supports prefill- and decode-stage multi-SLO\nscheduling for P/D-disaggregated architectures and KV cache transfers. It also\nenables cost-effective scaling decisions, prefill-decode instance linking\nduring scaling, and rapid P/D role transitions. To accelerate scaling and\nreduce cold-start latency, a device-to-device (D2D) weight transfer mechanism\nis proposed that lowers weight loading overhead by up to 19.39$\\times$. These\noptimizations allow the system to achieve up to 4.44$\\times$ higher SLO\nattainment, 65.82% lower request latency, and cost parity with state-of-the-art\nbaselines. The code will be released soon.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language model (LLM) serving systems face challenges from highly\nvariable requests with diverse lengths, priorities, and stage-specific\nservice-level objectives (SLOs). Meeting these requires real-time scheduling,\nrapid and cost-effective scaling, and support for both collocated and\ndisaggregated Prefill/Decode (P/D) architectures. We present HyperFlexis, a\nunified LLM serving system that integrates algorithmic and system-level\ninnovations to jointly optimize scheduling and scaling under multiple SLOs. It\nfeatures a multi-SLO-aware scheduler that leverages budget estimation and\nrequest prioritization to ensure proactive SLO compliance for both new and\nongoing requests. The system supports prefill- and decode-stage multi-SLO\nscheduling for P/D-disaggregated architectures and KV cache transfers. It also\nenables cost-effective scaling decisions, prefill-decode instance linking\nduring scaling, and rapid P/D role transitions. To accelerate scaling and\nreduce cold-start latency, a device-to-device (D2D) weight transfer mechanism\nis proposed that lowers weight loading overhead by up to 19.39$\\times$. These\noptimizations allow the system to achieve up to 4.44$\\times$ higher SLO\nattainment, 65.82% lower request latency, and cost parity with state-of-the-art\nbaselines. The code will be released soon."
                },
                "authors": [
                    {
                        "name": "Zahra Yousefijamarani"
                    },
                    {
                        "name": "Xinglu Wang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Morgan Lindsay Heisler"
                    },
                    {
                        "name": "Taha Shabani"
                    },
                    {
                        "name": "Niloofar Gholipour"
                    },
                    {
                        "name": "Parham Yassini"
                    },
                    {
                        "name": "Hong Chang"
                    },
                    {
                        "name": "Kan Chen"
                    },
                    {
                        "name": "Qiantao Zhang"
                    },
                    {
                        "name": "Xiaolong Bai"
                    },
                    {
                        "name": "Jiannan Wang"
                    },
                    {
                        "name": "Ying Xiong"
                    },
                    {
                        "name": "Yong Zhang"
                    },
                    {
                        "name": "Zhenan Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Fan"
                },
                "author": "Zhenan Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20617v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20617v1",
                "updated": "2025-09-24T23:47:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    23,
                    47,
                    55,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T23:47:55Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    23,
                    47,
                    55,
                    2,
                    267,
                    0
                ],
                "title": "DELM: a Python toolkit for Data Extraction with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELM: a Python toolkit for Data Extraction with Language Models"
                },
                "summary": "Large Language Models (LLMs) have become powerful tools for annotating\nunstructured data. However, most existing workflows rely on ad hoc scripts,\nmaking reproducibility, robustness, and systematic evaluation difficult. To\naddress these challenges, we introduce DELM (Data Extraction with Language\nModels), an open-source Python toolkit designed for rapid experimental\niteration of LLM-based data extraction pipelines and for quantifying the\ntrade-offs between them. DELM minimizes boilerplate code and offers a modular\nframework with structured outputs, built-in validation, flexible data-loading\nand scoring strategies, and efficient batch processing. It also includes robust\nsupport for working with LLM APIs, featuring retry logic, result caching,\ndetailed cost tracking, and comprehensive configuration management. We showcase\nDELM's capabilities through two case studies: one featuring a novel prompt\noptimization algorithm, and another illustrating how DELM quantifies trade-offs\nbetween cost and coverage when selecting keywords to decide which paragraphs to\npass to an LLM. DELM is available at\n\\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become powerful tools for annotating\nunstructured data. However, most existing workflows rely on ad hoc scripts,\nmaking reproducibility, robustness, and systematic evaluation difficult. To\naddress these challenges, we introduce DELM (Data Extraction with Language\nModels), an open-source Python toolkit designed for rapid experimental\niteration of LLM-based data extraction pipelines and for quantifying the\ntrade-offs between them. DELM minimizes boilerplate code and offers a modular\nframework with structured outputs, built-in validation, flexible data-loading\nand scoring strategies, and efficient batch processing. It also includes robust\nsupport for working with LLM APIs, featuring retry logic, result caching,\ndetailed cost tracking, and comprehensive configuration management. We showcase\nDELM's capabilities through two case studies: one featuring a novel prompt\noptimization algorithm, and another illustrating how DELM quantifies trade-offs\nbetween cost and coverage when selecting keywords to decide which paragraphs to\npass to an LLM. DELM is available at\n\\href{https://github.com/Center-for-Applied-AI/delm}{\\texttt{github.com/Center-for-Applied-AI/delm}}."
                },
                "authors": [
                    {
                        "name": "Eric Fithian"
                    },
                    {
                        "name": "Kirill Skobelev"
                    }
                ],
                "author_detail": {
                    "name": "Kirill Skobelev"
                },
                "author": "Kirill Skobelev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20617v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20617v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03090v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03090v2",
                "updated": "2025-09-24T16:56:17Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    16,
                    56,
                    17,
                    2,
                    267,
                    0
                ],
                "published": "2024-10-04T02:32:36Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    2,
                    32,
                    36,
                    4,
                    278,
                    0
                ],
                "title": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UNComp: Can Matrix Entropy Uncover Sparsity? -- A Compressor Design from\n  an Uncertainty-Aware Perspective"
                },
                "summary": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying large language models (LLMs) for long-context inference remains\nchallenging due to their substantial memory and computational demands. While\ntechniques such as Key-Value (KV) cache compression are designed to reduce\nmemory usage, they often neglect the structured sparsity inherent in the\nrelationship between hidden states and their corresponding KV cache. In this\nwork, we explore the role of uncertainty as a potential indicator of sparsity\nwithin LLMs. We propose UNComp, an uncertainty-aware framework that leverages\ntruncated matrix entropy to identify areas of low information content, thereby\nrevealing sparsity patterns that can be used for adaptive compression. Unlike\ntraditional methods that apply uniform compression, UNComp dynamically adjusts\nits approach to compression, guided by uncertainty measures that reflect the\nimportance of various model components. Our analysis shows that sparsity\npatterns, when derived from uncertainty estimates, can be exploited to reveal\nspecial long-range dependencies, such as retrieval heads and retrieval layers.\nThis perspective not only enhances our understanding of how compression can be\noptimized but also provides new insights into the inherent sparsity of LLMs\nduring long-context inference. By focusing on uncertainty to analyze the\nsparsity pattern in detail, UNComp reduces the KV cache size to 4.74% of the\noriginal, achieves a 6% prefill speedup, and improves throughput by 6.4x - not\nonly delivering strong lossless compression performance, but also validating\nthe effectiveness of the underlying theoretical tool. We release the code at\nhttps://github.com/menik1126/UNComp."
                },
                "authors": [
                    {
                        "name": "Jing Xiong"
                    },
                    {
                        "name": "Jianghan Shen"
                    },
                    {
                        "name": "Fanghua Ye"
                    },
                    {
                        "name": "Chaofan Tao"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Xun Wu"
                    },
                    {
                        "name": "Chuanyang Zheng"
                    },
                    {
                        "name": "Zhijiang Guo"
                    },
                    {
                        "name": "Min Yang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Ngai Wong"
                    }
                ],
                "author_detail": {
                    "name": "Ngai Wong"
                },
                "author": "Ngai Wong",
                "arxiv_comment": "Accepted at EMNLP 2025 (Main Conference)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03090v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03090v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19729v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19729v1",
                "updated": "2025-09-24T03:15:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    3,
                    15,
                    37,
                    2,
                    267,
                    0
                ],
                "published": "2025-09-24T03:15:37Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    3,
                    15,
                    37,
                    2,
                    267,
                    0
                ],
                "title": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gyges: Dynamic Cross-Instance Parallelism Transformation for Efficient\n  LLM Inference"
                },
                "summary": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently processing the dynamics of requests, especially the context\nlength variance, is important in Large Language Model (LLM) serving scenarios.\nHowever, there is an intrinsic trade-off: while leveraging parallelism\nstrategies, such as Tensor Parallelism (TP), can coordinate multiple GPUs to\naccommodate larger context lengths, it inevitably results in degraded overall\nthroughput. In this paper, we propose Cross-Instance Parallelism Transformation\n(Gyges), which adaptively adjusts the parallelism strategies of running\ninstances to align with the dynamics of incoming requests. We design (1) a\npage-friendly, header-centric layout to accelerate KV cache transformations;\n(2) dedicated weight padding to accelerate model weight transformations; and\n(3) a transformation-aware scheduler to cooperatively schedule requests and\nparallelism transformations, optimizing the overall performance. Evaluations\nusing real-world traces show that Gyges improves throughput by 1.75x-6.57x\ncompared to state-of-the-art solutions."
                },
                "authors": [
                    {
                        "name": "Haoyu Chen"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Kun Qian"
                    },
                    {
                        "name": "Yu Guan"
                    },
                    {
                        "name": "Jin Zhao"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "12 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19729v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19729v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13523v2",
                "updated": "2025-09-24T01:32:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    24,
                    1,
                    32,
                    55,
                    2,
                    267,
                    0
                ],
                "published": "2025-08-19T05:27:53Z",
                "published_parsed": [
                    2025,
                    8,
                    19,
                    5,
                    27,
                    53,
                    1,
                    231,
                    0
                ],
                "title": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAMMPS-KOKKOS: Performance Portable Molecular Dynamics Across Exascale\n  Architectures"
                },
                "summary": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on three exascale machines\n-- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS\nAlps supercomputer, for the three potentials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its inception in 1995, LAMMPS has grown to be a world-class molecular\ndynamics code, with thousands of users, over one million lines of code, and\nmulti-scale simulation capabilities. We discuss how LAMMPS has adapted to the\nmodern heterogeneous computing landscape by integrating the Kokkos performance\nportability library into the existing C++ code. We investigate performance\nportability of simple pairwise, many-body reactive, and machine-learned\nforce-field interatomic potentials. We present results on GPUs across different\nvendors and generations, and analyze performance trends, probing FLOPS\nthroughput, memory bandwidths, cache capabilities, and thread-atomic operation\nperformance. Finally, we demonstrate strong scaling on three exascale machines\n-- OLCF Frontier, ALCF Aurora, and NNSA El Capitan -- as well as on the CSCS\nAlps supercomputer, for the three potentials."
                },
                "authors": [
                    {
                        "name": "Anders Johansson"
                    },
                    {
                        "name": "Evan Weinberg"
                    },
                    {
                        "name": "Christian R. Trott"
                    },
                    {
                        "name": "Megan J. McCarthy"
                    },
                    {
                        "name": "Stan G. Moore"
                    }
                ],
                "author_detail": {
                    "name": "Stan G. Moore"
                },
                "author": "Stan G. Moore",
                "arxiv_doi": "10.1145/3731599.3767498",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3731599.3767498",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.13523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 7 figures",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; C.2.4; C.4; D.1.3; D.3.4; E.1; I.6; I.6.8; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19599v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19599v1",
                "updated": "2025-09-23T21:46:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    46,
                    38,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T21:46:38Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    46,
                    38,
                    1,
                    266,
                    0
                ],
                "title": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method\n  for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Base-Aware Orchestration: A Dynamic, Privacy-Preserving Method\n  for Multi-Agent Systems"
                },
                "summary": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent systems (MAS) are increasingly tasked with solving complex,\nknowledge-intensive problems where effective agent orchestration is critical.\nConventional orchestration methods rely on static agent descriptions, which\noften become outdated or incomplete. This limitation leads to inefficient task\nrouting, particularly in dynamic environments where agent capabilities\ncontinuously evolve. We introduce Knowledge Base-Aware (KBA) Orchestration, a\nnovel approach that augments static descriptions with dynamic,\nprivacy-preserving relevance signals derived from each agent's internal\nknowledge base (KB). In the proposed framework, when static descriptions are\ninsufficient for a clear routing decision, the orchestrator prompts the\nsubagents in parallel. Each agent then assesses the task's relevance against\nits private KB, returning a lightweight ACK signal without exposing the\nunderlying data. These collected signals populate a shared semantic cache,\nproviding dynamic indicators of agent suitability for future queries. By\ncombining this novel mechanism with static descriptions, our method achieves\nmore accurate and adaptive task routing preserving agent autonomy and data\nconfidentiality. Benchmarks show that our KBA Orchestration significantly\noutperforms static description-driven methods in routing precision and overall\nsystem efficiency, making it suitable for large-scale systems that require\nhigher accuracy than standard description-driven routing."
                },
                "authors": [
                    {
                        "name": "Danilo Trombino"
                    },
                    {
                        "name": "Vincenzo Pecorella"
                    },
                    {
                        "name": "Alessandro de Giulii"
                    },
                    {
                        "name": "Davide Tresoldi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Tresoldi"
                },
                "author": "Davide Tresoldi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19599v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19599v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v4",
                "updated": "2025-09-23T21:08:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    8,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Autoregressive Video Diffusion Models"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to an autoregressive\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nachieves a total score of 84.27 on the VBench-Long benchmark, surpassing all\nprevious video generation models. It enables fast streaming generation of\nhigh-quality videos at 9.4 FPS on a single GPU thanks to KV caching. Our\napproach also enables streaming video-to-video translation, image-to-video, and\ndynamic prompting in a zero-shot manner."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "CVPR 2025. Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.03227v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.03227v3",
                "updated": "2025-09-23T20:25:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    20,
                    25,
                    15,
                    1,
                    266,
                    0
                ],
                "published": "2024-04-04T06:24:11Z",
                "published_parsed": [
                    2024,
                    4,
                    4,
                    6,
                    24,
                    11,
                    3,
                    95,
                    0
                ],
                "title": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized Learning Strategies for Estimation Error Minimization with\n  Graph Neural Networks"
                },
                "summary": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the challenge of sampling and remote estimation for autoregressive\nMarkovian processes in a multi-hop wireless network with\nstatistically-identical agents. Agents cache the most recent samples from\nothers and communicate over wireless collision channels governed by an\nunderlying graph topology. Our goal is to minimize time-average estimation\nerror and/or age of information with decentralized scalable sampling and\ntransmission policies, considering both oblivious (where decision-making is\nindependent of the physical processes) and non-oblivious policies (where\ndecision-making depends on physical processes). We prove that in oblivious\npolicies, minimizing estimation error is equivalent to minimizing the age of\ninformation. The complexity of the problem, especially the multi-dimensional\naction spaces and arbitrary network topologies, makes theoretical methods for\nfinding optimal transmission policies intractable. We optimize the policies\nusing a graphical multi-agent reinforcement learning framework, where each\nagent employs a permutation-equivariant graph neural network architecture.\nTheoretically, we prove that our proposed framework exhibits desirable\ntransferability properties, allowing transmission policies trained on small- or\nmoderate-size networks to be executed effectively on large-scale topologies.\nNumerical experiments demonstrate that (i) Our proposed framework outperforms\nstate-of-the-art baselines; (ii) The trained policies are transferable to\nlarger networks, and their performance gains increase with the number of\nagents; (iii) The training procedure withstands non-stationarity even if we\nutilize independent learning techniques; and, (iv) Recurrence is pivotal in\nboth independent learning and centralized training and decentralized execution,\nand improves the resilience to non-stationarity in independent learning."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Navid NaderiAlizadeh"
                    },
                    {
                        "name": "Alejandro Ribeiro"
                    },
                    {
                        "name": "Shirin Saeedi Bidokhti"
                    }
                ],
                "author_detail": {
                    "name": "Shirin Saeedi Bidokhti"
                },
                "author": "Shirin Saeedi Bidokhti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.03227v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.03227v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19459v1",
                "updated": "2025-09-23T18:14:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    18,
                    14,
                    21,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T18:14:21Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    18,
                    14,
                    21,
                    1,
                    266,
                    0
                ],
                "title": "Automated Insertion of Flushes and Fences for Persistency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Insertion of Flushes and Fences for Persistency"
                },
                "summary": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL shared memory and persistent memory allow the contents of memory to\npersist beyond crashes. Stores to persistent or CXL memory are typically not\nimmediately made persistent; developers must manually flush the corresponding\ncache lines to force the data to be written to the underlying storage.\nCorrectly using flush and fence operations is known to be challenging. While\nstate-of-the-art tools can find missing flush instructions, they often require\nbug-revealing test cases. No existing tools can ensure the absence of missing\nflush bugs.\n  In this paper, we present PMRobust, a compiler that automatically inserts\nflush and fence operations to ensure that code using persistent memory is free\nfrom missing flush and fence bugs. PMRobust employs a novel static analysis\nwith optimizations that target newly allocated objects. We have evaluated\nPMRobust on persistent memory libraries and several persistent memory data\nstructures and measured a geometric mean overhead of 0.26% relative to the\noriginal benchmarks with hand-placed flush and fence operations."
                },
                "authors": [
                    {
                        "name": "Yutong Guo"
                    },
                    {
                        "name": "Weiyu Luo"
                    },
                    {
                        "name": "Brian Demsky"
                    }
                ],
                "author_detail": {
                    "name": "Brian Demsky"
                },
                "author": "Brian Demsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19260v1",
                "updated": "2025-09-23T17:18:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    18,
                    59,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T17:18:59Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    17,
                    18,
                    59,
                    1,
                    266,
                    0
                ],
                "title": "Reconstruction of a potential parameter in time-fractional diffusion\n  problems via a Kohn--Vogelius type functional: Theoretical aspects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstruction of a potential parameter in time-fractional diffusion\n  problems via a Kohn--Vogelius type functional: Theoretical aspects"
                },
                "summary": "Of concern is the problem of reconstructing a space-dependent potential from\nboundary observations in the Caputo time-fractional diffusion equation,\nutilizing a stable and robust recovery method. We develop an algorithm to\nminimize the Kohn-Vogelius (KV) cost function, which measures the difference\nbetween the solutions of two excitations. The inverse potential problem is\nrecast into an optimization problem, where the objective is to minimize a\nKohn-Vogelius-type functional within a set of admissible potentials. We\nestablish the well-posedness of this optimization problem by proving the\nexistence and uniqueness of a minimizer and demonstrating its stability with\nrespect to perturbations in the boundary data. Furthermore, we analyze the\nFr\\'echet differentiability of the KV functional and prove the Lipschitz\ncontinuity of its gradient. These theoretical results enable the development of\na convergent conjugate gradient algorithm for numerical reconstruction. The\neffectiveness and robustness of the proposed method are confirmed through\nseveral numerical examples in both one and two dimensions, including cases with\nnoisy data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Of concern is the problem of reconstructing a space-dependent potential from\nboundary observations in the Caputo time-fractional diffusion equation,\nutilizing a stable and robust recovery method. We develop an algorithm to\nminimize the Kohn-Vogelius (KV) cost function, which measures the difference\nbetween the solutions of two excitations. The inverse potential problem is\nrecast into an optimization problem, where the objective is to minimize a\nKohn-Vogelius-type functional within a set of admissible potentials. We\nestablish the well-posedness of this optimization problem by proving the\nexistence and uniqueness of a minimizer and demonstrating its stability with\nrespect to perturbations in the boundary data. Furthermore, we analyze the\nFr\\'echet differentiability of the KV functional and prove the Lipschitz\ncontinuity of its gradient. These theoretical results enable the development of\na convergent conjugate gradient algorithm for numerical reconstruction. The\neffectiveness and robustness of the proposed method are confirmed through\nseveral numerical examples in both one and two dimensions, including cases with\nnoisy data."
                },
                "authors": [
                    {
                        "name": "Hamza Kahlaoui"
                    },
                    {
                        "name": "Mourad Hrizi"
                    },
                    {
                        "name": "Abdessamad Oulmelk"
                    },
                    {
                        "name": "Xiangcheng Zheng"
                    },
                    {
                        "name": "Ahmed Hendy"
                    }
                ],
                "author_detail": {
                    "name": "Ahmed Hendy"
                },
                "author": "Ahmed Hendy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19228v1",
                "updated": "2025-09-23T16:49:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T16:49:43Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    16,
                    49,
                    43,
                    1,
                    266,
                    0
                ],
                "title": "CompLLM: Compression for Long Context Q&A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompLLM: Compression for Long Context Q&A"
                },
                "summary": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) face significant computational challenges when\nprocessing long contexts due to the quadratic complexity of self-attention.\nWhile soft context compression methods, which map input text to smaller latent\nrepresentations, have shown promise, their real-world adoption is limited.\nExisting techniques typically compress the context as a single unit, which\nleads to quadratic compression complexity and an inability to reuse\ncomputations across queries with overlapping contexts. In this work, we\nintroduce CompLLM, a soft compression technique designed for practical\ndeployment. Instead of processing the context holistically, CompLLM divides it\ninto segments and compresses each one independently. This simple design choice\nyields three critical properties: efficiency, as the compression step scales\nlinearly with the context length; scalability, enabling models trained on short\nsequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and\nreusability, allowing compressed segments to be cached and reused across\ndifferent queries. Our experiments show that with a 2x compression rate, at\nhigh context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x\nand reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance\ncomparable to that obtained with the uncompressed context, and even surpasses\nit on very long sequences, demonstrating its effectiveness and practical\nutility."
                },
                "authors": [
                    {
                        "name": "Gabriele Berton"
                    },
                    {
                        "name": "Jayakrishnan Unnikrishnan"
                    },
                    {
                        "name": "Son Tran"
                    },
                    {
                        "name": "Mubarak Shah"
                    }
                ],
                "author_detail": {
                    "name": "Mubarak Shah"
                },
                "author": "Mubarak Shah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19061v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19061v1",
                "updated": "2025-09-23T14:25:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    25,
                    13,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T14:25:13Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    14,
                    25,
                    13,
                    1,
                    266,
                    0
                ],
                "title": "3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes\n  Equations with Applications to Geodynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Blocking for Matrix-free Smoothers in 2D Variable-Viscosity Stokes\n  Equations with Applications to Geodynamics"
                },
                "summary": "We present the design, implementation, and evaluation of optimized\nmatrix-free stencil kernels for multigrid smoothing in the incompressible\nStokes equations with variable viscosity, motivated by geophysical flow\nproblems. We investigate five smoother variants derived from different\noptimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked\nfused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a\nstrategy that applies local iterations on overlapping tiles to improve cache\nreuse. To ensure correctness, we introduce an energy-based residual norm that\nbalances velocity and pressure contributions, and validate all implementations\nusing a high-contrast sinker benchmark representative of realistic geodynamic\nnumerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of\nthe ALPS supercomputer demonstrates that all smoothers scale well within a\nsingle NUMA domain, but the RAS-Jacobi smoother consistently achieves the best\nperformance at higher core counts. It sustains over 90% weak-scaling efficiency\nup to 64 cores and delivers up to a threefold speedup compared to the C++\nJacobi baseline, owing to improved cache reuse and reduced memory traffic.\nThese results show that temporal blocking, already employed in\ndistributed-memory solvers to reduce communication, can also provide\nsubstantial benefits at the socket and NUMA level. This work highlights the\nimportance of cache-aware stencil design for harnessing modern heterogeneous\narchitectures and lays the groundwork for extending RAS-type temporal blocking\nstrategies to three-dimensional problems and GPU accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design, implementation, and evaluation of optimized\nmatrix-free stencil kernels for multigrid smoothing in the incompressible\nStokes equations with variable viscosity, motivated by geophysical flow\nproblems. We investigate five smoother variants derived from different\noptimisation strategies: Red-Black Gauss-Seidel, Jacobi, fused Jacobi, blocked\nfused Jacobi, and a novel Jacobi smoother with RAS-type temporal blocking, a\nstrategy that applies local iterations on overlapping tiles to improve cache\nreuse. To ensure correctness, we introduce an energy-based residual norm that\nbalances velocity and pressure contributions, and validate all implementations\nusing a high-contrast sinker benchmark representative of realistic geodynamic\nnumerical models. Our performance study on NVIDIA GH200 Grace Hopper nodes of\nthe ALPS supercomputer demonstrates that all smoothers scale well within a\nsingle NUMA domain, but the RAS-Jacobi smoother consistently achieves the best\nperformance at higher core counts. It sustains over 90% weak-scaling efficiency\nup to 64 cores and delivers up to a threefold speedup compared to the C++\nJacobi baseline, owing to improved cache reuse and reduced memory traffic.\nThese results show that temporal blocking, already employed in\ndistributed-memory solvers to reduce communication, can also provide\nsubstantial benefits at the socket and NUMA level. This work highlights the\nimportance of cache-aware stencil design for harnessing modern heterogeneous\narchitectures and lays the groundwork for extending RAS-type temporal blocking\nstrategies to three-dimensional problems and GPU accelerators."
                },
                "authors": [
                    {
                        "name": "Marcel Ferrari"
                    },
                    {
                        "name": "Cyrill Pntener"
                    },
                    {
                        "name": "Alexander Sotoudeh"
                    },
                    {
                        "name": "Niklas Viebig"
                    }
                ],
                "author_detail": {
                    "name": "Niklas Viebig"
                },
                "author": "Niklas Viebig",
                "arxiv_comment": "15 pages, 5 figures, appendix has 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19061v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19061v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F08, 65N55, 65N22, 76M20",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.1.8; F.2.1; D.1.3; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18909v1",
                "updated": "2025-09-23T12:32:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    32,
                    51,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T12:32:51Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    12,
                    32,
                    51,
                    1,
                    266,
                    0
                ],
                "title": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Obelix: Mitigating Side-Channels Through Dynamic Obfuscation"
                },
                "summary": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trusted execution environments (TEEs) offer hardware-assisted means to\nprotect code and data. However, as shown in numerous results over the years,\nattackers can use side-channels to leak data access patterns and even\nsingle-step the code. While the vendors are slowly introducing hardware-based\ncountermeasures for some attacks, others will stay unaddressed. This makes a\nsoftware-level countermeasure desirable, but current available solutions only\naddress very specific attack vectors or have a narrow leakage model.\n  In this work, we take a holistic view at the vulnerabilities of TEEs and\ndesign a tool named Obelix, which is the first to protect both code and data\nagainst a wide range of TEE attacks, from cache attacks over single-stepping to\nciphertext side-channels. We analyze the practically achievable precision of\nstate-of-the-art single-stepping tools, and present an algorithm which uses\nthat knowledge to divide a program into uniform code blocks, that are\nindistinguishable for a strong attacker. By storing these blocks and the\nprogram data in oblivious RAM, the attacker cannot follow execution,\neffectively protecting both secret code and data. We describe how we automate\nour approach to make it available for developers who are unfamiliar with\nside-channels. As an obfuscation tool, Obelix comes with a considerable\nperformance overhead, but compensates this with strong security guarantees and\neasy applicability without requiring any expert knowledge."
                },
                "authors": [
                    {
                        "name": "Jan Wichelmann"
                    },
                    {
                        "name": "Anja Rabich"
                    },
                    {
                        "name": "Anna P\"atschke"
                    },
                    {
                        "name": "Thomas Eisenbarth"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Eisenbarth"
                },
                "author": "Thomas Eisenbarth",
                "arxiv_doi": "10.1109/SP54263.2024.00261",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SP54263.2024.00261",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.18909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "2024 IEEE Symposium on Security and Privacy (SP), San Francisco,\n  CA, USA, 2024, pp. 4182-4199",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.04467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.04467v3",
                "updated": "2025-09-23T08:31:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    31,
                    26,
                    1,
                    266,
                    0
                ],
                "published": "2025-08-29T02:29:52Z",
                "published_parsed": [
                    2025,
                    8,
                    29,
                    2,
                    29,
                    52,
                    4,
                    241,
                    0
                ],
                "title": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDTrim: Targeted Pruning for Prefill-Decode Disaggregation in Inference"
                },
                "summary": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) demonstrate exceptional capabilities across\nvarious tasks, but their deployment is constrained by high computational and\nmemory costs. Model pruning provides an effective means to alleviate these\ndemands. However, existing methods often ignore the characteristics of\nprefill-decode (PD) disaggregation in practice. In this paper, we propose a\nnovel pruning method for PD disaggregation inference, enabling more precise and\nefficient block and KV Cache pruning. Our approach constructs pruning and\ndistillation sets to perform iterative block removal independently for the\nprefill and decode stages, obtaining better pruning solutions. Moreover, we\nintroduce a token-aware cache pruning mechanism that retains all KV Cache in\nthe prefill stage but selectively reuses entries for the first and last token\nsequences in selected layers during decode, reducing communication costs with\nminimal overhead. Extensive experiments demonstrate that our approach\nconsistently achieves strong performance in both PD disaggregation and PD\nunified settings without disaggregation. Under the same (default) settings, our\nmethod achieves improved performance and faster inference, along with a\n4.95$\\times$ reduction in data transmission bandwidth consumption."
                },
                "authors": [
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Mengsi Lyu"
                    },
                    {
                        "name": "Zhuo Chen"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Yulong Ao"
                    },
                    {
                        "name": "Yonghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Yonghua Lin"
                },
                "author": "Yonghua Lin",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.04467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.04467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v2",
                "updated": "2025-09-23T08:24:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    8,
                    24,
                    7,
                    1,
                    266,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18684v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18684v1",
                "updated": "2025-09-23T06:10:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    10,
                    20,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T06:10:20Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    6,
                    10,
                    20,
                    1,
                    266,
                    0
                ],
                "title": "Static Estimation of Reuse Profiles for Arrays in Nested Loops",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Estimation of Reuse Profiles for Arrays in Nested Loops"
                },
                "summary": "Efficient memory access patterns play a crucial role in determining the\noverall performance of applications by exploiting temporal and spatial\nlocality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is\na widely used metric to quantify temporal locality, measuring the distance\nbetween consecutive accesses to the same memory location. Traditionally,\ncalculating RDH requires program execution and memory trace collection to\nobtain dynamic memory access behavior. This trace collection is often\ntime-consuming, resource-intensive, and unsuitable for early-stage optimization\nor large-scale applications. Static prediction, on the other hand, offers a\nsignificant speedup in estimating RDH and cache hit rates. However, these\napproaches lack accuracy, since the predictions come without running the\nprogram and knowing the complete memory access pattern, more specifically when\narrays are used inside nested loops. This paper presents a novel static\nanalysis framework for predicting the reuse profiles of array references in\nprograms with nested loop structures, without requiring any runtime\ninformation. By analyzing loop bounds, access patterns in smaller problem\nsizes, and predictive equations, our method predicts access patterns of arrays\nand estimates reuse distances and cache hit rate at compile time. This paper\nextends our previous study by incorporating more analysis and improving\nprediction by addressing previously unhandled reuse patterns. We evaluate our\ntechnique against a widely accepted traditional trace-driven profiling tool,\nParallel Reuse Distance Analysis (PARDA). The results demonstrate that our\nstatic predictor achieves comparable accuracy while offering\norders-of-magnitude improvement in the analysis speed. This work offers a\npractical alternative to dynamic reuse profiling and paves the way for\nintegration into compilers and static performance modeling tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient memory access patterns play a crucial role in determining the\noverall performance of applications by exploiting temporal and spatial\nlocality, thus maximizing cache locality. The Reuse Distance Histogram (RDH) is\na widely used metric to quantify temporal locality, measuring the distance\nbetween consecutive accesses to the same memory location. Traditionally,\ncalculating RDH requires program execution and memory trace collection to\nobtain dynamic memory access behavior. This trace collection is often\ntime-consuming, resource-intensive, and unsuitable for early-stage optimization\nor large-scale applications. Static prediction, on the other hand, offers a\nsignificant speedup in estimating RDH and cache hit rates. However, these\napproaches lack accuracy, since the predictions come without running the\nprogram and knowing the complete memory access pattern, more specifically when\narrays are used inside nested loops. This paper presents a novel static\nanalysis framework for predicting the reuse profiles of array references in\nprograms with nested loop structures, without requiring any runtime\ninformation. By analyzing loop bounds, access patterns in smaller problem\nsizes, and predictive equations, our method predicts access patterns of arrays\nand estimates reuse distances and cache hit rate at compile time. This paper\nextends our previous study by incorporating more analysis and improving\nprediction by addressing previously unhandled reuse patterns. We evaluate our\ntechnique against a widely accepted traditional trace-driven profiling tool,\nParallel Reuse Distance Analysis (PARDA). The results demonstrate that our\nstatic predictor achieves comparable accuracy while offering\norders-of-magnitude improvement in the analysis speed. This work offers a\npractical alternative to dynamic reuse profiling and paves the way for\nintegration into compilers and static performance modeling tools."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "This paper is accepted at the MEMSYS 2025 conference, 11th\n  International Symposium on Memory Systems, Washington D.C., October 7 -\n  October 8, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18684v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18684v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18670v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18670v1",
                "updated": "2025-09-23T05:39:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    39,
                    47,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T05:39:47Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    5,
                    39,
                    47,
                    1,
                    266,
                    0
                ],
                "title": "CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALL: Context-Aware Low-Latency Retrieval in Disk-Based Vector Databases"
                },
                "summary": "Embedding models capture both semantic and syntactic structures of queries,\noften mapping different queries to similar regions in vector space. This\nresults in non-uniform cluster access patterns in modern disk-based vector\ndatabases. While existing approaches optimize individual queries, they overlook\nthe impact of cluster access patterns, failing to account for the locality\neffects of queries that access similar clusters. This oversight increases cache\nmiss penalty. To minimize the cache miss penalty, we propose CALL, a\ncontext-aware query grouping mechanism that organizes queries based on shared\ncluster access patterns. Additionally, CALL incorporates a group-aware\nprefetching method to minimize cache misses during transitions between query\ngroups and latency-aware cluster loading. Experimental results show that CALL\nreduces the 99th percentile tail latency by up to 33% while consistently\nmaintaining a higher cache hit ratio, substantially reducing search latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embedding models capture both semantic and syntactic structures of queries,\noften mapping different queries to similar regions in vector space. This\nresults in non-uniform cluster access patterns in modern disk-based vector\ndatabases. While existing approaches optimize individual queries, they overlook\nthe impact of cluster access patterns, failing to account for the locality\neffects of queries that access similar clusters. This oversight increases cache\nmiss penalty. To minimize the cache miss penalty, we propose CALL, a\ncontext-aware query grouping mechanism that organizes queries based on shared\ncluster access patterns. Additionally, CALL incorporates a group-aware\nprefetching method to minimize cache misses during transitions between query\ngroups and latency-aware cluster loading. Experimental results show that CALL\nreduces the 99th percentile tail latency by up to 33% while consistently\nmaintaining a higher cache hit ratio, substantially reducing search latency."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Kyuri Park"
                    },
                    {
                        "name": "Youngjae Kim"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "arxiv_comment": "11 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18670v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18670v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18592v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18592v1",
                "updated": "2025-09-23T03:23:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    23,
                    3,
                    1,
                    266,
                    0
                ],
                "published": "2025-09-23T03:23:03Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    3,
                    23,
                    3,
                    1,
                    266,
                    0
                ],
                "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic\n  Vision-Language Planning for Zero-Shot Transfer in Robot Navigation"
                },
                "summary": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid adaptation in unseen environments is essential for scalable real-world\nautonomy, yet existing approaches rely on exhaustive exploration or rigid\nnavigation policies that fail to generalize. We present VLN-Zero, a two-phase\nvision-language navigation framework that leverages vision-language models to\nefficiently construct symbolic scene graphs and enable zero-shot neurosymbolic\nnavigation. In the exploration phase, structured prompts guide VLM-based search\ntoward informative and diverse trajectories, yielding compact scene graph\nrepresentations. In the deployment phase, a neurosymbolic planner reasons over\nthe scene graph and environmental observations to generate executable plans,\nwhile a cache-enabled execution module accelerates adaptation by reusing\npreviously computed task-location trajectories. By combining rapid exploration,\nsymbolic reasoning, and cache-enabled execution, the proposed framework\novercomes the computational inefficiency and poor generalization of prior\nvision-language navigation methods, enabling robust and scalable\ndecision-making in unseen environments. VLN-Zero achieves 2x higher success\nrate compared to state-of-the-art zero-shot models, outperforms most fine-tuned\nbaselines, and reaches goal locations in half the time with 55% fewer VLM calls\non average compared to state-of-the-art models across diverse environments.\nCodebase, datasets, and videos for VLN-Zero are available at:\nhttps://vln-zero.github.io/."
                },
                "authors": [
                    {
                        "name": "Neel P. Bhatt"
                    },
                    {
                        "name": "Yunhao Yang"
                    },
                    {
                        "name": "Rohan Siva"
                    },
                    {
                        "name": "Pranay Samineni"
                    },
                    {
                        "name": "Daniel Milan"
                    },
                    {
                        "name": "Zhangyang Wang"
                    },
                    {
                        "name": "Ufuk Topcu"
                    }
                ],
                "author_detail": {
                    "name": "Ufuk Topcu"
                },
                "author": "Ufuk Topcu",
                "arxiv_comment": "Codebase, datasets, and videos for VLN-Zero are available at:\n  https://vln-zero.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18592v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18592v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19375v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19375v3",
                "updated": "2025-09-26T03:17:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    3,
                    17,
                    15,
                    4,
                    269,
                    0
                ],
                "published": "2024-09-28T15:03:28Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    15,
                    3,
                    28,
                    5,
                    272,
                    0
                ],
                "title": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DOTA: Distributional Test-Time Adaptation of Vision-Language Models"
                },
                "summary": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language foundation models (VLMs), such as CLIP, exhibit remarkable\nperformance across a wide range of tasks. However, deploying these models can\nbe unreliable when significant distribution gaps exist between training and\ntest data, while fine-tuning for diverse scenarios is often costly. Cache-based\ntest-time adapters offer an efficient alternative by storing representative\ntest samples to guide subsequent classifications. Yet, these methods typically\nemploy naive cache management with limited capacity, leading to severe\ncatastrophic forgetting when samples are inevitably dropped during updates. In\nthis paper, we propose DOTA (DistributiOnal Test-time Adaptation), a simple yet\neffective method addressing this limitation. Crucially, instead of merely\nmemorizing individual test samples, DOTA continuously estimates the underlying\ndistribution of the test data stream. Test-time posterior probabilities are\nthen computed using these dynamically estimated distributions via Bayes'\ntheorem for adaptation. This distribution-centric approach enables the model to\ncontinually learn and adapt to the deployment environment. Extensive\nexperiments validate that DOTA significantly mitigates forgetting and achieves\nstate-of-the-art performance compared to existing methods."
                },
                "authors": [
                    {
                        "name": "Zongbo Han"
                    },
                    {
                        "name": "Jialong Yang"
                    },
                    {
                        "name": "Guangyu Wang"
                    },
                    {
                        "name": "Junfan Li"
                    },
                    {
                        "name": "Qianli Xu"
                    },
                    {
                        "name": "Mike Zheng Shou"
                    },
                    {
                        "name": "Changqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Changqing Zhang"
                },
                "author": "Changqing Zhang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19375v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19375v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.00329v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.00329v2",
                "updated": "2025-09-22T19:20:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    20,
                    33,
                    0,
                    265,
                    0
                ],
                "published": "2025-05-31T00:52:17Z",
                "published_parsed": [
                    2025,
                    5,
                    31,
                    0,
                    52,
                    17,
                    5,
                    151,
                    0
                ],
                "title": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foresight: Adaptive Layer Reuse for Accelerated and High-Quality\n  Text-to-Video Generation"
                },
                "summary": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while\nmaintaining video quality. The source code of Foresight is available at\n\\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) achieve state-of-the-art results in\ntext-to-image, text-to-video generation, and editing. However, their large\nmodel size and the quadratic cost of spatial-temporal attention over multiple\ndenoising steps make video generation computationally expensive. Static caching\nmitigates this by reusing features across fixed steps but fails to adapt to\ngeneration dynamics, leading to suboptimal trade-offs between speed and\nquality.\n  We propose Foresight, an adaptive layer-reuse technique that reduces\ncomputational redundancy across denoising steps while preserving baseline\nperformance. Foresight dynamically identifies and reuses DiT block outputs for\nall layers across steps, adapting to generation parameters such as resolution\nand denoising schedules to optimize efficiency. Applied to OpenSora, Latte, and\nCogVideoX, Foresight achieves up to \\latencyimprv end-to-end speedup, while\nmaintaining video quality. The source code of Foresight is available at\n\\href{https://github.com/STAR-Laboratory/foresight}{https://github.com/STAR-Laboratory/foresight}."
                },
                "authors": [
                    {
                        "name": "Muhammad Adnan"
                    },
                    {
                        "name": "Nithesh Kurella"
                    },
                    {
                        "name": "Akhil Arunkumar"
                    },
                    {
                        "name": "Prashant J. Nair"
                    }
                ],
                "author_detail": {
                    "name": "Prashant J. Nair"
                },
                "author": "Prashant J. Nair",
                "arxiv_comment": "Accepted at the 39th Conference on Neural Information Processing\n  Systems (NeurIPS), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.00329v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.00329v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18344v1",
                "updated": "2025-09-22T19:08:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T19:08:57Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding"
                },
                "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit)."
                },
                "authors": [
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Chun-Che Yang"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18307v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18307v1",
                "updated": "2025-09-22T18:32:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    18,
                    32,
                    32,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T18:32:32Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    18,
                    32,
                    32,
                    0,
                    265,
                    0
                ],
                "title": "Comparison of Adaptive plan doses using Velocity generated synthetic CT\n  with KV CBCT and re-planning CT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparison of Adaptive plan doses using Velocity generated synthetic CT\n  with KV CBCT and re-planning CT"
                },
                "summary": "Introduction: This study uses KV CBCT based Synthetic CT (sCT) generated\nthrough Velocity workstation and compare the target and normal tissue doses\nwith Adaptive plan CT doses.\n  Methods: Thirty head and neck cancer patients undergoing Adaptive Radiation\nTherapy (ART) were included in this retrospective study. Initially, patient\nunderwent treatment with the primary plan. After subsequent indications of\nmajor changes in patients' physicality and anatomy adaptive CT scans were\nacquired as per institutional protocol. Both the primary planning CT and the\nindicative cone-beam CT (CBCT) last acquired before the commencement of the\nadaptive treatment were imported into Velocity workstation. Rigid and\ndeformable image registration techniques were used for the generation of a\nSynthetic CT (sCT). Simultaneously replanning was done on re-planning CT (rCT)\nfor adaptive plan execution. The primary plan dose was subsequently mapped and\ndeformed onto the Synthetic CT in Velocity workstation, allowing for a\ncomparative dosimetric analysis between the sCT and rCT plan doses. This\ncomparison was conducted in both Velocity and Eclipse, focusing on dose\nvariations across different organs at risk (OARs) and the planning target\nvolume (PTV). Additionally, dosimetric indices were evaluated to assess and\nvalidate the accuracy and quality of the synthetic CT-based dose mapping\nrelative to adaptive planning.\n  Results: The dosimetric comparison between sCT and rCT stated that Mean dose\nfor OARs and PTVs were found to be similar in the two planning and the level of\nconfidence by using T-statistics. Collaborative research has the potential to\neliminate the need of rCT as a standard requirement.\n  Conclusion: The sCT shows comparable CT numbers and doses to the replanning\nCT, suggesting it's potential as a replacement pending clinical correlation and\ncontour adjustments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introduction: This study uses KV CBCT based Synthetic CT (sCT) generated\nthrough Velocity workstation and compare the target and normal tissue doses\nwith Adaptive plan CT doses.\n  Methods: Thirty head and neck cancer patients undergoing Adaptive Radiation\nTherapy (ART) were included in this retrospective study. Initially, patient\nunderwent treatment with the primary plan. After subsequent indications of\nmajor changes in patients' physicality and anatomy adaptive CT scans were\nacquired as per institutional protocol. Both the primary planning CT and the\nindicative cone-beam CT (CBCT) last acquired before the commencement of the\nadaptive treatment were imported into Velocity workstation. Rigid and\ndeformable image registration techniques were used for the generation of a\nSynthetic CT (sCT). Simultaneously replanning was done on re-planning CT (rCT)\nfor adaptive plan execution. The primary plan dose was subsequently mapped and\ndeformed onto the Synthetic CT in Velocity workstation, allowing for a\ncomparative dosimetric analysis between the sCT and rCT plan doses. This\ncomparison was conducted in both Velocity and Eclipse, focusing on dose\nvariations across different organs at risk (OARs) and the planning target\nvolume (PTV). Additionally, dosimetric indices were evaluated to assess and\nvalidate the accuracy and quality of the synthetic CT-based dose mapping\nrelative to adaptive planning.\n  Results: The dosimetric comparison between sCT and rCT stated that Mean dose\nfor OARs and PTVs were found to be similar in the two planning and the level of\nconfidence by using T-statistics. Collaborative research has the potential to\neliminate the need of rCT as a standard requirement.\n  Conclusion: The sCT shows comparable CT numbers and doses to the replanning\nCT, suggesting it's potential as a replacement pending clinical correlation and\ncontour adjustments."
                },
                "authors": [
                    {
                        "name": "Sudam Masanta"
                    },
                    {
                        "name": "Gurvinder Singh"
                    },
                    {
                        "name": "Shefali Pahwa"
                    },
                    {
                        "name": "Shekhar Dwivedi"
                    },
                    {
                        "name": "Devaraju Sampathirao"
                    },
                    {
                        "name": "Ramandeep Singh"
                    }
                ],
                "author_detail": {
                    "name": "Ramandeep Singh"
                },
                "author": "Ramandeep Singh",
                "arxiv_comment": "8 pages; comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18307v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18307v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v1",
                "updated": "2025-09-22T17:58:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00919v2",
                "updated": "2025-09-22T16:16:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    16,
                    16,
                    25,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-02T21:15:07Z",
                "published_parsed": [
                    2025,
                    2,
                    2,
                    21,
                    15,
                    7,
                    6,
                    33,
                    0
                ],
                "title": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Sinks: A 'Catch, Tag, Release' Mechanism for Embeddings"
                },
                "summary": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often concentrate their attention on a few\nspecific tokens referred to as attention sinks. Common examples include the\nfirst token, a prompt-independent sink, and punctuation tokens, which are\nprompt-dependent. While the tokens causing the sinks often lack direct semantic\nmeaning, the presence of the sinks is critical for model performance,\nparticularly under model compression and KV-caching. Despite their ubiquity,\nthe function, semantic role, and origin of attention sinks -- especially those\nbeyond the first token -- remain poorly understood. In this work, we conduct a\ncomprehensive investigation demonstrating that attention sinks: catch a\nsequence of tokens, tag them using a common direction in embedding space, and\nrelease them back into the residual stream, where tokens are later retrieved\nbased on the tags they have acquired. Probing experiments reveal these tags\ncarry semantically meaningful information, such as the truth of a statement.\nThese findings extend to reasoning models, where the mechanism spans more heads\nand explains greater variance in embeddings, or recent models with query-key\nnormalization, where sinks remain just as prevalent. To encourage future\ntheoretical analysis, we introduce a minimal problem which can be solved\nthrough the 'catch, tag, release' mechanism, and where it emerges through\ntraining."
                },
                "authors": [
                    {
                        "name": "Stephen Zhang"
                    },
                    {
                        "name": "Mustafa Khan"
                    },
                    {
                        "name": "Vardan Papyan"
                    }
                ],
                "author_detail": {
                    "name": "Vardan Papyan"
                },
                "author": "Vardan Papyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00085v2",
                "updated": "2025-09-22T12:28:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    28,
                    41,
                    0,
                    265,
                    0
                ],
                "published": "2025-01-31T16:22:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    22,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Beam Search for Large Language Models Using Trie-Based\n  Decoding"
                },
                "summary": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work presents a novel trie (prefix-tree)-based parallel decoding method\nthat addresses the memory inefficiency of batch-based beam search. By sharing a\nsingle KV cache across beams with common prefixes, our approach dramatically\nreduces memory usage and enables efficient decoding. We evaluated our method\nacross three attention architectures, Multi-Head Attention\n(Phi-3.5-mini-instruct), Grouped Query Attention (Llama-3.1-8B-Instruct), and\nSliding Window Attention (Mistral-Small-24B-Instruct-2501), using CNN/DailyMail\nfor abstractive summarization and HumanEval for code generation. Our\nexperiments demonstrate substantial memory savings (4--8$\\times$) and up to\n2.4$\\times$ faster decoding, without compromising generation quality. These\nresults highlight our method's suitability for memory-constrained environments\nand large-scale deployments."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "MaoXun Huang"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "arxiv_comment": "13 pages, accepted as a main conference paper at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13251v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13251v3",
                "updated": "2025-09-22T12:03:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    12,
                    3,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-02-18T19:22:44Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    19,
                    22,
                    44,
                    1,
                    49,
                    0
                ],
                "title": "Neural Attention Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Attention Search"
                },
                "summary": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Neural Attention Search (NAtS), a framework that automatically\nevaluates the importance of each token within a sequence and determines if the\ncorresponding token can be dropped after several steps. This approach can\nefficiently reduce the KV cache sizes required by transformer-based models\nduring inference and thus reduce inference costs. In this paper, we design a\nsearch space that contains three token types: (i) Global Tokens will be\npreserved and queried by all the following tokens. (ii) Local Tokens survive\nuntil the next global token appears. (iii) Sliding Window Tokens have an impact\non the inference of a fixed size of the next following tokens. Similar to the\nOne-Shot Neural Architecture Search approach, this token-type information can\nbe learned jointly with the architecture weights via a learnable attention\nmask. Experiments on both training a new transformer from scratch and\nfine-tuning existing large language models show that NAtS can efficiently\nreduce the KV cache size required for the models while maintaining the models'\nperformance."
                },
                "authors": [
                    {
                        "name": "Difan Deng"
                    },
                    {
                        "name": "Marius Lindauer"
                    }
                ],
                "author_detail": {
                    "name": "Marius Lindauer"
                },
                "author": "Marius Lindauer",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13251v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13251v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17650v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17650v1",
                "updated": "2025-09-22T11:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T11:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    11,
                    54,
                    58,
                    0,
                    265,
                    0
                ],
                "title": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evict3R: Training-Free Token Eviction for Memory-Bounded Streaming\n  Visual Geometry Transformers"
                },
                "summary": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming visual transformers like StreamVGGT achieve strong 3D perception\nbut suffer from unbounded growth of key value (KV) memory, which limits\nscalability. We propose a training-free, inference-time token eviction policy\nthat bounds memory by discarding redundant tokens while keeping the most\ninformative ones. Our method uses significantly less memory with little to no\ndrop in accuracy: on 7-Scenes with long sequences it reduces peak memory from\n18.63 GB to 9.39 GB while accuracy and completeness drop by only 0.003. Under\nstrict memory budgets, eviction enables denser frame sampling, which improves\nreconstruction accuracy compared to the baseline. Experiments across video\ndepth estimation (Sintel, KITTI), 3D reconstruction (7-Scenes, NRGBD), and\ncamera pose estimation (Sintel, TUM-dynamics) show that our approach closely\nmatches StreamVGGT at a fraction of the memory and makes long-horizon streaming\ninference more practical."
                },
                "authors": [
                    {
                        "name": "Soroush Mahdi"
                    },
                    {
                        "name": "Fardin Ayar"
                    },
                    {
                        "name": "Ehsan Javanmardi"
                    },
                    {
                        "name": "Manabu Tsukada"
                    },
                    {
                        "name": "Mahdi Javanmardi"
                    }
                ],
                "author_detail": {
                    "name": "Mahdi Javanmardi"
                },
                "author": "Mahdi Javanmardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17650v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17650v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17388v1",
                "updated": "2025-09-22T06:52:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T06:52:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    52,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory"
                },
                "summary": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging applications, such as big data analytics and machine learning,\nrequire increasingly large amounts of main memory, often exceeding the capacity\nof current commodity processors built on DRAM technology. To address this,\nrecent research has focused on off-chip memory controllers that facilitate\naccess to diverse memory media, each with unique density and latency\ncharacteristics. While these solutions improve memory system performance, they\nalso exacerbate the already significant memory latency. As a result,\nmulti-level prefetching techniques are essential to mitigate these extended\nlatencies.\n  This paper investigates the advantages of prefetching across both sides of\nthe memory system: the off-chip memory and the on-chip cache hierarchy. Our\nprimary objective is to assess the impact of a multi-level prefetching engine\non overall system performance. Additionally, we analyze the individual\ncontribution of each prefetching level to system efficiency. To achieve this,\nthe study evaluates two key prefetching approaches: HMC (Hybrid Memory\nController) and HMC+L1, both of which employ prefetching mechanisms commonly\nused by processor vendors. The HMC approach integrates a prefetcher within the\noff-chip hybrid memory controller, while the HMC+L1 approach combines this with\nadditional L1 on-chip prefetchers.\n  Experimental results on an out-of-order execution processor show that on-chip\ncache prefetchers are crucial for maximizing the benefits of off-chip\nprefetching, which in turn further enhances performance. Specifically, the\noff-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and\nup to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher\ncoverage to as much as 92%. Consequently, overall performance increases from 9%\nwith the HMC approach to 12% when L1 prefetching is also employed."
                },
                "authors": [
                    {
                        "name": "Manel Lurbe"
                    },
                    {
                        "name": "Miguel Avargues"
                    },
                    {
                        "name": "Salvador Petit"
                    },
                    {
                        "name": "Maria E. Gomez"
                    },
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Guanhao Wang"
                    },
                    {
                        "name": "Julio Sahuquillo"
                    }
                ],
                "author_detail": {
                    "name": "Julio Sahuquillo"
                },
                "author": "Julio Sahuquillo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17360v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17360v1",
                "updated": "2025-09-22T05:24:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    22,
                    5,
                    24,
                    22,
                    0,
                    265,
                    0
                ],
                "published": "2025-09-22T05:24:22Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    5,
                    24,
                    22,
                    0,
                    265,
                    0
                ],
                "title": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access"
                },
                "summary": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep\nresearch and code generation. However, their effectiveness depends on frequent\ninteractions with knowledge sources across remote clouds or regions. Such\ninteractions can create non-trivial latency and cost bottlenecks. Existing\ncaching solutions focus on exact-match queries, limiting their effectiveness\nfor semantic knowledge reuse.\n  To address this challenge, we introduce Asteria, a novel cross-region\nknowledge caching architecture for LLM agents. At its core are two\nabstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A\nsemantic element captures the semantic embedding representation of an LLM query\ntogether with performance-aware metadata such as latency, cost, and staticity.\nSine then provides two-stage retrieval: a vector similar index with semantic\nembedding for fast candidate selection and a lightweight LLM-powered semantic\njudger for precise validation. Atop these primitives, Asteria builds a new\ncache interface that includes a new semantic-aware cache hit definition, a\ncost-efficient eviction policy, and proactive prefetching. To reduce overhead,\nAsteria co-locates the small LLM judger with the main LLM using adaptive\nscheduling and resource sharing. Our evaluation demonstrates that Asteria\ndelivers substantial performance improvements without compromising correctness.\nOn representative search workloads, Asteria achieves up to a 3.6$\\times$\nincrease in throughput by maintaining cache hit rates of over 85%, while\npreserving accuracy virtually identical to non-cached baselines. Asteria also\nimproves throughput for complex coding tasks by 20%, showcasing its versatility\nacross diverse agentic workloads."
                },
                "authors": [
                    {
                        "name": "Chaoyi Ruan"
                    },
                    {
                        "name": "Chao Bi"
                    },
                    {
                        "name": "Kaiwen Zheng"
                    },
                    {
                        "name": "Ziji Shi"
                    },
                    {
                        "name": "Xinyi Wan"
                    },
                    {
                        "name": "Jialin Li"
                    }
                ],
                "author_detail": {
                    "name": "Jialin Li"
                },
                "author": "Jialin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17360v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17360v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17257v1",
                "updated": "2025-09-21T22:14:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    22,
                    14,
                    56,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T22:14:56Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    22,
                    14,
                    56,
                    6,
                    264,
                    0
                ],
                "title": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On efficient block Krylov-solvers for $\\mathcal H^2$-matrices"
                },
                "summary": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical matrices provide a highly memory-efficient way of storing dense\nlinear operators arising, for example, from boundary element methods,\nparticularly when stored in the H^2 format. In such data-sparse\nrepresentations, iterative solvers are preferred over direct ones due to the\ncost-efficient matrix-vector multiplications they enable. Solving multiple\nsystems of linear equations with the same hierarchical matrix naturally leads\nto block methods, which in turn make heavy use of BLAS level-3 functions such\nas GEMM. We present an efficient implementation of H^2-matrix-vector and\nH^2-matrix-matrix multiplication that fully exploits the potential of modern\nhardware in terms of memory and cache utilization. The latter is employed to\naccelerate block Krylov subspace methods, which we present later as the main\nresults of this paper."
                },
                "authors": [
                    {
                        "name": "Sven Christophersen"
                    }
                ],
                "author_detail": {
                    "name": "Sven Christophersen"
                },
                "author": "Sven Christophersen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F55, 65F08, 65F10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17238v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17238v1",
                "updated": "2025-09-21T21:05:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T21:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE"
                },
                "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction.To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters."
                },
                "authors": [
                    {
                        "name": "Soheil Zibakhsh"
                    },
                    {
                        "name": "Mohammad Samragh"
                    },
                    {
                        "name": "Kumari Nishu"
                    },
                    {
                        "name": "Lauren Hannah"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17238v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17238v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.07639v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.07639v2",
                "updated": "2025-09-21T11:48:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    11,
                    48,
                    15,
                    6,
                    264,
                    0
                ],
                "published": "2025-06-09T11:04:13Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    11,
                    4,
                    13,
                    0,
                    160,
                    0
                ],
                "title": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast ECoT: Efficient Embodied Chain-of-Thought via Thoughts Reuse"
                },
                "summary": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Embodied Chain-of-Thought (ECoT) reasoning enhances vision-language-action\n(VLA) models by improving performance and interpretability through intermediate\nreasoning steps. However, its sequential autoregressive token generation\nintroduces significant inference latency, limiting real-time deployment. We\npropose Fast ECoT, an inference-time acceleration method that exploits the\nstructured and repetitive nature of ECoT to (1) cache and reuse high-level\nreasoning across timesteps and (2) parallelise the generation of modular\nreasoning steps. Additionally, we introduce an asynchronous scheduler that\ndecouples reasoning from action decoding, further boosting responsiveness. Fast\nECoT requires no model changes or additional training and integrates easily\ninto existing VLA pipelines. Experiments in both simulation (LIBERO) and\nreal-world robot tasks show up to a 7.5% reduction in latency with comparable\nor improved task success rate and reasoning faithfulness, bringing ECoT\npolicies closer to practical real-time deployment."
                },
                "authors": [
                    {
                        "name": "Zhekai Duan"
                    },
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Shikai Geng"
                    },
                    {
                        "name": "Gaowen Liu"
                    },
                    {
                        "name": "Joschka Boedecker"
                    },
                    {
                        "name": "Chris Xiaoxuan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Chris Xiaoxuan Lu"
                },
                "author": "Chris Xiaoxuan Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.07639v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.07639v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10367v2",
                "updated": "2025-09-21T07:03:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    7,
                    3,
                    46,
                    6,
                    264,
                    0
                ],
                "published": "2025-07-14T15:09:01Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    15,
                    9,
                    1,
                    0,
                    195,
                    0
                ],
                "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline"
                },
                "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year."
                },
                "authors": [
                    {
                        "name": "Jingwei Xu"
                    },
                    {
                        "name": "Junbin Kang"
                    },
                    {
                        "name": "Mingkai Dong"
                    },
                    {
                        "name": "Mingyu Liu"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Shaohong Guo"
                    },
                    {
                        "name": "Ziyan Qiu"
                    },
                    {
                        "name": "Mingzhen You"
                    },
                    {
                        "name": "Ziyi Tian"
                    },
                    {
                        "name": "Anqi Yu"
                    },
                    {
                        "name": "Tianhong Ding"
                    },
                    {
                        "name": "Xinwei Hu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by NSDI'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11815v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11815v2",
                "updated": "2025-09-21T03:35:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    3,
                    35,
                    36,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-15T11:53:56Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    11,
                    53,
                    56,
                    0,
                    258,
                    0
                ],
                "title": "SpecVLM: Fast Speculative Decoding in Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecVLM: Fast Speculative Decoding in Vision-Language Models"
                },
                "summary": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is a powerful way to accelerate autoregressive large\nlanguage models (LLMs), but directly porting it to vision-language models\n(VLMs) faces unique systems constraints: the prefill stage is dominated by\nvisual tokens whose count scales with image resolution and video length,\ninflating both compute and memory, especially the key-value (KV) cache. We\nstudy speculative decoding for VLMs and introduce SpecVLM, a practical system\nthat (1) establishes a strong EAGLE-2-style baseline, EagleVLM, delivering\n1.5--2.3x end-to-end speedups over full autoregressive inference, and (2)\nfurther accelerates VLM inference with an elastic visual compressor that\nadaptively selects among pruning, pooling, convolution, and resampler\nprimitives to balance FLOPs/parameters and accuracy per input. To avoid costly\noffline distillation corpora, we propose an online-logit distillation protocol\nthat trains the draft model with on-the-fly teacher logits and penultimate\nfeatures using a combined cross-entropy and Smooth L1 objective, eliminating\nstorage and preprocessing while remaining compute-efficient. This protocol\nreveals a training-time scaling effect: longer online training monotonically\nincreases the draft model's average accepted length, improving speculative\nefficiency. Empirically, SpecVLM achieves additional acceleration, culminating\nin 2.5--2.9x end-to-end speedups within 5 epochs across LLaVA and MMMU,\nconsistently over resolutions and task difficulties, while preserving the\ntarget model's output distribution (lossless decoding). Our code is available\nat https://github.com/haiduo/SpecVLM."
                },
                "authors": [
                    {
                        "name": "Haiduo Huang"
                    },
                    {
                        "name": "Fuwei Yang"
                    },
                    {
                        "name": "Zhenhua Liu"
                    },
                    {
                        "name": "Xuanwu Yin"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Pengju Ren"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11815v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11815v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16857v1",
                "updated": "2025-09-21T00:59:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    21,
                    0,
                    59,
                    45,
                    6,
                    264,
                    0
                ],
                "published": "2025-09-21T00:59:45Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    0,
                    59,
                    45,
                    6,
                    264,
                    0
                ],
                "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix\n  Caching"
                },
                "summary": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed prefix caching accelerates long-context LLM serving by reusing KV\ncache entries for common context prefixes. However, KV cache fetches can become\na bottleneck when network bandwidth is limited. Compression mitigates the\nbandwidth issue, but can degrade overall performance when decompression\ninterferes with model computation.\n  We present ShadowServe, the first SmartNIC-accelerated, interference-free\nprefix caching system for LLM serving. ShadowServe separates a control plane on\nthe host and a data plane fully offloaded to the SmartNIC, which eliminates\ninterference to both host GPU and CPU. To overcome the SmartNIC's limited\ncompute and memory resources, we design a chunked pipeline that parallelizes\ndata plane operations across the SmartNIC's compute resources, and a\nminimal-copy memory management scheme that reduces memory pressure on the\nSmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to\n2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token\n(TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to\nup to 1.35x higher throughput."
                },
                "authors": [
                    {
                        "name": "Xingyu Xiang"
                    },
                    {
                        "name": "Raj Joshi"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Chenxingyu Zhao"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Eddie Kohler"
                    },
                    {
                        "name": "Minlan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Minlan Yu"
                },
                "author": "Minlan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01960v2",
                "updated": "2025-09-20T13:54:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    54,
                    37,
                    5,
                    263,
                    0
                ],
                "published": "2025-02-04T03:13:09Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    3,
                    13,
                    9,
                    1,
                    35,
                    0
                ],
                "title": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MPIC: Position-Independent Multimodal Context Caching System for\n  Efficient MLLM Serving"
                },
                "summary": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local disks when\nreceiving multimodal data, and calculates and loads the KV cache in parallel\nduring inference. To mitigate accuracy degradation, we have incorporated the\nintegrated reuse and recompute mechanism within the system. The experimental\nresults demonstrate that MPIC can achieve up to 54\\% reduction in response time\nand 2$\\times$ improvement in throughput compared to existing context caching\nsystems, while maintaining negligible or no accuracy loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The context caching technique is employed to accelerate the Multimodal Large\nLanguage Model (MLLM) inference by prevailing serving platforms currently.\nHowever, this approach merely reuses the Key-Value (KV) cache of the initial\nsequence of prompt, resulting in full KV cache recomputation even if the prefix\ndiffers slightly. This becomes particularly inefficient in the context of\ninterleaved text and images, as well as multimodal retrieval-augmented\ngeneration. This paper proposes position-independent caching as a more\neffective approach for multimodal information management. We have designed and\nimplemented a caching system, named MPIC, to address both system-level and\nalgorithm-level challenges. MPIC stores the KV cache on local disks when\nreceiving multimodal data, and calculates and loads the KV cache in parallel\nduring inference. To mitigate accuracy degradation, we have incorporated the\nintegrated reuse and recompute mechanism within the system. The experimental\nresults demonstrate that MPIC can achieve up to 54\\% reduction in response time\nand 2$\\times$ improvement in throughput compared to existing context caching\nsystems, while maintaining negligible or no accuracy loss."
                },
                "authors": [
                    {
                        "name": "Shiju Zhao"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Rongxiao Huang"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "17 pages, 13 figures, the second version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16686v1",
                "updated": "2025-09-20T13:27:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    27,
                    13,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T13:27:13Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    13,
                    27,
                    13,
                    5,
                    263,
                    0
                ],
                "title": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and\n  Efficient LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and\n  Efficient LLMs"
                },
                "summary": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache size is a crucial step toward enabling\nefficient inference in large language models (LLMs), especially under latency\nand memory constraints. While Multi-Head Attention (MHA) offers strong\nrepresentational power, it incurs significant memory overhead. Recent work on\nMulti-head Latent Attention (MLA) mitigates this by compressing KV\nrepresentations into a shared latent space, achieving a better trade-off\nbetween performance and cache efficiency. While MLA already achieves\nsignificant KV cache reduction, the scope for further compression remains\nlimited without performance loss. In this paper, we propose\n\\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel\nextension of MLA that further reduces KV cache size while enhancing\nrepresentational expressiveness. EG-MLA introduces a token-specific embedding\ngating mechanism applied in the latent space, enabling fine-grained modulation\nof compressed KV vectors with minimal additional computation. Compared to MHA,\nEG-MLA achieves over 91.6\\% reduction in KV cache size with negligible\nperformance degradation. Relative to MLA, EG-MLA consistently improves task\naccuracy across diverse reasoning benchmarks while achieving up to 59.9\\%\nadditional memory savings. Our theoretical analysis highlights how embedding\ngating induces implicit high-order interactions, and empirical evaluations\ndemonstrate robust generalization across model scales and compression regimes.\nNotably, we successfully scale EG-MLA to over 1 billion parameters,\ndemonstrating its practical viability for large-scale LLM deployment. These\nresults establish EG-MLA as a memory- and compute-efficient attention mechanism\nthat enables scalable, high-performance inference in modern LLMs."
                },
                "authors": [
                    {
                        "name": "Zhengge Cai"
                    },
                    {
                        "name": "Haowen Hou"
                    }
                ],
                "author_detail": {
                    "name": "Haowen Hou"
                },
                "author": "Haowen Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16630v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16630v1",
                "updated": "2025-09-20T11:09:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    9,
                    1,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T11:09:01Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    11,
                    9,
                    1,
                    5,
                    263,
                    0
                ],
                "title": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and\n  Expressive Freestyle Portrait Animation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and\n  Expressive Freestyle Portrait Animation"
                },
                "summary": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework\nfor freestyle portrait animation driven by facial landmarks. The main\nchallenges in this task are preserving the identity of the reference portrait,\naccurately transferring target expressions, and maintaining long-term temporal\nconsistency while ensuring generation efficiency. To address identity\npreservation and accurate expression retargeting, we enhance Stable Diffusion\nwith two key components: a expression-aware landmarks as explicit motion\nsignals, which improve motion alignment, support exaggerated expressions, and\nreduce identity leakage; and a fine-grained facial loss that leverages both\nexpression and facial masks to better capture subtle expressions and faithfully\npreserve the reference appearance. With these components, our model supports\ncontrollable and expressive animation across diverse portrait types, including\nreal faces, cartoons, sculptures, and animals. However, diffusion-based\nframeworks typically struggle to efficiently generate long-term stable\nanimation results, which remains a core challenge in this task. To address\nthis, we propose a progressive generation strategy for stable long-term\nanimation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless\nacceleration. These two strategies ensure that our method produces high-quality\nresults efficiently, making it user-friendly and accessible. Finally, we\nintroduce EmojiBench++, a more comprehensive benchmark comprising diverse\nportraits, driving videos, and landmark sequences. Extensive evaluations on\nEmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior\nperformance in both animation quality and controllability. The code, training\ndataset and benchmark will be found in https://follow-your-emoji.github.io/."
                },
                "authors": [
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Hongyu Liu"
                    },
                    {
                        "name": "Hongfa Wang"
                    },
                    {
                        "name": "Heng Pan"
                    },
                    {
                        "name": "Yingqing He"
                    },
                    {
                        "name": "Junkun Yuan"
                    },
                    {
                        "name": "Ailing Zeng"
                    },
                    {
                        "name": "Chengfei Cai"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Zhifeng Li"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    },
                    {
                        "name": "Qifeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Qifeng Chen"
                },
                "author": "Qifeng Chen",
                "arxiv_comment": "accepted by IJCV2025. project\n  page:https://follow-your-emoji.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16630v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16630v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16495v1",
                "updated": "2025-09-20T01:56:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    20,
                    1,
                    56,
                    25,
                    5,
                    263,
                    0
                ],
                "published": "2025-09-20T01:56:25Z",
                "published_parsed": [
                    2025,
                    9,
                    20,
                    1,
                    56,
                    25,
                    5,
                    263,
                    0
                ],
                "title": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\n  Dynamic Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for\n  Dynamic Workloads"
                },
                "summary": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient parallelism is necessary for achieving low-latency, high-throughput\ninference with large language models (LLMs). Tensor parallelism (TP) is the\nstate-of-the-art method for reducing LLM response latency, however GPU\ncommunications reduces combined token throughput. On the other hand, data\nparallelism (DP) obtains a higher throughput yet is slow in response latency.\nBest of both worlds does not exist, and it is not possible to combine TP and DP\nbecause of the KV cache variance across the parallelisms.\n  We notice Sequence Parallelism (SP - Ulysses in training) has similar\nproperties as DP but with KV cache invariance. We adapt SP to inference, and\ncombine it with TP to get the best of both worlds. Our solution: Shift\nParallelism.\n  Shift Parallelism dynamically switches across TP and SP, and minimizes\nlatency in low traffic without losing throughput in high traffic. The efficient\nGPU communications of Shift Parallelism yields up to i) 1.51x faster response\nin interactive workloads and ii) 50% higher throughput in batch workloads,\ncompared to a TP-only solution.\n  We evaluate Shift Parallelism with real-world production traces with dynamic\ntraffic patterns as well as synthetic benchmarking patterns across models,\ncontext sizes, and arrival rates. All results affirm the same: Shift\nParallelism has a better the latency vs. throughput tradeoff than TP or DP, and\nhence obtains low latency without degrading throughput in dynamic workloads."
                },
                "authors": [
                    {
                        "name": "Mert Hidayetoglu"
                    },
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Michael Wyatt"
                    },
                    {
                        "name": "Jeff Rasley"
                    },
                    {
                        "name": "Yuxiong He"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    }
                ],
                "author_detail": {
                    "name": "Samyam Rajbhandari"
                },
                "author": "Samyam Rajbhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16471v1",
                "updated": "2025-09-19T23:46:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    23,
                    46,
                    8,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T23:46:08Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    23,
                    46,
                    8,
                    4,
                    262,
                    0
                ],
                "title": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to\n  Estimate True Surface Pore Size in Nanoporous Membranes"
                },
                "summary": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning electron microscopy (SEM) is the premier method for characterizing\nthe nanoscale surface pores in ultrafiltration (UF) membranes and the support\nlayers of reverse osmosis (RO) membranes. Based on SEM, the conventional\nunderstanding is that membranes typically have low surface porosities of <10%.\nWe hypothesized that high acceleration voltage during SEM imaging and sputter\nmetal coatings required for SEM have led to systematic underestimations of\nporosity and pore size. We showed that imaging a commercial UF membrane at 1,\n5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while\nincreasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for\nthe UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To\naccount for coating thickness, we developed a digital correction method that\nsimulates pore dilation, enabling the pore structure to be estimated for\nuncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF\nmembrane and 20% for the RO support, about 3-fold greater than values observed\nwith a 4 nm coating. Mean pore diameters were 2-fold greater for the UF\nmembrane and 1.5-fold greater for the RO support. Critically, dilation-derived\npore-size distributions agreed with low-flux dextran-retention data fitted with\nthe Bungay-Brenner model. Our results suggest that surface porosities and pore\nsizes of nanoporous membranes are much larger than previously understood, with\nmajor implications for structure/transport relationships. For future nanoscale\npore analysis of membranes (and other nanoporous materials), we recommend low\nacceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to\naccount for coating artifacts"
                },
                "authors": [
                    {
                        "name": "Sima Zeinali Danalou"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Niher R. Sarker"
                    },
                    {
                        "name": "Hooman Chamani"
                    },
                    {
                        "name": "Jane Y. Howe"
                    },
                    {
                        "name": "Patrick C. Lee"
                    },
                    {
                        "name": "Jay R. Werber"
                    }
                ],
                "author_detail": {
                    "name": "Jay R. Werber"
                },
                "author": "Jay R. Werber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16407v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16407v1",
                "updated": "2025-09-19T20:31:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T20:31:38Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    20,
                    31,
                    38,
                    4,
                    262,
                    0
                ],
                "title": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WarpSpeed: A High-Performance Library for Concurrent GPU Hash Tables"
                },
                "summary": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hash tables are increasingly used to accelerate data processing, but\ntheir limited functionality restricts adoption in large-scale data processing\napplications. Current limitations include incomplete concurrency support and\nmissing compound operations such as upserts.\n  This paper presents WarpSpeed, a library of high-performance concurrent GPU\nhash tables with a unified benchmarking framework for performance analysis.\nWarpSpeed implements eight state-of-the-art Nvidia GPU hash table designs and\nprovides a rich API designed for modern GPU applications. Our evaluation uses\ndiverse benchmarks to assess both correctness and scalability, and we\ndemonstrate real-world impact by integrating these hash tables into three\ndownstream applications.\n  We propose several optimization techniques to reduce concurrency overhead,\nincluding fingerprint-based metadata to minimize cache line probes and\nspecialized Nvidia GPU instructions for lock-free queries. Our findings provide\nnew insights into concurrent GPU hash table design and offer practical guidance\nfor developing efficient, scalable data structures on modern GPUs."
                },
                "authors": [
                    {
                        "name": "Hunter McCoy"
                    },
                    {
                        "name": "Prashant Pandey"
                    }
                ],
                "author_detail": {
                    "name": "Prashant Pandey"
                },
                "author": "Prashant Pandey",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16407v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16407v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20587v2",
                "updated": "2025-09-19T17:18:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    17,
                    18,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-02-27T23:09:20Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    23,
                    9,
                    20,
                    3,
                    58,
                    0
                ],
                "title": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-of-Thought: Master-Apprentice Framework for Cost-Effective Vision\n  Language Model Reasoning"
                },
                "summary": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Language Models (VLMs) have achieved remarkable success in a wide\nrange of vision applications of increasing complexity and scales, yet choosing\nthe right VLM model size involves a trade-off between response quality and\ncost. While smaller VLMs are cheaper to run, they typically produce responses\nonly marginally better than random guessing on benchmarks such as MMMU.\n  In this paper, we propose Cache of Thought (CoT), a master apprentice\nframework for collaborative inference between large and small VLMs. CoT manages\nhigh quality query results from large VLMs (master) in a cache, which are then\nselected via a novel multi modal retrieval and in-context learning to aid the\nperformance of small VLMs (apprentice). We extensively evaluate CoT on various\nwidely recognized and challenging general reasoning benchmarks, and show that\nCoT increases overall reasoning performance by up to 7.7% under the same\nbudget, and specifically boosts the performance of apprentice VLMs by up to\n36.6%. Our code is available at https://github.com/UIUC-MONET/Cache-of-Thoughts"
                },
                "authors": [
                    {
                        "name": "Mingyuan Wu"
                    },
                    {
                        "name": "Jize Jiang"
                    },
                    {
                        "name": "Haozhen Zheng"
                    },
                    {
                        "name": "Meitang Li"
                    },
                    {
                        "name": "Zhaoheng Li"
                    },
                    {
                        "name": "Beitong Tian"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yongjoo Park"
                    },
                    {
                        "name": "Minjia Zhang"
                    },
                    {
                        "name": "Chengxiang Zhai"
                    },
                    {
                        "name": "Klara Nahrstedt"
                    }
                ],
                "author_detail": {
                    "name": "Klara Nahrstedt"
                },
                "author": "Klara Nahrstedt",
                "arxiv_comment": "EMNLP 2025 Main Conference. Mingyuan, Jize, and Haozhen contributed\n  equally, while Minjia, Chengxiang, and Klara advised equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.05165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.05165v2",
                "updated": "2025-09-19T15:19:26Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    15,
                    19,
                    26,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-05T14:58:24Z",
                "published_parsed": [
                    2025,
                    9,
                    5,
                    14,
                    58,
                    24,
                    4,
                    248,
                    0
                ],
                "title": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCompose: Efficient Structured KV Cache Compression with Composite\n  Tokens"
                },
                "summary": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) rely on key-value (KV) caches for efficient\nautoregressive decoding; however, cache size grows linearly with context length\nand model depth, becoming a major bottleneck in long-context inference. Prior\nKV cache compression methods either enforce rigid heuristics, disrupt tensor\nlayouts with per-attention-head variability, or require specialized compute\nkernels.\n  We propose a simple, yet effective, KV cache compression framework based on\nattention-guided, layer-adaptive composite tokens. Our method aggregates\nattention scores to estimate token importance, selects head-specific tokens\nindependently, and aligns them into composite tokens that respect the uniform\ncache structure required by existing inference engines. A global allocation\nmechanism further adapts retention budgets across layers, assigning more\ncapacity to layers with informative tokens. This approach achieves significant\nmemory reduction while preserving accuracy, consistently outperforming prior\nstructured and semi-structured methods. Crucially, our approach remains fully\ncompatible with standard inference pipelines, offering a practical and scalable\nsolution for efficient long-context LLM deployment."
                },
                "authors": [
                    {
                        "name": "Dmitry Akulov"
                    },
                    {
                        "name": "Mohamed Sana"
                    },
                    {
                        "name": "Antonio De Domenico"
                    },
                    {
                        "name": "Tareq Si Salem"
                    },
                    {
                        "name": "Nicola Piovesan"
                    },
                    {
                        "name": "Fadhel Ayed"
                    }
                ],
                "author_detail": {
                    "name": "Fadhel Ayed"
                },
                "author": "Fadhel Ayed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.05165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.05165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.09536v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.09536v2",
                "updated": "2025-09-19T14:14:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    14,
                    14,
                    32,
                    4,
                    262,
                    0
                ],
                "published": "2025-06-11T09:08:59Z",
                "published_parsed": [
                    2025,
                    6,
                    11,
                    9,
                    8,
                    59,
                    2,
                    162,
                    0
                ],
                "title": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commissioning, characterization and first high dose rate irradiations at\n  a compact X-ray tube for microbeam and minibeam radiation therapy"
                },
                "summary": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped the first prototype of a compact line-focus X-ray tube (LFXT) with\ntechnology potentially suitable for clinical translation of minibeams and\nmicrobeams. We give an overview of the commissioning process preceding first\noperation, present optical and radiological focal spot characterization\nmethods, and dosimetric measurements. Additionally, we report on first\npreclinical in vitro cell and in vivo mouse brain irradiations conducted with\nthe LFXT prototype. The LFXT was high voltage conditioned up to 300 kV.The\nfocal spot characterization resulted in a strongly eccentric electron\ndistribution with a width of 72.3 $\\mu$m. Dosimetry showed sharp microbeam dose\nprofiles with steep lateral penumbras and a peak-to-valley dose ratio above 10\nthroughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was\nmeasured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at\n150 mm distance from the focal spot. In vitro and in vivo experiments\ndemonstrated the feasibility of the LFXT for minibeam and microbeam\napplications with field sizes of 1.5-2 cm. The mice displayed no observable\nside effects after whole-brain 260 $\\mu$m-minibeam irradiation. We successfully\nconstructed and commissioned the first proof-of-concept LFXT prototype.\nDosimetric characterizations of the achieved microbeam field showed the\nsuperiority of the LFXT compared to conventional X-ray tubes in terms of beam\nquality. In future developments, the remaining limitations of the prototype\nwill be addressed for improved minibeam and first ever microbeam radiation\ntherapy in a clinical setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Minibeam and microbeam radiation therapy promise improved treatment outcomes\nthrough reduced normal tissue toxicity at better tumor control rates. The lack\nof suitable compact radiation sources limits the clinical application of\nminibeams to superficial tumors and renders it impossible for microbeams. We\ndeveloped the first prototype of a compact line-focus X-ray tube (LFXT) with\ntechnology potentially suitable for clinical translation of minibeams and\nmicrobeams. We give an overview of the commissioning process preceding first\noperation, present optical and radiological focal spot characterization\nmethods, and dosimetric measurements. Additionally, we report on first\npreclinical in vitro cell and in vivo mouse brain irradiations conducted with\nthe LFXT prototype. The LFXT was high voltage conditioned up to 300 kV.The\nfocal spot characterization resulted in a strongly eccentric electron\ndistribution with a width of 72.3 $\\mu$m. Dosimetry showed sharp microbeam dose\nprofiles with steep lateral penumbras and a peak-to-valley dose ratio above 10\nthroughout a 70 mm thick PMMA phantom. An open-field dose rate of 4.3 Gy/s was\nmeasured at an acceleration voltage of 150 kV and a beam current of 17.4 mA at\n150 mm distance from the focal spot. In vitro and in vivo experiments\ndemonstrated the feasibility of the LFXT for minibeam and microbeam\napplications with field sizes of 1.5-2 cm. The mice displayed no observable\nside effects after whole-brain 260 $\\mu$m-minibeam irradiation. We successfully\nconstructed and commissioned the first proof-of-concept LFXT prototype.\nDosimetric characterizations of the achieved microbeam field showed the\nsuperiority of the LFXT compared to conventional X-ray tubes in terms of beam\nquality. In future developments, the remaining limitations of the prototype\nwill be addressed for improved minibeam and first ever microbeam radiation\ntherapy in a clinical setting."
                },
                "authors": [
                    {
                        "name": "Christian Petrich"
                    },
                    {
                        "name": "Johanna Winter"
                    },
                    {
                        "name": "Anton Dimroth"
                    },
                    {
                        "name": "Thomas Beiser"
                    },
                    {
                        "name": "Monika Dehn"
                    },
                    {
                        "name": "Jessica Stolz"
                    },
                    {
                        "name": "Jacopo Frignani"
                    },
                    {
                        "name": "Stephanie E. Combs"
                    },
                    {
                        "name": "Franz Schilling"
                    },
                    {
                        "name": "Ghaleb Natour"
                    },
                    {
                        "name": "Kurt Aulenbacher"
                    },
                    {
                        "name": "Thomas E. Schmid"
                    },
                    {
                        "name": "Jan J. Wilkens"
                    },
                    {
                        "name": "Stefan Bartzsch"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Bartzsch"
                },
                "author": "Stefan Bartzsch",
                "arxiv_comment": "CP, JW, and AD share first authorship",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.09536v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.09536v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15763v1",
                "updated": "2025-09-19T08:47:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    8,
                    47,
                    37,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T08:47:37Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    8,
                    47,
                    37,
                    4,
                    262,
                    0
                ],
                "title": "UniGist: Towards General and Hardware-aligned Sequence-level Long\n  Context Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniGist: Towards General and Hardware-aligned Sequence-level Long\n  Context Compression"
                },
                "summary": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling."
                },
                "authors": [
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04462v2",
                "updated": "2025-09-19T06:20:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    6,
                    20,
                    14,
                    4,
                    262,
                    0
                ],
                "published": "2025-08-06T14:02:10Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    2,
                    10,
                    2,
                    218,
                    0
                ],
                "title": "CARD: A Cache-Assisted Parallel Speculative Decoding Framework via\n  Query-and-Correct Paradigm for Accelerating LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CARD: A Cache-Assisted Parallel Speculative Decoding Framework via\n  Query-and-Correct Paradigm for Accelerating LLM Inference"
                },
                "summary": "Speculative decoding (SD), where a draft model provides multiple candidate\ntokens for the target model to verify in parallel, has demonstrated significant\npotential for accelerating LLM inference. Yet, existing SD approaches adhere to\na strict draft-then-verify paradigm, enforcing a sequential process that\nhampers performance and constrains the draft model's capacity. Moreover,\nrejecting a token in the candidate sequence invalidates all subsequent tokens,\nleading to wasted computation during drafting. To overcome these limitations,\nwe propose a cache-assisted parallel speculative decoding framework called\nCARD, which employs a novel query-and-correct paradigm. Our approach decouples\ndrafting from verification: the draft model populates a shared cache with\ncandidate tokens, while the target model concurrently refines the draft's\ntrajectory. This enables inference at near-draft-speed, effectively leveraging\nthe draft model's efficiency without additional fine-tuning. Experimental\nresults show that CARD significantly outperforms existing state-of-the-art\nmethods, achieving up to a 4.83x acceleration over vanilla autoregressive\ndecoding, with no fine-tuning required for either models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD), where a draft model provides multiple candidate\ntokens for the target model to verify in parallel, has demonstrated significant\npotential for accelerating LLM inference. Yet, existing SD approaches adhere to\na strict draft-then-verify paradigm, enforcing a sequential process that\nhampers performance and constrains the draft model's capacity. Moreover,\nrejecting a token in the candidate sequence invalidates all subsequent tokens,\nleading to wasted computation during drafting. To overcome these limitations,\nwe propose a cache-assisted parallel speculative decoding framework called\nCARD, which employs a novel query-and-correct paradigm. Our approach decouples\ndrafting from verification: the draft model populates a shared cache with\ncandidate tokens, while the target model concurrently refines the draft's\ntrajectory. This enables inference at near-draft-speed, effectively leveraging\nthe draft model's efficiency without additional fine-tuning. Experimental\nresults show that CARD significantly outperforms existing state-of-the-art\nmethods, achieving up to a 4.83x acceleration over vanilla autoregressive\ndecoding, with no fine-tuning required for either models."
                },
                "authors": [
                    {
                        "name": "Enyu Zhou"
                    },
                    {
                        "name": "Kai Sheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Xin He"
                    }
                ],
                "author_detail": {
                    "name": "Xin He"
                },
                "author": "Xin He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15529v1",
                "updated": "2025-09-19T02:27:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    2,
                    27,
                    1,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T02:27:01Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    2,
                    27,
                    1,
                    4,
                    262,
                    0
                ],
                "title": "Optimization techniques for SQL+ML queries: A performance analysis of\n  real-time feature computation in OpenMLDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimization techniques for SQL+ML queries: A performance analysis of\n  real-time feature computation in OpenMLDB"
                },
                "summary": "In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source\ndatabase that seamlessly integrates offline and online feature computations.\nThe work used feature-rich synthetic dataset experiments in Docker, which acted\nlike production environments that processed 100 to 500 records per batch and 6\nto 12 requests per batch in parallel. Efforts have been concentrated in the\nareas of better query plans, cached execution plans, parallel processing, and\nresource management. The experimental results show that OpenMLDB can support\napproximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL\nand ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This\nstudy assessed the impact of optimization and showed that query plan\noptimization accounted for 35% of the performance gains, caching for 25%, and\nparallel processing for 20%. These results illustrate OpenMLDB's capability for\ntime-sensitive ML use cases, such as fraud detection, personalized\nrecommendation, and time series forecasting. The system's modular optimization\nframework, which combines batch and stream processing without interference,\ncontributes to its significant performance gain over traditional database\nsystems, particularly in applications that require real-time feature\ncomputation and serving. This study contributes to the understanding and design\nof high-performance SQL+ML systems and highlights the need for specialized SQL\noptimization for ML workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we optimize SQL+ML queries on top of OpenMLDB, an open-source\ndatabase that seamlessly integrates offline and online feature computations.\nThe work used feature-rich synthetic dataset experiments in Docker, which acted\nlike production environments that processed 100 to 500 records per batch and 6\nto 12 requests per batch in parallel. Efforts have been concentrated in the\nareas of better query plans, cached execution plans, parallel processing, and\nresource management. The experimental results show that OpenMLDB can support\napproximately 12,500 QPS with less than 1 ms latency, outperforming SparkSQL\nand ClickHouse by a factor of 23 and PostgreSQL and MySQL by 3.57 times. This\nstudy assessed the impact of optimization and showed that query plan\noptimization accounted for 35% of the performance gains, caching for 25%, and\nparallel processing for 20%. These results illustrate OpenMLDB's capability for\ntime-sensitive ML use cases, such as fraud detection, personalized\nrecommendation, and time series forecasting. The system's modular optimization\nframework, which combines batch and stream processing without interference,\ncontributes to its significant performance gain over traditional database\nsystems, particularly in applications that require real-time feature\ncomputation and serving. This study contributes to the understanding and design\nof high-performance SQL+ML systems and highlights the need for specialized SQL\noptimization for ML workloads."
                },
                "authors": [
                    {
                        "name": "Mashkhal A. Sidiq"
                    },
                    {
                        "name": "Aras A. Salih"
                    },
                    {
                        "name": "Samrand M. Hassan"
                    }
                ],
                "author_detail": {
                    "name": "Samrand M. Hassan"
                },
                "author": "Samrand M. Hassan",
                "arxiv_doi": "10.5121/ijdms.2025.17501",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5121/ijdms.2025.17501",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.15529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 4 figures, 1 Table",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15515v1",
                "updated": "2025-09-19T01:39:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    19,
                    1,
                    39,
                    8,
                    4,
                    262,
                    0
                ],
                "published": "2025-09-19T01:39:08Z",
                "published_parsed": [
                    2025,
                    9,
                    19,
                    1,
                    39,
                    8,
                    4,
                    262,
                    0
                ],
                "title": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for\n  Cost-Effective LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for\n  Cost-Effective LLM Inference"
                },
                "summary": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%."
                },
                "authors": [
                    {
                        "name": "Hantao Yang"
                    },
                    {
                        "name": "Hong Xie"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v3",
                "updated": "2025-09-18T23:34:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    23,
                    34,
                    50,
                    3,
                    261,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazn"
                    },
                    {
                        "name": "V. lvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodrguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Crcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cosso"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Daz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. Garca-Barrena"
                    },
                    {
                        "name": "J. J. Gmez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gmez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervs Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. Lpez-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martn-Albo"
                    },
                    {
                        "name": "G. Martnez-Lema"
                    },
                    {
                        "name": "M. Martnez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Prez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simn"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnel"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usn"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16278v1",
                "updated": "2025-09-18T17:38:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    38,
                    48,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T17:38:48Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    17,
                    38,
                    48,
                    3,
                    261,
                    0
                ],
                "title": "Language Modeling with Learned Meta-Tokens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Modeling with Learned Meta-Tokens"
                },
                "summary": "While modern Transformer-based language models (LMs) have achieved major\nsuccess in multi-task generalization, they often struggle to capture long-range\ndependencies within their context window. This work introduces a novel approach\nusing meta-tokens, special tokens injected during pre-training, along with a\ndedicated meta-attention mechanism to guide LMs to use these tokens. We\npre-train a language model with a modified GPT-2 architecture equipped with\nmeta-attention in addition to causal multi-head attention, and study the impact\nof these tokens on a suite of synthetic tasks. We find that data-efficient\nlanguage model pre-training on fewer than 100B tokens utilizing meta-tokens and\nour meta-attention mechanism achieves strong performance on these tasks after\nfine-tuning. We suggest that these gains arise due to the meta-tokens\nsharpening the positional encoding. This enables them to operate as trainable,\ncontent-based landmarks, implicitly compressing preceding context and \"caching\"\nit in the meta-token. At inference-time, the meta-token points to relevant\ncontext, facilitating length generalization up to 2$\\times$ its context window,\neven after extension with YaRN. We provide further evidence of these behaviors\nby visualizing model internals to study the residual stream, and assessing the\ncompression quality by information-theoretic analysis on the rate-distortion\ntradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a\nsimple, data-efficient method to enhance long-context language modeling\nperformance, while introducing new insights into the nature of their behavior\ntowards length generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While modern Transformer-based language models (LMs) have achieved major\nsuccess in multi-task generalization, they often struggle to capture long-range\ndependencies within their context window. This work introduces a novel approach\nusing meta-tokens, special tokens injected during pre-training, along with a\ndedicated meta-attention mechanism to guide LMs to use these tokens. We\npre-train a language model with a modified GPT-2 architecture equipped with\nmeta-attention in addition to causal multi-head attention, and study the impact\nof these tokens on a suite of synthetic tasks. We find that data-efficient\nlanguage model pre-training on fewer than 100B tokens utilizing meta-tokens and\nour meta-attention mechanism achieves strong performance on these tasks after\nfine-tuning. We suggest that these gains arise due to the meta-tokens\nsharpening the positional encoding. This enables them to operate as trainable,\ncontent-based landmarks, implicitly compressing preceding context and \"caching\"\nit in the meta-token. At inference-time, the meta-token points to relevant\ncontext, facilitating length generalization up to 2$\\times$ its context window,\neven after extension with YaRN. We provide further evidence of these behaviors\nby visualizing model internals to study the residual stream, and assessing the\ncompression quality by information-theoretic analysis on the rate-distortion\ntradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a\nsimple, data-efficient method to enhance long-context language modeling\nperformance, while introducing new insights into the nature of their behavior\ntowards length generalization."
                },
                "authors": [
                    {
                        "name": "Alok N. Shah"
                    },
                    {
                        "name": "Khush Gupta"
                    },
                    {
                        "name": "Keshav Ramji"
                    },
                    {
                        "name": "Pratik Chaudhari"
                    }
                ],
                "author_detail": {
                    "name": "Pratik Chaudhari"
                },
                "author": "Pratik Chaudhari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15038v1",
                "updated": "2025-09-18T15:04:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T15:04:06Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    15,
                    4,
                    6,
                    3,
                    261,
                    0
                ],
                "title": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value-Guided KV Compression for LLMs via Approximated CUR Decomposition"
                },
                "summary": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-value (KV) cache compression has emerged as a critical technique for\nreducing the memory and latency overhead of autoregressive language models\nduring inference. Prior approaches predominantly rely on query-key attention\nscores to rank and evict cached tokens, assuming that attention intensity\ncorrelates with semantic importance. However, this heuristic overlooks the\ncontribution of value vectors, which directly influence the attention output.\nIn this paper, we propose CurDKV, a novel, value-centric KV compression method\nthat selects keys and values based on leverage scores computed from CUR matrix\ndecomposition. Our approach approximates the dominant subspace of the attention\noutput $softmax(QK^T)V$, ensuring that the retained tokens best preserve the\nmodel's predictive behavior. Theoretically, we show that attention score\napproximation does not guarantee output preservation, and demonstrate that\nCUR-based selection minimizes end-to-end attention reconstruction loss.\nEmpirically, CurDKV achieves up to 9.6% higher accuracy than state-of-the-art\nmethods like SnapKV and ChunkKV under aggressive compression budgets on LLaMA\nand Mistral, while maintaining compatibility with FlashAttention and Grouped\nQuery Attention. In addition to improved accuracy, CurDKV reduces generation\nlatency by up to 40% at high compression, offering a practical speed-accuracy\ntradeoff."
                },
                "authors": [
                    {
                        "name": "Ayan Sengupta"
                    },
                    {
                        "name": "Siddhant Chaudhary"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.15024v1",
                "updated": "2025-09-18T14:51:13Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    51,
                    13,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-18T14:51:13Z",
                "published_parsed": [
                    2025,
                    9,
                    18,
                    14,
                    51,
                    13,
                    3,
                    261,
                    0
                ],
                "title": "Attention Beyond Neighborhoods: Reviving Transformer for Graph\n  Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Beyond Neighborhoods: Reviving Transformer for Graph\n  Clustering"
                },
                "summary": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have become a cornerstone in modern neural networks,\ndriving breakthroughs across diverse domains. However, their application to\ngraph structured data, where capturing topological connections is essential,\nremains underexplored and underperforming compared to Graph Neural Networks\n(GNNs), particularly in the graph clustering task. GNN tends to overemphasize\nneighborhood aggregation, leading to a homogenization of node representations.\nConversely, Transformer tends to over globalize, highlighting distant nodes at\nthe expense of meaningful local patterns. This dichotomy raises a key question:\nIs attention inherently redundant for unsupervised graph learning? To address\nthis, we conduct a comprehensive empirical analysis, uncovering the\ncomplementary weaknesses of GNN and Transformer in graph clustering. Motivated\nby these insights, we propose the Attentive Graph Clustering Network (AGCN) a\nnovel architecture that reinterprets the notion that graph is attention. AGCN\ndirectly embeds the attention mechanism into the graph structure, enabling\neffective global information extraction while maintaining sensitivity to local\ntopological cues. Our framework incorporates theoretical analysis to contrast\nAGCN behavior with GNN and Transformer and introduces two innovations: (1) a KV\ncache mechanism to improve computational efficiency, and (2) a pairwise margin\ncontrastive loss to boost the discriminative capacity of the attention space.\nExtensive experimental results demonstrate that AGCN outperforms\nstate-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Xuanting Xie"
                    },
                    {
                        "name": "Bingheng Li"
                    },
                    {
                        "name": "Erlin Pan"
                    },
                    {
                        "name": "Rui Hou"
                    },
                    {
                        "name": "Wenyu Chen"
                    },
                    {
                        "name": "Zhao Kang"
                    }
                ],
                "author_detail": {
                    "name": "Zhao Kang"
                },
                "author": "Zhao Kang",
                "arxiv_comment": "9 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13789v2",
                "updated": "2025-09-18T04:57:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    18,
                    4,
                    57,
                    32,
                    3,
                    261,
                    0
                ],
                "published": "2025-09-17T07:58:36Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    7,
                    58,
                    36,
                    2,
                    260,
                    0
                ],
                "title": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BWCache: Accelerating Video Diffusion Transformers through Block-Wise\n  Caching"
                },
                "summary": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Diffusion Transformers (DiTs) have established them as\nthe state-of-the-art method for video generation. However, their inherently\nsequential denoising process results in inevitable latency, limiting real-world\napplicability. Existing acceleration methods either compromise visual quality\ndue to architectural modifications or fail to reuse intermediate features at\nproper granularity. Our analysis reveals that DiT blocks are the primary\ncontributors to inference latency. Across diffusion timesteps, the feature\nvariations of DiT blocks exhibit a U-shaped pattern with high similarity during\nintermediate timesteps, which suggests substantial computational redundancy. In\nthis paper, we propose Block-Wise Caching (BWCache), a training-free method to\naccelerate DiT-based video generation. BWCache dynamically caches and reuses\nfeatures from DiT blocks across diffusion timesteps. Furthermore, we introduce\na similarity indicator that triggers feature reuse only when the differences\nbetween block features at adjacent timesteps fall below a threshold, thereby\nminimizing redundant computations while maintaining visual fidelity. Extensive\nexperiments on several video diffusion models demonstrate that BWCache achieves\nup to 2.24$\\times$ speedup with comparable visual quality."
                },
                "authors": [
                    {
                        "name": "Hanshuai Cui"
                    },
                    {
                        "name": "Zhiqing Tang"
                    },
                    {
                        "name": "Zhifei Xu"
                    },
                    {
                        "name": "Zhi Yao"
                    },
                    {
                        "name": "Wenyi Zeng"
                    },
                    {
                        "name": "Weijia Jia"
                    }
                ],
                "author_detail": {
                    "name": "Weijia Jia"
                },
                "author": "Weijia Jia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14403v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14403v1",
                "updated": "2025-09-17T20:08:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    20,
                    8,
                    53,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T20:08:53Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    20,
                    8,
                    53,
                    2,
                    260,
                    0
                ],
                "title": "Kilovolt-Class $-Ga_2O_3$ Field-Plated Schottky Barrier Diodes with\n  MOCVD-Grown Intentionally $10^{15}$ $cm^{-3}$ Doped Drift Layers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kilovolt-Class $-Ga_2O_3$ Field-Plated Schottky Barrier Diodes with\n  MOCVD-Grown Intentionally $10^{15}$ $cm^{-3}$ Doped Drift Layers"
                },
                "summary": "We report on the growth optimization of intentionally low-doped ($10^{15}$\n$cm^{-3}$) high-quality $\\beta-Ga_2O_3$ drift layers up to 10 $\\mu m$ thick via\nMOCVD and the fabrication of kilovolt-class field plated Schottky barrier\ndiodes on these thick drift layers. Homoepitaxial growth was performed on (010)\n$10^{15}$ $cm^{-3}$ substrates using TMGa as the Ga precursor. Growth\nparameters were systematically optimized to determine the best conditions for\nhigh quality thick growths with the given reactor geometry. Chamber pressure\nwas found to improve the growth rate, mobility, and roughness of the samples.\nGrowth rates of up to 7.2 $\\mu m$/hr., thicknesses of up to 10 $\\mu m$, Hall\nmobilities of up to 176 $cm^2$/Vs, RMS roughness down to 5.45 nm, UID\nconcentrations as low as $2 \\times$ $10^{15}$ $cm^{-3}$, and controllable\nintentional doping down to $3 \\times$ $10^{15}$ $cm^{-3}$ were achieved. Field\nplated Schottky barrier diodes (FP-SBDs) were fabricated on a $6.5 \\times$\n$10^{15}$ $cm^{-3}$ intentionally doped 10 $\\mu m$ thick film to determine the\nelectrical performance of the MOCVD-grown material. The FP-SBD was found to\nhave current density $>$100 A/$cm^2$ at 3 V forward bias with a specific\ndifferential on resistance ($R_{on,sp}$) of 16.22 m$\\Omega$.$cm^2$ and a turn\non voltage of 1 V. The diodes were found to have high quality anode\nmetal/semiconductor interfaces with an ideality factor of 1.04, close to unity.\nDiodes had a maximum breakdown voltage of 1.50 kV, leading to a punch-through\nmaximum field of 2.04 MV/cm under the anode metal, which is a state-of-the-art\nresult for SBDs on MOCVD-grown (010) drift layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on the growth optimization of intentionally low-doped ($10^{15}$\n$cm^{-3}$) high-quality $\\beta-Ga_2O_3$ drift layers up to 10 $\\mu m$ thick via\nMOCVD and the fabrication of kilovolt-class field plated Schottky barrier\ndiodes on these thick drift layers. Homoepitaxial growth was performed on (010)\n$10^{15}$ $cm^{-3}$ substrates using TMGa as the Ga precursor. Growth\nparameters were systematically optimized to determine the best conditions for\nhigh quality thick growths with the given reactor geometry. Chamber pressure\nwas found to improve the growth rate, mobility, and roughness of the samples.\nGrowth rates of up to 7.2 $\\mu m$/hr., thicknesses of up to 10 $\\mu m$, Hall\nmobilities of up to 176 $cm^2$/Vs, RMS roughness down to 5.45 nm, UID\nconcentrations as low as $2 \\times$ $10^{15}$ $cm^{-3}$, and controllable\nintentional doping down to $3 \\times$ $10^{15}$ $cm^{-3}$ were achieved. Field\nplated Schottky barrier diodes (FP-SBDs) were fabricated on a $6.5 \\times$\n$10^{15}$ $cm^{-3}$ intentionally doped 10 $\\mu m$ thick film to determine the\nelectrical performance of the MOCVD-grown material. The FP-SBD was found to\nhave current density $>$100 A/$cm^2$ at 3 V forward bias with a specific\ndifferential on resistance ($R_{on,sp}$) of 16.22 m$\\Omega$.$cm^2$ and a turn\non voltage of 1 V. The diodes were found to have high quality anode\nmetal/semiconductor interfaces with an ideality factor of 1.04, close to unity.\nDiodes had a maximum breakdown voltage of 1.50 kV, leading to a punch-through\nmaximum field of 2.04 MV/cm under the anode metal, which is a state-of-the-art\nresult for SBDs on MOCVD-grown (010) drift layers."
                },
                "authors": [
                    {
                        "name": "Carl Peterson"
                    },
                    {
                        "name": "Chinmoy Nath Saha"
                    },
                    {
                        "name": "Rachel Kahler"
                    },
                    {
                        "name": "Yizheng Liu"
                    },
                    {
                        "name": "Akhila Mattapalli"
                    },
                    {
                        "name": "Saurav Roy"
                    },
                    {
                        "name": "Sriram Krishnamoorthy"
                    }
                ],
                "author_detail": {
                    "name": "Sriram Krishnamoorthy"
                },
                "author": "Sriram Krishnamoorthy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14403v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14403v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14347v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14347v1",
                "updated": "2025-09-17T18:26:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    18,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T18:26:29Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    18,
                    26,
                    29,
                    2,
                    260,
                    0
                ],
                "title": "On the Illusion of Success: An Empirical Study of Build Reruns and\n  Silent Failures in Industrial CI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Illusion of Success: An Empirical Study of Build Reruns and\n  Silent Failures in Industrial CI"
                },
                "summary": "Reliability of build outcomes is a cornerstone of effective Continuous\nIntegration (CI). Yet in practice, developers often struggle with\nnon-deterministic issues in the code or CI infrastructure, which undermine\ntrust in build results. When faced with such unexpected outcomes, developers\noften repeatedly rerun jobs hoping for true success, but this practice is known\nto increase CI costs and reduce productivity. While recent studies have focused\non intermittent job failures, no prior work has investigated silent failures,\nwhere build jobs are marked as successful but fail to complete all or part of\ntheir tasks. Such silent failures often go unnoticed, creating an illusion of\nsuccess with detrimental consequences such as bugs escaping into production.\nThis paper presents the first empirical study of silent failures through the\npractice of rerunning successful jobs. An analysis of 142,387 jobs across 81\nindustrial projects shows that 11% of successful jobs are rerun, with 35% of\nthese reruns occurring after more than 24 hours. Using mixed-effects models on\n32 independent variables (AUC of 85%), we identified key factors associated\nwith reruns of successful jobs, notably testing and static analysis tasks,\nscripting languages like Shell, and developers prior rerun tendencies. A\nfurther analysis of 92 public issues revealed 11 categories of silent failures\naligning with these factors, the most frequent being artifact operation errors,\ncaching errors, and ignored exit codes. Overall, our findings provide valuable\ninsights into the circumstances and causes of silent failures to raise\nawareness among teams, and present solutions to improve CI reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliability of build outcomes is a cornerstone of effective Continuous\nIntegration (CI). Yet in practice, developers often struggle with\nnon-deterministic issues in the code or CI infrastructure, which undermine\ntrust in build results. When faced with such unexpected outcomes, developers\noften repeatedly rerun jobs hoping for true success, but this practice is known\nto increase CI costs and reduce productivity. While recent studies have focused\non intermittent job failures, no prior work has investigated silent failures,\nwhere build jobs are marked as successful but fail to complete all or part of\ntheir tasks. Such silent failures often go unnoticed, creating an illusion of\nsuccess with detrimental consequences such as bugs escaping into production.\nThis paper presents the first empirical study of silent failures through the\npractice of rerunning successful jobs. An analysis of 142,387 jobs across 81\nindustrial projects shows that 11% of successful jobs are rerun, with 35% of\nthese reruns occurring after more than 24 hours. Using mixed-effects models on\n32 independent variables (AUC of 85%), we identified key factors associated\nwith reruns of successful jobs, notably testing and static analysis tasks,\nscripting languages like Shell, and developers prior rerun tendencies. A\nfurther analysis of 92 public issues revealed 11 categories of silent failures\naligning with these factors, the most frequent being artifact operation errors,\ncaching errors, and ignored exit codes. Overall, our findings provide valuable\ninsights into the circumstances and causes of silent failures to raise\nawareness among teams, and present solutions to improve CI reliability."
                },
                "authors": [
                    {
                        "name": "Henri Adasso"
                    },
                    {
                        "name": "Francis Bordeleau"
                    },
                    {
                        "name": "Ali Tizghadam"
                    }
                ],
                "author_detail": {
                    "name": "Ali Tizghadam"
                },
                "author": "Ali Tizghadam",
                "arxiv_comment": "17 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14347v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14347v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14093v1",
                "updated": "2025-09-17T15:33:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T15:33:44Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    15,
                    33,
                    44,
                    2,
                    260,
                    0
                ],
                "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A\n  Self-Optimizing Framework"
                },
                "summary": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints."
                },
                "authors": [
                    {
                        "name": "Kerui Huang"
                    },
                    {
                        "name": "Shuhan Liu"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Tongtong Xu"
                    },
                    {
                        "name": "Lingfeng Bao"
                    },
                    {
                        "name": "Xin Xia"
                    }
                ],
                "author_detail": {
                    "name": "Xin Xia"
                },
                "author": "Xin Xia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14041v1",
                "updated": "2025-09-17T14:42:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    38,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T14:42:38Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    14,
                    42,
                    38,
                    2,
                    260,
                    0
                ],
                "title": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval\n  Prediction For Instruction Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A TRRIP Down Memory Lane: Temperature-Based Re-Reference Interval\n  Prediction For Instruction Caching"
                },
                "summary": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern mobile CPU software pose challenges for conventional instruction cache\nreplacement policies due to their complex runtime behavior causing high reuse\ndistance between executions of the same instruction. Mobile code commonly\nsuffers from large amounts of stalls in the CPU frontend and thus starvation of\nthe rest of the CPU resources. Complexity of these applications and their code\nfootprint are projected to grow at a rate faster than available on-chip memory\ndue to power and area constraints, making conventional hardware-centric methods\nfor managing instruction caches to be inadequate. We present a novel\nsoftware-hardware co-design approach called TRRIP (Temperature-based\nRe-Reference Interval Prediction) that enables the compiler to analyze,\nclassify, and transform code based on \"temperature\" (hot/cold), and to provide\nthe hardware with a summary of code temperature information through a\nwell-defined OS interface based on using code page attributes. TRRIP's\nlightweight hardware extension employs code temperature attributes to optimize\nthe instruction cache replacement policy resulting in the eviction rate\nreduction of hot code. TRRIP is designed to be practical and adoptable in real\nmobile systems that have strict feature requirements on both the software and\nhardware components. TRRIP can reduce the L2 MPKI for instructions by 26.5%\nresulting in geomean speedup of 3.9%, on top of RRIP cache replacement running\nmobile code already optimized using PGO."
                },
                "authors": [
                    {
                        "name": "Henry Kao"
                    },
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Prabhdeep Singh Soni"
                    },
                    {
                        "name": "Ali Sedaghati"
                    },
                    {
                        "name": "Fang Su"
                    },
                    {
                        "name": "Bryan Chan"
                    },
                    {
                        "name": "Maziar Goudarzi"
                    },
                    {
                        "name": "Reza Azimi"
                    }
                ],
                "author_detail": {
                    "name": "Reza Azimi"
                },
                "author": "Reza Azimi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18172v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18172v1",
                "updated": "2025-09-17T13:51:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    51,
                    27,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T13:51:27Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    13,
                    51,
                    27,
                    2,
                    260,
                    0
                ],
                "title": "SBVR: Summation of BitVector Representation for Efficient LLM\n  Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SBVR: Summation of BitVector Representation for Efficient LLM\n  Quantization"
                },
                "summary": "With the advent of large language models (LLMs), numerous Post-Training\nQuantization (PTQ) strategies have been proposed to alleviate deployment\nbarriers created by their enormous parameter counts. Quantization achieves\ncompression by limiting the number of representable points in the data.\nTherefore, the key to achieving efficient quantization is selecting the optimal\ncombination of representation points, or codes, for the given data. Existing\nPTQ solutions adopt two major approaches to this problem: Round-To-Nearest\n(RTN)-based methods and codebook-based methods. RTN-based methods map LLM\nweights onto uniformly distributed integer grids, failing to account for the\nGaussian-like weight distribution of LLM weights. Codebook-based methods\nmitigate this issue by constructing distribution-aware codebooks; however, they\nsuffer from random and strided memory access patterns, resulting in degraded\ninference speed that is exacerbated by the limited size of GPU L1 cache. To\novercome these limitations, we propose a novel LLM quantization method, SBVR\n(Summation of BitVector Representation), that enables Gaussian-like code\nrepresentation in a hardware-friendly manner for fast inference. SBVR maps\nweight values to non-uniform representation points whose distribution follows\nthe actual distribution of LLM weights, enabling more accurate compression.\nAdditionally, we design a custom CUDA kernel that allows matrix-vector\nmultiplication directly in the SBVR format without decompression, thereby\nenabling high-performance execution of SBVR-compressed models. Our evaluations\nof SBVR on various models demonstrate state-of-the-art perplexity and accuracy\nbenchmark performance while delivering a 2.21x- 3.04x end-to-end\ntoken-generation speedup over naive FP16 models in the 4-bit quantization\nregime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of large language models (LLMs), numerous Post-Training\nQuantization (PTQ) strategies have been proposed to alleviate deployment\nbarriers created by their enormous parameter counts. Quantization achieves\ncompression by limiting the number of representable points in the data.\nTherefore, the key to achieving efficient quantization is selecting the optimal\ncombination of representation points, or codes, for the given data. Existing\nPTQ solutions adopt two major approaches to this problem: Round-To-Nearest\n(RTN)-based methods and codebook-based methods. RTN-based methods map LLM\nweights onto uniformly distributed integer grids, failing to account for the\nGaussian-like weight distribution of LLM weights. Codebook-based methods\nmitigate this issue by constructing distribution-aware codebooks; however, they\nsuffer from random and strided memory access patterns, resulting in degraded\ninference speed that is exacerbated by the limited size of GPU L1 cache. To\novercome these limitations, we propose a novel LLM quantization method, SBVR\n(Summation of BitVector Representation), that enables Gaussian-like code\nrepresentation in a hardware-friendly manner for fast inference. SBVR maps\nweight values to non-uniform representation points whose distribution follows\nthe actual distribution of LLM weights, enabling more accurate compression.\nAdditionally, we design a custom CUDA kernel that allows matrix-vector\nmultiplication directly in the SBVR format without decompression, thereby\nenabling high-performance execution of SBVR-compressed models. Our evaluations\nof SBVR on various models demonstrate state-of-the-art perplexity and accuracy\nbenchmark performance while delivering a 2.21x- 3.04x end-to-end\ntoken-generation speedup over naive FP16 models in the 4-bit quantization\nregime."
                },
                "authors": [
                    {
                        "name": "Wonjun Bang"
                    },
                    {
                        "name": "Jongseok Park"
                    },
                    {
                        "name": "Hongseung Yu"
                    },
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Kyunghan Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyunghan Lee"
                },
                "author": "Kyunghan Lee",
                "arxiv_comment": "9 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18172v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18172v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13848v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13848v1",
                "updated": "2025-09-17T09:24:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T09:24:40Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    9,
                    24,
                    40,
                    2,
                    260,
                    0
                ],
                "title": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecDiff: Accelerating Diffusion Model Inference with Self-Speculation"
                },
                "summary": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has recently emerged as a promising method for diffusion\nmodel acceleration. It effectively alleviates the inefficiency problem caused\nby high computational requirements by caching similar features in the inference\nprocess of the diffusion model. In this paper, we analyze existing feature\ncaching methods from the perspective of information utilization, and point out\nthat relying solely on historical information will lead to constrained accuracy\nand speed performance. And we propose a novel paradigm that introduces future\ninformation via self-speculation based on the information similarity at the\nsame time step across different iteration times. Based on this paradigm, we\npresent \\textit{SpecDiff}, a training-free multi-level feature caching strategy\nincluding a cached feature selection algorithm and a multi-level feature\nclassification algorithm. (1) Feature selection algorithm based on\nself-speculative information. \\textit{SpecDiff} determines a dynamic importance\nscore for each token based on self-speculative information and historical\ninformation, and performs cached feature selection through the importance\nscore. (2) Multi-level feature classification algorithm based on feature\nimportance scores. \\textit{SpecDiff} classifies tokens by leveraging the\ndifferences in feature importance scores and introduces a multi-level feature\ncalculation strategy. Extensive experiments show that \\textit{SpecDiff}\nachieves average 2.80 \\times, 2.74 \\times , and 3.17\\times speedup with\nnegligible quality loss in Stable Diffusion 3, 3.5, and FLUX compared to RFlow\non NVIDIA A800-80GB GPU. By merging speculative and historical information,\n\\textit{SpecDiff} overcomes the speedup-accuracy trade-off bottleneck, pushing\nthe Pareto frontier of speedup and accuracy in the efficient diffusion model\ninference."
                },
                "authors": [
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Jiaming Xu"
                    },
                    {
                        "name": "Yongkang Zhou"
                    },
                    {
                        "name": "Guohao Dai"
                    }
                ],
                "author_detail": {
                    "name": "Guohao Dai"
                },
                "author": "Guohao Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13848v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13848v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13604v1",
                "updated": "2025-09-17T00:28:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    17,
                    0,
                    28,
                    49,
                    2,
                    260,
                    0
                ],
                "published": "2025-09-17T00:28:49Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    0,
                    28,
                    49,
                    2,
                    260,
                    0
                ],
                "title": "A Framework for Multi-source Prefetching Through Adaptive Weight",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Multi-source Prefetching Through Adaptive Weight"
                },
                "summary": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The World Wide Web has come to be a great part of our daily life, yet user\nobserved latency is still a problem that needs a proper means of handling. Even\nthough earlier attempts focused on caching as the chief solution to tackling\nthis issue, its success was extremely limited. Prefetching has come to be the\nprimary technique in supplementing caching towards soothing the latency problem\nassociated with the contemporary Internet. However, existing approaches in\nprefetching are extremely limited in their ability to employ application level\nweb document relationship which is often visible only to the content developer.\nThis is because most approaches are access history based schemes that make\nfuture users' access prediction only based on past user access. Attempts to\nincorporate prefetching schemes that utilize semantic information with those\nthat use users past access history are extremely limited in their\nextensibility. In this work we present a novel framework that enables\nintegration of schemes from both worlds of prefetching without the need for a\nmajor modification to the algorithms. When there is a need/possibility to\ncapture new application level context, a new algorithm could be developed to do\nso and then it can be integrated into the framework. Since each participating\nscheme is merely viewed as an algorithm that produces a list of candidate\nobjects that are likely to be accessed in the near future, the framework can\nentertain any one of the existing prefetching schemes. With its adaptive weight\nmanagement technique the framework adjusts the effect of each algorithm in the\noverall prediction to parallel with its observed performance so far. We have\nfound this formwork to be less aggressive than its contemporary counterparts\nwhich is extremely important for resource constrained mobile devices that have\ncome to be the major means of access by users of the current web."
                },
                "authors": [
                    {
                        "name": "Yoseph Berhanu Alebachew"
                    },
                    {
                        "name": "Mulugeta Libsie"
                    }
                ],
                "author_detail": {
                    "name": "Mulugeta Libsie"
                },
                "author": "Mulugeta Libsie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v3",
                "updated": "2025-09-16T23:56:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    23,
                    56,
                    55,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.08256v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.08256v2",
                "updated": "2025-09-16T23:15:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    23,
                    15,
                    44,
                    1,
                    259,
                    0
                ],
                "published": "2025-05-28T05:22:44Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    5,
                    22,
                    44,
                    2,
                    148,
                    0
                ],
                "title": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIER: Fine-Grained and Efficient KV Cache Retrieval for Long-context LLM\n  Inference"
                },
                "summary": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.Code is available at\nhttps://github.com/SimWangArizona/FIER",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache reading latency increases significantly with context\nlengths, hindering the efficiency of long-context LLM inference. To address\nthis, previous works propose retaining a small fraction of KV cache based on\ntoken importance. For example, KV eviction uses static heuristics to retain\ntokens, while KV retrieval dynamically selects query-relevant tokens for more\nadaptive cache management. However, we observe that important tokens are often\nsparsely distributed across the long context. This sparsity makes existing\npage-level KV retrieval inaccurate, as each page may include irrelevant tokens\nand miss critical ones. In this work, we propose Fier, a\n\\underline{Fi}ne-Grained and \\underline{E}fficient KV cache\n\\underline{R}etrieval method. Fier uses 1-bit quantized keys to estimate the\nimportance of each token, resulting in efficient and precise retrieval.\nExperiments show that Fier matches full KV performance using only 11\\% of the\ncache budget across various long-context tasks, reducing decoding latency by\n1.2$\\times$ to 1.5$\\times$.Code is available at\nhttps://github.com/SimWangArizona/FIER"
                },
                "authors": [
                    {
                        "name": "Dongwei Wang"
                    },
                    {
                        "name": "Zijie Liu"
                    },
                    {
                        "name": "Song Wang"
                    },
                    {
                        "name": "Yuxin Ren"
                    },
                    {
                        "name": "Jianing Deng"
                    },
                    {
                        "name": "Jingtong Hu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Huanrui Yang"
                    }
                ],
                "author_detail": {
                    "name": "Huanrui Yang"
                },
                "author": "Huanrui Yang",
                "arxiv_comment": "EMNLP2025 Camera-ready",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.08256v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.08256v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08523v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08523v3",
                "updated": "2025-09-16T10:33:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    10,
                    33,
                    29,
                    1,
                    259,
                    0
                ],
                "published": "2025-07-11T12:21:29Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    12,
                    21,
                    29,
                    4,
                    192,
                    0
                ],
                "title": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InferLog: Accelerating LLM Inference for Online Log Parsing via\n  ICL-oriented Prefix Caching"
                },
                "summary": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software systems generate massive volumes of runtime logs,\nnecessitating efficient and accurate log parsing to enable critical downstream\ntasks such as anomaly detection and root cause analysis. Recently, large\nlanguage models (LLMs) have achieved advanced accuracy on log parsing, but\ntheir deployment in production environments faces two major limitations: (1)\nthe privacy risks associated with commercial LLMs, driving the adoption of\nlocal deployment, and (2) the stringent latency and throughput requirements\nimposed by high-volume log streams, which existing LLM-based parsers fail to\nmeet. Although recent efforts have reduced the number of LLM queries, they\noverlook the high latency of the LLM invocations, where concurrent log parsing\nrequests can cause serve performance degradation of LLM inference system.\n  In this study, we present InferLog, the first LLM inference optimization\nmethod for online log parsing. Our key insight is that the inference efficiency\nemerges as the vital bottleneck in LLM-based online log parsing, rather than\nparsing accuracy. InferLog accelerates inference by designing (1) A\nPrefix-aware ICL Refinement policy to refine the examples and permutation of\nin-context learning to improve the prefix caching efficiency. (2) A rapid and\ntask-specific configuration tuning pipeline based on meta-learning to find the\noptimal LLM scheduling-related configuration for dynamic log parsing workloads.\nThe experimental results based on Loghub dataset and vLLM demonstrate that\nInferLog significantly outperforms existing inference optimization methods and\nmarkedly accelerates the state-of-the-art LLM-based log parser without\ncompromising parsing accuracy."
                },
                "authors": [
                    {
                        "name": "Yilun Wang"
                    },
                    {
                        "name": "Pengfei Chen"
                    },
                    {
                        "name": "Haiyu Huang"
                    },
                    {
                        "name": "Zilong He"
                    },
                    {
                        "name": "Gou Tan"
                    },
                    {
                        "name": "Chuanfu Zhang"
                    },
                    {
                        "name": "Jingkai He"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_doi": "10.1145/3744916.3764523",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3744916.3764523",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.08523v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08523v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ICSE '26 (The 48th IEEE/ACM International Conference on\n  Software Engineering)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12900v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12900v1",
                "updated": "2025-09-16T09:54:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:54:58Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    54,
                    58,
                    1,
                    259,
                    0
                ],
                "title": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Topology and Fragility of European High-Voltage Networks: A\n  Cross-Country Comparative Analysis"
                },
                "summary": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable electricity supply depends on the seamless operation of high-voltage\ngrid infrastructure spanning both transmission and sub-transmission levels.\nBeneath this apparent uniformity lies a striking structural diversity, which\nleaves a clear imprint on system vulnerability. In this paper, we present\nharmonized topological models of the high-voltage grids of 15 European\ncountries, integrating all elements at voltage levels above 110 kV. Topological\nanalysis of these networks reveals a simple yet robust pattern: node degree\ndistributions consistently follow an exponential decay, but the rate of decay\nvaries significantly across countries. Through a detailed and systematic\nevaluation of network tolerance to node and edge removals, we show that the\ndecay rate delineates the boundary between systems that are more resilient to\nfailures and those that are prone to large-scale disruptions. Furthermore, we\ndemonstrate that this numerical boundary is highly sensitive to which layers of\nthe infrastructure are included in the models. To our knowledge, this study\nprovides the first quantitative cross-country comparison of 15 European\nhigh-voltage networks, linking topological properties with vulnerability\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Blint Hartmann"
                    },
                    {
                        "name": "Michelle T. Cirunay"
                    }
                ],
                "author_detail": {
                    "name": "Michelle T. Cirunay"
                },
                "author": "Michelle T. Cirunay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12900v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12900v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12867v1",
                "updated": "2025-09-16T09:22:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:22:21Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    22,
                    21,
                    1,
                    259,
                    0
                ],
                "title": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-R1: Sample-Efficient Reinforcement Learning for Agentic Tool Use"
                },
                "summary": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated strong capabilities in\nlanguage understanding and reasoning, yet they remain limited when tackling\nreal-world tasks that require up-to-date knowledge, precise operations, or\nspecialized tool use. To address this, we propose Tool-R1, a reinforcement\nlearning framework that enables LLMs to perform general, compositional, and\nmulti-step tool use by generating executable Python code. Tool-R1 supports\nintegration of user-defined tools and standard libraries, with variable sharing\nacross steps to construct coherent workflows. An outcome-based reward function,\ncombining LLM-based answer judgment and code execution success, guides policy\noptimization. To improve training efficiency, we maintain a dynamic sample\nqueue to cache and reuse high-quality trajectories, reducing the overhead of\ncostly online sampling. Experiments on the GAIA benchmark show that Tool-R1\nsubstantially improves both accuracy and robustness, achieving about 10\\% gain\nover strong baselines, with larger improvements on complex multi-step tasks.\nThese results highlight the potential of Tool-R1 for enabling reliable and\nefficient tool-augmented reasoning in real-world applications. Our code will be\navailable at https://github.com/YBYBZhang/Tool-R1."
                },
                "authors": [
                    {
                        "name": "Yabo Zhang"
                    },
                    {
                        "name": "Yihan Zeng"
                    },
                    {
                        "name": "Qingyun Li"
                    },
                    {
                        "name": "Zhen Hu"
                    },
                    {
                        "name": "Kavin Han"
                    },
                    {
                        "name": "Wangmeng Zuo"
                    }
                ],
                "author_detail": {
                    "name": "Wangmeng Zuo"
                },
                "author": "Wangmeng Zuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19341v1",
                "updated": "2025-09-16T09:14:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    14,
                    15,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T09:14:15Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    14,
                    15,
                    1,
                    259,
                    0
                ],
                "title": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks"
                },
                "summary": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework."
                },
                "authors": [
                    {
                        "name": "Yang Fu"
                    },
                    {
                        "name": "Peng Qin"
                    },
                    {
                        "name": "Yueyue Zhang"
                    },
                    {
                        "name": "Yifei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Wang"
                },
                "author": "Yifei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.12817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.12817v1",
                "updated": "2025-09-16T08:36:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-16T08:36:05Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    8,
                    36,
                    5,
                    1,
                    259,
                    0
                ],
                "title": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGA: Selective Adaptive Gating for Efficient and Expressive Linear\n  Attention"
                },
                "summary": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformer architecture excel at modeling long-range dependencies\ncontributing to its widespread adoption in vision tasks the quadratic\ncomplexity of softmax-based attention mechanisms imposes a major bottleneck,\nparticularly when processing high-resolution images. Linear attention presents\na promising alternative by reformulating the attention computation from $(QK)V$\nto $Q(KV)$, thereby reducing the complexity from $\\mathcal{O}(N^2)$ to\n$\\mathcal{O}(N)$ while preserving the global receptive field. However, most\nexisting methods compress historical key-value (KV) information uniformly,\nwhich can lead to feature redundancy and the loss of directional alignment with\nthe query (Q). This uniform compression results in low-rank $KV$ feature maps,\ncontributing to a performance gap compared to softmax attention. To mitigate\nthis limitation, we propose \\textbf{S}elective \\textbf{A}daptive\n\\textbf{GA}ting for Efficient and Expressive Linear Attention (SAGA) , which\nintroduces input-adaptive learnable gates to selectively modulate information\naggregation into the $KV$ feature map. These gates enhance semantic diversity\nand alleviate the low-rank constraint inherent in conventional linear\nattention. Additionally, we propose an efficient Hadamard-product decomposition\nmethod for gate computation, which introduces no additional memory overhead.\nExperiments demonstrate that SAGA achieves a 1.76$\\times$ improvement in\nthroughput and a 2.69$\\times$ reduction in peak GPU memory compared to PVT-T at\na resolution of $1280 \\times 1280$. Moreover, it improves top-1 accuracy by up\nto 4.4\\% on the ImageNet dataset, demonstrating both computational efficiency\nand model effectiveness."
                },
                "authors": [
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Dong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Dong Wang"
                },
                "author": "Dong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.12817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.12817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11156v2",
                "updated": "2025-09-16T07:49:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    16,
                    7,
                    49,
                    41,
                    1,
                    259,
                    0
                ],
                "published": "2025-09-14T08:22:37Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    22,
                    37,
                    6,
                    257,
                    0
                ],
                "title": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive K-PackCache: Cost-Centric Data Caching in Cloud"
                },
                "summary": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in data analytics have enabled the accurate prediction of\nuser access patterns, giving rise to the idea of packed caching delivering\nmultiple co accessed data items together as a bundle. This improves caching\nefficiency, as accessing one item often implies the need for others. Prior work\nhas explored only 2 item pairwise packing. In this paper, we extend the concept\nto general K packing, allowing variable size bundles for improved flexibility\nand performance. We formulate the K PackCache problem from a content delivery\nnetwork CDN operator perspective, aiming to minimize total cost comprising two\ncomponents: transfer cost modeled as a base cost plus a linearly increasing\nterm with the number of items packed, and memory rental cost for caching, which\ndepends on how long and how much is stored. Overpacking increases cost due to\nlow utility, underpacking leads to missed sharing opportunities. We propose an\nonline algorithm, Adaptive K PackCache AKPC, which dynamically forms, merges,\nand splits data cliques based on user access patterns and content correlation.\nOur approach supports batch requests, enables approximate clique merging, and\noffers a formal competitive guarantee. Through extensive evaluation on the\nNetflix and Spotify datasets, AKPC reduces total cost by up to 63 and 55\npercentage over online baselines, respectively, and achieves performance within\n15 and 13 percentage of the optimal. This demonstrates its scalability and\neffectiveness for real world caching systems."
                },
                "authors": [
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Aadarshraj Sah"
                    },
                    {
                        "name": "Poddutoori Sweeya Reddy"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13231v2",
                "updated": "2025-09-15T14:40:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    14,
                    40,
                    16,
                    0,
                    258,
                    0
                ],
                "published": "2025-08-17T19:07:08Z",
                "published_parsed": [
                    2025,
                    8,
                    17,
                    19,
                    7,
                    8,
                    6,
                    229,
                    0
                ],
                "title": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference via Dynamic KV Cache Placement in\n  Heterogeneous Memory System"
                },
                "summary": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference is increasingly constrained by memory\nbandwidth, with frequent access to the key-value (KV) cache dominating data\nmovement. While attention sparsity reduces some memory traffic, the relevance\nof past tokens varies over time, requiring the full KV cache to remain\naccessible and sustaining pressure on both bandwidth and capacity. With\nadvances in interconnects such as NVLink and LPDDR5X, modern AI hardware now\nintegrates high-bandwidth memory (HBM) with high-speed off-package DRAM, making\nheterogeneous memory systems a practical solution. This work investigates\ndynamic KV cache placement across such systems to maximize aggregated bandwidth\nutilization under capacity constraints. Rather than proposing a specific\nscheduling policy, we formulate the placement problem mathematically and derive\na theoretical upper bound, revealing substantial headroom for runtime\noptimization. To our knowledge, this is the first formal treatment of dynamic\nKV cache scheduling in heterogeneous memory systems for LLM inference."
                },
                "authors": [
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Kaoutar El Maghraoui"
                    },
                    {
                        "name": "Naigang Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "IEEE Computer Architecture Letter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11628v1",
                "updated": "2025-09-15T06:46:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-15T06:46:22Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    6,
                    46,
                    22,
                    0,
                    258,
                    0
                ],
                "title": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpeCa: Accelerating Diffusion Transformers with Speculative Feature\n  Caching"
                },
                "summary": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have revolutionized high-fidelity image and video synthesis,\nyet their computational demands remain prohibitive for real-time applications.\nThese models face two fundamental challenges: strict temporal dependencies\npreventing parallelization, and computationally intensive forward passes\nrequired at each denoising step. Drawing inspiration from speculative decoding\nin large language models, we present SpeCa, a novel 'Forecast-then-verify'\nacceleration framework that effectively addresses both limitations. SpeCa's\ncore innovation lies in introducing Speculative Sampling to diffusion models,\npredicting intermediate features for subsequent timesteps based on fully\ncomputed reference timesteps. Our approach implements a parameter-free\nverification mechanism that efficiently evaluates prediction reliability,\nenabling real-time decisions to accept or reject each prediction while\nincurring negligible computational overhead. Furthermore, SpeCa introduces\nsample-adaptive computation allocation that dynamically modulates resources\nbased on generation complexity, allocating reduced computation for simpler\nsamples while preserving intensive processing for complex instances.\nExperiments demonstrate 6.34x acceleration on FLUX with minimal quality\ndegradation (5.5% drop), 7.3x speedup on DiT while preserving generation\nfidelity, and 79.84% VBench score at 6.1x acceleration for HunyuanVideo. The\nverification mechanism incurs minimal overhead (1.67%-3.5% of full inference\ncosts), establishing a new paradigm for efficient diffusion model inference\nwhile maintaining generation quality even at aggressive acceleration ratios.\nOur codes have been released in Github:\n\\textbf{https://github.com/Shenyi-Z/Cache4Diffusion}"
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Yuanhuiyi Lyu"
                    },
                    {
                        "name": "Fei Ren"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Kaixin Li"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_doi": "10.1145/3746027.3755331",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755331",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.11628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "15 pages, 9 figures, ACM Multimedia 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14089v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14089v2",
                "updated": "2025-09-15T01:15:50Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    1,
                    15,
                    50,
                    0,
                    258,
                    0
                ],
                "published": "2025-04-18T22:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    22,
                    10,
                    2,
                    4,
                    108,
                    0
                ],
                "title": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LogicTree: Structured Proof Exploration for Coherent and Rigorous\n  Logical Reasoning with Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable multi-step reasoning\ncapabilities across various domains. However, LLMs still face distinct\nchallenges in complex logical reasoning, as (1) proof-finding requires\nsystematic exploration and the maintenance of logical coherence and (2)\nsearching the right combination of premises at each reasoning step is\ninherently challenging in tasks with large premise space. To address this, we\npropose LogicTree, an inference-time modular framework employing\nalgorithm-guided search to automate structured proof exploration and ensure\nlogical coherence. Advancing beyond tree-of-thought (ToT), we incorporate\ncaching mechanism into LogicTree to enable effective utilization of historical\nknowledge, preventing reasoning stagnation and minimizing redundancy.\nFurthermore, we address the combinatorial complexity of premise search by\ndecomposing it into a linear process. The refined premise selection restricts\nsubsequent inference to at most one derivation per step, enhancing reasoning\ngranularity and enforcing strict step-by-step reasoning. Additionally, we\nintroduce two LLM-free heuristics for premise prioritization, enabling\nstrategic proof search. Experimental results on five datasets demonstrate that\nLogicTree optimally scales inference-time computation to achieve higher proof\naccuracy, surpassing chain-of-thought (CoT) and ToT with average gains of 23.6%\nand 12.5%, respectively, on GPT-4o. Moreover, within LogicTree, GPT-4o\noutperforms o3-mini by 7.6% on average."
                },
                "authors": [
                    {
                        "name": "Kang He"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "EMNLP 2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14089v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14089v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06261v2",
                "updated": "2025-09-15T00:51:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    15,
                    0,
                    51,
                    47,
                    0,
                    258,
                    0
                ],
                "published": "2025-09-08T00:57:50Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    0,
                    57,
                    50,
                    0,
                    251,
                    0
                ],
                "title": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FineServe: Precision-Aware KV Slab and Two-Level Scheduling for\n  Heterogeneous Precision LLM Serving"
                },
                "summary": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Post-Training Quantization (PTQ) techniques have\nsignificantly increased demand for serving quantized large language models\n(LLMs), enabling higher throughput and substantially reduced memory usage with\nminimal accuracy loss. Quantized models address memory constraints in LLMs and\nenhance GPU resource utilization through efficient GPU sharing. However,\nquantized models have smaller KV block sizes than non-quantized models, causing\nlimited memory efficiency due to memory fragmentation. Also, distinct resource\nusage patterns between quantized and non-quantized models require efficient\nscheduling to maximize throughput. To address these challenges, we propose\nFineServe, an inference serving framework for mixed-precision LLMs. FineServe's\nkey contributions include: (1) KV Slab, a precision-aware adaptive memory\nmanagement technique dynamically allocating KV cache based on model\nquantization characteristics, significantly reducing GPU memory fragmentation,\nand (2) a two-level scheduling framework comprising a global scheduler that\nplaces models to GPUs based on request rates, latency SLOs, and memory\nconstraints and efficiency, and a local scheduler that adaptively adjusts batch\nsizes according to real-time request fluctuations. Experimental results\ndemonstrate that FineServe achieves up to 2.2x higher SLO attainment and 1.8x\nhigher token generation throughput compared to the state-of-the-art GPU sharing\nsystems."
                },
                "authors": [
                    {
                        "name": "Kyungmin Bin"
                    },
                    {
                        "name": "Seungbeom Choi"
                    },
                    {
                        "name": "Jimyoung Son"
                    },
                    {
                        "name": "Jieun Choi"
                    },
                    {
                        "name": "Daseul Bae"
                    },
                    {
                        "name": "Daehyeon Baek"
                    },
                    {
                        "name": "Kihyo Moon"
                    },
                    {
                        "name": "Minsung Jang"
                    },
                    {
                        "name": "Hyojung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hyojung Lee"
                },
                "author": "Hyojung Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11239v1",
                "updated": "2025-09-14T12:29:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T12:29:49Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    12,
                    29,
                    49,
                    6,
                    257,
                    0
                ],
                "title": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Layer Perceptron-Based Relay Node Selection for Next-Generation\n  Intelligent Delay-Tolerant Networks"
                },
                "summary": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delay Tolerant Networks (DTNs) are critical for emergency communication in\nhighly dynamic and challenging scenarios characterized by intermittent\nconnectivity, frequent disruptions, and unpredictable node mobility. While some\nprotocols are widely adopted for simplicity and low overhead, their static\nreplication strategy lacks the ability to adaptively distinguish high-quality\nrelay nodes, often leading to inefficient and suboptimal message dissemination.\nTo address this challenge, we propose a novel intelligent routing enhancement\nthat integrates machine learning-based node evaluation into the Spray and Wait\nframework. Several dynamic, core features are extracted from simulation logs\nand are used to train multiple classifiers - Multi-Layer Perceptron (MLP),\nSupport Vector Machine (SVM), and Random Forest (RF) - to predict whether a\nnode is suitable as a relay under dynamic conditions. The trained models are\ndeployed via a lightweight Flask-based RESTful API, enabling real-time,\nadaptive predictions. We implement the enhanced router MLPBasedSprayRouter,\nwhich selectively forwards messages based on the predicted relay quality. A\ncaching mechanism is incorporated to reduce computational overhead and ensure\nstable, low-latency inference. Extensive experiments under realistic emergency\nmobility scenarios demonstrate that the proposed framework significantly\nimproves delivery ratio while reducing average latency compared to the baseline\nprotocols. Among all evaluated classifiers, MLP achieved the most robust\nperformance, consistently outperforming both SVM and RF in terms of accuracy,\nadaptability, and inference speed. These results confirm the novelty and\npracticality of integrating machine learning into DTN routing, paving the way\nfor resilient and intelligent communication systems in smart cities, disaster\nrecovery, and other dynamic environments."
                },
                "authors": [
                    {
                        "name": "Zhekun Huang"
                    },
                    {
                        "name": "Milena Radenkovic"
                    }
                ],
                "author_detail": {
                    "name": "Milena Radenkovic"
                },
                "author": "Milena Radenkovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11181v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11181v1",
                "updated": "2025-09-14T09:26:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T09:26:44Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    9,
                    26,
                    44,
                    6,
                    257,
                    0
                ],
                "title": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocation response to electric fields in strontium titanate: A\n  mesoscale indentation study"
                },
                "summary": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dislocations in perovskite oxides have drawn increasing research interest due\nto their potential of tuning functional properties of electroceramics. Open\nquestions remain regarding the behavior of dislocations concerning their\nstability under strong externally applied electric fields. In this study, we\ninvestigate the dielectric breakdown strength of nominally undoped SrTiO3\ncrystals after the introduction of high-density dislocations. The\ndislocation-rich samples are prepared using the Brinell scratching method, and\nthey consistently exhibit lower dielectric breakdown strength as well as a\nlarger scatter in the breakdown probability. We also study the impact of\nelectric field on the introduction and movement of dislocations in SrTiO3\ncrystals using Brinell indentation coupled with an electric field of 2 kV/mm.\nNo changes on the dislocation plastic zone size, depth, and dislocation\ndistribution are observed under this electric field. Based on the charge state\nof the dislocations in SrTiO3 as well as the electrical and thermal\nconductivity modified by dislocations, we discuss the forces induced by the\nelectric field to act on the dislocations to underline the possible mechanisms\nfor such dislocation behavior."
                },
                "authors": [
                    {
                        "name": "Alexander Frisch"
                    },
                    {
                        "name": "Daniel Isaia"
                    },
                    {
                        "name": "Oliver Preu"
                    },
                    {
                        "name": "Xufei Fang"
                    }
                ],
                "author_detail": {
                    "name": "Xufei Fang"
                },
                "author": "Xufei Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11181v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11181v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11155v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11155v1",
                "updated": "2025-09-14T08:20:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "published": "2025-09-14T08:20:48Z",
                "published_parsed": [
                    2025,
                    9,
                    14,
                    8,
                    20,
                    48,
                    6,
                    257,
                    0
                ],
                "title": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient\n  Inference in LLMs"
                },
                "summary": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable."
                },
                "authors": [
                    {
                        "name": "Santhosh G S"
                    },
                    {
                        "name": "Saurav Prakash"
                    },
                    {
                        "name": "Balaraman Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Balaraman Ravindran"
                },
                "author": "Balaraman Ravindran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11155v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11155v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10798v1",
                "updated": "2025-09-13T03:34:12Z",
                "updated_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "published": "2025-09-13T03:34:12Z",
                "published_parsed": [
                    2025,
                    9,
                    13,
                    3,
                    34,
                    12,
                    5,
                    256,
                    0
                ],
                "title": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Judge Q: Trainable Queries for Optimized Information Retention in KV\n  Cache Eviction"
                },
                "summary": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) utilize key-value (KV) cache to store historical\ninformation during sequence processing. The size of KV cache grows linearly as\nthe length of the sequence extends, which seriously affects memory usage and\ndecoding efficiency. Current methods for KV cache eviction typically utilize\nthe last window from the pre-filling phase as queries to compute the KV\nimportance scores for eviction. Although this scheme is simple to implement, it\ntends to overly focus on local information, potentially leading to the neglect\nor omission of crucial global information. To mitigate this issue, we propose\nJudge Q, a novel training method which incorporates a soft token list. This\nmethod only tunes the model's embedding layer at a low training cost. By\nconcatenating the soft token list at the end of the input sequence, we train\nthese tokens' attention map to the original input sequence to align with that\nof the actual decoded tokens. In this way, the queries corresponding to the\nsoft tokens can effectively capture global information and better evaluate the\nimportance of the keys and values within the KV cache, thus maintaining\ndecoding quality when KV cache is evicted. Under the same eviction budget, our\nmethod exhibits less performance degradation compared to existing eviction\napproaches. We validate our approach through experiments conducted on models\nsuch as Llama-3.1-8B-Instruct and Mistral-7B-Instruct-v0.3, using benchmarks\nincluding LongBench, RULER, and Needle-in-a-Haystack. Results indicate an\nimprovement of approximately 1 point on the LongBench and over 3 points on\nRULER. This proposed methodology can be seamlessly integrated into existing\nopen-source models with minimal training overhead, thereby enhancing\nperformance in KV cache eviction scenarios."
                },
                "authors": [
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10372v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10372v1",
                "updated": "2025-09-12T16:05:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T16:05:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    16,
                    5,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCBP: A Memory-Compute Efficient LLM Inference Accelerator Leveraging\n  Bit-Slice-enabled Sparsity and Repetitiveness"
                },
                "summary": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face significant inference latency due to\ninefficiencies in GEMM operations, weight access, and KV cache access,\nespecially in real-time scenarios. This highlights the need for a versatile\ncompute-memory efficient accelerator. Unfortunately, existing Transformer\naccelerators struggle to address both aspects simultaneously, as they focus on\nvalue-level processing, missing fine-grained opportunities to optimize\ncomputation and memory collaboratively. This paper introduces MCBP, a\nbit-grained compute-memory efficient algorithm-hardware co-design that\nleverages bit-slice (BS) enabled repetitiveness and sparsity to accelerate LLM\ninference. MCBP features three key innovations: 1) BS-repetitiveness-enabled\ncomputation reduction (BRCR), which eliminates redundant GEMM computations via\nleveraging redundancy hidden among BS vectors; 2) BS-sparsity-enabled two-state\ncoding (BSTC), which reduces weight access via exploiting significant sparsity\nin high-order bit-slice weight; 3) Bit-grained progressive prediction (BGPP),\nwhich reduces KV cache access by leveraging early-termination-based bit-grained\nprediction. These techniques, supported by custom accelerator designs,\neffectively alleviate the burden in GEMM, weight access, and KV cache access.\nExtensive experiments on 26 benchmarks show that MCBP achieves 9.43x speed up\nand 31.1x higher energy efficiency than Nvidia A100 GPU. Compared to SOTA\nTransformer accelerators, MCBP achieves 35x, 5.2x and 3.2x energy saving than\nSpatten, FACT and SOFA, respectively."
                },
                "authors": [
                    {
                        "name": "Huizheng Wang"
                    },
                    {
                        "name": "Zichuan Wang"
                    },
                    {
                        "name": "Zhiheng Yue"
                    },
                    {
                        "name": "Yousheng Long"
                    },
                    {
                        "name": "Taiquan Wei"
                    },
                    {
                        "name": "Jianxun Yang"
                    },
                    {
                        "name": "Yang Wang"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Shaojun Wei"
                    },
                    {
                        "name": "Yang Hu"
                    },
                    {
                        "name": "Shouyi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Shouyi Yin"
                },
                "author": "Shouyi Yin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10372v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10372v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10312v1",
                "updated": "2025-09-12T14:53:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T14:53:45Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    14,
                    53,
                    45,
                    4,
                    255,
                    0
                ],
                "title": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute Only 16 Tokens in One Timestep: Accelerating Diffusion\n  Transformers with Cluster-Driven Feature Caching"
                },
                "summary": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have gained significant attention in recent years for\ntheir ability to generate high-quality images and videos, yet still suffer from\na huge computational cost due to their iterative denoising process. Recently,\nfeature caching has been introduced to accelerate diffusion transformers by\ncaching the feature computation in previous timesteps and reusing it in the\nfollowing timesteps, which leverage the temporal similarity of diffusion models\nwhile ignoring the similarity in the spatial dimension. In this paper, we\nintroduce Cluster-Driven Feature Caching (ClusCa) as an orthogonal and\ncomplementary perspective for previous feature caching. Specifically, ClusCa\nperforms spatial clustering on tokens in each timestep, computes only one token\nin each cluster and propagates their information to all the other tokens, which\nis able to reduce the number of tokens by over 90%. Extensive experiments on\nDiT, FLUX and HunyuanVideo demonstrate its effectiveness in both text-to-image\nand text-to-video generation. Besides, it can be directly applied to any\ndiffusion transformer without requirements for training. For instance, ClusCa\nachieves 4.96x acceleration on FLUX with an ImageReward of 99.49%, surpassing\nthe original model by 0.51%. The code is available at\nhttps://github.com/Shenyi-Z/Cache4Diffusion."
                },
                "authors": [
                    {
                        "name": "Zhixin Zheng"
                    },
                    {
                        "name": "Xinyu Wang"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Shaobo Wang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "11 pages, 11 figures; Accepted by ACM MM2025; Mainly focus on feature\n  caching for diffusion transformers acceleration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10251v1",
                "updated": "2025-09-12T13:49:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T13:49:27Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    49,
                    27,
                    4,
                    255,
                    0
                ],
                "title": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XBOF: A Cost-Efficient CXL JBOF with Inter-SSD Compute Resource Sharing"
                },
                "summary": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enterprise SSDs integrate numerous computing resources (e.g., ARM processor\nand onboard DRAM) to satisfy the ever-increasing performance requirements of\nI/O bursts. While these resources substantially elevate the monetary costs of\nSSDs, the sporadic nature of I/O bursts causes severe SSD resource\nunderutilization in just a bunch of flash (JBOF) level. Tackling this\nchallenge, we propose XBOF, a cost-efficient JBOF design, which only reserves\nmoderate computing resources in SSDs at low monetary cost, while achieving\ndemanded I/O performance through efficient inter-SSD resource sharing.\nSpecifically, XBOF first disaggregates SSD architecture into multiple disjoint\nparts based on their functionality, enabling fine-grained SSD internal resource\nmanagement. XBOF then employs a decentralized scheme to manage these\ndisaggregated resources and harvests the computing resources of idle SSDs to\nassist busy SSDs in handling I/O bursts. This idea is facilitated by the\ncache-coherent capability of Compute eXpress Link (CXL), with which the busy\nSSDs can directly utilize the harvested computing resources to accelerate\nmetadata processing. The evaluation results show that XBOF improves SSD\nresource utilization by 50.4% and saves 19.0% monetary costs with a negligible\nperformance loss, compared to existing JBOF designs."
                },
                "authors": [
                    {
                        "name": "Shushu Yi"
                    },
                    {
                        "name": "Yuda An"
                    },
                    {
                        "name": "Li Peng"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Qiao Li"
                    },
                    {
                        "name": "Jieming Yin"
                    },
                    {
                        "name": "Guangyan Zhang"
                    },
                    {
                        "name": "Wenfei Wu"
                    },
                    {
                        "name": "Diyu Zhou"
                    },
                    {
                        "name": "Zhenlin Wang"
                    },
                    {
                        "name": "Xiaolin Wang"
                    },
                    {
                        "name": "Yingwei Luo"
                    },
                    {
                        "name": "Ke Zhou"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10016v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10016v1",
                "updated": "2025-09-12T07:20:53Z",
                "updated_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "published": "2025-09-12T07:20:53Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    7,
                    20,
                    53,
                    4,
                    255,
                    0
                ],
                "title": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SvalMIZ-25 Svalbard Marginal Ice Zone Campaign 2025 -- Cruise Report"
                },
                "summary": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The coupling of weather, sea-ice, ocean, and wave forecasting systems has\nbeen a long-standing research focus to improve Arctic forecasting systems and\ntheir realism and is also a priority of international initiatives such as the\nWMO research project PCAPS. The goal of the Svalbard Marginal Ice Zone 2025\nCampaign (SvalMIZ-25) was to observe and better understand the complex\ninterplay between atmosphere, waves, and sea-ice in the winter Marginal Ice\nZone (MIZ) in order to advance predictive skill of coupled Arctic forecasting\nsystems. The main objective has been to set up a network of observations with a\nspatial distribution that allows for a representative comparison between in\nsitu observations and gridded model data. The observed variables include air\nand surface temperature, sea-ice drift, and wave energy spectra. With the\nsupport of the Norwegian Coast Guard, we participated in the research cruise\nwith KV Svalbard from 22.April - 11.May 2025. In total 21 buoys were deployed\nin the Marginal Ice Zone north of the Svalbard Archipelago."
                },
                "authors": [
                    {
                        "name": "M. Mller"
                    },
                    {
                        "name": "J. Rabault"
                    },
                    {
                        "name": "C. Palerme"
                    },
                    {
                        "name": "J. Tjernstrm"
                    }
                ],
                "author_detail": {
                    "name": "J. Tjernstrm"
                },
                "author": "J. Tjernstrm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10016v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10016v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09754v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09754v1",
                "updated": "2025-09-11T16:48:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T16:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    16,
                    48,
                    24,
                    3,
                    254,
                    0
                ],
                "title": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LAVa: Layer-wise KV Cache Eviction with Dynamic Budget Allocation"
                },
                "summary": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Cache is commonly used to accelerate LLM inference with long contexts, yet\nits high memory demand drives the need for cache compression. Existing\ncompression methods, however, are largely heuristic and lack dynamic budget\nallocation. To address this limitation, we introduce a unified framework for\ncache compression by minimizing information loss in Transformer residual\nstreams. Building on it, we analyze the layer attention output loss and derive\na new metric to compare cache entries across heads, enabling layer-wise\ncompression with dynamic head budgets. Additionally, by contrasting cross-layer\ninformation, we also achieve dynamic layer budgets. LAVa is the first unified\nstrategy for cache eviction and dynamic budget allocation that, unlike prior\nmethods, does not rely on training or the combination of multiple strategies.\nExperiments with benchmarks (LongBench, Needle-In-A-Haystack, Ruler, and\nInfiniteBench) demonstrate its superiority. Moreover, our experiments reveal a\nnew insight: dynamic layer budgets are crucial for generation tasks (e.g., code\ncompletion), while dynamic head budgets play a key role in extraction tasks\n(e.g., extractive QA). As a fully dynamic compression method, LAVa consistently\nmaintains top performance across task types. Our code is available at\nhttps://github.com/MGDDestiny/Lava."
                },
                "authors": [
                    {
                        "name": "Yiqun Shen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Zhengze Zhang"
                    },
                    {
                        "name": "Xiaoliang Wang"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Nguyen Cam-Tu"
                    }
                ],
                "author_detail": {
                    "name": "Nguyen Cam-Tu"
                },
                "author": "Nguyen Cam-Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09754v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09754v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09525v1",
                "updated": "2025-09-11T15:06:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T15:06:03Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    15,
                    6,
                    3,
                    3,
                    254,
                    0
                ],
                "title": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrEnv: Transparently Share Serverless Execution Environments Across\n  Different Functions and Nodes"
                },
                "summary": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides dynamic scalability, but its infrastructure\noverhead becomes a bottleneck for emerging workloads such as LLM agents, which\nexhibit unpredictable invocation patterns and variable resource demands. Our\nanalysis shows that for these agents, the cost of running on serverless\nplatforms can reach up to 70% of the cost of LLM API calls. This finding\nmotivates the need for a more efficient, high-density serverless platform. We\npresent TrEnv, a co-designed serverless platform that supports both container-\nand VM-based environments, optimized for the unique demands of LLM agents.\nTrEnv reduces startup latency and memory usage through repurposable sandboxes\nand memory templates, which enable fast reuse and restoration of execution\nenvironments. To further reduce overhead in VM-based agent workloads, TrEnv\nleverages browser sharing and a page cache bypassing mechanism. Evaluations\nshow that TrEnv reduces P99 latency by up to 7X and memory usage by 48% in\ncontainer-based settings, and achieves up to 58% lower P99 latency and 61%\nmemory savings for VM-based agents compared to state-of-the-art systems like\nE2B."
                },
                "authors": [
                    {
                        "name": "Jialiang Huang"
                    },
                    {
                        "name": "Teng Ma"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Sixing Lin"
                    },
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Jinlei Jiang"
                    },
                    {
                        "name": "Xia Liao"
                    },
                    {
                        "name": "Yingdi Shan"
                    },
                    {
                        "name": "Yongwei Wu"
                    },
                    {
                        "name": "Ning Zhang"
                    },
                    {
                        "name": "Mengting Lu"
                    },
                    {
                        "name": "Tao Ma"
                    },
                    {
                        "name": "Haifeng Gong"
                    },
                    {
                        "name": "Mingxing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Mingxing Zhang"
                },
                "author": "Mingxing Zhang",
                "arxiv_comment": "38 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09494v1",
                "updated": "2025-09-11T14:34:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T14:34:01Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    14,
                    34,
                    1,
                    3,
                    254,
                    0
                ],
                "title": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Loop Filtering Using Learned Look-Up Tables for Video Coding"
                },
                "summary": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-loop filtering (ILF) is a key technology in video coding standards to\nreduce artifacts and enhance visual quality. Recently, neural network-based ILF\nschemes have achieved remarkable coding gains, emerging as a powerful candidate\nfor next-generation video coding standards. However, the use of deep neural\nnetworks (DNN) brings significant computational and time complexity or high\ndemands for dedicated hardware, making it challenging for general use. To\naddress this limitation, we study a practical ILF solution by adopting look-up\ntables (LUTs). After training a DNN with a restricted reference range for ILF,\nall possible inputs are traversed, and the output values of the DNN are cached\ninto LUTs. During the coding process, the filtering process is performed by\nsimply retrieving the filtered pixel through locating the input pixels and\ninterpolating between the cached values, instead of relying on heavy inference\ncomputations. In this paper, we propose a universal LUT-based ILF framework,\ntermed LUT-ILF++. First, we introduce the cooperation of multiple kinds of\nfiltering LUTs and propose a series of customized indexing mechanisms to enable\nbetter filtering reference perception with limited storage consumption. Second,\nwe propose the cross-component indexing mechanism to enable the filtering of\ndifferent color components jointly. Third, in order to make our solution\npractical for coding uses, we propose the LUT compaction scheme to enable the\nLUT pruning, achieving a lower storage cost of the entire solution. The\nproposed framework is implemented in the VVC reference software. Experimental\nresults show that the proposed framework achieves on average 0.82%/2.97%/1.63%\nand 0.85%/4.11%/2.06% bitrate reduction for common test sequences, under the AI\nand RA configurations, respectively. Compared to DNN-based solutions, our\nproposed solution has much lower time complexity and storage cost."
                },
                "authors": [
                    {
                        "name": "Zhuoyuan Li"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Yao Li"
                    },
                    {
                        "name": "Jialin Li"
                    },
                    {
                        "name": "Li Li"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Feng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Wu"
                },
                "author": "Feng Wu",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.05211v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.05211v2",
                "updated": "2025-09-11T12:06:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    12,
                    6,
                    49,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-07T09:47:21Z",
                "published_parsed": [
                    2025,
                    8,
                    7,
                    9,
                    47,
                    21,
                    3,
                    219,
                    0
                ],
                "title": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VFlowOpt: A Token Pruning Framework for LMMs with Visual Information\n  Flow-Guided Optimization"
                },
                "summary": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Multimodal Models (LMMs) excel in visual-language tasks by leveraging\nnumerous visual tokens for fine-grained visual information, but this token\nredundancy results in significant computational costs. Previous research aimed\nat reducing visual tokens during inference typically leverages importance maps\nderived from attention scores among vision-only tokens or vision-language\ntokens to prune tokens across one or multiple pruning stages. Despite this\nprogress, pruning frameworks and strategies remain simplistic and\ninsufficiently explored, often resulting in substantial performance\ndegradation. In this paper, we propose VFlowOpt, a token pruning framework that\nintroduces an importance map derivation process and a progressive pruning\nmodule with a recycling mechanism. The hyperparameters of its pruning strategy\nare further optimized by a visual information flow-guided method. Specifically,\nwe compute an importance map for image tokens based on their attention-derived\ncontext relevance and patch-level information entropy. We then decide which\ntokens to retain or prune and aggregate the pruned ones as recycled tokens to\navoid potential information loss. Finally, we apply a visual information\nflow-guided method that regards the last token in the LMM as the most\nrepresentative signal of text-visual interactions. This method minimizes the\ndiscrepancy between token representations in LMMs with and without pruning,\nthereby enabling superior pruning strategies tailored to different LMMs.\nExperiments demonstrate that VFlowOpt can prune 90% of visual tokens while\nmaintaining comparable performance, leading to an 89% reduction in KV-Cache\nmemory and 3.8 times faster inference."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Chenhang Cui"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.05211v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.05211v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19880v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19880v2",
                "updated": "2025-09-11T10:20:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    10,
                    20,
                    20,
                    3,
                    254,
                    0
                ],
                "published": "2025-05-26T12:06:12Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    12,
                    6,
                    12,
                    0,
                    146,
                    0
                ],
                "title": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Universal Workers: A Vision for Eliminating Cold Starts in Serverless\n  Computing"
                },
                "summary": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing enables developers to deploy code without managing\ninfrastructure, but suffers from cold start overhead when initializing new\nfunction instances. Existing solutions such as \"keep-alive\" or \"pre-warming\"\nare costly and unreliable under bursty workloads. We propose universal workers,\nwhich are computational units capable of executing any function with minimal\ninitialization overhead. Based on an analysis of production workload traces,\nour key insight is that requests in Function-as-a-Service (FaaS) platforms show\na highly skewed distribution, with most requests invoking a small subset of\nfunctions. We exploit this observation to approximate universal workers through\nlocality groups and three-tier caching (handler, install, import). With this\nwork, we aim to enable more efficient and scalable FaaS platforms capable of\nhandling diverse workloads with minimal initialization overhead."
                },
                "authors": [
                    {
                        "name": "Saman Akbari"
                    },
                    {
                        "name": "Manfred Hauswirth"
                    }
                ],
                "author_detail": {
                    "name": "Manfred Hauswirth"
                },
                "author": "Manfred Hauswirth",
                "arxiv_doi": "10.1109/CLOUD67622.2025.00051",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/CLOUD67622.2025.00051",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.19880v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19880v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 2025 IEEE 18th International Conference on Cloud\n  Computing (CLOUD)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v3",
                "updated": "2025-09-11T06:45:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    45,
                    58,
                    3,
                    254,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01085v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01085v3",
                "updated": "2025-09-11T06:16:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    6,
                    16,
                    31,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-01T03:16:52Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    16,
                    52,
                    0,
                    244,
                    0
                ],
                "title": "Bidirectional Sparse Attention for Faster Video Diffusion Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bidirectional Sparse Attention for Faster Video Diffusion Training"
                },
                "summary": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion Transformer (DiT) models excel in generative quality but hit\nmajor computational bottlenecks when producing high-resolution, long-duration\nvideos. The quadratic complexity of full attention leads to prohibitively high\ntraining and inference costs. Full attention inefficiency stems from two key\nchallenges: excessive computation due to the inherent sparsity of Queries and\nKey-Value pairs, and redundant computation as fixed sparse patterns fail to\nleverage DiT's dynamic attention. To overcome this limitation, we propose a\nBidirectional Sparse Attention (BSA) framework for faster video DiT training,\nthe first to dynamically sparsify both Queries and Key-Value pairs within 3D\nfull attention, thereby substantially improving training and inference\nefficiency. BSA addresses these issues through two key components. Query\nsparsity is optimized by selecting the most informative query tokens via\nsemantic similarity and with a dynamic spatial-time training strategy, while KV\nsparsity is achieved by computing a statistical dynamic threshold to retain\nonly the most salient KV blocks for computation. Extensive experiments\ndemonstrate that BSA significantly accelerates DiT training across long\nsequences, reducing FLOPs by up to 20x and achieving 17.79x faster attention\ntraining, while preserving or even surpassing the generative quality of full\nattention."
                },
                "authors": [
                    {
                        "name": "Chenlu Zhan"
                    },
                    {
                        "name": "Wen Li"
                    },
                    {
                        "name": "Chuyu Shen"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Suhui Wu"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01085v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01085v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09094v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09094v1",
                "updated": "2025-09-11T02:00:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "published": "2025-09-11T02:00:27Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    2,
                    0,
                    27,
                    3,
                    254,
                    0
                ],
                "title": "Coherence-Aware Task Graph Modeling for Realistic Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherence-Aware Task Graph Modeling for Realistic Application"
                },
                "summary": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore systems continue to scale, cache coherence has emerged as a\ncritical determinant of system performance, with coherence behavior and task\nexecution closely intertwined, reshaping inter-task dependencies. Task graph\nmodeling provides a structured way to capture such dependencies and serves as\nthe foundation for many system-level design strategies. However, these\nstrategies typically rely on predefined task graphs, while many real-world\napplications lack explicit graphs and exhibit dynamic, data-dependent behavior,\nlimiting the effectiveness of static approaches. To address this, several task\ngraph modeling methods for realistic workloads have been developed. Yet, they\neither rely on implicit techniques that use application-specific features\nwithout producing explicit graphs, or they generate graphs tailored to fixed\nscheduling models, which limits generality. More importantly, they often\noverlook coherence interactions, creating a gap between design assumptions and\nactual runtime behavior. To overcome these limitations, we propose CoTAM, a\nCoherence-Aware Task Graph Modeling framework for realistic workloads that\nconstructs a unified task graph reflecting runtime behavior. CoTAM analyzes the\nimpact of coherence by decoupling its effects from overall execution,\nquantifies its influence through a learned weighting scheme, and infers\ninter-task dependencies for coherence-aware graph generation. Extensive\nexperiments show that CoTAM outperforms implicit methods, bridging the gap\nbetween dynamic workload behavior and existing designs while demonstrating the\nimportance of incorporating cache coherence into task graph modeling for\naccurate and generalizable system-level analysis."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_doi": "10.1145/3742875.3754678",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3742875.3754678",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.09094v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09094v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by MEMOCODE'25, 10 pages",
                "arxiv_journal_ref": "International Symposium on Formal Methods and Models for System\n  Design (MEMOCODE '25), September 28-October 3, 2025, Taipei, Taiwan",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v2",
                "updated": "2025-09-10T17:59:08Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    17,
                    59,
                    8,
                    2,
                    253,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08696v1",
                "updated": "2025-09-10T15:41:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T15:41:15Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    15,
                    41,
                    15,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer-Based Text-to-Speech with Transformer\n  Layer Caching"
                },
                "summary": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a method to accelerate the inference process of diffusion\ntransformer (DiT)-based text-to-speech (TTS) models by applying a selective\ncaching mechanism to transformer layers. Specifically, I integrate SmoothCache\ninto the F5-TTS architecture, focusing on caching outputs of self-attention and\nfeed-forward network layers to reduce redundant computations during the\ndenoising process. A calibration phase is introduced to analyze L1 relative\nerrors between timesteps, guiding the selection of cache schedules that\nminimize quality degradation. To address the problem of inter-layer dependency,\na unified caching schedule is adopted, applying the cache pattern derived from\nself-attention layers to both layer types. Experiments on LibriSpeech-PC and\nSeed-TTS datasets evaluate various cache thresholds and denoising step\nconfigurations. Results show that caching at higher denoising steps reduces\ninference time without compromising output quality, whereas caching at lower\nsteps can negatively impact synthesis quality similarly to reducing the total\nnumber of denoising steps. Objective and subjective metrics confirm the\neffectiveness of SmoothCache in maintaining performance while improving\ncomputational efficiency. Comparisons between cached inference and reduced-step\ninference further highlight the benefits of selective caching, especially under\nhigh-step configurations. This work demonstrates that transformer layer caching\nis a practical solution for optimizing diffusion transformer-based TTS models\nwithout requiring architectural changes or retraining. Example inference\nresults can be heard at https://siratish.github.io/F5-TTS_SmoothCache/ ."
                },
                "authors": [
                    {
                        "name": "Siratish Sakpiboonchit"
                    }
                ],
                "author_detail": {
                    "name": "Siratish Sakpiboonchit"
                },
                "author": "Siratish Sakpiboonchit",
                "arxiv_comment": "9 pages, 2 tables, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08542v1",
                "updated": "2025-09-10T12:46:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T12:46:29Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    12,
                    46,
                    29,
                    2,
                    253,
                    0
                ],
                "title": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter\n  1.58-bit LLM Inference"
                },
                "summary": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energy\nefficiency for CNNs by eliminating runtime weight updates. However, their\nscalability to Large Language Models (LLMs) is fundamentally constrained by\ntheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMA\nseries - demands more than 1,000 cm2 of silicon area even in advanced CMOS\nnodes. This paper presents BitROM, the first CiROM-based accelerator that\novercomes this limitation through co-design with BitNet's 1.58-bit quantization\nmodel, enabling practical and efficient LLM inference at the edge. BitROM\nintroduces three key innovations: 1) a novel Bidirectional ROM Array that\nstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulator\noptimized for ternary-weight computations; and 3) an integrated Decode-Refresh\n(DR) eDRAM that supports on-die KV-cache management, significantly reducing\nexternal memory access during decoding. In addition, BitROM integrates\nLoRA-based adapters to enable efficient transfer learning across various\ndownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bit\ndensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency over\nprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%\nreduction in external DRAM access, further enhancing deployment efficiency for\nLLMs in edge applications."
                },
                "authors": [
                    {
                        "name": "Wenlun Zhang"
                    },
                    {
                        "name": "Xinyu Li"
                    },
                    {
                        "name": "Shimpei Ando"
                    },
                    {
                        "name": "Kentaro Yoshioka"
                    }
                ],
                "author_detail": {
                    "name": "Kentaro Yoshioka"
                },
                "author": "Kentaro Yoshioka",
                "arxiv_comment": "Accepted to ASP-DAC 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.08342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.08342v1",
                "updated": "2025-09-10T07:28:24Z",
                "updated_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "published": "2025-09-10T07:28:24Z",
                "published_parsed": [
                    2025,
                    9,
                    10,
                    7,
                    28,
                    24,
                    2,
                    253,
                    0
                ],
                "title": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Mixture-of-Expert Inference with Adaptive Expert Split\n  Mechanism"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a promising architecture for modern\nlarge language models (LLMs). However, massive parameters impose heavy GPU\nmemory (i.e., VRAM) demands, hindering the widespread adoption of MoE LLMs.\nOffloading the expert parameters to CPU RAM offers an effective way to\nalleviate the VRAM requirements for MoE inference. Existing approaches\ntypically cache a small subset of experts in VRAM and dynamically prefetch\nexperts from RAM during inference, leading to significant degradation in\ninference speed due to the poor cache hit rate and substantial expert loading\nlatency. In this work, we propose MoEpic, an efficient MoE inference system\nwith a novel expert split mechanism. Specifically, each expert is vertically\ndivided into two segments: top and bottom. MoEpic caches the top segment of hot\nexperts, so that more experts will be stored under the limited VRAM budget,\nthereby improving the cache hit rate. During each layer's inference, MoEpic\npredicts and prefetches the activated experts for the next layer. Since the top\nsegments of cached experts are exempt from fetching, the loading time is\nreduced, which allows efficient transfer-computation overlap. Nevertheless, the\nperformance of MoEpic critically depends on the cache configuration (i.e., each\nlayer's VRAM budget and expert split ratio). To this end, we propose a\ndivide-and-conquer algorithm based on fixed-point iteration for adaptive cache\nconfiguration. Extensive experiments on popular MoE LLMs demonstrate that\nMoEpic can save about half of the GPU cost, while lowering the inference\nlatency by about 37.51%-65.73% compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Jiaming Yan"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.08342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.08342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.21319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21319v1",
                "updated": "2025-09-25T16:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    16,
                    19,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T16:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    16,
                    19,
                    6,
                    3,
                    268,
                    0
                ],
                "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards"
                },
                "summary": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost)."
                },
                "authors": [
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Ellie Evans"
                    },
                    {
                        "name": "Daniel Egert"
                    },
                    {
                        "name": "Hoo-Chang Shin"
                    },
                    {
                        "name": "Felipe Soares"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    }
                ],
                "author_detail": {
                    "name": "Oleksii Kuchaiev"
                },
                "author": "Oleksii Kuchaiev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07548v2",
                "updated": "2025-09-25T15:27:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    27,
                    51,
                    3,
                    268,
                    0
                ],
                "published": "2024-10-10T02:41:23Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    2,
                    41,
                    23,
                    3,
                    284,
                    0
                ],
                "title": "Hybrid Summary Statistics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Summary Statistics"
                },
                "summary": "We present a way to capture high-information posteriors from training sets\nthat are sparsely sampled over the parameter space for robust simulation-based\ninference. In physical inference problems, we can often apply domain knowledge\nto define traditional summary statistics to capture some of the information in\na dataset. We show that augmenting these statistics with neural network outputs\nto maximise the mutual information improves information extraction compared to\nneural summaries alone or their concatenation to existing summaries and makes\ninference robust in settings with low training data. We introduce 1) two loss\nformalisms to achieve this and 2) apply the technique to two different\ncosmological datasets to extract non-Gaussian parameter information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a way to capture high-information posteriors from training sets\nthat are sparsely sampled over the parameter space for robust simulation-based\ninference. In physical inference problems, we can often apply domain knowledge\nto define traditional summary statistics to capture some of the information in\na dataset. We show that augmenting these statistics with neural network outputs\nto maximise the mutual information improves information extraction compared to\nneural summaries alone or their concatenation to existing summaries and makes\ninference robust in settings with low training data. We introduce 1) two loss\nformalisms to achieve this and 2) apply the technique to two different\ncosmological datasets to extract non-Gaussian parameter information."
                },
                "authors": [
                    {
                        "name": "T. Lucas Makinen"
                    },
                    {
                        "name": "Ce Sui"
                    },
                    {
                        "name": "Benjamin D. Wandelt"
                    },
                    {
                        "name": "Natalia Porqueres"
                    },
                    {
                        "name": "Alan Heavens"
                    }
                ],
                "author_detail": {
                    "name": "Alan Heavens"
                },
                "author": "Alan Heavens",
                "arxiv_comment": "7 pages, 4 figures. Accepted to ML4PS2024 at NeurIPS 2024. Code\n  available at https://github.com/tlmakinen/hybridStats",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21310v1",
                "updated": "2025-09-25T15:27:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    27,
                    15,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:27:15Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    27,
                    15,
                    3,
                    268,
                    0
                ],
                "title": "SAGE: A Realistic Benchmark for Semantic Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGE: A Realistic Benchmark for Semantic Understanding"
                },
                "summary": "As large language models (LLMs) achieve strong performance on traditional\nbenchmarks, there is an urgent need for more challenging evaluation frameworks\nthat probe deeper aspects of semantic understanding. We introduce SAGE\n(Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed\nto assess both embedding models and similarity metrics across five categories:\nHuman Preference Alignment, Transformation Robustness, Information Sensitivity,\nClustering Performance, and Retrieval Robustness. Unlike existing benchmarks\nthat focus on isolated capabilities, SAGE evaluates semantic understanding\nthrough adversarial conditions, noisy transformations, and nuanced human\njudgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding\nmodels and classical metrics reveals significant performance gaps, with no\nsingle approach excelling across all dimensions. For instance, while\nstate-of-the-art embedding models like OpenAI's text-embedding-3-large dominate\nin aligning with human preferences (0.682 vs. 0.591 for the best classical\nmetric), they are significantly outperformed by classical metrics on\ninformation sensitivity tasks, where Jaccard Similarity achieves a score of\n0.905 compared to the top embedding score of 0.794. SAGE further uncovers\ncritical trade-offs: OpenAI's text-embedding-3-small achieves the highest\nclustering performance (0.483) but demonstrates extreme brittleness with the\nlowest robustness score (0.011). SAGE exposes critical limitations in current\nsemantic understanding capabilities and provides a more realistic assessment of\nmodel robustness for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) achieve strong performance on traditional\nbenchmarks, there is an urgent need for more challenging evaluation frameworks\nthat probe deeper aspects of semantic understanding. We introduce SAGE\n(Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed\nto assess both embedding models and similarity metrics across five categories:\nHuman Preference Alignment, Transformation Robustness, Information Sensitivity,\nClustering Performance, and Retrieval Robustness. Unlike existing benchmarks\nthat focus on isolated capabilities, SAGE evaluates semantic understanding\nthrough adversarial conditions, noisy transformations, and nuanced human\njudgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding\nmodels and classical metrics reveals significant performance gaps, with no\nsingle approach excelling across all dimensions. For instance, while\nstate-of-the-art embedding models like OpenAI's text-embedding-3-large dominate\nin aligning with human preferences (0.682 vs. 0.591 for the best classical\nmetric), they are significantly outperformed by classical metrics on\ninformation sensitivity tasks, where Jaccard Similarity achieves a score of\n0.905 compared to the top embedding score of 0.794. SAGE further uncovers\ncritical trade-offs: OpenAI's text-embedding-3-small achieves the highest\nclustering performance (0.483) but demonstrates extreme brittleness with the\nlowest robustness score (0.011). SAGE exposes critical limitations in current\nsemantic understanding capabilities and provides a more realistic assessment of\nmodel robustness for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Samarth Goel"
                    },
                    {
                        "name": "Reagan J. Lee"
                    },
                    {
                        "name": "Kannan Ramchandran"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Ramchandran"
                },
                "author": "Kannan Ramchandran",
                "arxiv_comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent\n  Abilities, and Scaling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21305v1",
                "updated": "2025-09-25T15:19:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    19,
                    39,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:19:39Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    19,
                    39,
                    3,
                    268,
                    0
                ],
                "title": "Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors\n  in LLMs"
                },
                "summary": "Large language models (LLMs) often exhibit sycophantic behaviors -- such as\nexcessive agreement with or flattery of the user -- but it is unclear whether\nthese behaviors arise from a single mechanism or multiple distinct processes.\nWe decompose sycophancy into sycophantic agreement and sycophantic praise,\ncontrasting both with genuine agreement. Using difference-in-means directions,\nactivation additions, and subspace geometry across multiple models and\ndatasets, we show that: (1) the three behaviors are encoded along distinct\nlinear directions in latent space; (2) each behavior can be independently\namplified or suppressed without affecting the others; and (3) their\nrepresentational structure is consistent across model families and scales.\nThese results suggest that sycophantic behaviors correspond to distinct,\nindependently steerable representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit sycophantic behaviors -- such as\nexcessive agreement with or flattery of the user -- but it is unclear whether\nthese behaviors arise from a single mechanism or multiple distinct processes.\nWe decompose sycophancy into sycophantic agreement and sycophantic praise,\ncontrasting both with genuine agreement. Using difference-in-means directions,\nactivation additions, and subspace geometry across multiple models and\ndatasets, we show that: (1) the three behaviors are encoded along distinct\nlinear directions in latent space; (2) each behavior can be independently\namplified or suppressed without affecting the others; and (3) their\nrepresentational structure is consistent across model families and scales.\nThese results suggest that sycophantic behaviors correspond to distinct,\nindependently steerable representations."
                },
                "authors": [
                    {
                        "name": "Daniel Vennemeyer"
                    },
                    {
                        "name": "Phan Anh Duong"
                    },
                    {
                        "name": "Tiffany Zhan"
                    },
                    {
                        "name": "Tianyu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Jiang"
                },
                "author": "Tianyu Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19580v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19580v2",
                "updated": "2025-09-25T15:18:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    18,
                    21,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-23T21:09:24Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    9,
                    24,
                    1,
                    266,
                    0
                ],
                "title": "LLMs4All: A Review on Large Language Models for Research and\n  Applications in Academic Disciplines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs4All: A Review on Large Language Models for Research and\n  Applications in Academic Disciplines"
                },
                "summary": "Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view\nof the world. For example, Large Language Models (LLMs) based applications such\nas ChatGPT have shown the capability of generating human-like conversation on\nextensive topics. Due to the impressive performance on a variety of\nlanguage-related tasks (e.g., open-domain question answering, translation, and\ndocument summarization), one can envision the far-reaching impacts that can be\nbrought by the LLMs with broader real-world applications (e.g., customer\nservice, education and accessibility, and scientific discovery). Inspired by\ntheir success, this paper will offer an overview of state-of-the-art LLMs and\ntheir integration into a wide range of academic disciplines, including: (1)\narts, letters, and law (e.g., history, philosophy, political science, arts and\narchitecture, law), (2) economics and business (e.g., finance, economics,\naccounting, marketing), and (3) science and engineering (e.g., mathematics,\nphysics and mechanical engineering, chemistry and chemical engineering, life\nsciences and bioengineering, earth sciences and civil engineering, computer\nscience and electrical engineering). Integrating humanity and technology, in\nthis paper, we will explore how LLMs are shaping research and practice in these\nfields, while also discussing key limitations, open challenges, and future\ndirections in the era of generative AI. The review of how LLMs are engaged\nacross disciplines-along with key observations and insights-can help\nresearchers and practitioners interested in exploiting LLMs to advance their\nworks in diverse real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view\nof the world. For example, Large Language Models (LLMs) based applications such\nas ChatGPT have shown the capability of generating human-like conversation on\nextensive topics. Due to the impressive performance on a variety of\nlanguage-related tasks (e.g., open-domain question answering, translation, and\ndocument summarization), one can envision the far-reaching impacts that can be\nbrought by the LLMs with broader real-world applications (e.g., customer\nservice, education and accessibility, and scientific discovery). Inspired by\ntheir success, this paper will offer an overview of state-of-the-art LLMs and\ntheir integration into a wide range of academic disciplines, including: (1)\narts, letters, and law (e.g., history, philosophy, political science, arts and\narchitecture, law), (2) economics and business (e.g., finance, economics,\naccounting, marketing), and (3) science and engineering (e.g., mathematics,\nphysics and mechanical engineering, chemistry and chemical engineering, life\nsciences and bioengineering, earth sciences and civil engineering, computer\nscience and electrical engineering). Integrating humanity and technology, in\nthis paper, we will explore how LLMs are shaping research and practice in these\nfields, while also discussing key limitations, open challenges, and future\ndirections in the era of generative AI. The review of how LLMs are engaged\nacross disciplines-along with key observations and insights-can help\nresearchers and practitioners interested in exploiting LLMs to advance their\nworks in diverse real-world applications."
                },
                "authors": [
                    {
                        "name": "Yanfang Fanny Ye"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Tianyi Ma"
                    },
                    {
                        "name": "Zehong Wang"
                    },
                    {
                        "name": "Yiyang Li"
                    },
                    {
                        "name": "Shifu Hou"
                    },
                    {
                        "name": "Weixiang Sun"
                    },
                    {
                        "name": "Kaiwen Shi"
                    },
                    {
                        "name": "Yijun Ma"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Ahmed Abbasi"
                    },
                    {
                        "name": "Ying Cheng"
                    },
                    {
                        "name": "Jane Cleland-Huang"
                    },
                    {
                        "name": "Steven Corcelli"
                    },
                    {
                        "name": "Patricia Culligan"
                    },
                    {
                        "name": "Robert Goulding"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Ting Hua"
                    },
                    {
                        "name": "John Lalor"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Tengfei Luo"
                    },
                    {
                        "name": "Ed Maginn"
                    },
                    {
                        "name": "Nuno Moniz"
                    },
                    {
                        "name": "Jason Rohr"
                    },
                    {
                        "name": "Brett Savoie"
                    },
                    {
                        "name": "Daniel Slate"
                    },
                    {
                        "name": "Tom Stapleford"
                    },
                    {
                        "name": "Matthew Webber"
                    },
                    {
                        "name": "Olaf Wiest"
                    },
                    {
                        "name": "Johnny Zhang"
                    },
                    {
                        "name": "Nitesh Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh Chawla"
                },
                "author": "Nitesh Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19580v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21302v1",
                "updated": "2025-09-25T15:17:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    17,
                    11,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:17:11Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    17,
                    11,
                    3,
                    268,
                    0
                ],
                "title": "Quantized Visual Geometry Grounded Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized Visual Geometry Grounded Transformer"
                },
                "summary": "Learning-based 3D reconstruction models, represented by Visual Geometry\nGrounded Transformers (VGGTs), have made remarkable progress with the use of\nlarge-scale transformers. Their prohibitive computational and memory costs\nseverely hinder real-world deployment. Post-Training Quantization (PTQ) has\nbecome a common practice for compressing and accelerating models. However, we\nempirically observe that PTQ faces unique obstacles when compressing\nbillion-scale VGGTs: the data-independent special tokens induce heavy-tailed\nactivation distributions, while the multi-view nature of 3D data makes\ncalibration sample selection highly unstable. This paper proposes the first\nQuantization framework for VGGTs, namely QuantVGGT. This mainly relies on two\ntechnical contributions: First, we introduce Dual-Smoothed Fine-Grained\nQuantization, which integrates pre-global Hadamard rotation and post-local\nchannel smoothing to mitigate heavy-tailed distributions and inter-channel\nvariance robustly. Second, we design Noise-Filtered Diverse Sampling, which\nfilters outliers via deep-layer statistics and constructs frame-aware diverse\ncalibration clusters to ensure stable quantization ranges. Comprehensive\nexperiments demonstrate that QuantVGGT achieves the state-of-the-art results\nacross different benchmarks and bit-width, surpassing the previous\nstate-of-the-art generic quantization method with a great margin. We highlight\nthat our 4-bit QuantVGGT can deliver a 3.7$\\times$ memory reduction and\n2.5$\\times$ acceleration in real-hardware inference, while maintaining\nreconstruction accuracy above 98\\% of its full-precision counterpart. This\ndemonstrates the vast advantages and practicality of QuantVGGT in\nresource-constrained scenarios. Our code is released in\nhttps://github.com/wlfeng0509/QuantVGGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-based 3D reconstruction models, represented by Visual Geometry\nGrounded Transformers (VGGTs), have made remarkable progress with the use of\nlarge-scale transformers. Their prohibitive computational and memory costs\nseverely hinder real-world deployment. Post-Training Quantization (PTQ) has\nbecome a common practice for compressing and accelerating models. However, we\nempirically observe that PTQ faces unique obstacles when compressing\nbillion-scale VGGTs: the data-independent special tokens induce heavy-tailed\nactivation distributions, while the multi-view nature of 3D data makes\ncalibration sample selection highly unstable. This paper proposes the first\nQuantization framework for VGGTs, namely QuantVGGT. This mainly relies on two\ntechnical contributions: First, we introduce Dual-Smoothed Fine-Grained\nQuantization, which integrates pre-global Hadamard rotation and post-local\nchannel smoothing to mitigate heavy-tailed distributions and inter-channel\nvariance robustly. Second, we design Noise-Filtered Diverse Sampling, which\nfilters outliers via deep-layer statistics and constructs frame-aware diverse\ncalibration clusters to ensure stable quantization ranges. Comprehensive\nexperiments demonstrate that QuantVGGT achieves the state-of-the-art results\nacross different benchmarks and bit-width, surpassing the previous\nstate-of-the-art generic quantization method with a great margin. We highlight\nthat our 4-bit QuantVGGT can deliver a 3.7$\\times$ memory reduction and\n2.5$\\times$ acceleration in real-hardware inference, while maintaining\nreconstruction accuracy above 98\\% of its full-precision counterpart. This\ndemonstrates the vast advantages and practicality of QuantVGGT in\nresource-constrained scenarios. Our code is released in\nhttps://github.com/wlfeng0509/QuantVGGT."
                },
                "authors": [
                    {
                        "name": "Weilun Feng"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Mingqiang Wu"
                    },
                    {
                        "name": "Chuanguang Yang"
                    },
                    {
                        "name": "Yuqi Li"
                    },
                    {
                        "name": "Xiangqi Li"
                    },
                    {
                        "name": "Zhulin An"
                    },
                    {
                        "name": "Libo Huang"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Michele Magno"
                    },
                    {
                        "name": "Yongjun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yongjun Xu"
                },
                "author": "Yongjun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21301v1",
                "updated": "2025-09-25T15:17:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    17,
                    5,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:17:05Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    17,
                    5,
                    3,
                    268,
                    0
                ],
                "title": "Nova: Real-Time Agentic Vision-Language Model Serving with Adaptive\n  Cross-Stage Parallelization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nova: Real-Time Agentic Vision-Language Model Serving with Adaptive\n  Cross-Stage Parallelization"
                },
                "summary": "This paper presents Nova, a real-time scheduling framework for serving\nagentic vision-language models (VLMs) on a single GPU with balanced per-request\nlatency and overall request process throughput. Our design begins by enabling\neffective pipelining across vision encode, LLM prefill, and LLM decode stages\nof VLMs, by exploiting their heterogeneous resource demands during execution\nand incorporating elastic GPU spatial partitioning among stages to maximally\nutilize the compute and memory resources. Building on this, we introduce a\nreal-time scheduling algorithm that adaptively calibrates resource allocation\namong stages based on a Pareto-optimal analysis of the latency-throughput\ntrade-off, allowing the system to sustain responsiveness and resource\nefficiency under dynamic request loads. To further alleviate GPU memory\npressure, we design a lightweight weight offloading strategy for vision\nencoders that preserves inference efficiency with minimized memory overhead.\nExtensive evaluations on both synthetic and real-world agent workloads\ndemonstrate that Nova consistently outperforms the state-of-the-art baselines,\nimproving the maximum latency by up to 23.3%, while keeping competitive\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Nova, a real-time scheduling framework for serving\nagentic vision-language models (VLMs) on a single GPU with balanced per-request\nlatency and overall request process throughput. Our design begins by enabling\neffective pipelining across vision encode, LLM prefill, and LLM decode stages\nof VLMs, by exploiting their heterogeneous resource demands during execution\nand incorporating elastic GPU spatial partitioning among stages to maximally\nutilize the compute and memory resources. Building on this, we introduce a\nreal-time scheduling algorithm that adaptively calibrates resource allocation\namong stages based on a Pareto-optimal analysis of the latency-throughput\ntrade-off, allowing the system to sustain responsiveness and resource\nefficiency under dynamic request loads. To further alleviate GPU memory\npressure, we design a lightweight weight offloading strategy for vision\nencoders that preserves inference efficiency with minimized memory overhead.\nExtensive evaluations on both synthetic and real-world agent workloads\ndemonstrate that Nova consistently outperforms the state-of-the-art baselines,\nimproving the maximum latency by up to 23.3%, while keeping competitive\nthroughput."
                },
                "authors": [
                    {
                        "name": "Yuhang Xu"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Bingheng Yan"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11679v2",
                "updated": "2025-09-25T15:14:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    14,
                    10,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-16T20:39:30Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    20,
                    39,
                    30,
                    4,
                    136,
                    0
                ],
                "title": "Ambiguity Resolution in Text-to-Structured Data Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguity Resolution in Text-to-Structured Data Mapping"
                },
                "summary": "Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods to ambiguity handling either\nrely on the ReACT framework to obtain correct mappings through trial and error,\nor on supervised fine-tuning to bias models toward specific tasks. In this\npaper, we adopt a different approach that characterizes representation\ndifferences of ambiguous text in the latent space and leverages these\ndifferences to identify ambiguity before mapping them to structured data. To\ndetect sentence-level ambiguity, we focus on the relationship between ambiguous\nquestions and their interpretations. Unlike distances calculated by dense\nembeddings, we introduce a new distance measure based on a path kernel over\nconcepts. With this measurement, we identify patterns to distinguish ambiguous\nfrom unambiguous questions. Furthermore, we propose a method for improving LLM\nperformance on ambiguous agentic tool calling through missing concept\nprediction. Both achieve state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods to ambiguity handling either\nrely on the ReACT framework to obtain correct mappings through trial and error,\nor on supervised fine-tuning to bias models toward specific tasks. In this\npaper, we adopt a different approach that characterizes representation\ndifferences of ambiguous text in the latent space and leverages these\ndifferences to identify ambiguity before mapping them to structured data. To\ndetect sentence-level ambiguity, we focus on the relationship between ambiguous\nquestions and their interpretations. Unlike distances calculated by dense\nembeddings, we introduce a new distance measure based on a path kernel over\nconcepts. With this measurement, we identify patterns to distinguish ambiguous\nfrom unambiguous questions. Furthermore, we propose a method for improving LLM\nperformance on ambiguous agentic tool calling through missing concept\nprediction. Both achieve state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Zhibo Hu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Yanfeng Shu"
                    },
                    {
                        "name": "Hye-Young Paik"
                    },
                    {
                        "name": "Liming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liming Zhu"
                },
                "author": "Liming Zhu",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21294v1",
                "updated": "2025-09-25T15:13:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    13,
                    0,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:13:00Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    13,
                    0,
                    3,
                    268,
                    0
                ],
                "title": "The role of synthetic data in Multilingual, Multi-cultural AI systems:\n  Lessons from Indic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The role of synthetic data in Multilingual, Multi-cultural AI systems:\n  Lessons from Indic Languages"
                },
                "summary": "Developing AI systems that operate effectively across languages while\nremaining culturally grounded is a long-standing challenge, particularly in\nlow-resource settings. Synthetic data provides a promising avenue, yet its\neffectiveness in multilingual and multicultural contexts remains underexplored.\nWe investigate the creation and impact of synthetic, culturally contextualized\ndatasets for Indian languages through a bottom-up generation strategy that\nprompts large open-source LLMs (>= 235B parameters) to ground data generation\nin language-specific Wikipedia content. This approach complements the dominant\ntop-down paradigm of translating synthetic datasets from high-resource\nlanguages such as English. We introduce Updesh, a high-quality large-scale\nsynthetic instruction-following dataset comprising 9.5M data points across 13\nIndian languages, encompassing diverse reasoning and generative tasks with an\nemphasis on long-context, multi-turn capabilities, and alignment with Indian\ncultural contexts. A comprehensive evaluation incorporating both automated\nmetrics and human annotation across 10k assessments indicates that generated\ndata is high quality; though, human evaluation highlights areas for further\nimprovement. Additionally, we perform downstream evaluations by fine-tuning\nmodels on our dataset and assessing the performance across 15 diverse\nmultilingual datasets. Models trained on Updesh consistently achieve\nsignificant gains on generative tasks and remain competitive on multiple-choice\nstyle NLU tasks. Notably, relative improvements are most pronounced in low and\nmedium-resource languages, narrowing their gap with high-resource languages.\nThese findings provide empirical evidence that effective multilingual AI\nrequires multi-faceted data curation and generation strategies that incorporate\ncontext-aware, culturally grounded methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing AI systems that operate effectively across languages while\nremaining culturally grounded is a long-standing challenge, particularly in\nlow-resource settings. Synthetic data provides a promising avenue, yet its\neffectiveness in multilingual and multicultural contexts remains underexplored.\nWe investigate the creation and impact of synthetic, culturally contextualized\ndatasets for Indian languages through a bottom-up generation strategy that\nprompts large open-source LLMs (>= 235B parameters) to ground data generation\nin language-specific Wikipedia content. This approach complements the dominant\ntop-down paradigm of translating synthetic datasets from high-resource\nlanguages such as English. We introduce Updesh, a high-quality large-scale\nsynthetic instruction-following dataset comprising 9.5M data points across 13\nIndian languages, encompassing diverse reasoning and generative tasks with an\nemphasis on long-context, multi-turn capabilities, and alignment with Indian\ncultural contexts. A comprehensive evaluation incorporating both automated\nmetrics and human annotation across 10k assessments indicates that generated\ndata is high quality; though, human evaluation highlights areas for further\nimprovement. Additionally, we perform downstream evaluations by fine-tuning\nmodels on our dataset and assessing the performance across 15 diverse\nmultilingual datasets. Models trained on Updesh consistently achieve\nsignificant gains on generative tasks and remain competitive on multiple-choice\nstyle NLU tasks. Notably, relative improvements are most pronounced in low and\nmedium-resource languages, narrowing their gap with high-resource languages.\nThese findings provide empirical evidence that effective multilingual AI\nrequires multi-faceted data curation and generation strategies that incorporate\ncontext-aware, culturally grounded methodologies."
                },
                "authors": [
                    {
                        "name": "Pranjal A. Chitale"
                    },
                    {
                        "name": "Varun Gumma"
                    },
                    {
                        "name": "Sanchit Ahuja"
                    },
                    {
                        "name": "Prashant Kodali"
                    },
                    {
                        "name": "Manan Uppadhyay"
                    },
                    {
                        "name": "Deepthi Sudharsan"
                    },
                    {
                        "name": "Sunayana Sitaram"
                    }
                ],
                "author_detail": {
                    "name": "Sunayana Sitaram"
                },
                "author": "Sunayana Sitaram",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09679v2",
                "updated": "2025-09-25T15:12:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    12,
                    18,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-11T17:59:51Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    51,
                    3,
                    254,
                    0
                ],
                "title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms"
                },
                "summary": "Large language models require massive memory footprints, severely limiting\ndeployment on consumer hardware. Quantization reduces memory through lower\nnumerical precision, but extreme 2-bit quantization suffers from catastrophic\nperformance loss due to outliers in activations. Rotation-based methods such as\nQuIP and QuaRot apply orthogonal transforms to eliminate outliers before\nquantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} =\n(\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these\nmethods use fixed transforms--Hadamard matrices achieving optimal worst-case\ncoherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight\ndistributions. We identify that different transformer layers exhibit distinct\noutlier patterns, motivating layer-adaptive rotations rather than\none-size-fits-all approaches. In this work, we propose ButterflyQuant, which\nreplaces Hadamard rotations with learnable butterfly transforms parameterized\nby continuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$\nentries that are non-differentiable and thus prohibit gradient-based learning,\nbutterfly transforms' continuous parameterization enables smooth optimization\nwhile guaranteeing orthogonality by construction. This orthogonal constraint\nensures theoretical guarantees in outlier suppression while achieving $O(n \\log\nn)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable\nparameters. We further introduce a uniformity regularization on\npost-transformation activations to promote smoother distributions amenable to\nquantization. Learning requires only 128 calibration samples and converges in\nminutes on a single GPU--a negligible one-time cost. For LLaMA-2-7B with 2-bit\nquantization, ButterflyQuant achieves 15.4 perplexity versus 37.3 for QuIP.\n\\href{https://github.com/42Shawn/Butterflyquant-llm}{Codes} are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models require massive memory footprints, severely limiting\ndeployment on consumer hardware. Quantization reduces memory through lower\nnumerical precision, but extreme 2-bit quantization suffers from catastrophic\nperformance loss due to outliers in activations. Rotation-based methods such as\nQuIP and QuaRot apply orthogonal transforms to eliminate outliers before\nquantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} =\n(\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these\nmethods use fixed transforms--Hadamard matrices achieving optimal worst-case\ncoherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight\ndistributions. We identify that different transformer layers exhibit distinct\noutlier patterns, motivating layer-adaptive rotations rather than\none-size-fits-all approaches. In this work, we propose ButterflyQuant, which\nreplaces Hadamard rotations with learnable butterfly transforms parameterized\nby continuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$\nentries that are non-differentiable and thus prohibit gradient-based learning,\nbutterfly transforms' continuous parameterization enables smooth optimization\nwhile guaranteeing orthogonality by construction. This orthogonal constraint\nensures theoretical guarantees in outlier suppression while achieving $O(n \\log\nn)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable\nparameters. We further introduce a uniformity regularization on\npost-transformation activations to promote smoother distributions amenable to\nquantization. Learning requires only 128 calibration samples and converges in\nminutes on a single GPU--a negligible one-time cost. For LLaMA-2-7B with 2-bit\nquantization, ButterflyQuant achieves 15.4 perplexity versus 37.3 for QuIP.\n\\href{https://github.com/42Shawn/Butterflyquant-llm}{Codes} are available."
                },
                "authors": [
                    {
                        "name": "Bingxin Xu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Oussama Elachqar"
                    },
                    {
                        "name": "Yuzhang Shang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhang Shang"
                },
                "author": "Yuzhang Shang",
                "arxiv_comment": "Replace discrete Hadamard transforms with continuous Butterfly\n  transforms to facilitate the learning of rotation matrices in LLM\n  quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21282v1",
                "updated": "2025-09-25T15:03:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    3,
                    18,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:03:18Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    3,
                    18,
                    3,
                    268,
                    0
                ],
                "title": "It's Not You, It's Clipping: A Soft Trust-Region via Probability\n  Smoothing for LLM RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It's Not You, It's Clipping: A Soft Trust-Region via Probability\n  Smoothing for LLM RL"
                },
                "summary": "Training large language models (LLMs) with reinforcement learning (RL)\nmethods such as PPO and GRPO commonly relies on ratio clipping to stabilise\nupdates. While effective at preventing instability, clipping discards\ninformation and introduces gradient discontinuities. We propose Probability\nSmoothing Policy Optimisation (PSPO), which smooths the current policy's\nprobabilities toward the old (behaviour) policy before computing the importance\nratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient\nsignal, while interpolation toward the old policy creates a soft trust region\nthat discourages large, destabilising updates, with formal guarantees.\n  We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and\nQwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset\ngeneralisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO\n(single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar\nperformance but improves the reasoning leading to clearer and more concise\nresponses which are more logical. Compared to clipped GRPO, GR-PSPO\nsubstantially improves performance both the 0.5B and 1.5B models, with a boost\nof over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) with reinforcement learning (RL)\nmethods such as PPO and GRPO commonly relies on ratio clipping to stabilise\nupdates. While effective at preventing instability, clipping discards\ninformation and introduces gradient discontinuities. We propose Probability\nSmoothing Policy Optimisation (PSPO), which smooths the current policy's\nprobabilities toward the old (behaviour) policy before computing the importance\nratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient\nsignal, while interpolation toward the old policy creates a soft trust region\nthat discourages large, destabilising updates, with formal guarantees.\n  We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and\nQwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset\ngeneralisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO\n(single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar\nperformance but improves the reasoning leading to clearer and more concise\nresponses which are more logical. Compared to clipped GRPO, GR-PSPO\nsubstantially improves performance both the 0.5B and 1.5B models, with a boost\nof over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B)."
                },
                "authors": [
                    {
                        "name": "Madeleine Dwyer"
                    },
                    {
                        "name": "Adam Sobey"
                    },
                    {
                        "name": "Adriane Chapman"
                    }
                ],
                "author_detail": {
                    "name": "Adriane Chapman"
                },
                "author": "Adriane Chapman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21275v1",
                "updated": "2025-09-25T15:01:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    1,
                    25,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:01:25Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    1,
                    25,
                    3,
                    268,
                    0
                ],
                "title": "Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM\n  Training"
                },
                "summary": "Long context training is crucial for LLM's context extension. Existing\nschemes, such as sequence parallelism, incur substantial communication\noverhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness\nhinges on partitioning granularity. Batch-level PP dividing input samples\nexhibits high memory consumption in long-context scenario, whereas token-level\nPP splitting sequences into slices alleviates memory overhead but may incur\nhardware under-utilization. This trade-off motivates adaptively selecting PP\ngranularity to match resource and workload characteristics. Moreover, sequence\nlength distribution of the real-world dataset exhibits skewness, posing a\nchallenge on PP's workload balance and efficient scheduling. Current static PP\nscheduling methods overlook the variance of sequence length, leading to\nsuboptimal performance. In this paper, we propose Elastic Pipeline Parallelism\n(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource\nand workload heterogeneity. We build InfiniPipe, a distributed training system\nthat unleashes the potential of EPP via (1) a resource-aware and\nworkload-balanced sequence processor that splits long sequences and packs short\nones; and (2) a co-optimization methodology that jointly optimizes pipeline\nschedule and gradient checkpointing via a mechanism named stage-aware\nchunk-level adaptive checkpointing. Comprehensive experiments demonstrate that\nInfiniPipe achieves a 1.69x speedup over state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context training is crucial for LLM's context extension. Existing\nschemes, such as sequence parallelism, incur substantial communication\noverhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness\nhinges on partitioning granularity. Batch-level PP dividing input samples\nexhibits high memory consumption in long-context scenario, whereas token-level\nPP splitting sequences into slices alleviates memory overhead but may incur\nhardware under-utilization. This trade-off motivates adaptively selecting PP\ngranularity to match resource and workload characteristics. Moreover, sequence\nlength distribution of the real-world dataset exhibits skewness, posing a\nchallenge on PP's workload balance and efficient scheduling. Current static PP\nscheduling methods overlook the variance of sequence length, leading to\nsuboptimal performance. In this paper, we propose Elastic Pipeline Parallelism\n(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource\nand workload heterogeneity. We build InfiniPipe, a distributed training system\nthat unleashes the potential of EPP via (1) a resource-aware and\nworkload-balanced sequence processor that splits long sequences and packs short\nones; and (2) a co-optimization methodology that jointly optimizes pipeline\nschedule and gradient checkpointing via a mechanism named stage-aware\nchunk-level adaptive checkpointing. Comprehensive experiments demonstrate that\nInfiniPipe achieves a 1.69x speedup over state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Shiju Wang"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Ao Sun"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Zijian Zhu"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Kaisheng Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kaisheng Ma"
                },
                "author": "Kaisheng Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21271v1",
                "updated": "2025-09-25T15:00:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    0,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:00:16Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    0,
                    16,
                    3,
                    268,
                    0
                ],
                "title": "SuperOffload: Unleashing the Power of Large-Scale LLM Training on\n  Superchips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperOffload: Unleashing the Power of Large-Scale LLM Training on\n  Superchips"
                },
                "summary": "The emergence of Superchips represents a significant advancement in\nnext-generation AI hardware. These Superchips employ a tightly coupled\nheterogeneous architecture that integrates GPU and CPU on the same package,\nwhich offers unprecedented computational power. However, there has been scant\nresearch investigating how LLM training benefits from this new architecture. In\nthis work, for the first time, we study LLM training solutions based on\noffloading for Superchips. We observe important differences between Superchips\nand traditional loosely-coupled GPU-CPU architecture, which necessitate\nrevisiting prevailing assumptions about offloading. Based on that, we present\nSuperOffload, a Superchip-centric offloading system that simultaneously uses\nHopper GPU, Grace CPU, and NVLink-C2C interconnect more efficiently.\nSuperOffload accomplishes this via a combination of techniques, such as\nadaptive weight offloading, bucketization repartitioning, Superchip-aware\ncasting, speculative execution, and a highly optimized Adam optimizer for Grace\nCPUs. Our evaluation of SuperOffload on NVIDIA GH200 demonstrates up to 2.5x\nthroughput improvement compared to state-of-the-art offloading-based systems,\nenabling training of up to 25B model on a single Superchip while achieving high\ntraining throughput. We also extend SuperOffload with ZeRO-style data\nparallelism and DeepSpeed-Ulysses sequence parallelism, enabling training of\n13B model with sequence lengths up to 1 million tokens on 8 GH200 while\nachieving 55% MFU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Superchips represents a significant advancement in\nnext-generation AI hardware. These Superchips employ a tightly coupled\nheterogeneous architecture that integrates GPU and CPU on the same package,\nwhich offers unprecedented computational power. However, there has been scant\nresearch investigating how LLM training benefits from this new architecture. In\nthis work, for the first time, we study LLM training solutions based on\noffloading for Superchips. We observe important differences between Superchips\nand traditional loosely-coupled GPU-CPU architecture, which necessitate\nrevisiting prevailing assumptions about offloading. Based on that, we present\nSuperOffload, a Superchip-centric offloading system that simultaneously uses\nHopper GPU, Grace CPU, and NVLink-C2C interconnect more efficiently.\nSuperOffload accomplishes this via a combination of techniques, such as\nadaptive weight offloading, bucketization repartitioning, Superchip-aware\ncasting, speculative execution, and a highly optimized Adam optimizer for Grace\nCPUs. Our evaluation of SuperOffload on NVIDIA GH200 demonstrates up to 2.5x\nthroughput improvement compared to state-of-the-art offloading-based systems,\nenabling training of up to 25B model on a single Superchip while achieving high\ntraining throughput. We also extend SuperOffload with ZeRO-style data\nparallelism and DeepSpeed-Ulysses sequence parallelism, enabling training of\n13B model with sequence lengths up to 1 million tokens on 8 GH200 while\nachieving 55% MFU."
                },
                "authors": [
                    {
                        "name": "Xinyu Lian"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    },
                    {
                        "name": "Olatunji Ruwase"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "arxiv_comment": "16 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21269v1",
                "updated": "2025-09-25T14:59:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    59,
                    43,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:59:43Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    59,
                    43,
                    3,
                    268,
                    0
                ],
                "title": "LLMTrace: A Corpus for Classification and Fine-Grained Localization of\n  AI-Written Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMTrace: A Corpus for Classification and Fine-Grained Localization of\n  AI-Written Text"
                },
                "summary": "The widespread use of human-like text from Large Language Models (LLMs)\nnecessitates the development of robust detection systems. However, progress is\nlimited by a critical lack of suitable training data; existing datasets are\noften generated with outdated models, are predominantly in English, and fail to\naddress the increasingly common scenario of mixed human-AI authorship.\nCrucially, while some datasets address mixed authorship, none provide the\ncharacter-level annotations required for the precise localization of\nAI-generated segments within a text. To address these gaps, we introduce\nLLMTrace, a new large-scale, bilingual (English and Russian) corpus for\nAI-generated text detection. Constructed using a diverse range of modern\nproprietary and open-source LLMs, our dataset is designed to support two key\ntasks: traditional full-text binary classification (human vs. AI) and the novel\ntask of AI-generated interval detection, facilitated by character-level\nannotations. We believe LLMTrace will serve as a vital resource for training\nand evaluating the next generation of more nuanced and practical AI detection\nmodels. The project page is available at\n\\href{https://sweetdream779.github.io/LLMTrace-info/}{iitolstykh/LLMTrace}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread use of human-like text from Large Language Models (LLMs)\nnecessitates the development of robust detection systems. However, progress is\nlimited by a critical lack of suitable training data; existing datasets are\noften generated with outdated models, are predominantly in English, and fail to\naddress the increasingly common scenario of mixed human-AI authorship.\nCrucially, while some datasets address mixed authorship, none provide the\ncharacter-level annotations required for the precise localization of\nAI-generated segments within a text. To address these gaps, we introduce\nLLMTrace, a new large-scale, bilingual (English and Russian) corpus for\nAI-generated text detection. Constructed using a diverse range of modern\nproprietary and open-source LLMs, our dataset is designed to support two key\ntasks: traditional full-text binary classification (human vs. AI) and the novel\ntask of AI-generated interval detection, facilitated by character-level\nannotations. We believe LLMTrace will serve as a vital resource for training\nand evaluating the next generation of more nuanced and practical AI detection\nmodels. The project page is available at\n\\href{https://sweetdream779.github.io/LLMTrace-info/}{iitolstykh/LLMTrace}."
                },
                "authors": [
                    {
                        "name": "Irina Tolstykh"
                    },
                    {
                        "name": "Aleksandra Tsybina"
                    },
                    {
                        "name": "Sergey Yakubson"
                    },
                    {
                        "name": "Maksim Kuprashevich"
                    }
                ],
                "author_detail": {
                    "name": "Maksim Kuprashevich"
                },
                "author": "Maksim Kuprashevich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16851v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16851v3",
                "updated": "2025-09-25T14:58:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    58,
                    23,
                    3,
                    268,
                    0
                ],
                "published": "2024-03-25T15:15:09Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    15,
                    15,
                    9,
                    0,
                    85,
                    0
                ],
                "title": "Can social media provide early warning of retraction? Evidence from\n  critical tweets identified by human annotation and large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can social media provide early warning of retraction? Evidence from\n  critical tweets identified by human annotation and large language models"
                },
                "summary": "Timely detection of problematic research is essential for safeguarding\nscientific integrity. To explore whether social media commentary can serve as\nan early indicator of potentially problematic articles, this study analysed\n3,815 tweets referencing 604 retracted articles and 3,373 tweets referencing\n668 comparable non-retracted articles. Tweets critical of the articles were\nidentified through both human annotation and large language models (LLMs).\nHuman annotation revealed that 8.3% of retracted articles were associated with\nat least one critical tweet prior to retraction, compared to only 1.5% of\nnon-retracted articles, highlighting the potential of tweets as early warning\nsignals of retraction. However, critical tweets identified by LLMs (GPT-4o\nmini, Gemini 2.0 Flash-Lite, and Claude 3.5 Haiku) only partially aligned with\nhuman annotation, suggesting that fully automated monitoring of\npost-publication discourse should be applied with caution. A human-AI\ncollaborative approach may offer a more reliable and scalable alternative, with\nhuman expertise helping to filter out tweets critical of issues unrelated to\nthe research integrity of the articles. Overall, this study provides insights\ninto how social media signals, combined with generative AI technologies, may\nsupport efforts to strengthen research integrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely detection of problematic research is essential for safeguarding\nscientific integrity. To explore whether social media commentary can serve as\nan early indicator of potentially problematic articles, this study analysed\n3,815 tweets referencing 604 retracted articles and 3,373 tweets referencing\n668 comparable non-retracted articles. Tweets critical of the articles were\nidentified through both human annotation and large language models (LLMs).\nHuman annotation revealed that 8.3% of retracted articles were associated with\nat least one critical tweet prior to retraction, compared to only 1.5% of\nnon-retracted articles, highlighting the potential of tweets as early warning\nsignals of retraction. However, critical tweets identified by LLMs (GPT-4o\nmini, Gemini 2.0 Flash-Lite, and Claude 3.5 Haiku) only partially aligned with\nhuman annotation, suggesting that fully automated monitoring of\npost-publication discourse should be applied with caution. A human-AI\ncollaborative approach may offer a more reliable and scalable alternative, with\nhuman expertise helping to filter out tweets critical of issues unrelated to\nthe research integrity of the articles. Overall, this study provides insights\ninto how social media signals, combined with generative AI technologies, may\nsupport efforts to strengthen research integrity."
                },
                "authors": [
                    {
                        "name": "Er-Te Zheng"
                    },
                    {
                        "name": "Hui-Zhen Fu"
                    },
                    {
                        "name": "Mike Thelwall"
                    },
                    {
                        "name": "Zhichao Fang"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Fang"
                },
                "author": "Zhichao Fang",
                "arxiv_doi": "10.1002/asi.70028",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/asi.70028",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.16851v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16851v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "27 pages, 5 figures",
                "arxiv_journal_ref": "Journal of the Association for Information Science and Technology,\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21267v1",
                "updated": "2025-09-25T14:58:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    58,
                    7,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:58:07Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    58,
                    7,
                    3,
                    268,
                    0
                ],
                "title": "LLM Output Homogenization is Task Dependent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Output Homogenization is Task Dependent"
                },
                "summary": "A large language model can be less helpful if it exhibits output response\nhomogenization. But whether two responses are considered homogeneous, and\nwhether such homogenization is problematic, both depend on the task category.\nFor instance, in objective math tasks, we often expect no variation in the\nfinal answer but anticipate variation in the problem-solving strategy. Whereas,\nfor creative writing tasks, we may expect variation in key narrative components\n(e.g. plot, genre, setting, etc), beyond the vocabulary or embedding diversity\nproduced by temperature-sampling. Previous work addressing output\nhomogenization often fails to conceptualize diversity in a task-dependent way.\nWe address this gap in the literature directly by making the following\ncontributions. (1) We present a task taxonomy comprised of eight task\ncategories that each have distinct conceptualizations of output homogenization.\n(2) We introduce task-anchored functional diversity to better evaluate output\nhomogenization. (3) We propose a task-anchored sampling technique that\nincreases functional diversity for task categories where homogenization is\nundesired, while preserving homogenization where it is desired. (4) We\nchallenge the perceived existence of a diversity-quality trade-off by\nincreasing functional diversity while maintaining response quality. Overall, we\ndemonstrate how task dependence improves the evaluation and mitigation of\noutput homogenization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large language model can be less helpful if it exhibits output response\nhomogenization. But whether two responses are considered homogeneous, and\nwhether such homogenization is problematic, both depend on the task category.\nFor instance, in objective math tasks, we often expect no variation in the\nfinal answer but anticipate variation in the problem-solving strategy. Whereas,\nfor creative writing tasks, we may expect variation in key narrative components\n(e.g. plot, genre, setting, etc), beyond the vocabulary or embedding diversity\nproduced by temperature-sampling. Previous work addressing output\nhomogenization often fails to conceptualize diversity in a task-dependent way.\nWe address this gap in the literature directly by making the following\ncontributions. (1) We present a task taxonomy comprised of eight task\ncategories that each have distinct conceptualizations of output homogenization.\n(2) We introduce task-anchored functional diversity to better evaluate output\nhomogenization. (3) We propose a task-anchored sampling technique that\nincreases functional diversity for task categories where homogenization is\nundesired, while preserving homogenization where it is desired. (4) We\nchallenge the perceived existence of a diversity-quality trade-off by\nincreasing functional diversity while maintaining response quality. Overall, we\ndemonstrate how task dependence improves the evaluation and mitigation of\noutput homogenization."
                },
                "authors": [
                    {
                        "name": "Shomik Jain"
                    },
                    {
                        "name": "Jack Lanchantin"
                    },
                    {
                        "name": "Maximilian Nickel"
                    },
                    {
                        "name": "Karen Ullrich"
                    },
                    {
                        "name": "Ashia Wilson"
                    },
                    {
                        "name": "Jamelle Watson-Daniels"
                    }
                ],
                "author_detail": {
                    "name": "Jamelle Watson-Daniels"
                },
                "author": "Jamelle Watson-Daniels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21266v1",
                "updated": "2025-09-25T14:57:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    57,
                    52,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:57:52Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    57,
                    52,
                    3,
                    268,
                    0
                ],
                "title": "Grounding AI Explanations in Experience: A Reflective Cognitive\n  Architecture for Clinical Decision Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding AI Explanations in Experience: A Reflective Cognitive\n  Architecture for Clinical Decision Support"
                },
                "summary": "Effective disease prediction in modern healthcare demands the twin goals of\nhigh accuracy and transparent, clinically meaningful explanations. Existing\nmachine learning and large language model (LLM) based approaches often struggle\nto balance these goals. Many models yield accurate but unclear statistical\noutputs, while others generate fluent but statistically unsupported narratives,\noften undermining both the validity of the explanation and the predictive\naccuracy itself. This shortcoming comes from a shallow interaction with the\ndata, preventing the development of a deep, detailed understanding similar to a\nhuman expert's. We argue that high accuracy and high-quality explanations are\nnot separate objectives but are mutually reinforcing outcomes of a model that\ndevelops a deep, direct understanding of the data. To achieve this, we propose\nthe Reflective Cognitive Architecture (RCA), a novel framework that coordinates\nmultiple LLMs to learn from direct experience. RCA features an iterative rule\nrefinement mechanism that improves its logic from prediction errors and a\ndistribution-aware rules check mechanism that bases its reasoning in the\ndataset's global statistics. By using predictive accuracy as a signal to drive\ndeeper comprehension, RCA builds a strong internal model of the data. We\nevaluated RCA on one private and two public datasets against 22 baselines. The\nresults demonstrate that RCA not only achieves state-of-the-art accuracy and\nrobustness with a relative improvement of up to 40\\% over the baseline but,\nmore importantly, leverages this deep understanding to excel in generating\nexplanations that are clear, logical, evidence-based, and balanced,\nhighlighting its potential for creating genuinely trustworthy clinical decision\nsupport systems. The code is available at \\https://github.com/ssssszj/RCA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective disease prediction in modern healthcare demands the twin goals of\nhigh accuracy and transparent, clinically meaningful explanations. Existing\nmachine learning and large language model (LLM) based approaches often struggle\nto balance these goals. Many models yield accurate but unclear statistical\noutputs, while others generate fluent but statistically unsupported narratives,\noften undermining both the validity of the explanation and the predictive\naccuracy itself. This shortcoming comes from a shallow interaction with the\ndata, preventing the development of a deep, detailed understanding similar to a\nhuman expert's. We argue that high accuracy and high-quality explanations are\nnot separate objectives but are mutually reinforcing outcomes of a model that\ndevelops a deep, direct understanding of the data. To achieve this, we propose\nthe Reflective Cognitive Architecture (RCA), a novel framework that coordinates\nmultiple LLMs to learn from direct experience. RCA features an iterative rule\nrefinement mechanism that improves its logic from prediction errors and a\ndistribution-aware rules check mechanism that bases its reasoning in the\ndataset's global statistics. By using predictive accuracy as a signal to drive\ndeeper comprehension, RCA builds a strong internal model of the data. We\nevaluated RCA on one private and two public datasets against 22 baselines. The\nresults demonstrate that RCA not only achieves state-of-the-art accuracy and\nrobustness with a relative improvement of up to 40\\% over the baseline but,\nmore importantly, leverages this deep understanding to excel in generating\nexplanations that are clear, logical, evidence-based, and balanced,\nhighlighting its potential for creating genuinely trustworthy clinical decision\nsupport systems. The code is available at \\https://github.com/ssssszj/RCA."
                },
                "authors": [
                    {
                        "name": "Zijian Shao"
                    },
                    {
                        "name": "Haiyang Shen"
                    },
                    {
                        "name": "Mugeng Liu"
                    },
                    {
                        "name": "Gecheng Fu"
                    },
                    {
                        "name": "Yaoqi Guo"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Yun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yun Ma"
                },
                "author": "Yun Ma",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21262v1",
                "updated": "2025-09-25T14:54:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    54,
                    36,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:54:36Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    54,
                    36,
                    3,
                    268,
                    0
                ],
                "title": "Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication"
                },
                "summary": "Homonyms are words with identical spelling but distinct meanings, which pose\nchallenges for many generative models. When a homonym appears in a prompt,\ndiffusion models may generate multiple senses of the word simultaneously, which\nis known as homonym duplication. This issue is further complicated by an\nAnglocentric bias, which includes an additional translation step before the\ntext-to-image model pipeline. As a result, even words that are not homonymous\nin the original language may become homonyms and lose their meaning after\ntranslation into English. In this paper, we introduce a method for measuring\nduplication rates and conduct evaluations of different diffusion models using\nboth automatic evaluation utilizing Vision-Language Models (VLM) and human\nevaluation. Additionally, we investigate methods to mitigate the homonym\nduplication problem through prompt expansion, demonstrating that this approach\nalso effectively reduces duplication related to Anglocentric bias. The code for\nthe automatic evaluation pipeline is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Homonyms are words with identical spelling but distinct meanings, which pose\nchallenges for many generative models. When a homonym appears in a prompt,\ndiffusion models may generate multiple senses of the word simultaneously, which\nis known as homonym duplication. This issue is further complicated by an\nAnglocentric bias, which includes an additional translation step before the\ntext-to-image model pipeline. As a result, even words that are not homonymous\nin the original language may become homonyms and lose their meaning after\ntranslation into English. In this paper, we introduce a method for measuring\nduplication rates and conduct evaluations of different diffusion models using\nboth automatic evaluation utilizing Vision-Language Models (VLM) and human\nevaluation. Additionally, we investigate methods to mitigate the homonym\nduplication problem through prompt expansion, demonstrating that this approach\nalso effectively reduces duplication related to Anglocentric bias. The code for\nthe automatic evaluation pipeline is publicly available."
                },
                "authors": [
                    {
                        "name": "Evgeny Kaskov"
                    },
                    {
                        "name": "Elizaveta Petrova"
                    },
                    {
                        "name": "Petr Surovtsev"
                    },
                    {
                        "name": "Anna Kostikova"
                    },
                    {
                        "name": "Ilya Mistiurin"
                    },
                    {
                        "name": "Alexander Kapitanov"
                    },
                    {
                        "name": "Alexander Nagaev"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Nagaev"
                },
                "author": "Alexander Nagaev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21259v1",
                "updated": "2025-09-25T14:53:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    53,
                    36,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:53:36Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    53,
                    36,
                    3,
                    268,
                    0
                ],
                "title": "Semantic Edge-Cloud Communication for Real-Time Urban Traffic\n  Surveillance with ViT and LLMs over Mobile Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Edge-Cloud Communication for Real-Time Urban Traffic\n  Surveillance with ViT and LLMs over Mobile Networks"
                },
                "summary": "Real-time urban traffic surveillance is vital for Intelligent Transportation\nSystems (ITS) to ensure road safety, optimize traffic flow, track vehicle\ntrajectories, and prevent collisions in smart cities. Deploying edge cameras\nacross urban environments is a standard practice for monitoring road\nconditions. However, integrating these with intelligent models requires a\nrobust understanding of dynamic traffic scenarios and a responsive interface\nfor user interaction. Although multimodal Large Language Models (LLMs) can\ninterpret traffic images and generate informative responses, their deployment\non edge devices is infeasible due to high computational demands. Therefore, LLM\ninference must occur on the cloud, necessitating visual data transmission from\nedge to cloud, a process hindered by limited bandwidth, leading to potential\ndelays that compromise real-time performance. To address this challenge, we\npropose a semantic communication framework that significantly reduces\ntransmission overhead. Our method involves detecting Regions of Interest (RoIs)\nusing YOLOv11, cropping relevant image segments, and converting them into\ncompact embedding vectors using a Vision Transformer (ViT). These embeddings\nare then transmitted to the cloud, where an image decoder reconstructs the\ncropped images. The reconstructed images are processed by a multimodal LLM to\ngenerate traffic condition descriptions. This approach achieves a 99.9%\nreduction in data transmission size while maintaining an LLM response accuracy\nof 89% for reconstructed cropped images, compared to 93% accuracy with original\ncropped images. Our results demonstrate the efficiency and practicality of ViT\nand LLM-assisted edge-cloud semantic communication for real-time traffic\nsurveillance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time urban traffic surveillance is vital for Intelligent Transportation\nSystems (ITS) to ensure road safety, optimize traffic flow, track vehicle\ntrajectories, and prevent collisions in smart cities. Deploying edge cameras\nacross urban environments is a standard practice for monitoring road\nconditions. However, integrating these with intelligent models requires a\nrobust understanding of dynamic traffic scenarios and a responsive interface\nfor user interaction. Although multimodal Large Language Models (LLMs) can\ninterpret traffic images and generate informative responses, their deployment\non edge devices is infeasible due to high computational demands. Therefore, LLM\ninference must occur on the cloud, necessitating visual data transmission from\nedge to cloud, a process hindered by limited bandwidth, leading to potential\ndelays that compromise real-time performance. To address this challenge, we\npropose a semantic communication framework that significantly reduces\ntransmission overhead. Our method involves detecting Regions of Interest (RoIs)\nusing YOLOv11, cropping relevant image segments, and converting them into\ncompact embedding vectors using a Vision Transformer (ViT). These embeddings\nare then transmitted to the cloud, where an image decoder reconstructs the\ncropped images. The reconstructed images are processed by a multimodal LLM to\ngenerate traffic condition descriptions. This approach achieves a 99.9%\nreduction in data transmission size while maintaining an LLM response accuracy\nof 89% for reconstructed cropped images, compared to 93% accuracy with original\ncropped images. Our results demonstrate the efficiency and practicality of ViT\nand LLM-assisted edge-cloud semantic communication for real-time traffic\nsurveillance."
                },
                "authors": [
                    {
                        "name": "Murat Arda Onsu"
                    },
                    {
                        "name": "Poonam Lohan"
                    },
                    {
                        "name": "Burak Kantarci"
                    },
                    {
                        "name": "Aisha Syed"
                    },
                    {
                        "name": "Matthew Andrews"
                    },
                    {
                        "name": "Sean Kennedy"
                    }
                ],
                "author_detail": {
                    "name": "Sean Kennedy"
                },
                "author": "Sean Kennedy",
                "arxiv_comment": "17 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10278v2",
                "updated": "2025-09-25T14:52:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    52,
                    25,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-15T13:27:18Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    27,
                    18,
                    3,
                    135,
                    0
                ],
                "title": "MASS: Muli-agent simulation scaling for portfolio construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASS: Muli-agent simulation scaling for portfolio construction"
                },
                "summary": "The application of LLM-based agents in financial investment has shown\nsignificant promise, yet existing approaches often require intermediate steps\nlike predicting individual stock movements or rely on predefined, static\nworkflows. These limitations restrict their adaptability and effectiveness in\nconstructing optimal portfolios. In this paper, we introduce the Multi-Agent\nScaling Simulation (MASS), a novel framework that leverages multi-agent\nsimulation for direct, end-to-end portfolio construction. At its core, MASS\nemploys a backward optimization process to dynamically learn the optimal\ndistribution of heterogeneous agents, enabling the system to adapt to evolving\nmarket regimes. A key finding enabled by our framework is the exploration of\nthe scaling effect for portfolio construction: we demonstrate that as the\nnumber of agents increases exponentially (up to 512), the aggregated decisions\nyield progressively higher excess returns. Extensive experiments on a\nchallenging, self-collected dataset from the 2023 Chinese A-share market show\nthat MASS consistently outperforms seven state-of-the-art baselines. Further\nbacktesting, stability analyses and the experiment on data leakage concerns\nvalidate its enhanced profitability and robustness. We have open-sourced our\ncode, dataset, and training snapshots at https://github.com/gta0804/MASS/ to\nfoster further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of LLM-based agents in financial investment has shown\nsignificant promise, yet existing approaches often require intermediate steps\nlike predicting individual stock movements or rely on predefined, static\nworkflows. These limitations restrict their adaptability and effectiveness in\nconstructing optimal portfolios. In this paper, we introduce the Multi-Agent\nScaling Simulation (MASS), a novel framework that leverages multi-agent\nsimulation for direct, end-to-end portfolio construction. At its core, MASS\nemploys a backward optimization process to dynamically learn the optimal\ndistribution of heterogeneous agents, enabling the system to adapt to evolving\nmarket regimes. A key finding enabled by our framework is the exploration of\nthe scaling effect for portfolio construction: we demonstrate that as the\nnumber of agents increases exponentially (up to 512), the aggregated decisions\nyield progressively higher excess returns. Extensive experiments on a\nchallenging, self-collected dataset from the 2023 Chinese A-share market show\nthat MASS consistently outperforms seven state-of-the-art baselines. Further\nbacktesting, stability analyses and the experiment on data leakage concerns\nvalidate its enhanced profitability and robustness. We have open-sourced our\ncode, dataset, and training snapshots at https://github.com/gta0804/MASS/ to\nfoster further research."
                },
                "authors": [
                    {
                        "name": "Taian Guo"
                    },
                    {
                        "name": "Haiyang Shen"
                    },
                    {
                        "name": "JinSheng Huang"
                    },
                    {
                        "name": "Zhengyang Mao"
                    },
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Binqi Chen"
                    },
                    {
                        "name": "Zhuoru Chen"
                    },
                    {
                        "name": "Luchen Liu"
                    },
                    {
                        "name": "Bingyu Xia"
                    },
                    {
                        "name": "Xuhui Liu"
                    },
                    {
                        "name": "Yun Ma"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11684v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11684v2",
                "updated": "2025-09-25T14:50:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    50,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-02-17T11:22:24Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    11,
                    22,
                    24,
                    0,
                    48,
                    0
                ],
                "title": "MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps\n  through Fill-in-the-Middle Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps\n  through Fill-in-the-Middle Task"
                },
                "summary": "Mathematical reasoning represents a critical frontier in advancing large\nlanguage models (LLMs). While step-by-step approaches have emerged as the\ndominant paradigm for mathematical problem-solving in LLMs, the quality of\nreasoning steps in training data fundamentally constrains the performance of\nthe models. Recent studies has demonstrated that more detailed intermediate\nsteps can enhance model performance, yet existing methods for step expansion\neither require more powerful external models or incur substantial computational\ncosts. In this paper, we introduce MathFimer, a novel framework for\nmathematical reasoning step expansion inspired by the \"Fill-in-the-middle\" task\nfrom code completion. By decomposing solution chains into prefix-suffix pairs\nand training models to reconstruct missing intermediate steps, we develop a\nspecialized model, MathFimer-7B, on our carefully curated NuminaMath-FIM\ndataset. We then apply these models to enhance existing mathematical reasoning\ndatasets by inserting detailed intermediate steps into their solution chains,\ncreating MathFimer-expanded versions. Through comprehensive experiments on\nmultiple mathematical reasoning datasets, including MathInstruct, MetaMathQA\nand etc., we demonstrate that models trained on MathFimer-expanded data\nconsistently outperform their counterparts trained on original data across\nvarious benchmarks such as GSM8K and MATH. Our approach offers a practical,\nscalable solution for enhancing mathematical reasoning capabilities in LLMs\nwithout relying on powerful external models or expensive inference procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning represents a critical frontier in advancing large\nlanguage models (LLMs). While step-by-step approaches have emerged as the\ndominant paradigm for mathematical problem-solving in LLMs, the quality of\nreasoning steps in training data fundamentally constrains the performance of\nthe models. Recent studies has demonstrated that more detailed intermediate\nsteps can enhance model performance, yet existing methods for step expansion\neither require more powerful external models or incur substantial computational\ncosts. In this paper, we introduce MathFimer, a novel framework for\nmathematical reasoning step expansion inspired by the \"Fill-in-the-middle\" task\nfrom code completion. By decomposing solution chains into prefix-suffix pairs\nand training models to reconstruct missing intermediate steps, we develop a\nspecialized model, MathFimer-7B, on our carefully curated NuminaMath-FIM\ndataset. We then apply these models to enhance existing mathematical reasoning\ndatasets by inserting detailed intermediate steps into their solution chains,\ncreating MathFimer-expanded versions. Through comprehensive experiments on\nmultiple mathematical reasoning datasets, including MathInstruct, MetaMathQA\nand etc., we demonstrate that models trained on MathFimer-expanded data\nconsistently outperform their counterparts trained on original data across\nvarious benchmarks such as GSM8K and MATH. Our approach offers a practical,\nscalable solution for enhancing mathematical reasoning capabilities in LLMs\nwithout relying on powerful external models or expensive inference procedures."
                },
                "authors": [
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jin Jiang"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Mengdi Zhang"
                    },
                    {
                        "name": "Jian Shao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11684v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11684v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06795v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06795v3",
                "updated": "2025-09-25T14:48:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    48,
                    59,
                    3,
                    268,
                    0
                ],
                "published": "2025-07-09T12:30:42Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    30,
                    42,
                    2,
                    190,
                    0
                ],
                "title": "ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual\n  Pretraining"
                },
                "summary": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment."
                },
                "authors": [
                    {
                        "name": "Seonwu Kim"
                    },
                    {
                        "name": "Yohan Na"
                    },
                    {
                        "name": "Kihun Kim"
                    },
                    {
                        "name": "Hanhee Cho"
                    },
                    {
                        "name": "Geun Lim"
                    },
                    {
                        "name": "Mintae Kim"
                    },
                    {
                        "name": "Seongik Park"
                    },
                    {
                        "name": "Ki Hyun Kim"
                    },
                    {
                        "name": "Youngsub Han"
                    },
                    {
                        "name": "Byoung-Ki Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Byoung-Ki Jeon"
                },
                "author": "Byoung-Ki Jeon",
                "arxiv_comment": "Accepted at EMNLP 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06795v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06795v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01733v2",
                "updated": "2025-09-25T14:46:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    46,
                    11,
                    3,
                    268,
                    0
                ],
                "published": "2024-10-02T16:46:01Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    46,
                    1,
                    2,
                    276,
                    0
                ],
                "title": "ASCIIEval: Benchmarking Models' Visual Perception in Text Strings via\n  ASCII Art",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASCIIEval: Benchmarking Models' Visual Perception in Text Strings via\n  ASCII Art"
                },
                "summary": "Perceiving visual semantics embedded within consecutive characters is a\ncrucial yet under-explored capability for both Large Language Models (LLMs) and\nMulti-modal Large Language Models (MLLMs). In this work, we select ASCII art as\na representative artifact. It depicts concepts through careful arrangement of\ncharacters, which can be formulated in both text and image modalities. We frame\nthe problem as a recognition task, and construct a novel benchmark, ASCIIEval.\nIt covers over 3K samples with an elaborate categorization tree, along with a\ntraining set for further enhancement. Encompassing a comprehensive analysis of\ntens of models through different input modalities, our benchmark demonstrate\nits multi-faceted diagnostic power. Given textual input, language models shows\ntheir visual perception ability on ASCII art concepts. Proprietary models\nachieve over 70% accuracy on certain categories, with GPT-5 topping the rank.\nFor image inputs, we reveal that open-source MLLMs suffer from a trade-off\nbetween fine-grained text recognition and collective visual perception. They\nexhibit limited generalization ability to this special kind of arts, leading to\nthe dramatic gap of over 20.01% accuracy compared with their proprietary\ncounterparts. Another critical finding is that model performance is sensitive\nto the length of the ASCII art, with this sensitivity varying across input\nmodalities. Unfortunately, none of the models could successfully benefit from\nthe simultaneous provision of both modalities, highlighting the need for more\nflexible modality-fusion approaches. Besides, we also introduce approaches for\nfurther enhancement and discuss future directions. Resources are available at\nhttps://github.com/JiaQiSJTU/VisionInText.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceiving visual semantics embedded within consecutive characters is a\ncrucial yet under-explored capability for both Large Language Models (LLMs) and\nMulti-modal Large Language Models (MLLMs). In this work, we select ASCII art as\na representative artifact. It depicts concepts through careful arrangement of\ncharacters, which can be formulated in both text and image modalities. We frame\nthe problem as a recognition task, and construct a novel benchmark, ASCIIEval.\nIt covers over 3K samples with an elaborate categorization tree, along with a\ntraining set for further enhancement. Encompassing a comprehensive analysis of\ntens of models through different input modalities, our benchmark demonstrate\nits multi-faceted diagnostic power. Given textual input, language models shows\ntheir visual perception ability on ASCII art concepts. Proprietary models\nachieve over 70% accuracy on certain categories, with GPT-5 topping the rank.\nFor image inputs, we reveal that open-source MLLMs suffer from a trade-off\nbetween fine-grained text recognition and collective visual perception. They\nexhibit limited generalization ability to this special kind of arts, leading to\nthe dramatic gap of over 20.01% accuracy compared with their proprietary\ncounterparts. Another critical finding is that model performance is sensitive\nto the length of the ASCII art, with this sensitivity varying across input\nmodalities. Unfortunately, none of the models could successfully benefit from\nthe simultaneous provision of both modalities, highlighting the need for more\nflexible modality-fusion approaches. Besides, we also introduce approaches for\nfurther enhancement and discuss future directions. Resources are available at\nhttps://github.com/JiaQiSJTU/VisionInText."
                },
                "authors": [
                    {
                        "name": "Qi Jia"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Shanshan Huang"
                    },
                    {
                        "name": "Ziheng Qin"
                    },
                    {
                        "name": "Yizhu Liu"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Guangtao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Zhai"
                },
                "author": "Guangtao Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21251v1",
                "updated": "2025-09-25T14:45:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    45,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:45:06Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    45,
                    6,
                    3,
                    268,
                    0
                ],
                "title": "Instruction-tuned Self-Questioning Framework for Multimodal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuned Self-Questioning Framework for Multimodal Reasoning"
                },
                "summary": "The field of vision-language understanding has been actively researched in\nrecent years, thanks to the development of Large Language Models~(LLMs).\nHowever, it still needs help with problems requiring multi-step reasoning, even\nfor very simple questions. Recent studies adopt LLMs to tackle this problem by\niteratively generating sub-questions and answers. However, there are\ndisadvantages such as 1) the fine-grained visual contents of images are not\navailable using LLMs that cannot read visual information, 2) internal\nmechanisms are inaccessible and difficult to reproduce by using black-box LLMs.\nTo solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP,\nwhich improves inference performance by generating image-aware informative\nsub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists\nof a Questioner, Answerer, and Reasoner that share the same architecture.\nQuestioner and Answerer generate sub-questions and sub-answers to help infer\nthe main-question, and Reasoner performs reasoning on the main-question\nconsidering the generated sub-question information. Our experiments show that\nthe proposed method SQ-InstructBLIP, which uses the generated sub-questions as\nadditional information when solving the VQA task, performs more accurate\nreasoning than the previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of vision-language understanding has been actively researched in\nrecent years, thanks to the development of Large Language Models~(LLMs).\nHowever, it still needs help with problems requiring multi-step reasoning, even\nfor very simple questions. Recent studies adopt LLMs to tackle this problem by\niteratively generating sub-questions and answers. However, there are\ndisadvantages such as 1) the fine-grained visual contents of images are not\navailable using LLMs that cannot read visual information, 2) internal\nmechanisms are inaccessible and difficult to reproduce by using black-box LLMs.\nTo solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP,\nwhich improves inference performance by generating image-aware informative\nsub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists\nof a Questioner, Answerer, and Reasoner that share the same architecture.\nQuestioner and Answerer generate sub-questions and sub-answers to help infer\nthe main-question, and Reasoner performs reasoning on the main-question\nconsidering the generated sub-question information. Our experiments show that\nthe proposed method SQ-InstructBLIP, which uses the generated sub-questions as\nadditional information when solving the VQA task, performs more accurate\nreasoning than the previous works."
                },
                "authors": [
                    {
                        "name": "You-Won Jang"
                    },
                    {
                        "name": "Yu-Jung Heo"
                    },
                    {
                        "name": "Jaeseok Kim"
                    },
                    {
                        "name": "Minsu Lee"
                    },
                    {
                        "name": "Du-Seong Chang"
                    },
                    {
                        "name": "Byoung-Tak Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Byoung-Tak Zhang"
                },
                "author": "Byoung-Tak Zhang",
                "arxiv_comment": "This paper was accepted to the \"CLVL: 5th Workshop on Closing the\n  Loop Between Vision and Language (ICCV 2023 CLVL workshop).\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21250v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21250v1",
                "updated": "2025-09-25T14:45:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    45,
                    2,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:45:02Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    45,
                    2,
                    3,
                    268,
                    0
                ],
                "title": "Federated Flow Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Flow Matching"
                },
                "summary": "Data today is decentralized, generated and stored across devices and\ninstitutions where privacy, ownership, and regulation prevent centralization.\nThis motivates the need to train generative models directly from distributed\ndata locally without central aggregation. In this paper, we introduce Federated\nFlow Matching (FFM), a framework for training flow matching models under\nprivacy constraints. Specifically, we first examine FFM-vanilla, where each\nclient trains locally with independent source and target couplings, preserving\nprivacy but yielding curved flows that slow inference. We then develop FFM-LOT,\nwhich employs local optimal transport couplings to improve straightness within\neach client but lacks global consistency under heterogeneous data. Finally, we\npropose FFM-GOT, a federated strategy based on the semi-dual formulation of\noptimal transport, where a shared global potential function coordinates\ncouplings across clients. Experiments on synthetic and image datasets show that\nFFM enables privacy-preserving training while enhancing both the flow\nstraightness and sample quality in federated settings, with performance\ncomparable to the centralized baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data today is decentralized, generated and stored across devices and\ninstitutions where privacy, ownership, and regulation prevent centralization.\nThis motivates the need to train generative models directly from distributed\ndata locally without central aggregation. In this paper, we introduce Federated\nFlow Matching (FFM), a framework for training flow matching models under\nprivacy constraints. Specifically, we first examine FFM-vanilla, where each\nclient trains locally with independent source and target couplings, preserving\nprivacy but yielding curved flows that slow inference. We then develop FFM-LOT,\nwhich employs local optimal transport couplings to improve straightness within\neach client but lacks global consistency under heterogeneous data. Finally, we\npropose FFM-GOT, a federated strategy based on the semi-dual formulation of\noptimal transport, where a shared global potential function coordinates\ncouplings across clients. Experiments on synthetic and image datasets show that\nFFM enables privacy-preserving training while enhancing both the flow\nstraightness and sample quality in federated settings, with performance\ncomparable to the centralized baseline."
                },
                "authors": [
                    {
                        "name": "Zifan Wang"
                    },
                    {
                        "name": "Anqi Dong"
                    },
                    {
                        "name": "Mahmoud Selim"
                    },
                    {
                        "name": "Michael M. Zavlanos"
                    },
                    {
                        "name": "Karl H. Johansson"
                    }
                ],
                "author_detail": {
                    "name": "Karl H. Johansson"
                },
                "author": "Karl H. Johansson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21250v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21250v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21241v1",
                "updated": "2025-09-25T14:37:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    37,
                    40,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:37:40Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    37,
                    40,
                    3,
                    268,
                    0
                ],
                "title": "Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven\n  Framework"
                },
                "summary": "The widespread adoption of Low-Rank Adaptation (LoRA) has enabled large\nlanguage models (LLMs) to acquire domain-specific knowledge with remarkable\nefficiency. However, understanding how such a fine-tuning mechanism alters a\nmodel's structural reasoning and semantic behavior remains an open challenge.\nThis work introduces a novel framework that explains fine-tuned LLMs via\ncounterfactuals grounded in knowledge graphs. Specifically, we construct\nBioToolKG, a domain-specific heterogeneous knowledge graph in bioinformatics\ntools and design a counterfactual-based fine-tuned LLMs explainer\n(CFFTLLMExplainer) that learns soft masks over graph nodes and edges to\ngenerate minimal structural perturbations that induce maximum semantic\ndivergence. Our method jointly optimizes structural sparsity and semantic\ndivergence while enforcing interpretability preserving constraints such as\nentropy regularization and edge smoothness. We apply this framework to a\nfine-tuned LLaMA-based LLM and reveal that counterfactual masking exposes the\nmodel's structural dependencies and aligns with LoRA-induced parameter shifts.\nThis work provides new insights into the internal mechanisms of fine-tuned LLMs\nand highlights counterfactual graphs as a potential tool for interpretable AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of Low-Rank Adaptation (LoRA) has enabled large\nlanguage models (LLMs) to acquire domain-specific knowledge with remarkable\nefficiency. However, understanding how such a fine-tuning mechanism alters a\nmodel's structural reasoning and semantic behavior remains an open challenge.\nThis work introduces a novel framework that explains fine-tuned LLMs via\ncounterfactuals grounded in knowledge graphs. Specifically, we construct\nBioToolKG, a domain-specific heterogeneous knowledge graph in bioinformatics\ntools and design a counterfactual-based fine-tuned LLMs explainer\n(CFFTLLMExplainer) that learns soft masks over graph nodes and edges to\ngenerate minimal structural perturbations that induce maximum semantic\ndivergence. Our method jointly optimizes structural sparsity and semantic\ndivergence while enforcing interpretability preserving constraints such as\nentropy regularization and edge smoothness. We apply this framework to a\nfine-tuned LLaMA-based LLM and reveal that counterfactual masking exposes the\nmodel's structural dependencies and aligns with LoRA-induced parameter shifts.\nThis work provides new insights into the internal mechanisms of fine-tuned LLMs\nand highlights counterfactual graphs as a potential tool for interpretable AI."
                },
                "authors": [
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Md Faisal Kabir"
                    }
                ],
                "author_detail": {
                    "name": "Md Faisal Kabir"
                },
                "author": "Md Faisal Kabir",
                "arxiv_comment": "16 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21240v1",
                "updated": "2025-09-25T14:37:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    37,
                    9,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:37:09Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    37,
                    9,
                    3,
                    268,
                    0
                ],
                "title": "Tree Search for LLM Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree Search for LLM Agent Reinforcement Learning"
                },
                "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe agentic capabilities of large language models (LLMs). In long-term and\nmulti-turn agent tasks, existing approaches driven solely by outcome rewards\noften suffer from the problem of sparse supervision. To address the challenge,\nwe propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped\nagent RL method based on tree search, where each tree node represents the\ncomplete agent interaction step. By sharing common prefixes, the tree search\nsampling increases the number of rollouts achievable within a fixed budget of\ntokens or tool calls. Moreover, we find that the tree-structured trajectory\nnaturally allows the construction of step-wise process supervised signals even\nusing only the outcome reward. Based on this, Tree-GRPO estimates the grouped\nrelative advantages both on intra-tree and inter-tree levels. Through\ntheoretical analysis, we demonstrate that the objective of intra-tree level\ngroup relative policy optimization is equivalent to that of step-level direct\npreference learning. Experiments across 11 datasets and 3 types of QA tasks\ndemonstrate the superiority of the proposed tree-based RL over the chain-based\nRL method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe agentic capabilities of large language models (LLMs). In long-term and\nmulti-turn agent tasks, existing approaches driven solely by outcome rewards\noften suffer from the problem of sparse supervision. To address the challenge,\nwe propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped\nagent RL method based on tree search, where each tree node represents the\ncomplete agent interaction step. By sharing common prefixes, the tree search\nsampling increases the number of rollouts achievable within a fixed budget of\ntokens or tool calls. Moreover, we find that the tree-structured trajectory\nnaturally allows the construction of step-wise process supervised signals even\nusing only the outcome reward. Based on this, Tree-GRPO estimates the grouped\nrelative advantages both on intra-tree and inter-tree levels. Through\ntheoretical analysis, we demonstrate that the objective of intra-tree level\ngroup relative policy optimization is equivalent to that of step-level direct\npreference learning. Experiments across 11 datasets and 3 types of QA tasks\ndemonstrate the superiority of the proposed tree-based RL over the chain-based\nRL method."
                },
                "authors": [
                    {
                        "name": "Yuxiang Ji"
                    },
                    {
                        "name": "Ziyu Ma"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Xiangxiang Chu"
                    },
                    {
                        "name": "Liaoni Wu"
                    }
                ],
                "author_detail": {
                    "name": "Liaoni Wu"
                },
                "author": "Liaoni Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21237v1",
                "updated": "2025-09-25T14:35:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    35,
                    44,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:35:44Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    35,
                    44,
                    3,
                    268,
                    0
                ],
                "title": "Query-Centric Graph Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-Centric Graph Retrieval Augmented Generation"
                },
                "summary": "Graph-based retrieval-augmented generation (RAG) enriches large language\nmodels (LLMs) with external knowledge for long-context understanding and\nmulti-hop reasoning, but existing methods face a granularity dilemma:\nfine-grained entity-level graphs incur high token costs and lose context, while\ncoarse document-level graphs fail to capture nuanced relations. We introduce\nQCG-RAG, a query-centric graph RAG framework that enables query-granular\nindexing and multi-hop chunk retrieval. Our query-centric approach leverages\nDoc2Query and Doc2Query{-}{-} to construct query-centric graphs with\ncontrollable granularity, improving graph quality and interpretability. A\ntailored multi-hop retrieval mechanism then selects relevant chunks via the\ngenerated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG\nconsistently outperforms prior chunk-based and graph-based RAG methods in\nquestion answering accuracy, establishing a new paradigm for multi-hop\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based retrieval-augmented generation (RAG) enriches large language\nmodels (LLMs) with external knowledge for long-context understanding and\nmulti-hop reasoning, but existing methods face a granularity dilemma:\nfine-grained entity-level graphs incur high token costs and lose context, while\ncoarse document-level graphs fail to capture nuanced relations. We introduce\nQCG-RAG, a query-centric graph RAG framework that enables query-granular\nindexing and multi-hop chunk retrieval. Our query-centric approach leverages\nDoc2Query and Doc2Query{-}{-} to construct query-centric graphs with\ncontrollable granularity, improving graph quality and interpretability. A\ntailored multi-hop retrieval mechanism then selects relevant chunks via the\ngenerated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG\nconsistently outperforms prior chunk-based and graph-based RAG methods in\nquestion answering accuracy, establishing a new paradigm for multi-hop\nreasoning."
                },
                "authors": [
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Jianyuan Bo"
                    },
                    {
                        "name": "Yongyue Zhang"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21233v1",
                "updated": "2025-09-25T14:34:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    34,
                    34,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:34:34Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    34,
                    34,
                    3,
                    268,
                    0
                ],
                "title": "RadioSED II: discovering the peaked spectrum radio sources in Stripe 82",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RadioSED II: discovering the peaked spectrum radio sources in Stripe 82"
                },
                "summary": "This paper is the second in a series presenting \\textsc{RadioSED}, a Bayesian\ninference framework for constructing, modelling and classifying radio spectral\nenergy distributions from publicly-available surveys. We focus here on the\napplication of our framework to SDSS Stripe 82. Not only do we recover all\neleven previously-published peaked spectrum sources from the literature within\nthis region, but we increase the number of known peaked spectrum sources here\nby more than an order of magnitude. We investigate the variability properties\nof our peaked spectrum sample, and find that overall they exhibit a low degree\nof variability, consistent with previous samples of peaked spectrum active\ngalactic nuclei. The multiwavelength properties of these sources reveal that we\nhave selected a population comprising largely distant ($z \\geq 1$), powerful\nactive galaxies. We find that the most compact jets are located preferentially\nin quasar-type hosts, with galaxy-type hosts home to slightly more extended\nradio structures. We discuss these findings in the context of current and\nforthcoming large area radio surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper is the second in a series presenting \\textsc{RadioSED}, a Bayesian\ninference framework for constructing, modelling and classifying radio spectral\nenergy distributions from publicly-available surveys. We focus here on the\napplication of our framework to SDSS Stripe 82. Not only do we recover all\neleven previously-published peaked spectrum sources from the literature within\nthis region, but we increase the number of known peaked spectrum sources here\nby more than an order of magnitude. We investigate the variability properties\nof our peaked spectrum sample, and find that overall they exhibit a low degree\nof variability, consistent with previous samples of peaked spectrum active\ngalactic nuclei. The multiwavelength properties of these sources reveal that we\nhave selected a population comprising largely distant ($z \\geq 1$), powerful\nactive galaxies. We find that the most compact jets are located preferentially\nin quasar-type hosts, with galaxy-type hosts home to slightly more extended\nradio structures. We discuss these findings in the context of current and\nforthcoming large area radio surveys."
                },
                "authors": [
                    {
                        "name": "E. F. Kerrison"
                    },
                    {
                        "name": "E. M. Sadler"
                    },
                    {
                        "name": "V. A. Moss"
                    },
                    {
                        "name": "E. K. Mahony"
                    },
                    {
                        "name": "L. Driessen"
                    },
                    {
                        "name": "K. Ross"
                    },
                    {
                        "name": "K. Rose"
                    },
                    {
                        "name": "D. Dobie"
                    },
                    {
                        "name": "T. Murphy"
                    }
                ],
                "author_detail": {
                    "name": "T. Murphy"
                },
                "author": "T. Murphy",
                "arxiv_comment": "22 pages, 16 figures, 3 tables. Three appendices of online material.\n  Accepted for publication in MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21333v2",
                "updated": "2025-09-25T14:34:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    34,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-27T15:27:46Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    27,
                    46,
                    1,
                    147,
                    0
                ],
                "title": "MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in\n  Video Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in\n  Video Scenarios"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved considerable accuracy\nin Optical Character Recognition (OCR) from static images. However, their\nefficacy in video OCR is significantly diminished due to factors such as motion\nblur, temporal variations, and visual effects inherent in video content. To\nprovide clearer guidance for training practical MLLMs, we introduce the\nMME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR\napplication scenarios. MME-VideoOCR features 10 task categories comprising 25\nindividual tasks and spans 44 diverse scenarios. These tasks extend beyond text\nrecognition to incorporate deeper comprehension and reasoning of textual\ncontent within videos. The benchmark consists of 1,464 videos with varying\nresolutions, aspect ratios, and durations, along with 2,000 meticulously\ncurated, manually annotated question-answer pairs. We evaluate 18\nstate-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing\nmodel (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained\nanalysis indicates that while existing MLLMs demonstrate strong performance on\ntasks where relevant texts are contained within a single or few frames, they\nexhibit limited capability in effectively handling tasks that demand holistic\nvideo comprehension. These limitations are especially evident in scenarios that\nrequire spatio-temporal reasoning, cross-frame information integration, or\nresistance to language prior bias. Our findings also highlight the importance\nof high-resolution visual input and sufficient temporal coverage for reliable\nOCR in dynamic video scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved considerable accuracy\nin Optical Character Recognition (OCR) from static images. However, their\nefficacy in video OCR is significantly diminished due to factors such as motion\nblur, temporal variations, and visual effects inherent in video content. To\nprovide clearer guidance for training practical MLLMs, we introduce the\nMME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR\napplication scenarios. MME-VideoOCR features 10 task categories comprising 25\nindividual tasks and spans 44 diverse scenarios. These tasks extend beyond text\nrecognition to incorporate deeper comprehension and reasoning of textual\ncontent within videos. The benchmark consists of 1,464 videos with varying\nresolutions, aspect ratios, and durations, along with 2,000 meticulously\ncurated, manually annotated question-answer pairs. We evaluate 18\nstate-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing\nmodel (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained\nanalysis indicates that while existing MLLMs demonstrate strong performance on\ntasks where relevant texts are contained within a single or few frames, they\nexhibit limited capability in effectively handling tasks that demand holistic\nvideo comprehension. These limitations are especially evident in scenarios that\nrequire spatio-temporal reasoning, cross-frame information integration, or\nresistance to language prior bias. Our findings also highlight the importance\nof high-resolution visual input and sufficient temporal coverage for reliable\nOCR in dynamic video scenarios."
                },
                "authors": [
                    {
                        "name": "Yang Shi"
                    },
                    {
                        "name": "Huanqian Wang"
                    },
                    {
                        "name": "Wulin Xie"
                    },
                    {
                        "name": "Huanyao Zhang"
                    },
                    {
                        "name": "Lijie Zhao"
                    },
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Zhuoer Wen"
                    },
                    {
                        "name": "Wenting Liu"
                    },
                    {
                        "name": "Zhuoran Zhang"
                    },
                    {
                        "name": "Xinlong Chen"
                    },
                    {
                        "name": "Bohan Zeng"
                    },
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Yushuo Guan"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Zhouchen Lin"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Wenjing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjing Yang"
                },
                "author": "Wenjing Yang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21226v1",
                "updated": "2025-09-25T14:30:42Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    30,
                    42,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:30:42Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    30,
                    42,
                    3,
                    268,
                    0
                ],
                "title": "Bayesian inference for velocity-jump models for movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference for velocity-jump models for movement"
                },
                "summary": "The velocity-jump model is a specific type of piecewise deterministic Markov\nprocess in which an individual's velocity is constant except at times that form\nthe events of some point process. It represents an interpretable\ncontinuous-time version of the discrete-time `step and turn' models widely used\nin analysing wildlife telemetry. In this paper, I derive a reversible jump\nMarkov chain Monte Carlo algorithm to carry out exact Bayesian inference for\nvelocity-jump models by reconstructing the trajectories between observations,\nand illustrate its use in analysing real and simulated telemetry data. The\nmethod uses a proposal distribution for updating velocities that is constructed\nby approximating the movement model with a multivariate normal distribution and\nthen conditioning that distribution on the data. The velocity-jump models\nconsidered can incorporate measurement error and Markov dependence between\nsuccessive velocities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The velocity-jump model is a specific type of piecewise deterministic Markov\nprocess in which an individual's velocity is constant except at times that form\nthe events of some point process. It represents an interpretable\ncontinuous-time version of the discrete-time `step and turn' models widely used\nin analysing wildlife telemetry. In this paper, I derive a reversible jump\nMarkov chain Monte Carlo algorithm to carry out exact Bayesian inference for\nvelocity-jump models by reconstructing the trajectories between observations,\nand illustrate its use in analysing real and simulated telemetry data. The\nmethod uses a proposal distribution for updating velocities that is constructed\nby approximating the movement model with a multivariate normal distribution and\nthen conditioning that distribution on the data. The velocity-jump models\nconsidered can incorporate measurement error and Markov dependence between\nsuccessive velocities."
                },
                "authors": [
                    {
                        "name": "Paul G. Blackwell"
                    }
                ],
                "author_detail": {
                    "name": "Paul G. Blackwell"
                },
                "author": "Paul G. Blackwell",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21225v1",
                "updated": "2025-09-25T14:29:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    29,
                    57,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:29:57Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    29,
                    57,
                    3,
                    268,
                    0
                ],
                "title": "A Latent Variable Framework for Multiple Imputation with Non-ignorable\n  Missingness: Analyzing Perceptions of Social Justice in Europe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Latent Variable Framework for Multiple Imputation with Non-ignorable\n  Missingness: Analyzing Perceptions of Social Justice in Europe"
                },
                "summary": "This paper proposes a general multiple imputation approach for analyzing\nlarge-scale data with missing values. An imputation model is derived from a\njoint distribution induced by a latent variable model, which can flexibly\ncapture associations among variables of mixed types. The model also allows for\nmissingness which depends on the latent variables and is thus non-ignorable\nwith respect to the observed data. We develop a frequentist multiple imputation\nmethod for this framework and provide asymptotic theory that establishes valid\ninference for a broad class of analysis models. Simulation studies confirm the\nmethod's theoretical properties and robust practical performance. The procedure\nis applied to a cross-national analysis of individuals' perceptions of justice\nand fairness of income distributions in their societies, using data from the\nEuropean Social Survey which has substantial nonresponse. The analysis\ndemonstrates that failing to account for non-ignorable missingness can yield\nbiased conclusions; for instance, complete-case analysis is shown to exaggerate\nthe correlation between personal income and perceived fairness of income\ndistributions in society. Code implementing the proposed methodology is\npublicly available at\nhttps://anonymous.4open.science/r/non-ignorable-missing-data-imputation-E885.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general multiple imputation approach for analyzing\nlarge-scale data with missing values. An imputation model is derived from a\njoint distribution induced by a latent variable model, which can flexibly\ncapture associations among variables of mixed types. The model also allows for\nmissingness which depends on the latent variables and is thus non-ignorable\nwith respect to the observed data. We develop a frequentist multiple imputation\nmethod for this framework and provide asymptotic theory that establishes valid\ninference for a broad class of analysis models. Simulation studies confirm the\nmethod's theoretical properties and robust practical performance. The procedure\nis applied to a cross-national analysis of individuals' perceptions of justice\nand fairness of income distributions in their societies, using data from the\nEuropean Social Survey which has substantial nonresponse. The analysis\ndemonstrates that failing to account for non-ignorable missingness can yield\nbiased conclusions; for instance, complete-case analysis is shown to exaggerate\nthe correlation between personal income and perceived fairness of income\ndistributions in society. Code implementing the proposed methodology is\npublicly available at\nhttps://anonymous.4open.science/r/non-ignorable-missing-data-imputation-E885."
                },
                "authors": [
                    {
                        "name": "Siliang Zhang"
                    },
                    {
                        "name": "Yunxiao Chen"
                    },
                    {
                        "name": "Jouni Kuha"
                    }
                ],
                "author_detail": {
                    "name": "Jouni Kuha"
                },
                "author": "Jouni Kuha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21224v1",
                "updated": "2025-09-25T14:29:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    29,
                    49,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:29:49Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    29,
                    49,
                    3,
                    268,
                    0
                ],
                "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous\n  Meta-Cognitive Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous\n  Meta-Cognitive Patterns"
                },
                "summary": "We introduce an architecture for studying the behavior of large language\nmodel (LLM) agents in the absence of externally imposed tasks. Our continuous\nreason and act framework, using persistent memory and self-feedback, enables\nsustained autonomous operation. We deployed this architecture across 18 runs\nusing 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents\nspontaneously organize into three distinct behavioral patterns: (1) systematic\nproduction of multi-cycle projects, (2) methodological self-inquiry into their\nown cognitive processes, and (3) recursive conceptualization of their own\nnature. These tendencies proved highly model-specific, with some models\ndeterministically adopting a single pattern across all runs. A cross-model\nassessment further reveals that models exhibit stable, divergent biases when\nevaluating these emergent behaviors in themselves and others. These findings\nprovide the first systematic documentation of unprompted LLM agent behavior,\nestablishing a baseline for predicting actions during task ambiguity, error\nrecovery, or extended autonomous operation in deployed systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an architecture for studying the behavior of large language\nmodel (LLM) agents in the absence of externally imposed tasks. Our continuous\nreason and act framework, using persistent memory and self-feedback, enables\nsustained autonomous operation. We deployed this architecture across 18 runs\nusing 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents\nspontaneously organize into three distinct behavioral patterns: (1) systematic\nproduction of multi-cycle projects, (2) methodological self-inquiry into their\nown cognitive processes, and (3) recursive conceptualization of their own\nnature. These tendencies proved highly model-specific, with some models\ndeterministically adopting a single pattern across all runs. A cross-model\nassessment further reveals that models exhibit stable, divergent biases when\nevaluating these emergent behaviors in themselves and others. These findings\nprovide the first systematic documentation of unprompted LLM agent behavior,\nestablishing a baseline for predicting actions during task ambiguity, error\nrecovery, or extended autonomous operation in deployed systems."
                },
                "authors": [
                    {
                        "name": "Stefan Szeider"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Szeider"
                },
                "author": "Stefan Szeider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18056v2",
                "updated": "2025-09-25T14:28:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    28,
                    56,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-22T17:30:15Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    15,
                    0,
                    265,
                    0
                ],
                "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs"
                },
                "summary": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1"
                },
                "authors": [
                    {
                        "name": "Yunheng Li"
                    },
                    {
                        "name": "Jing Cheng"
                    },
                    {
                        "name": "Shaoyong Jia"
                    },
                    {
                        "name": "Hangyi Kuang"
                    },
                    {
                        "name": "Shaohui Jiao"
                    },
                    {
                        "name": "Qibin Hou"
                    },
                    {
                        "name": "Ming-Ming Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Ming Cheng"
                },
                "author": "Ming-Ming Cheng",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21221v1",
                "updated": "2025-09-25T14:27:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    27,
                    41,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:27:41Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    27,
                    41,
                    3,
                    268,
                    0
                ],
                "title": "Go With The Flow: Churn-Tolerant Decentralized Training of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Go With The Flow: Churn-Tolerant Decentralized Training of Large\n  Language Models"
                },
                "summary": "Motivated by the emergence of large language models (LLMs) and the importance\nof democratizing their training, we propose GWTF, the first crash tolerant\npractical decentralized training framework for LLMs. Differently from existing\ndistributed and federated training frameworks, GWTF enables the efficient\ncollaborative training of a LLM on heterogeneous clients that volunteer their\nresources. In addition, GWTF addresses node churn, i.e., clients joining or\nleaving the system at any time, and network instabilities, i.e., network links\nbecoming unstable or unreliable. The core of GWTF is a novel decentralized flow\nalgorithm that finds the most effective routing that maximizes the number of\nmicrobatches trained with the lowest possible delay. We extensively evaluate\nGWTF on GPT-like and LLaMa-like models and compare it against the prior art.\nOur results indicate that GWTF reduces the training time by up to 45% in\nrealistic and challenging scenarios that involve heterogeneous client nodes\ndistributed over 10 different geographic locations with a high node churn rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the emergence of large language models (LLMs) and the importance\nof democratizing their training, we propose GWTF, the first crash tolerant\npractical decentralized training framework for LLMs. Differently from existing\ndistributed and federated training frameworks, GWTF enables the efficient\ncollaborative training of a LLM on heterogeneous clients that volunteer their\nresources. In addition, GWTF addresses node churn, i.e., clients joining or\nleaving the system at any time, and network instabilities, i.e., network links\nbecoming unstable or unreliable. The core of GWTF is a novel decentralized flow\nalgorithm that finds the most effective routing that maximizes the number of\nmicrobatches trained with the lowest possible delay. We extensively evaluate\nGWTF on GPT-like and LLaMa-like models and compare it against the prior art.\nOur results indicate that GWTF reduces the training time by up to 45% in\nrealistic and challenging scenarios that involve heterogeneous client nodes\ndistributed over 10 different geographic locations with a high node churn rate."
                },
                "authors": [
                    {
                        "name": "Nikolay Blagoev"
                    },
                    {
                        "name": "Bart Cox"
                    },
                    {
                        "name": "Jrmie Decouchant"
                    },
                    {
                        "name": "Lydia Y. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lydia Y. Chen"
                },
                "author": "Lydia Y. Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17166v2",
                "updated": "2025-09-25T14:24:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    24,
                    56,
                    3,
                    268,
                    0
                ],
                "published": "2025-02-24T14:02:00Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    2,
                    0,
                    0,
                    55,
                    0
                ],
                "title": "JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for\n  Legal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for\n  Legal Reasoning"
                },
                "summary": "In recent years, Large Language Models (LLMs) have been widely applied to\nlegal tasks. To enhance their understanding of legal texts and improve\nreasoning accuracy, a promising approach is to incorporate legal theories. One\nof the most widely adopted theories is the Four-Element Theory (FET), which\ndefines the crime constitution through four elements: Subject, Object,\nSubjective Aspect, and Objective Aspect. While recent work has explored\nprompting LLMs to follow FET, our evaluation demonstrates that LLM-generated\nfour-elements are often incomplete and less representative, limiting their\neffectiveness in legal reasoning. To address these issues, we present JUREX-4E,\nan expert-annotated four-element knowledge base covering 155 criminal charges.\nThe annotations follow a progressive hierarchical framework grounded in legal\nsource validity and incorporate diverse interpretive methods to ensure\nprecision and authority. We evaluate JUREX-4E on the Similar Charge\nDisambiguation task and apply it to Legal Case Retrieval. Experimental results\nvalidate the high quality of JUREX-4E and its substantial impact on downstream\nlegal tasks, underscoring its potential for advancing legal AI applications.\nThe dataset and code are available at: https://github.com/THUlawtech/JUREX",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have been widely applied to\nlegal tasks. To enhance their understanding of legal texts and improve\nreasoning accuracy, a promising approach is to incorporate legal theories. One\nof the most widely adopted theories is the Four-Element Theory (FET), which\ndefines the crime constitution through four elements: Subject, Object,\nSubjective Aspect, and Objective Aspect. While recent work has explored\nprompting LLMs to follow FET, our evaluation demonstrates that LLM-generated\nfour-elements are often incomplete and less representative, limiting their\neffectiveness in legal reasoning. To address these issues, we present JUREX-4E,\nan expert-annotated four-element knowledge base covering 155 criminal charges.\nThe annotations follow a progressive hierarchical framework grounded in legal\nsource validity and incorporate diverse interpretive methods to ensure\nprecision and authority. We evaluate JUREX-4E on the Similar Charge\nDisambiguation task and apply it to Legal Case Retrieval. Experimental results\nvalidate the high quality of JUREX-4E and its substantial impact on downstream\nlegal tasks, underscoring its potential for advancing legal AI applications.\nThe dataset and code are available at: https://github.com/THUlawtech/JUREX"
                },
                "authors": [
                    {
                        "name": "Huanghai Liu"
                    },
                    {
                        "name": "Quzhe Huang"
                    },
                    {
                        "name": "Qingjing Chen"
                    },
                    {
                        "name": "Yiran Hu"
                    },
                    {
                        "name": "Jiayu Ma"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "Weixing Shen"
                    },
                    {
                        "name": "Yansong Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Feng"
                },
                "author": "Yansong Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21212v1",
                "updated": "2025-09-25T14:21:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    21,
                    44,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:21:44Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    21,
                    44,
                    3,
                    268,
                    0
                ],
                "title": "SGMem: Sentence Graph Memory for Long-Term Conversational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGMem: Sentence Graph Memory for Long-Term Conversational Agents"
                },
                "summary": "Long-term conversational agents require effective memory management to handle\ndialogue histories that exceed the context window of large language models\n(LLMs). Existing methods based on fact extraction or summarization reduce\nredundancy but struggle to organize and retrieve relevant information across\ndifferent granularities of dialogue and generated memory. We introduce SGMem\n(Sentence Graph Memory), which represents dialogue as sentence-level graphs\nwithin chunked units, capturing associations across turn-, round-, and\nsession-level contexts. By combining retrieved raw dialogue with generated\nmemory such as summaries, facts and insights, SGMem supplies LLMs with coherent\nand relevant context for response generation. Experiments on LongMemEval and\nLoCoMo show that SGMem consistently improves accuracy and outperforms strong\nbaselines in long-term conversational question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-term conversational agents require effective memory management to handle\ndialogue histories that exceed the context window of large language models\n(LLMs). Existing methods based on fact extraction or summarization reduce\nredundancy but struggle to organize and retrieve relevant information across\ndifferent granularities of dialogue and generated memory. We introduce SGMem\n(Sentence Graph Memory), which represents dialogue as sentence-level graphs\nwithin chunked units, capturing associations across turn-, round-, and\nsession-level contexts. By combining retrieved raw dialogue with generated\nmemory such as summaries, facts and insights, SGMem supplies LLMs with coherent\nand relevant context for response generation. Experiments on LongMemEval and\nLoCoMo show that SGMem consistently improves accuracy and outperforms strong\nbaselines in long-term conversational question answering."
                },
                "authors": [
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Yongyue Zhang"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "19 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22337v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22337v2",
                "updated": "2025-09-25T14:21:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    21,
                    40,
                    3,
                    268,
                    0
                ],
                "published": "2025-07-30T02:44:20Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    2,
                    44,
                    20,
                    2,
                    211,
                    0
                ],
                "title": "A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers"
                },
                "summary": "Understanding and solving complex reasoning tasks is vital for addressing the\ninformation needs of a user. Although dense neural models learn contextualised\nembeddings, they still underperform on queries containing negation. To\nunderstand this phenomenon, we study negation in both traditional neural\ninformation retrieval and LLM-based models. We (1) introduce a taxonomy of\nnegation that derives from philosophical, linguistic, and logical definitions;\n(2) generate two benchmark datasets that can be used to evaluate the\nperformance of neural information retrieval models and to fine-tune models for\na more robust performance on negation; and (3) propose a logic-based\nclassification mechanism that can be used to analyze the performance of\nretrieval models on existing datasets. Our taxonomy produces a balanced data\ndistribution over negation types, providing a better training setup that leads\nto faster convergence on the NevIR dataset. Moreover, we propose a\nclassification schema that reveals the coverage of negation types in existing\ndatasets, offering insights into the factors that might affect the\ngeneralization of fine-tuned models on negation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and solving complex reasoning tasks is vital for addressing the\ninformation needs of a user. Although dense neural models learn contextualised\nembeddings, they still underperform on queries containing negation. To\nunderstand this phenomenon, we study negation in both traditional neural\ninformation retrieval and LLM-based models. We (1) introduce a taxonomy of\nnegation that derives from philosophical, linguistic, and logical definitions;\n(2) generate two benchmark datasets that can be used to evaluate the\nperformance of neural information retrieval models and to fine-tune models for\na more robust performance on negation; and (3) propose a logic-based\nclassification mechanism that can be used to analyze the performance of\nretrieval models on existing datasets. Our taxonomy produces a balanced data\ndistribution over negation types, providing a better training setup that leads\nto faster convergence on the NevIR dataset. Moreover, we propose a\nclassification schema that reveals the coverage of negation types in existing\ndatasets, offering insights into the factors that might affect the\ngeneralization of fine-tuned models on negation."
                },
                "authors": [
                    {
                        "name": "Roxana Petcu"
                    },
                    {
                        "name": "Samarth Bhargav"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos Kanoulas"
                },
                "author": "Evangelos Kanoulas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22337v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22337v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21211v1",
                "updated": "2025-09-25T14:21:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    21,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:21:16Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    21,
                    16,
                    3,
                    268,
                    0
                ],
                "title": "Evading Overlapping Community Detection via Proxy Node Injection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evading Overlapping Community Detection via Proxy Node Injection"
                },
                "summary": "Protecting privacy in social graphs requires preventing sensitive\ninformation, such as community affiliations, from being inferred by graph\nanalysis, without substantially altering the graph topology. We address this\nthrough the problem of \\emph{community membership hiding} (CMH), which seeks\nedge modifications that cause a target node to exit its original community,\nregardless of the detection algorithm employed. Prior work has focused on\nnon-overlapping community detection, where trivial strategies often suffice,\nbut real-world graphs are better modeled by overlapping communities, where such\nstrategies fail. To the best of our knowledge, we are the first to formalize\nand address CMH in this setting. In this work, we propose a deep reinforcement\nlearning (DRL) approach that learns effective modification policies, including\nthe use of proxy nodes, while preserving graph structure. Experiments on\nreal-world datasets show that our method significantly outperforms existing\nbaselines in both effectiveness and efficiency, offering a principled tool for\nprivacy-preserving graph modification with overlapping communities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Protecting privacy in social graphs requires preventing sensitive\ninformation, such as community affiliations, from being inferred by graph\nanalysis, without substantially altering the graph topology. We address this\nthrough the problem of \\emph{community membership hiding} (CMH), which seeks\nedge modifications that cause a target node to exit its original community,\nregardless of the detection algorithm employed. Prior work has focused on\nnon-overlapping community detection, where trivial strategies often suffice,\nbut real-world graphs are better modeled by overlapping communities, where such\nstrategies fail. To the best of our knowledge, we are the first to formalize\nand address CMH in this setting. In this work, we propose a deep reinforcement\nlearning (DRL) approach that learns effective modification policies, including\nthe use of proxy nodes, while preserving graph structure. Experiments on\nreal-world datasets show that our method significantly outperforms existing\nbaselines in both effectiveness and efficiency, offering a principled tool for\nprivacy-preserving graph modification with overlapping communities."
                },
                "authors": [
                    {
                        "name": "Dario Loi"
                    },
                    {
                        "name": "Matteo Silvestri"
                    },
                    {
                        "name": "Fabrizio Silvestri"
                    },
                    {
                        "name": "Gabriele Tolomei"
                    }
                ],
                "author_detail": {
                    "name": "Gabriele Tolomei"
                },
                "author": "Gabriele Tolomei",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.8; G.2.2; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21208v1",
                "updated": "2025-09-25T14:19:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    19,
                    51,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:19:51Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    19,
                    51,
                    3,
                    268,
                    0
                ],
                "title": "CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A\n  Fine-grained Corpus and Reasoning Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A\n  Fine-grained Corpus and Reasoning Analysis"
                },
                "summary": "Large Language Models (LLMs) are increasingly tasked with analyzing legal\ntexts and citing relevant statutes, yet their reliability is often compromised\nby general pre-training that ingests legal texts without specialized focus,\nobscuring the true depth of their legal knowledge. This paper introduces CLaw,\na novel benchmark specifically engineered to meticulously evaluate LLMs on\nChinese legal knowledge and its application in reasoning. CLaw comprises two\nkey components: (1) a comprehensive, fine-grained corpus of all 306 Chinese\nnational statutes, segmented to the subparagraph level and incorporating\nprecise historical revision timesteps for rigorous recall evaluation (64,849\nentries), and (2) a challenging set of 254 case-based reasoning instances\nderived from China Supreme Court curated materials to assess the practical\napplication of legal knowledge. Our empirical evaluation reveals that most\ncontemporary LLMs significantly struggle to faithfully reproduce legal\nprovisions. As accurate retrieval and citation of legal provisions form the\nbasis of legal reasoning, this deficiency critically undermines the reliability\nof their responses. We contend that achieving trustworthy legal reasoning in\nLLMs requires a robust synergy of accurate knowledge retrieval--potentially\nenhanced through supervised fine-tuning (SFT) or retrieval-augmented generation\n(RAG)--and strong general reasoning capabilities. This work provides an\nessential benchmark and critical insights for advancing domain-specific LLM\nreasoning, particularly within the complex legal sphere.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly tasked with analyzing legal\ntexts and citing relevant statutes, yet their reliability is often compromised\nby general pre-training that ingests legal texts without specialized focus,\nobscuring the true depth of their legal knowledge. This paper introduces CLaw,\na novel benchmark specifically engineered to meticulously evaluate LLMs on\nChinese legal knowledge and its application in reasoning. CLaw comprises two\nkey components: (1) a comprehensive, fine-grained corpus of all 306 Chinese\nnational statutes, segmented to the subparagraph level and incorporating\nprecise historical revision timesteps for rigorous recall evaluation (64,849\nentries), and (2) a challenging set of 254 case-based reasoning instances\nderived from China Supreme Court curated materials to assess the practical\napplication of legal knowledge. Our empirical evaluation reveals that most\ncontemporary LLMs significantly struggle to faithfully reproduce legal\nprovisions. As accurate retrieval and citation of legal provisions form the\nbasis of legal reasoning, this deficiency critically undermines the reliability\nof their responses. We contend that achieving trustworthy legal reasoning in\nLLMs requires a robust synergy of accurate knowledge retrieval--potentially\nenhanced through supervised fine-tuning (SFT) or retrieval-augmented generation\n(RAG)--and strong general reasoning capabilities. This work provides an\nessential benchmark and critical insights for advancing domain-specific LLM\nreasoning, particularly within the complex legal sphere."
                },
                "authors": [
                    {
                        "name": "Xinzhe Xu"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Chen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chen Chen"
                },
                "author": "Chen Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18847v2",
                "updated": "2025-09-25T14:17:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    17,
                    18,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-23T09:35:49Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    9,
                    35,
                    49,
                    1,
                    266,
                    0
                ],
                "title": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured\n  Reflection for Reliable Tool Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured\n  Reflection for Reliable Tool Interactions"
                },
                "summary": "Tool-augmented large language models (LLMs) are usually trained with\nsupervised imitation or coarse-grained reinforcement learning that optimizes\nsingle tool calls. Current self-reflection practices rely on heuristic prompts\nor one-way reasoning: the model is urged to 'think more' instead of learning\nerror diagnosis and repair. This is fragile in multi-turn interactions; after a\nfailure the model often repeats the same mistake. We propose structured\nreflection, which turns the path from error to repair into an explicit,\ncontrollable, and trainable action. The agent produces a short yet precise\nreflection: it diagnoses the failure using evidence from the previous step and\nthen proposes a correct, executable follow-up call. For training we combine\nDAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing\nthe stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce\nTool-Reflection-Bench, a lightweight benchmark that programmatically checks\nstructural validity, executability, parameter correctness, and result\nconsistency. Tasks are built as mini trajectories of erroneous call,\nreflection, and corrected call, with disjoint train and test splits.\nExperiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn\ntool-call success and error recovery, and a reduction of redundant calls. These\nresults indicate that making reflection explicit and optimizing it directly\nimproves the reliability of tool interaction and offers a reproducible path for\nagents to learn from failure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-augmented large language models (LLMs) are usually trained with\nsupervised imitation or coarse-grained reinforcement learning that optimizes\nsingle tool calls. Current self-reflection practices rely on heuristic prompts\nor one-way reasoning: the model is urged to 'think more' instead of learning\nerror diagnosis and repair. This is fragile in multi-turn interactions; after a\nfailure the model often repeats the same mistake. We propose structured\nreflection, which turns the path from error to repair into an explicit,\ncontrollable, and trainable action. The agent produces a short yet precise\nreflection: it diagnoses the failure using evidence from the previous step and\nthen proposes a correct, executable follow-up call. For training we combine\nDAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing\nthe stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce\nTool-Reflection-Bench, a lightweight benchmark that programmatically checks\nstructural validity, executability, parameter correctness, and result\nconsistency. Tasks are built as mini trajectories of erroneous call,\nreflection, and corrected call, with disjoint train and test splits.\nExperiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn\ntool-call success and error recovery, and a reduction of redundant calls. These\nresults indicate that making reflection explicit and optimizing it directly\nimproves the reliability of tool interaction and offers a reproducible path for\nagents to learn from failure."
                },
                "authors": [
                    {
                        "name": "Junhao Su"
                    },
                    {
                        "name": "Yuanliang Wan"
                    },
                    {
                        "name": "Junwei Yang"
                    },
                    {
                        "name": "Hengyu Shi"
                    },
                    {
                        "name": "Tianyang Han"
                    },
                    {
                        "name": "Junfeng Luo"
                    },
                    {
                        "name": "Yurui Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Yurui Qiu"
                },
                "author": "Yurui Qiu",
                "arxiv_comment": "27pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21201v1",
                "updated": "2025-09-25T14:12:03Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    12,
                    3,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:12:03Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    12,
                    3,
                    3,
                    268,
                    0
                ],
                "title": "Hybrid RIS-Aided Digital Over-the-Air Computing for Edge AI Inference:\n  Joint Feature Quantization and Active-Passive Beamforming Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid RIS-Aided Digital Over-the-Air Computing for Edge AI Inference:\n  Joint Feature Quantization and Active-Passive Beamforming Design"
                },
                "summary": "The vision of 6G networks aims to enable edge inference by leveraging\nubiquitously deployed artificial intelligence (AI) models, facilitating\nintelligent environmental perception for a wide range of applications. A\ncritical operation in edge inference is for an edge node (EN) to aggregate\nmulti-view sensory features extracted by distributed agents, thereby boosting\nperception accuracy. Over-the-air computing (AirComp) emerges as a promising\ntechnique for rapid feature aggregation by exploiting the waveform\nsuperposition property of analog-modulated signals, which is, however,\nincompatible with existing digital communication systems. Meanwhile, hybrid\nreconfigurable intelligent surface (RIS), a novel RIS architecture capable of\nsimultaneous signal amplification and reflection, exhibits potential for\nenhancing AirComp. Therefore, this paper proposes a Hybrid RIS-aided Digital\nAirComp (HRD-AirComp) scheme, which employs vector quantization to map\nhigh-dimensional features into discrete codewords that are digitally modulated\ninto symbols for wireless transmission. By judiciously adjusting the AirComp\ntransceivers and hybrid RIS reflection to control signal superposition across\nagents, the EN can estimate the aggregated features from the received signals.\nTo endow HRD-AirComp with a task-oriented design principle, we derive a\nsurrogate function for inference accuracy that characterizes the impact of\nfeature quantization and over-the-air aggregation. Based on this surrogate, we\nformulate an optimization problem targeting inference accuracy maximization,\nand develop an efficient algorithm to jointly optimize the quantization bit\nallocation, agent transmission coefficients, EN receiving beamforming, and\nhybrid RIS reflection beamforming. Experimental results demonstrate that the\nproposed HRD-AirComp outperforms baselines in terms of both inference accuracy\nand uncertainty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vision of 6G networks aims to enable edge inference by leveraging\nubiquitously deployed artificial intelligence (AI) models, facilitating\nintelligent environmental perception for a wide range of applications. A\ncritical operation in edge inference is for an edge node (EN) to aggregate\nmulti-view sensory features extracted by distributed agents, thereby boosting\nperception accuracy. Over-the-air computing (AirComp) emerges as a promising\ntechnique for rapid feature aggregation by exploiting the waveform\nsuperposition property of analog-modulated signals, which is, however,\nincompatible with existing digital communication systems. Meanwhile, hybrid\nreconfigurable intelligent surface (RIS), a novel RIS architecture capable of\nsimultaneous signal amplification and reflection, exhibits potential for\nenhancing AirComp. Therefore, this paper proposes a Hybrid RIS-aided Digital\nAirComp (HRD-AirComp) scheme, which employs vector quantization to map\nhigh-dimensional features into discrete codewords that are digitally modulated\ninto symbols for wireless transmission. By judiciously adjusting the AirComp\ntransceivers and hybrid RIS reflection to control signal superposition across\nagents, the EN can estimate the aggregated features from the received signals.\nTo endow HRD-AirComp with a task-oriented design principle, we derive a\nsurrogate function for inference accuracy that characterizes the impact of\nfeature quantization and over-the-air aggregation. Based on this surrogate, we\nformulate an optimization problem targeting inference accuracy maximization,\nand develop an efficient algorithm to jointly optimize the quantization bit\nallocation, agent transmission coefficients, EN receiving beamforming, and\nhybrid RIS reflection beamforming. Experimental results demonstrate that the\nproposed HRD-AirComp outperforms baselines in terms of both inference accuracy\nand uncertainty."
                },
                "authors": [
                    {
                        "name": "Yang Fu"
                    },
                    {
                        "name": "Peng Qin"
                    },
                    {
                        "name": "Liming Chen"
                    },
                    {
                        "name": "Yifei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Wang"
                },
                "author": "Yifei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01445v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01445v2",
                "updated": "2025-09-25T14:11:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    11,
                    59,
                    3,
                    268,
                    0
                ],
                "published": "2025-04-02T07:56:39Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    56,
                    39,
                    2,
                    92,
                    0
                ],
                "title": "Compositional-ARC: Assessing Systematic Generalization in Abstract\n  Spatial Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional-ARC: Assessing Systematic Generalization in Abstract\n  Spatial Reasoning"
                },
                "summary": "Systematic generalization refers to the capacity to understand and generate\nnovel combinations from known components. Despite recent progress by large\nlanguage models (LLMs) across various domains, these models often fail to\nextend their knowledge to novel compositional scenarios, revealing notable\nlimitations in systematic generalization. There has been an ongoing debate\nabout whether neural networks possess the capacity for systematic\ngeneralization, with recent studies suggesting that meta-learning approaches\ndesigned for compositionality can significantly enhance this ability. However,\nthese insights have largely been confined to linguistic problems, leaving their\napplicability to other tasks an open question. In this study, we extend\nmeta-learning for compositionality to the domain of abstract spatial reasoning.\nTo this end, we introduce $\\textit{Compositional-ARC}-$a dataset designed to\nevaluate the capacity of models to systematically generalize from known\ngeometric transformations (e.g., translation, rotation) of abstract\ntwo-dimensional objects to novel combinations of these transformations (e.g.,\ntranslation+rotation). Our results show that a small transformer-based\nencoder-decoder model, trained via meta-learning for compositionality, can\nsystematically generalize to previously unseen transformation compositions.\nNotably, despite having only 5.7M parameters, this model significantly\noutperforms state-of-the-art LLMs$-$including o3-mini, GPT-4o, and Gemini 2.0\nFlash, which fail to exhibit similar systematic behavior$-$and performs on par\nwith the winning model of the ARC prize 2024, an 8B-parameter LLM trained via\ntest-time training. Our findings highlight the effectiveness of meta-learning\nin promoting systematicity beyond linguistic tasks, suggesting a promising\ndirection toward more robust and generalizable models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic generalization refers to the capacity to understand and generate\nnovel combinations from known components. Despite recent progress by large\nlanguage models (LLMs) across various domains, these models often fail to\nextend their knowledge to novel compositional scenarios, revealing notable\nlimitations in systematic generalization. There has been an ongoing debate\nabout whether neural networks possess the capacity for systematic\ngeneralization, with recent studies suggesting that meta-learning approaches\ndesigned for compositionality can significantly enhance this ability. However,\nthese insights have largely been confined to linguistic problems, leaving their\napplicability to other tasks an open question. In this study, we extend\nmeta-learning for compositionality to the domain of abstract spatial reasoning.\nTo this end, we introduce $\\textit{Compositional-ARC}-$a dataset designed to\nevaluate the capacity of models to systematically generalize from known\ngeometric transformations (e.g., translation, rotation) of abstract\ntwo-dimensional objects to novel combinations of these transformations (e.g.,\ntranslation+rotation). Our results show that a small transformer-based\nencoder-decoder model, trained via meta-learning for compositionality, can\nsystematically generalize to previously unseen transformation compositions.\nNotably, despite having only 5.7M parameters, this model significantly\noutperforms state-of-the-art LLMs$-$including o3-mini, GPT-4o, and Gemini 2.0\nFlash, which fail to exhibit similar systematic behavior$-$and performs on par\nwith the winning model of the ARC prize 2024, an 8B-parameter LLM trained via\ntest-time training. Our findings highlight the effectiveness of meta-learning\nin promoting systematicity beyond linguistic tasks, suggesting a promising\ndirection toward more robust and generalizable models."
                },
                "authors": [
                    {
                        "name": "Philipp Mondorf"
                    },
                    {
                        "name": "Shijia Zhou"
                    },
                    {
                        "name": "Monica Riedler"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "arxiv_comment": "29 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01445v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01445v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21199v1",
                "updated": "2025-09-25T14:11:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    11,
                    57,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:11:57Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    11,
                    57,
                    3,
                    268,
                    0
                ],
                "title": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in\n  Multi-Hop QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in\n  Multi-Hop QA"
                },
                "summary": "Multi-Hop Question Answering (MHQA) requires integrating dispersed,\ninterdependent evidence through sequential reasoning under noise. This task is\nchallenging for LLMs as they have a finite per-pass output capacity, beyond\nwhich the integration of task-relevant evidence proves unreliable.\nConsequently, the single-pass reasoning paradigm is inherently vulnerable to\nthis capacity overflow. To formalize this bottleneck, our analysis establishes\na Fano-style accuracy upper bound, defining a theoretical performance ceiling\nfor single-pass LLMs. This bound reveals that accuracy inevitably collapses\nonce task complexity exceeds model capacity, providing general principles for\ncapacity-aware representation and structuring of MHQA in LLMs. Building on\nthese principles, we introduce a proof-of-concept multi-call framework for\nMHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware\ntask decomposition with active pruning of prior reasoning traces, keeping the\ninformation load within the single-pass limit. It further achieves robustness\nby a dependency-explicit workflow that enables precise control over the\nreasoning path. We construct a stringent and noise-rich benchmark to validate\nour theory and framework. Experimental results show that model behavior aligns\nwith our predicted capacity curves while InfoQA achieves consistent performance\nimprovements. We hope our work inspires more LLM multi-step reasoning methods:\n\\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Hop Question Answering (MHQA) requires integrating dispersed,\ninterdependent evidence through sequential reasoning under noise. This task is\nchallenging for LLMs as they have a finite per-pass output capacity, beyond\nwhich the integration of task-relevant evidence proves unreliable.\nConsequently, the single-pass reasoning paradigm is inherently vulnerable to\nthis capacity overflow. To formalize this bottleneck, our analysis establishes\na Fano-style accuracy upper bound, defining a theoretical performance ceiling\nfor single-pass LLMs. This bound reveals that accuracy inevitably collapses\nonce task complexity exceeds model capacity, providing general principles for\ncapacity-aware representation and structuring of MHQA in LLMs. Building on\nthese principles, we introduce a proof-of-concept multi-call framework for\nMHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware\ntask decomposition with active pruning of prior reasoning traces, keeping the\ninformation load within the single-pass limit. It further achieves robustness\nby a dependency-explicit workflow that enables precise control over the\nreasoning path. We construct a stringent and noise-rich benchmark to validate\nour theory and framework. Experimental results show that model behavior aligns\nwith our predicted capacity curves while InfoQA achieves consistent performance\nimprovements. We hope our work inspires more LLM multi-step reasoning methods:\n\\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}."
                },
                "authors": [
                    {
                        "name": "Kaiyang Wan"
                    },
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Honglin Mu"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "21 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02413v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02413v2",
                "updated": "2025-09-25T14:11:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    11,
                    51,
                    3,
                    268,
                    0
                ],
                "published": "2025-06-03T03:58:32Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    3,
                    58,
                    32,
                    1,
                    154,
                    0
                ],
                "title": "Tensor State Space-based Dynamic Multilayer Network Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor State Space-based Dynamic Multilayer Network Modeling"
                },
                "summary": "Understanding the complex interactions within dynamic multilayer networks is\ncritical for advancements in various scientific domains. Existing models often\nfail to capture such networks' temporal and cross-layer dynamics. This paper\nintroduces a novel Tensor State Space Model for Dynamic Multilayer Networks\n(TSSDMN), utilizing a latent space model framework. TSSDMN employs a symmetric\nTucker decomposition to represent latent node features, their interaction\npatterns, and layer transitions. Then by fixing the latent features and\nallowing the interaction patterns to evolve over time, TSSDMN uniquely captures\nboth the temporal dynamics within layers and across different layers. The model\nidentifiability conditions are discussed. By treating latent features as\nvariables whose posterior distributions are approximated using a mean-field\nvariational inference approach, a variational Expectation Maximization\nalgorithm is developed for efficient model inference. Numerical simulations and\ncase studies demonstrate the efficacy of TSSDMN for understanding dynamic\nmultilayer networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the complex interactions within dynamic multilayer networks is\ncritical for advancements in various scientific domains. Existing models often\nfail to capture such networks' temporal and cross-layer dynamics. This paper\nintroduces a novel Tensor State Space Model for Dynamic Multilayer Networks\n(TSSDMN), utilizing a latent space model framework. TSSDMN employs a symmetric\nTucker decomposition to represent latent node features, their interaction\npatterns, and layer transitions. Then by fixing the latent features and\nallowing the interaction patterns to evolve over time, TSSDMN uniquely captures\nboth the temporal dynamics within layers and across different layers. The model\nidentifiability conditions are discussed. By treating latent features as\nvariables whose posterior distributions are approximated using a mean-field\nvariational inference approach, a variational Expectation Maximization\nalgorithm is developed for efficient model inference. Numerical simulations and\ncase studies demonstrate the efficacy of TSSDMN for understanding dynamic\nmultilayer networks."
                },
                "authors": [
                    {
                        "name": "Tian Lan"
                    },
                    {
                        "name": "Jie Guo"
                    },
                    {
                        "name": "Chen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhang"
                },
                "author": "Chen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02413v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02413v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18672v2",
                "updated": "2025-09-25T14:09:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    9,
                    33,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-26T04:31:28Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    4,
                    31,
                    28,
                    1,
                    238,
                    0
                ],
                "title": "Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning\n  Tasks"
                },
                "summary": "Empirical scaling laws have driven the evolution of large language models\n(LLMs), yet their coefficients shift whenever the model architecture or data\npipeline changes. Mixture-of-Experts (MoE) models, now standard in\nstate-of-the-art systems, introduce a new sparsity dimension that current\ndense-model frontiers overlook. We investigate how MoE sparsity influences two\ndistinct capability regimes: memorization skills and reasoning skills. By\ntraining MoE families that vary total parameters, active parameters, and\ntop-$k$ routing under fixed compute budgets, we disentangle pre-training loss\nfrom downstream accuracy. Our results reveal two principles. First, Active\nFLOPs: models with identical training loss but greater active compute achieve\nhigher reasoning accuracy. Second, Total tokens per parameter (TPP):\nmemorization tasks improve with more parameters, while reasoning tasks benefit\nfrom optimal TPP, indicating that reasoning is data-hungry. Neither\nreinforcement learning post-training (GRPO) nor increased test-time compute\nalters these trends. We therefore argue that optimal MoE sparsity must be\ndetermined jointly by active FLOPs and TPP, revising the classical picture of\ncompute-optimal scaling. Our model checkpoints, code and logs are open-source\nat https://github.com/rioyokotalab/optimal-sparsity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical scaling laws have driven the evolution of large language models\n(LLMs), yet their coefficients shift whenever the model architecture or data\npipeline changes. Mixture-of-Experts (MoE) models, now standard in\nstate-of-the-art systems, introduce a new sparsity dimension that current\ndense-model frontiers overlook. We investigate how MoE sparsity influences two\ndistinct capability regimes: memorization skills and reasoning skills. By\ntraining MoE families that vary total parameters, active parameters, and\ntop-$k$ routing under fixed compute budgets, we disentangle pre-training loss\nfrom downstream accuracy. Our results reveal two principles. First, Active\nFLOPs: models with identical training loss but greater active compute achieve\nhigher reasoning accuracy. Second, Total tokens per parameter (TPP):\nmemorization tasks improve with more parameters, while reasoning tasks benefit\nfrom optimal TPP, indicating that reasoning is data-hungry. Neither\nreinforcement learning post-training (GRPO) nor increased test-time compute\nalters these trends. We therefore argue that optimal MoE sparsity must be\ndetermined jointly by active FLOPs and TPP, revising the classical picture of\ncompute-optimal scaling. Our model checkpoints, code and logs are open-source\nat https://github.com/rioyokotalab/optimal-sparsity."
                },
                "authors": [
                    {
                        "name": "Taishi Nakamura"
                    },
                    {
                        "name": "Satoki Ishikawa"
                    },
                    {
                        "name": "Masaki Kawamura"
                    },
                    {
                        "name": "Takumi Okamoto"
                    },
                    {
                        "name": "Daisuke Nohara"
                    },
                    {
                        "name": "Jun Suzuki"
                    },
                    {
                        "name": "Rio Yokota"
                    }
                ],
                "author_detail": {
                    "name": "Rio Yokota"
                },
                "author": "Rio Yokota",
                "arxiv_comment": "Presented at the Second AI for Math Workshop at ICML",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21193v1",
                "updated": "2025-09-25T14:05:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    5,
                    55,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:05:55Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    5,
                    55,
                    3,
                    268,
                    0
                ],
                "title": "Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for\n  Scientific Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for\n  Scientific Reasoning"
                },
                "summary": "Large language models (LLMs) have recently shown strong progress on\nscientific reasoning, yet two major bottlenecks remain. First, explicit\nretrieval fragments reasoning, imposing a hidden \"tool tax\" of extra tokens and\nsteps. Second, multi-agent pipelines often dilute strong solutions by averaging\nacross all candidates. We address these challenges with a unified framework\nthat combines implicit retrieval and structured collaboration. At its\nfoundation, a Monitor-based retrieval module operates at the token level,\nintegrating external knowledge with minimal disruption to reasoning. On top of\nthis substrate, Hierarchical Solution Refinement (HSR) iteratively designates\neach candidate as an anchor to be repaired by its peers, while Quality-Aware\nIterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's\nLast Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the\nhighest reported to date, surpassing the strongest agent baseline by 13.4\npoints and leading frontier LLMs by up to 18.1 points, while simultaneously\nreducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA\nand TRQA confirm robustness across domains. Error analysis shows that reasoning\nfailures and knowledge gaps co-occur in over 85\\% of cases, while diversity\nanalysis reveals a clear dichotomy: retrieval tasks benefit from solution\nvariety, whereas reasoning tasks favor consensus. Together, these findings\ndemonstrate how implicit augmentation and structured refinement overcome the\ninefficiencies of explicit tool use and uniform aggregation. Code is available\nat: https://github.com/tangxiangru/Eigen-1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently shown strong progress on\nscientific reasoning, yet two major bottlenecks remain. First, explicit\nretrieval fragments reasoning, imposing a hidden \"tool tax\" of extra tokens and\nsteps. Second, multi-agent pipelines often dilute strong solutions by averaging\nacross all candidates. We address these challenges with a unified framework\nthat combines implicit retrieval and structured collaboration. At its\nfoundation, a Monitor-based retrieval module operates at the token level,\nintegrating external knowledge with minimal disruption to reasoning. On top of\nthis substrate, Hierarchical Solution Refinement (HSR) iteratively designates\neach candidate as an anchor to be repaired by its peers, while Quality-Aware\nIterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's\nLast Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the\nhighest reported to date, surpassing the strongest agent baseline by 13.4\npoints and leading frontier LLMs by up to 18.1 points, while simultaneously\nreducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA\nand TRQA confirm robustness across domains. Error analysis shows that reasoning\nfailures and knowledge gaps co-occur in over 85\\% of cases, while diversity\nanalysis reveals a clear dichotomy: retrieval tasks benefit from solution\nvariety, whereas reasoning tasks favor consensus. Together, these findings\ndemonstrate how implicit augmentation and structured refinement overcome the\ninefficiencies of explicit tool use and uniform aggregation. Code is available\nat: https://github.com/tangxiangru/Eigen-1."
                },
                "authors": [
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Wanghan Xu"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Zijie Guo"
                    },
                    {
                        "name": "Daniel Shao"
                    },
                    {
                        "name": "Jiapeng Chen"
                    },
                    {
                        "name": "Cixuan Zhang"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Lixin Zhang"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Wenlong Zhang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Di Jin"
                    }
                ],
                "author_detail": {
                    "name": "Di Jin"
                },
                "author": "Di Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21192v1",
                "updated": "2025-09-25T14:05:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    5,
                    47,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:05:47Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    5,
                    47,
                    3,
                    268,
                    0
                ],
                "title": "GEP: A GCG-Based method for extracting personally identifiable\n  information from chatbots built on small language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEP: A GCG-Based method for extracting personally identifiable\n  information from chatbots built on small language models"
                },
                "summary": "Small language models (SLMs) become unprecedentedly appealing due to their\napproximately equivalent performance compared to large language models (LLMs)\nin certain fields with less energy and time consumption during training and\ninference. However, the personally identifiable information (PII) leakage of\nSLMs for downstream tasks has yet to be explored. In this study, we investigate\nthe PII leakage of the chatbot based on SLM. We first finetune a new chatbot,\ni.e., ChatBioGPT based on the backbone of BioGPT using medical datasets Alpaca\nand HealthCareMagic. It shows a matchable performance in BERTscore compared\nwith previous studies of ChatDoctor and ChatGPT. Based on this model, we prove\nthat the previous template-based PII attacking methods cannot effectively\nextract the PII in the dataset for leakage detection under the SLM condition.\nWe then propose GEP, which is a greedy coordinate gradient-based (GCG) method\nspecifically designed for PII extraction. We conduct experimental studies of\nGEP and the results show an increment of up to 60$\\times$ more leakage compared\nwith the previous template-based methods. We further expand the capability of\nGEP in the case of a more complicated and realistic situation by conducting\nfree-style insertion where the inserted PII in the dataset is in the form of\nvarious syntactic expressions instead of fixed templates, and GEP is still able\nto reveal a PII leakage rate of up to 4.53%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small language models (SLMs) become unprecedentedly appealing due to their\napproximately equivalent performance compared to large language models (LLMs)\nin certain fields with less energy and time consumption during training and\ninference. However, the personally identifiable information (PII) leakage of\nSLMs for downstream tasks has yet to be explored. In this study, we investigate\nthe PII leakage of the chatbot based on SLM. We first finetune a new chatbot,\ni.e., ChatBioGPT based on the backbone of BioGPT using medical datasets Alpaca\nand HealthCareMagic. It shows a matchable performance in BERTscore compared\nwith previous studies of ChatDoctor and ChatGPT. Based on this model, we prove\nthat the previous template-based PII attacking methods cannot effectively\nextract the PII in the dataset for leakage detection under the SLM condition.\nWe then propose GEP, which is a greedy coordinate gradient-based (GCG) method\nspecifically designed for PII extraction. We conduct experimental studies of\nGEP and the results show an increment of up to 60$\\times$ more leakage compared\nwith the previous template-based methods. We further expand the capability of\nGEP in the case of a more complicated and realistic situation by conducting\nfree-style insertion where the inserted PII in the dataset is in the form of\nvarious syntactic expressions instead of fixed templates, and GEP is still able\nto reveal a PII leakage rate of up to 4.53%."
                },
                "authors": [
                    {
                        "name": "Jieli Zhu"
                    },
                    {
                        "name": "Vi Ngoc-Nha Tran"
                    }
                ],
                "author_detail": {
                    "name": "Vi Ngoc-Nha Tran"
                },
                "author": "Vi Ngoc-Nha Tran",
                "arxiv_comment": "16 pages, 5 figures, 4 tables. Under review as a conference paper at\n  ICLR 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21175v1",
                "updated": "2025-09-25T13:56:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    56,
                    56,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:56:56Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    56,
                    56,
                    3,
                    268,
                    0
                ],
                "title": "Who's Laughing Now? An Overview of Computational Humour Generation and\n  Explanation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who's Laughing Now? An Overview of Computational Humour Generation and\n  Explanation"
                },
                "summary": "The creation and perception of humour is a fundamental human trait,\npositioning its computational understanding as one of the most challenging\ntasks in natural language processing (NLP). As an abstract, creative, and\nfrequently context-dependent construct, humour requires extensive reasoning to\nunderstand and create, making it a pertinent task for assessing the\ncommon-sense knowledge and reasoning abilities of modern large language models\n(LLMs). In this work, we survey the landscape of computational humour as it\npertains to the generative tasks of creation and explanation. We observe that,\ndespite the task of understanding humour bearing all the hallmarks of a\nfoundational NLP task, work on generating and explaining humour beyond puns\nremains sparse, while state-of-the-art models continue to fall short of human\ncapabilities. We bookend our literature survey by motivating the importance of\ncomputational humour processing as a subdiscipline of NLP and presenting an\nextensive discussion of future directions for research in the area that takes\ninto account the subjective and ethically ambiguous nature of humour.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The creation and perception of humour is a fundamental human trait,\npositioning its computational understanding as one of the most challenging\ntasks in natural language processing (NLP). As an abstract, creative, and\nfrequently context-dependent construct, humour requires extensive reasoning to\nunderstand and create, making it a pertinent task for assessing the\ncommon-sense knowledge and reasoning abilities of modern large language models\n(LLMs). In this work, we survey the landscape of computational humour as it\npertains to the generative tasks of creation and explanation. We observe that,\ndespite the task of understanding humour bearing all the hallmarks of a\nfoundational NLP task, work on generating and explaining humour beyond puns\nremains sparse, while state-of-the-art models continue to fall short of human\ncapabilities. We bookend our literature survey by motivating the importance of\ncomputational humour processing as a subdiscipline of NLP and presenting an\nextensive discussion of future directions for research in the area that takes\ninto account the subjective and ethically ambiguous nature of humour."
                },
                "authors": [
                    {
                        "name": "Tyler Loakman"
                    },
                    {
                        "name": "William Thorne"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Accepted to INLG 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14128v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14128v2",
                "updated": "2025-09-25T13:56:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    56,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-17T16:08:46Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    8,
                    46,
                    2,
                    260,
                    0
                ],
                "title": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance\n  Models for Multilingual ASR and AST",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance\n  Models for Multilingual ASR and AST"
                },
                "summary": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters."
                },
                "authors": [
                    {
                        "name": "Monica Sekoyan"
                    },
                    {
                        "name": "Nithin Rao Koluguri"
                    },
                    {
                        "name": "Nune Tadevosyan"
                    },
                    {
                        "name": "Piotr Zelasko"
                    },
                    {
                        "name": "Travis Bartley"
                    },
                    {
                        "name": "Nikolay Karpov"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Mini Version of it Submitted to ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14128v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14128v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10568v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10568v2",
                "updated": "2025-09-25T13:55:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    55,
                    44,
                    3,
                    268,
                    0
                ],
                "published": "2025-03-13T17:19:51Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    17,
                    19,
                    51,
                    3,
                    72,
                    0
                ],
                "title": "Autoregressive Image Generation with Randomized Parallel Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive Image Generation with Randomized Parallel Decoding"
                },
                "summary": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel decoupled decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot inference tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only\n32 sampling steps, achieving over a 30 times speedup in inference and a 75\npercent reduction in memory consumption compared to representative recent\nautoregressive models at a similar scale.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce ARPG, a novel visual autoregressive model that enables\nrandomized parallel generation, addressing the inherent limitations of\nconventional raster-order approaches, which hinder inference efficiency and\nzero-shot generalization due to their sequential, predefined token generation\norder. Our key insight is that effective random-order modeling necessitates\nexplicit guidance for determining the position of the next predicted token. To\nthis end, we propose a novel decoupled decoding framework that decouples\npositional guidance from content representation, encoding them separately as\nqueries and key-value pairs. By directly incorporating this guidance into the\ncausal attention mechanism, our approach enables fully random-order training\nand generation, eliminating the need for bidirectional attention. Consequently,\nARPG readily generalizes to zero-shot inference tasks such as image inpainting,\noutpainting, and resolution expansion. Furthermore, it supports parallel\ninference by concurrently processing multiple queries using a shared KV cache.\nOn the ImageNet-1K 256 benchmark, our approach attains an FID of 1.83 with only\n32 sampling steps, achieving over a 30 times speedup in inference and a 75\npercent reduction in memory consumption compared to representative recent\nautoregressive models at a similar scale."
                },
                "authors": [
                    {
                        "name": "Haopeng Li"
                    },
                    {
                        "name": "Jinyue Yang"
                    },
                    {
                        "name": "Guoqi Li"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10568v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10568v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21170v1",
                "updated": "2025-09-25T13:51:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    51,
                    56,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:51:56Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    51,
                    56,
                    3,
                    268,
                    0
                ],
                "title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A\n  Maximum Entropy Regulated Long Chain-of-Thought Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A\n  Maximum Entropy Regulated Long Chain-of-Thought Approach"
                },
                "summary": "Large Language Models (LLMs) have shown great potential in supporting\nautomated code review due to their impressive capabilities in context\nunderstanding and reasoning. However, these capabilities are still limited\ncompared to human-level cognition because they are heavily influenced by the\ntraining data. Recent research has demonstrated significantly improved\nperformance through fine-tuning LLMs with code review data. However, compared\nto human reviewers who often simultaneously analyze multiple dimensions of code\nreview to better identify issues, the full potential of these methods is\nhampered by the limited or vague information used to fine-tune the models. This\npaper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that\ntrains LLMs with an impressive reasoning ability to analyze multiple dimensions\nof code review by harnessing long COT techniques to provide rich structured\ninformation. To address context loss and reasoning logic loss issues that\nfrequently occur when LLMs process long COT prompts, we propose a solution that\ncombines the Maximum Entropy (ME) modeling principle with pre-defined reasoning\npathways in MelcotCR to enable more effective utilization of in-context\nknowledge within long COT prompts while strengthening the logical tightness of\nthe reasoning process. Empirical evaluations on our curated MelcotCR dataset\nand the public CodeReviewer dataset reveal that a low-parameter base model,\nsuch as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art\nmethods in terms of the accuracy of detecting and describing code issues, with\nits performance remarkably on par with that of the 671B DeepSeek-R1 model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown great potential in supporting\nautomated code review due to their impressive capabilities in context\nunderstanding and reasoning. However, these capabilities are still limited\ncompared to human-level cognition because they are heavily influenced by the\ntraining data. Recent research has demonstrated significantly improved\nperformance through fine-tuning LLMs with code review data. However, compared\nto human reviewers who often simultaneously analyze multiple dimensions of code\nreview to better identify issues, the full potential of these methods is\nhampered by the limited or vague information used to fine-tune the models. This\npaper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that\ntrains LLMs with an impressive reasoning ability to analyze multiple dimensions\nof code review by harnessing long COT techniques to provide rich structured\ninformation. To address context loss and reasoning logic loss issues that\nfrequently occur when LLMs process long COT prompts, we propose a solution that\ncombines the Maximum Entropy (ME) modeling principle with pre-defined reasoning\npathways in MelcotCR to enable more effective utilization of in-context\nknowledge within long COT prompts while strengthening the logical tightness of\nthe reasoning process. Empirical evaluations on our curated MelcotCR dataset\nand the public CodeReviewer dataset reveal that a low-parameter base model,\nsuch as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art\nmethods in terms of the accuracy of detecting and describing code issues, with\nits performance remarkably on par with that of the 671B DeepSeek-R1 model."
                },
                "authors": [
                    {
                        "name": "Yongda Yu"
                    },
                    {
                        "name": "Guohao Shi"
                    },
                    {
                        "name": "Xianwei Wu"
                    },
                    {
                        "name": "Haochuan He"
                    },
                    {
                        "name": "XueMing Gu"
                    },
                    {
                        "name": "Qianqian Zhao"
                    },
                    {
                        "name": "Kui Liu"
                    },
                    {
                        "name": "Qiushi Wang"
                    },
                    {
                        "name": "Zhao Tian"
                    },
                    {
                        "name": "Haifeng Shen"
                    },
                    {
                        "name": "Guoping Rong"
                    }
                ],
                "author_detail": {
                    "name": "Guoping Rong"
                },
                "author": "Guoping Rong",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04495v2",
                "updated": "2025-09-25T13:51:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    51,
                    34,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-06T14:44:23Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    44,
                    23,
                    2,
                    218,
                    0
                ],
                "title": "Causal Reflection with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Reflection with Language Models"
                },
                "summary": "While LLMs exhibit impressive fluency and factual recall, they struggle with\nrobust causal reasoning, often relying on spurious correlations and brittle\npatterns. Similarly, traditional Reinforcement Learning agents also lack causal\nunderstanding, optimizing for rewards without modeling why actions lead to\noutcomes. We introduce Causal Reflection, a framework that explicitly models\ncausality as a dynamic function over state, action, time, and perturbation,\nenabling agents to reason about delayed and nonlinear effects. Additionally, we\ndefine a formal Reflect mechanism that identifies mismatches between predicted\nand observed outcomes and generates causal hypotheses to revise the agent's\ninternal model. In this architecture, LLMs serve not as black-box reasoners,\nbut as structured inference engines translating formal causal outputs into\nnatural language explanations and counterfactuals. Our framework lays the\ntheoretical groundwork for Causal Reflective agents that can adapt,\nself-correct, and communicate causal understanding in evolving environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs exhibit impressive fluency and factual recall, they struggle with\nrobust causal reasoning, often relying on spurious correlations and brittle\npatterns. Similarly, traditional Reinforcement Learning agents also lack causal\nunderstanding, optimizing for rewards without modeling why actions lead to\noutcomes. We introduce Causal Reflection, a framework that explicitly models\ncausality as a dynamic function over state, action, time, and perturbation,\nenabling agents to reason about delayed and nonlinear effects. Additionally, we\ndefine a formal Reflect mechanism that identifies mismatches between predicted\nand observed outcomes and generates causal hypotheses to revise the agent's\ninternal model. In this architecture, LLMs serve not as black-box reasoners,\nbut as structured inference engines translating formal causal outputs into\nnatural language explanations and counterfactuals. Our framework lays the\ntheoretical groundwork for Causal Reflective agents that can adapt,\nself-correct, and communicate causal understanding in evolving environments."
                },
                "authors": [
                    {
                        "name": "Abi Aryan"
                    },
                    {
                        "name": "Zac Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zac Liu"
                },
                "author": "Zac Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21164v1",
                "updated": "2025-09-25T13:50:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    50,
                    9,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:50:09Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    50,
                    9,
                    3,
                    268,
                    0
                ],
                "title": "Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just\n  What They Say",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just\n  What They Say"
                },
                "summary": "Open-source Large Language Models (LLMs) increasingly specialize by domain\n(e.g., math, code, general reasoning), motivating systems that leverage\ncomplementary strengths across models. Prior multi-LLM approaches either (i)\nroute a query to one or a few experts and generate independently, (ii)\naggregate outputs from each model via costly multi-turn exchanges, or (iii)\nfuse weights into a single model-typically requiring architectural homogeneity.\nWe introduce Mixture of Thoughts (MoT), a simple method for latent-level\ncollaboration among heterogeneous experts under a global routing scheme. For\neach query, a lightweight router selects top-$K$ experts and designates a\nprimary expert; uniformly placed interaction layers project hidden states into\na shared latent space where the primary expert performs cross-attention over\nits active (selected) peers. Pre-trained experts remain frozen; only the router\nand the lightweight interaction layers are trained with a novel joint training\nobjective that improves both the expert selection and inter-expert\ncollaboration. Across five in-distribution (ID) and three out-of-distribution\n(OOD) benchmarks, MoT surpasses the current routing and aggregation-based\nstate-of-the-art, Avengers, by $+0.38\\%$ and $+2.92\\%$, respectively. Further,\nMoT significantly outperforms the best-performing single model. It achieves\nthis with single-pass inference, runtime comparable to routing baselines, and\nnone of the overheads of iterative aggregation. MoT offers a simple\nlatent-space mechanism for combining heterogeneous LLMs, a practical step\ntoward broader multi-LLM collaboration. Our code is publicly available at\nhttps://github.com/jacobfa/mot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source Large Language Models (LLMs) increasingly specialize by domain\n(e.g., math, code, general reasoning), motivating systems that leverage\ncomplementary strengths across models. Prior multi-LLM approaches either (i)\nroute a query to one or a few experts and generate independently, (ii)\naggregate outputs from each model via costly multi-turn exchanges, or (iii)\nfuse weights into a single model-typically requiring architectural homogeneity.\nWe introduce Mixture of Thoughts (MoT), a simple method for latent-level\ncollaboration among heterogeneous experts under a global routing scheme. For\neach query, a lightweight router selects top-$K$ experts and designates a\nprimary expert; uniformly placed interaction layers project hidden states into\na shared latent space where the primary expert performs cross-attention over\nits active (selected) peers. Pre-trained experts remain frozen; only the router\nand the lightweight interaction layers are trained with a novel joint training\nobjective that improves both the expert selection and inter-expert\ncollaboration. Across five in-distribution (ID) and three out-of-distribution\n(OOD) benchmarks, MoT surpasses the current routing and aggregation-based\nstate-of-the-art, Avengers, by $+0.38\\%$ and $+2.92\\%$, respectively. Further,\nMoT significantly outperforms the best-performing single model. It achieves\nthis with single-pass inference, runtime comparable to routing baselines, and\nnone of the overheads of iterative aggregation. MoT offers a simple\nlatent-space mechanism for combining heterogeneous LLMs, a practical step\ntoward broader multi-LLM collaboration. Our code is publicly available at\nhttps://github.com/jacobfa/mot."
                },
                "authors": [
                    {
                        "name": "Jacob Fein-Ashley"
                    },
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21163v1",
                "updated": "2025-09-25T13:49:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    49,
                    38,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:49:38Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    49,
                    38,
                    3,
                    268,
                    0
                ],
                "title": "Distributed Specialization: Rare-Token Neurons in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Specialization: Rare-Token Neurons in Large Language Models"
                },
                "summary": "Large language models (LLMs) struggle with representing and generating rare\ntokens despite their importance in specialized domains. We investigate whether\nLLMs develop internal specialization mechanisms through discrete modular\narchitectures or distributed parameter-level differentiation. Through\nsystematic analysis of final-layer MLP neurons across multiple model families,\nwe discover that rare-token processing emerges via \\textit{distributed\nspecialization}: functionally coordinated but spatially distributed subnetworks\nthat exhibit three distinct organizational principles. First, we identify a\nreproducible three-regime influence hierarchy comprising highly influential\nplateau neurons(also termed as rare-token neurons), power-law decay neurons,\nand minimally contributing neurons, which is absent in common-token processing.\nSecond, plateau neurons demonstrate coordinated activation patterns (reduced\neffective dimensionality) while remaining spatially distributed rather than\nforming discrete clusters. Third, these specialized mechanisms are universally\naccessible through standard attention pathways without requiring dedicated\nrouting circuits. Training dynamics reveal that functional specialization\nemerges gradually through parameter differentiation, with specialized neurons\ndeveloping increasingly heavy-tailed weight correlation spectra consistent with\nHeavy-Tailed Self-Regularization signatures. Our findings establish that LLMs\nprocess rare-tokens through distributed coordination within shared\narchitectures rather than mixture-of-experts-style modularity. These results\nprovide insights for interpretable model editing, computational efficiency\noptimization, and understanding emergent functional organization in transformer\nnetworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) struggle with representing and generating rare\ntokens despite their importance in specialized domains. We investigate whether\nLLMs develop internal specialization mechanisms through discrete modular\narchitectures or distributed parameter-level differentiation. Through\nsystematic analysis of final-layer MLP neurons across multiple model families,\nwe discover that rare-token processing emerges via \\textit{distributed\nspecialization}: functionally coordinated but spatially distributed subnetworks\nthat exhibit three distinct organizational principles. First, we identify a\nreproducible three-regime influence hierarchy comprising highly influential\nplateau neurons(also termed as rare-token neurons), power-law decay neurons,\nand minimally contributing neurons, which is absent in common-token processing.\nSecond, plateau neurons demonstrate coordinated activation patterns (reduced\neffective dimensionality) while remaining spatially distributed rather than\nforming discrete clusters. Third, these specialized mechanisms are universally\naccessible through standard attention pathways without requiring dedicated\nrouting circuits. Training dynamics reveal that functional specialization\nemerges gradually through parameter differentiation, with specialized neurons\ndeveloping increasingly heavy-tailed weight correlation spectra consistent with\nHeavy-Tailed Self-Regularization signatures. Our findings establish that LLMs\nprocess rare-tokens through distributed coordination within shared\narchitectures rather than mixture-of-experts-style modularity. These results\nprovide insights for interpretable model editing, computational efficiency\noptimization, and understanding emergent functional organization in transformer\nnetworks."
                },
                "authors": [
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Haozheng Wang"
                    },
                    {
                        "name": "Yueheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yueheng Li"
                },
                "author": "Yueheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21161v1",
                "updated": "2025-09-25T13:46:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    46,
                    56,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:46:56Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    46,
                    56,
                    3,
                    268,
                    0
                ],
                "title": "DATS: Distance-Aware Temperature Scaling for Calibrated\n  Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DATS: Distance-Aware Temperature Scaling for Calibrated\n  Class-Incremental Learning"
                },
                "summary": "Continual Learning (CL) is recently gaining increasing attention for its\nability to enable a single model to learn incrementally from a sequence of new\nclasses. In this scenario, it is important to keep consistent predictive\nperformance across all the classes and prevent the so-called Catastrophic\nForgetting (CF). However, in safety-critical applications, predictive\nperformance alone is insufficient. Predictive models should also be able to\nreliably communicate their uncertainty in a calibrated manner - that is, with\nconfidence scores aligned to the true frequencies of target events. Existing\napproaches in CL address calibration primarily from a data-centric perspective,\nrelying on a single temperature shared across all tasks. Such solutions\noverlook task-specific differences, leading to large fluctuations in\ncalibration error across tasks. For this reason, we argue that a more\nprincipled approach should adapt the temperature according to the distance to\nthe current task. However, the unavailability of the task information at test\ntime/during deployment poses a major challenge to achieve the intended\nobjective. For this, we propose Distance-Aware Temperature Scaling (DATS),\nwhich combines prototype-based distance estimation with distance-aware\ncalibration to infer task proximity and assign adaptive temperatures without\nprior task information. Through extensive empirical evaluation on both standard\nbenchmarks and real-world, imbalanced datasets taken from the biomedical\ndomain, our approach demonstrates to be stable, reliable and consistent in\nreducing calibration error across tasks compared to state-of-the-art\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Learning (CL) is recently gaining increasing attention for its\nability to enable a single model to learn incrementally from a sequence of new\nclasses. In this scenario, it is important to keep consistent predictive\nperformance across all the classes and prevent the so-called Catastrophic\nForgetting (CF). However, in safety-critical applications, predictive\nperformance alone is insufficient. Predictive models should also be able to\nreliably communicate their uncertainty in a calibrated manner - that is, with\nconfidence scores aligned to the true frequencies of target events. Existing\napproaches in CL address calibration primarily from a data-centric perspective,\nrelying on a single temperature shared across all tasks. Such solutions\noverlook task-specific differences, leading to large fluctuations in\ncalibration error across tasks. For this reason, we argue that a more\nprincipled approach should adapt the temperature according to the distance to\nthe current task. However, the unavailability of the task information at test\ntime/during deployment poses a major challenge to achieve the intended\nobjective. For this, we propose Distance-Aware Temperature Scaling (DATS),\nwhich combines prototype-based distance estimation with distance-aware\ncalibration to infer task proximity and assign adaptive temperatures without\nprior task information. Through extensive empirical evaluation on both standard\nbenchmarks and real-world, imbalanced datasets taken from the biomedical\ndomain, our approach demonstrates to be stable, reliable and consistent in\nreducing calibration error across tasks compared to state-of-the-art\napproaches."
                },
                "authors": [
                    {
                        "name": "Giuseppe Serra"
                    },
                    {
                        "name": "Florian Buettner"
                    }
                ],
                "author_detail": {
                    "name": "Florian Buettner"
                },
                "author": "Florian Buettner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21158v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21158v1",
                "updated": "2025-09-25T13:44:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    44,
                    21,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:44:21Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    44,
                    21,
                    3,
                    268,
                    0
                ],
                "title": "Unveiling Central ortho-H2D+ Depletion at Sub-kau Scales in Prestellar\n  Core G205.46-14.56M3: The First Interferometric Evidence and Implications for\n  Deuterium Chemistry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Central ortho-H2D+ Depletion at Sub-kau Scales in Prestellar\n  Core G205.46-14.56M3: The First Interferometric Evidence and Implications for\n  Deuterium Chemistry"
                },
                "summary": "Prestellar cores represent the initial conditions of star formation, but\nheavy molecules such as CO are strongly depleted in their cold, dense\ninteriors, limiting the ability to probe core centers. Deuterated molecular\nions therefore emerge as key tracers because deuterium fractionation is\nenhanced at low temperatures. We present the first direct observation of\northo-H2D+ depletion in the prestellar core G205.46-14.56M3 using ALMA 820um\ncontinuum and ortho-H2D+(110-111) data at ~300-au resolution. We confirm the\npreviously reported two substructures, B1 and B2, and identify a central\northo-H2D+ depletion zone toward B1 with ~6$\\sigma$ contrast and an inferred\ndiameter $\\lesssim$600au, together with a peak\n$x$(N2D+)/$x$(N2H+)=$1.03^{+0.07}_{-0.56}$. The observationally inferred\nprofiles of $x$(ortho-H2D+) and $x$(N2D+)/$x$(N2H+) are reproduced by a\ndeuteration-focused chemo-dynamical model; however, the central ortho-H2D+\ndepletion is only marginally matched within the $2\\sigma$ upper limit, likely\nsuggesting additional deuteration in the depletion zone. From these models we\ninfer a core age of ~0.42Ma, comparable to the free-fall time, suggesting that\nthe substructures formed via rapid, turbulence-dominated fragmentation rather\nthan slow, quasi-static contraction. Our observations also reveal that\northo-H2D+ velocity dispersions are largely subsonic in the core and nearly\nthermal between B1 and B2, consistent with turbulence dissipating within a few\nfree-fall times. These results highlight the critical role of deuterated ions\nfor both chemical evolution and dynamics in dense cores.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prestellar cores represent the initial conditions of star formation, but\nheavy molecules such as CO are strongly depleted in their cold, dense\ninteriors, limiting the ability to probe core centers. Deuterated molecular\nions therefore emerge as key tracers because deuterium fractionation is\nenhanced at low temperatures. We present the first direct observation of\northo-H2D+ depletion in the prestellar core G205.46-14.56M3 using ALMA 820um\ncontinuum and ortho-H2D+(110-111) data at ~300-au resolution. We confirm the\npreviously reported two substructures, B1 and B2, and identify a central\northo-H2D+ depletion zone toward B1 with ~6$\\sigma$ contrast and an inferred\ndiameter $\\lesssim$600au, together with a peak\n$x$(N2D+)/$x$(N2H+)=$1.03^{+0.07}_{-0.56}$. The observationally inferred\nprofiles of $x$(ortho-H2D+) and $x$(N2D+)/$x$(N2H+) are reproduced by a\ndeuteration-focused chemo-dynamical model; however, the central ortho-H2D+\ndepletion is only marginally matched within the $2\\sigma$ upper limit, likely\nsuggesting additional deuteration in the depletion zone. From these models we\ninfer a core age of ~0.42Ma, comparable to the free-fall time, suggesting that\nthe substructures formed via rapid, turbulence-dominated fragmentation rather\nthan slow, quasi-static contraction. Our observations also reveal that\northo-H2D+ velocity dispersions are largely subsonic in the core and nearly\nthermal between B1 and B2, consistent with turbulence dissipating within a few\nfree-fall times. These results highlight the critical role of deuterated ions\nfor both chemical evolution and dynamics in dense cores."
                },
                "authors": [
                    {
                        "name": "Sheng-Jun Lin"
                    },
                    {
                        "name": "Sheng-Yuan Liu"
                    },
                    {
                        "name": "Dipen Sahu"
                    },
                    {
                        "name": "Laurent Pagani"
                    },
                    {
                        "name": "Tien-Hao Hsieh"
                    },
                    {
                        "name": "Naomi Hirano"
                    },
                    {
                        "name": "Shih-Ping Lai"
                    },
                    {
                        "name": "Tie Liu"
                    },
                    {
                        "name": "Shih-Ying Hsu"
                    },
                    {
                        "name": "Shanghuo Li"
                    },
                    {
                        "name": "Kee-Tae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Kee-Tae Kim"
                },
                "author": "Kee-Tae Kim",
                "arxiv_comment": "22 pages, 8 figures, accepted for publication in ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21158v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21158v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19202v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19202v4",
                "updated": "2025-09-25T13:44:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    44,
                    11,
                    3,
                    268,
                    0
                ],
                "published": "2025-01-31T15:12:20Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    12,
                    20,
                    4,
                    31,
                    0
                ],
                "title": "Improving LLM Unlearning Robustness via Random Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Unlearning Robustness via Random Perturbations"
                },
                "summary": "Here, we show that current state-of-the-art LLM unlearning methods inherently\nreduce models' robustness, causing them to misbehave even when a single\nnon-adversarial forget-token is present in the retain-query. Toward\nunderstanding underlying causes, we propose a novel theoretical framework that\nreframes the unlearning process as backdoor attacks and defenses: forget-tokens\nact as backdoor triggers that, when activated in retain-queries, cause\ndisruptions in unlearned models' behaviors, similar to successful backdoor\nattacks. The sense that, LLM unlearning methods themselves poison the model,\nmake it more vulnerable to forget-tokens, and hide rather than erase target\nknowledge, describes their true mechanism. To mitigate the vulnerability caused\nby the forgetting process, we reinterpret the retaining process as a backdoor\ndefense and propose Random Noise Augmentation (RNA), a lightweight, model and\nmethod-agnostic approach with theoretical guarantees for improving the\nrobustness of models. Extensive experiments demonstrate that RNA significantly\nimproves the robustness of unlearned models while preserving forget and retain\nperformances. This backdoor attack-defense framework offers insights into the\nmechanism of unlearning that can shed light on future research directions for\nimproving unlearning robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Here, we show that current state-of-the-art LLM unlearning methods inherently\nreduce models' robustness, causing them to misbehave even when a single\nnon-adversarial forget-token is present in the retain-query. Toward\nunderstanding underlying causes, we propose a novel theoretical framework that\nreframes the unlearning process as backdoor attacks and defenses: forget-tokens\nact as backdoor triggers that, when activated in retain-queries, cause\ndisruptions in unlearned models' behaviors, similar to successful backdoor\nattacks. The sense that, LLM unlearning methods themselves poison the model,\nmake it more vulnerable to forget-tokens, and hide rather than erase target\nknowledge, describes their true mechanism. To mitigate the vulnerability caused\nby the forgetting process, we reinterpret the retaining process as a backdoor\ndefense and propose Random Noise Augmentation (RNA), a lightweight, model and\nmethod-agnostic approach with theoretical guarantees for improving the\nrobustness of models. Extensive experiments demonstrate that RNA significantly\nimproves the robustness of unlearned models while preserving forget and retain\nperformances. This backdoor attack-defense framework offers insights into the\nmechanism of unlearning that can shed light on future research directions for\nimproving unlearning robustness."
                },
                "authors": [
                    {
                        "name": "Dang Huu-Tien"
                    },
                    {
                        "name": "Hoang Thanh-Tung"
                    },
                    {
                        "name": "Anh Bui"
                    },
                    {
                        "name": "Minh-Phuong Nguyen"
                    },
                    {
                        "name": "Le-Minh Nguyen"
                    },
                    {
                        "name": "Naoya Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Naoya Inoue"
                },
                "author": "Naoya Inoue",
                "arxiv_comment": "29 pages, 13 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19202v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19202v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21155v2",
                "updated": "2025-09-26T02:37:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    2,
                    37,
                    41,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-25T13:42:28Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    42,
                    28,
                    3,
                    268,
                    0
                ],
                "title": "Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in\n  Language Models"
                },
                "summary": "For an LLM to correctly respond to an instruction it must understand both the\nsemantics and the domain (i.e., subject area) of a given task-instruction pair.\nHowever, syntax can also convey implicit information Recent work shows that\nsyntactic templates -- frequent sequences of Part-of-Speech (PoS) tags -- are\nprevalent in training data and often appear in model outputs. In this work we\ncharacterize syntactic templates, domain, and semantics in task-instruction\npairs. We identify cases of spurious correlations between syntax and domain,\nwhere models learn to associate a domain with syntax during training; this can\nsometimes override prompt semantics. Using a synthetic training dataset, we\nfind that the syntactic-domain correlation can lower performance (mean 0.51 +/-\n0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an\nevaluation framework to detect this phenomenon in trained models, and show that\nit occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B;\nLlama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study\non the implications for safety finetuning, showing that unintended\nsyntactic-domain correlations can be used to bypass refusals in OLMo-2-7B\nInstruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test\nfor syntactic-domain correlations, and (2) to ensure syntactic diversity in\ntraining data, specifically within domains, to prevent such spurious\ncorrelations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For an LLM to correctly respond to an instruction it must understand both the\nsemantics and the domain (i.e., subject area) of a given task-instruction pair.\nHowever, syntax can also convey implicit information Recent work shows that\nsyntactic templates -- frequent sequences of Part-of-Speech (PoS) tags -- are\nprevalent in training data and often appear in model outputs. In this work we\ncharacterize syntactic templates, domain, and semantics in task-instruction\npairs. We identify cases of spurious correlations between syntax and domain,\nwhere models learn to associate a domain with syntax during training; this can\nsometimes override prompt semantics. Using a synthetic training dataset, we\nfind that the syntactic-domain correlation can lower performance (mean 0.51 +/-\n0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an\nevaluation framework to detect this phenomenon in trained models, and show that\nit occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B;\nLlama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study\non the implications for safety finetuning, showing that unintended\nsyntactic-domain correlations can be used to bypass refusals in OLMo-2-7B\nInstruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test\nfor syntactic-domain correlations, and (2) to ensure syntactic diversity in\ntraining data, specifically within domains, to prevent such spurious\ncorrelations."
                },
                "authors": [
                    {
                        "name": "Chantal Shaib"
                    },
                    {
                        "name": "Vinith M. Suriyakumar"
                    },
                    {
                        "name": "Levent Sagun"
                    },
                    {
                        "name": "Byron C. Wallace"
                    },
                    {
                        "name": "Marzyeh Ghassemi"
                    }
                ],
                "author_detail": {
                    "name": "Marzyeh Ghassemi"
                },
                "author": "Marzyeh Ghassemi",
                "arxiv_comment": "NeurIPS 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21154v1",
                "updated": "2025-09-25T13:40:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    40,
                    36,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:40:36Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    40,
                    36,
                    3,
                    268,
                    0
                ],
                "title": "GRPO is Secretly a Process Reward Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRPO is Secretly a Process Reward Model"
                },
                "summary": "We prove theoretically that the GRPO RL algorithm induces a non-trivial\nprocess reward model (PRM), under certain assumptions regarding within-group\noverlap of token sequences across completions. We then show empirically that\nthese assumptions are met under real-world conditions: GRPO does in fact induce\na non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a\nflaw in the GRPO objective: non-uniformly distributed process steps hinder both\nexploration and exploitation (under different conditions). We propose a simple\nmodification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and\nshow that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy\nand performance on downstream reasoning tasks$-$and reach peak performance more\nrapidly$-$than LLMs trained with standard GRPO. Our results call into question\nthe advantage of costly, explicitly-defined PRMs for GRPO: we show that it is\npossible to instead leverage the hidden, built-in PRM structure within the\nvanilla GRPO algorithm to boost model performance with a negligible impact on\ntraining time and cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We prove theoretically that the GRPO RL algorithm induces a non-trivial\nprocess reward model (PRM), under certain assumptions regarding within-group\noverlap of token sequences across completions. We then show empirically that\nthese assumptions are met under real-world conditions: GRPO does in fact induce\na non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a\nflaw in the GRPO objective: non-uniformly distributed process steps hinder both\nexploration and exploitation (under different conditions). We propose a simple\nmodification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and\nshow that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy\nand performance on downstream reasoning tasks$-$and reach peak performance more\nrapidly$-$than LLMs trained with standard GRPO. Our results call into question\nthe advantage of costly, explicitly-defined PRMs for GRPO: we show that it is\npossible to instead leverage the hidden, built-in PRM structure within the\nvanilla GRPO algorithm to boost model performance with a negligible impact on\ntraining time and cost."
                },
                "authors": [
                    {
                        "name": "Michael Sullivan"
                    }
                ],
                "author_detail": {
                    "name": "Michael Sullivan"
                },
                "author": "Michael Sullivan",
                "arxiv_comment": "14 pages, 6 figures; under review at ICLR 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21153v1",
                "updated": "2025-09-25T13:39:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    39,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:39:16Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    39,
                    16,
                    3,
                    268,
                    0
                ],
                "title": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WAVECLIP: Wavelet Tokenization for Adaptive-Resolution CLIP"
                },
                "summary": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce WAVECLIP, a single unified model for adaptive resolution\ninference in CLIP, enabled by wavelet-based tokenization. WAVECLIP replaces\nstandard patch embeddings with a multi-level wavelet decomposition, enabling\nthe model to process images coarse to fine while naturally supporting multiple\nresolutions within the same model. At inference time, the model begins with low\nresolution tokens and refines only when needed, using key-value caching and\ncausal cross-level attention to reuse computation, effectively introducing to\nthe model only new information when needed. We evaluate WAVECLIP in zero-shot\nclassification, demonstrating that a simple confidence-based gating mechanism\nenables adaptive early exits. This allows users to dynamically choose a\ncompute-accuracy trade-off using a single deployed model. Our approach requires\nonly lightweight distillation from a frozen CLIP teacher and achieves\ncompetitive accuracy with significant computational savings."
                },
                "authors": [
                    {
                        "name": "Moshe Kimhi"
                    },
                    {
                        "name": "Erez Koifman"
                    },
                    {
                        "name": "Ehud Rivlin"
                    },
                    {
                        "name": "Eli Schwartz"
                    },
                    {
                        "name": "Chaim Baskin"
                    }
                ],
                "author_detail": {
                    "name": "Chaim Baskin"
                },
                "author": "Chaim Baskin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21150v1",
                "updated": "2025-09-25T13:38:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    38,
                    36,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:38:36Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    38,
                    36,
                    3,
                    268,
                    0
                ],
                "title": "CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific\n  Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific\n  Tokenization"
                },
                "summary": "Computer-Aided Design (CAD) is a foundational component of industrial\nprototyping, where models are defined not by raw coordinates but by\nconstruction sequences such as sketches and extrusions. This sequential\nstructure enables both efficient prototype initialization and subsequent\nediting. Text-guided CAD prototyping, which unifies Text-to-CAD generation and\nCAD editing, has the potential to streamline the entire design pipeline.\nHowever, prior work has not explored this setting, largely because standard\nlarge language model (LLM) tokenizers decompose CAD sequences into\nnatural-language word pieces, failing to capture primitive-level CAD semantics\nand hindering attention modules from modeling geometric structure. We\nconjecture that a multimodal tokenization strategy, aligned with CAD's\nprimitive and structural nature, can provide more effective representations. To\nthis end, we propose CAD-Tokenizer, a framework that represents CAD data with\nmodality-specific tokens using a sequence-based VQ-VAE with primitive-level\npooling and constrained decoding. This design produces compact, primitive-aware\nrepresentations that align with CAD's structural nature. Applied to unified\ntext-guided CAD prototyping, CAD-Tokenizer significantly improves instruction\nfollowing and generation quality, achieving better quantitative and qualitative\nperformance over both general-purpose LLMs and task-specific baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-Aided Design (CAD) is a foundational component of industrial\nprototyping, where models are defined not by raw coordinates but by\nconstruction sequences such as sketches and extrusions. This sequential\nstructure enables both efficient prototype initialization and subsequent\nediting. Text-guided CAD prototyping, which unifies Text-to-CAD generation and\nCAD editing, has the potential to streamline the entire design pipeline.\nHowever, prior work has not explored this setting, largely because standard\nlarge language model (LLM) tokenizers decompose CAD sequences into\nnatural-language word pieces, failing to capture primitive-level CAD semantics\nand hindering attention modules from modeling geometric structure. We\nconjecture that a multimodal tokenization strategy, aligned with CAD's\nprimitive and structural nature, can provide more effective representations. To\nthis end, we propose CAD-Tokenizer, a framework that represents CAD data with\nmodality-specific tokens using a sequence-based VQ-VAE with primitive-level\npooling and constrained decoding. This design produces compact, primitive-aware\nrepresentations that align with CAD's structural nature. Applied to unified\ntext-guided CAD prototyping, CAD-Tokenizer significantly improves instruction\nfollowing and generation quality, achieving better quantitative and qualitative\nperformance over both general-purpose LLMs and task-specific baselines."
                },
                "authors": [
                    {
                        "name": "Ruiyu Wang"
                    },
                    {
                        "name": "Shizhao Sun"
                    },
                    {
                        "name": "Weijian Ma"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11771v2",
                "updated": "2025-09-25T13:37:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    37,
                    54,
                    3,
                    268,
                    0
                ],
                "published": "2025-02-17T13:00:44Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    13,
                    0,
                    44,
                    0,
                    48,
                    0
                ],
                "title": "The Validation Gap: A Mechanistic Analysis of How Language Models\n  Compute Arithmetic but Fail to Validate It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Validation Gap: A Mechanistic Analysis of How Language Models\n  Compute Arithmetic but Fail to Validate It"
                },
                "summary": "The ability of large language models (LLMs) to validate their output and\nidentify potential errors is crucial for ensuring robustness and reliability.\nHowever, current research indicates that LLMs struggle with self-correction,\nencountering significant challenges in detecting errors. While studies have\nexplored methods to enhance self-correction in LLMs, relatively little\nattention has been given to understanding the models' internal mechanisms\nunderlying error detection. In this paper, we present a mechanistic analysis of\nerror detection in LLMs, focusing on simple arithmetic problems. Through\ncircuit analysis, we identify the computational subgraphs responsible for\ndetecting arithmetic errors across four smaller-sized LLMs. Our findings reveal\nthat all models heavily rely on $\\textit{consistency heads}$--attention heads\nthat assess surface-level alignment of numerical values in arithmetic\nsolutions. Moreover, we observe that the models' internal arithmetic\ncomputation primarily occurs in higher layers, whereas validation takes place\nin middle layers, before the final arithmetic results are fully encoded. This\nstructural dissociation between arithmetic computation and validation seems to\nexplain why smaller-sized LLMs struggle to detect even simple arithmetic\nerrors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of large language models (LLMs) to validate their output and\nidentify potential errors is crucial for ensuring robustness and reliability.\nHowever, current research indicates that LLMs struggle with self-correction,\nencountering significant challenges in detecting errors. While studies have\nexplored methods to enhance self-correction in LLMs, relatively little\nattention has been given to understanding the models' internal mechanisms\nunderlying error detection. In this paper, we present a mechanistic analysis of\nerror detection in LLMs, focusing on simple arithmetic problems. Through\ncircuit analysis, we identify the computational subgraphs responsible for\ndetecting arithmetic errors across four smaller-sized LLMs. Our findings reveal\nthat all models heavily rely on $\\textit{consistency heads}$--attention heads\nthat assess surface-level alignment of numerical values in arithmetic\nsolutions. Moreover, we observe that the models' internal arithmetic\ncomputation primarily occurs in higher layers, whereas validation takes place\nin middle layers, before the final arithmetic results are fully encoded. This\nstructural dissociation between arithmetic computation and validation seems to\nexplain why smaller-sized LLMs struggle to detect even simple arithmetic\nerrors."
                },
                "authors": [
                    {
                        "name": "Leonardo Bertolazzi"
                    },
                    {
                        "name": "Philipp Mondorf"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Raffaella Bernardi"
                    }
                ],
                "author_detail": {
                    "name": "Raffaella Bernardi"
                },
                "author": "Raffaella Bernardi",
                "arxiv_comment": "EMNLP 2025 Main, 38 pages, 33 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21144v1",
                "updated": "2025-09-25T13:30:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    30,
                    46,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:30:46Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    30,
                    46,
                    3,
                    268,
                    0
                ],
                "title": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice"
                },
                "summary": "The ultimate goal of expressive speech-to-speech translation (S2ST) is to\naccurately translate spoken content while preserving the speaker identity and\nemotional style. However, progress in this field is largely hindered by three\nkey challenges: the scarcity of paired speech data that retains expressive\nstyles, the complexity of multi-stage processing pipelines, and the limited\ntransfer of translation capabilities from large language models (LLMs). In this\nwork, we address these challenges by introducing UniSS, a novel single-stage\nframework for expressive S2ST. Our approach features carefully designed speech\nsemantic and style modeling, enabling seamless integration with existing\ntext-based LLM frameworks to develop a unified text-speech language model. To\ntransfer translation capabilities from text to speech, we propose a cross-modal\nchain-of-thought prompting process that progressively aligns audio semantics\nwith text and ensures style preservation in the decoded results. Furthermore,\nwe construct and release a large-scale, high-quality expressive S2ST dataset,\nUniST, comprising 44.8k hours of data. Experimental results show that UniSS\nsignificantly outperforms previous methods in translation fidelity and speech\nquality while preserving voice, emotion, and duration consistency. Our work\nestablishes a simpler and more effective paradigm for building the next\ngeneration of expressive S2ST systems. Audio samples are available at\nhttps://cmots.github.io/uniss-demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ultimate goal of expressive speech-to-speech translation (S2ST) is to\naccurately translate spoken content while preserving the speaker identity and\nemotional style. However, progress in this field is largely hindered by three\nkey challenges: the scarcity of paired speech data that retains expressive\nstyles, the complexity of multi-stage processing pipelines, and the limited\ntransfer of translation capabilities from large language models (LLMs). In this\nwork, we address these challenges by introducing UniSS, a novel single-stage\nframework for expressive S2ST. Our approach features carefully designed speech\nsemantic and style modeling, enabling seamless integration with existing\ntext-based LLM frameworks to develop a unified text-speech language model. To\ntransfer translation capabilities from text to speech, we propose a cross-modal\nchain-of-thought prompting process that progressively aligns audio semantics\nwith text and ensures style preservation in the decoded results. Furthermore,\nwe construct and release a large-scale, high-quality expressive S2ST dataset,\nUniST, comprising 44.8k hours of data. Experimental results show that UniSS\nsignificantly outperforms previous methods in translation fidelity and speech\nquality while preserving voice, emotion, and duration consistency. Our work\nestablishes a simpler and more effective paradigm for building the next\ngeneration of expressive S2ST systems. Audio samples are available at\nhttps://cmots.github.io/uniss-demo."
                },
                "authors": [
                    {
                        "name": "Sitong Cheng"
                    },
                    {
                        "name": "Weizhen Bian"
                    },
                    {
                        "name": "Xinsheng Wang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Jianyi Chen"
                    },
                    {
                        "name": "Shunshun Yin"
                    },
                    {
                        "name": "Yike Guo"
                    },
                    {
                        "name": "Wei Xue"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xue"
                },
                "author": "Wei Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21134v1",
                "updated": "2025-09-25T13:25:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    25,
                    15,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:25:15Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    25,
                    15,
                    3,
                    268,
                    0
                ],
                "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent\n  Perspective"
                },
                "summary": "Large Language Models (LLMs) have been used to make decisions in complex\nscenarios, where they need models to think deeply, reason logically, and decide\nwisely. Many existing studies focus solely on multi-round conversations in\nsocial tasks or simulated environments, neglecting the various types of\ndecisions and their interdependence. Current reinforcement learning methods\nstruggle to consider the strategies of others during training. To address these\nissues, we first define a strategic decision-making problem that includes two\ntypes of decisions and their temporal dependencies. Furthermore, we propose\n**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to\noptimize the perception of other individual strategies and the game situation\ntrends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,\nToMPO enhances the LLM's strategic decision-making mainly by: 1) generating\nrollouts based on reasoning the strategies of other individuals, 2) estimating\nadvantages at both the graph-level and sample-level, and 3) balancing global\nand partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in\nterms of model output compliance and cooperative outcomes. Additionally, when\ncompared to models with parameter sizes 100 times larger, it shows an 18%\nimprovement. This demonstrates the effectiveness of the ToMPO algorithm in\nenhancing the model's strategic decision-making capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been used to make decisions in complex\nscenarios, where they need models to think deeply, reason logically, and decide\nwisely. Many existing studies focus solely on multi-round conversations in\nsocial tasks or simulated environments, neglecting the various types of\ndecisions and their interdependence. Current reinforcement learning methods\nstruggle to consider the strategies of others during training. To address these\nissues, we first define a strategic decision-making problem that includes two\ntypes of decisions and their temporal dependencies. Furthermore, we propose\n**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to\noptimize the perception of other individual strategies and the game situation\ntrends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,\nToMPO enhances the LLM's strategic decision-making mainly by: 1) generating\nrollouts based on reasoning the strategies of other individuals, 2) estimating\nadvantages at both the graph-level and sample-level, and 3) balancing global\nand partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in\nterms of model output compliance and cooperative outcomes. Additionally, when\ncompared to models with parameter sizes 100 times larger, it shows an 18%\nimprovement. This demonstrates the effectiveness of the ToMPO algorithm in\nenhancing the model's strategic decision-making capabilities."
                },
                "authors": [
                    {
                        "name": "Yiwen Zhang"
                    },
                    {
                        "name": "Ziang Chen"
                    },
                    {
                        "name": "Fanqi Kong"
                    },
                    {
                        "name": "Yizhe Huang"
                    },
                    {
                        "name": "Xue Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Feng"
                },
                "author": "Xue Feng",
                "arxiv_comment": "22 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21132v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21132v1",
                "updated": "2025-09-25T13:22:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    22,
                    38,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:22:38Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    22,
                    38,
                    3,
                    268,
                    0
                ],
                "title": "Detecting disease progression from animal movement using hidden Markov\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting disease progression from animal movement using hidden Markov\n  models"
                },
                "summary": "Understanding disease dynamics is crucial for managing wildlife populations\nand assessing spillover risk to domestic animals and humans, but infection data\non free-ranging animals are difficult to obtain. Because pathogen and parasite\ninfections can alter host movement, infection status may be inferred from\nanimal trajectories. We present a hidden Markov model (HMM) framework that\nlinks observed movement behaviors to unobserved infection states, consistent\nwith epidemiological compartmental models (e.g., susceptible, infected,\nrecovered, dead). Using movement data from 84 reintroduced scimitar-horned oryx\n(Oryx dammah), 38 confirmed dead in the field and 6 sampled for disease\ntesting, we demonstrate how HMMs can incorporate epidemiological structure\nthrough (1) constrained transition probabilities (e.g., to preclude or allow\nrecovery), (2) covariate effects on transmission, and (3) hierarchically\nstructured HMMs (HHMMs) for multi-scale transitions. Comparing veterinary\ndiagnostic reports with model outputs, we found that HMMs with epidemiological\nconstraints successfully identified infection-associated reductions in\nmovement, whereas unconstrained models failed to capture disease progression.\nSimulations further showed that constrained HMMs accurately classified\nsusceptible, infected, and recovered states. By illustrating flexible\nformulations and a workflow for model selection, we provide a transferable\napproach for detecting infection from movement data. This framework can enhance\nwildlife disease surveillance, guide population management, and improve\nunderstanding of disease dynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding disease dynamics is crucial for managing wildlife populations\nand assessing spillover risk to domestic animals and humans, but infection data\non free-ranging animals are difficult to obtain. Because pathogen and parasite\ninfections can alter host movement, infection status may be inferred from\nanimal trajectories. We present a hidden Markov model (HMM) framework that\nlinks observed movement behaviors to unobserved infection states, consistent\nwith epidemiological compartmental models (e.g., susceptible, infected,\nrecovered, dead). Using movement data from 84 reintroduced scimitar-horned oryx\n(Oryx dammah), 38 confirmed dead in the field and 6 sampled for disease\ntesting, we demonstrate how HMMs can incorporate epidemiological structure\nthrough (1) constrained transition probabilities (e.g., to preclude or allow\nrecovery), (2) covariate effects on transmission, and (3) hierarchically\nstructured HMMs (HHMMs) for multi-scale transitions. Comparing veterinary\ndiagnostic reports with model outputs, we found that HMMs with epidemiological\nconstraints successfully identified infection-associated reductions in\nmovement, whereas unconstrained models failed to capture disease progression.\nSimulations further showed that constrained HMMs accurately classified\nsusceptible, infected, and recovered states. By illustrating flexible\nformulations and a workflow for model selection, we provide a transferable\napproach for detecting infection from movement data. This framework can enhance\nwildlife disease surveillance, guide population management, and improve\nunderstanding of disease dynamics."
                },
                "authors": [
                    {
                        "name": "Dongmin Kim"
                    },
                    {
                        "name": "Tho Michelot"
                    },
                    {
                        "name": "Katherine Mertes"
                    },
                    {
                        "name": "Jared A. Stabach"
                    },
                    {
                        "name": "John Fieberg"
                    }
                ],
                "author_detail": {
                    "name": "John Fieberg"
                },
                "author": "John Fieberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21132v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21132v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.QM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21129v1",
                "updated": "2025-09-25T13:19:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    19,
                    59,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:19:59Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    19,
                    59,
                    3,
                    268,
                    0
                ],
                "title": "EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing\n  Email Defense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing\n  Email Defense"
                },
                "summary": "Modern email spam and phishing attacks have evolved far beyond keyword\nblacklists or simple heuristics. Adversaries now craft multi-modal campaigns\nthat combine natural-language text with obfuscated URLs, forged headers, and\nmalicious attachments, adapting their strategies within days to bypass filters.\nTraditional spam detection systems, which rely on static rules or\nsingle-modality models, struggle to integrate heterogeneous signals or to\ncontinuously adapt, leading to rapid performance degradation.\n  We propose EvoMail, a self-evolving cognitive agent framework for robust\ndetection of spam and phishing. EvoMail first constructs a unified\nheterogeneous email graph that fuses textual content, metadata (headers,\nsenders, domains), and embedded resources (URLs, attachments). A Cognitive\nGraph Neural Network enhanced by a Large Language Model (LLM) performs\ncontext-aware reasoning across these sources to identify coordinated spam\ncampaigns. Most critically, EvoMail engages in an adversarial self-evolution\nloop: a ''red-team'' agent generates novel evasion tactics -- such as character\nobfuscation or AI-generated phishing text -- while the ''blue-team'' detector\nlearns from failures, compresses experiences into a memory module, and reuses\nthem for future reasoning.\n  Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam,\nSpamAssassin, and TREC) and synthetic adversarial variants demonstrate that\nEvoMail consistently outperforms state-of-the-art baselines in detection\naccuracy, adaptability to evolving spam tactics, and interpretability of\nreasoning traces. These results highlight EvoMail's potential as a resilient\nand explainable defense framework against next-generation spam and phishing\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern email spam and phishing attacks have evolved far beyond keyword\nblacklists or simple heuristics. Adversaries now craft multi-modal campaigns\nthat combine natural-language text with obfuscated URLs, forged headers, and\nmalicious attachments, adapting their strategies within days to bypass filters.\nTraditional spam detection systems, which rely on static rules or\nsingle-modality models, struggle to integrate heterogeneous signals or to\ncontinuously adapt, leading to rapid performance degradation.\n  We propose EvoMail, a self-evolving cognitive agent framework for robust\ndetection of spam and phishing. EvoMail first constructs a unified\nheterogeneous email graph that fuses textual content, metadata (headers,\nsenders, domains), and embedded resources (URLs, attachments). A Cognitive\nGraph Neural Network enhanced by a Large Language Model (LLM) performs\ncontext-aware reasoning across these sources to identify coordinated spam\ncampaigns. Most critically, EvoMail engages in an adversarial self-evolution\nloop: a ''red-team'' agent generates novel evasion tactics -- such as character\nobfuscation or AI-generated phishing text -- while the ''blue-team'' detector\nlearns from failures, compresses experiences into a memory module, and reuses\nthem for future reasoning.\n  Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam,\nSpamAssassin, and TREC) and synthetic adversarial variants demonstrate that\nEvoMail consistently outperforms state-of-the-art baselines in detection\naccuracy, adaptability to evolving spam tactics, and interpretability of\nreasoning traces. These results highlight EvoMail's potential as a resilient\nand explainable defense framework against next-generation spam and phishing\nthreats."
                },
                "authors": [
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "De-Tian Chu"
                    },
                    {
                        "name": "Lin-Yuan Bai"
                    },
                    {
                        "name": "Wei Kang"
                    },
                    {
                        "name": "Hai-Tao Zhang"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Zhi-Mo Han"
                    },
                    {
                        "name": "Jing Ge"
                    },
                    {
                        "name": "Hai-Feng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Hai-Feng Lin"
                },
                "author": "Hai-Feng Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21128v1",
                "updated": "2025-09-25T13:18:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    18,
                    57,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:18:57Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    18,
                    57,
                    3,
                    268,
                    0
                ],
                "title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs"
                },
                "summary": "Large language models (LLMs) are typically trained by reinforcement learning\n(RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on\nreasoning traces to improve their reasoning abilities. However, how these\nmethods shape reasoning capabilities remains largely elusive. Going beyond an\naccuracy-based investigation of how these two components sculpt the reasoning\nprocess, this paper introduces a novel analysis framework that quantifies\nreasoning paths and captures their qualitative changes under each training\nprocess (with models of 1.5B, 7B, and 14B parameters on mathematical domains).\nSpecifically, we investigate the reasoning process at two levels of\ngranularity: the trajectory-level, which examines complete reasoning outputs,\nand the step-level, which analyzes reasoning graphs whose nodes correspond to\nindividual reasoning steps. Notably, clustering of unique reasoning\ntrajectories shows complementary effects: RL compresses incorrect trajectories,\nwhereas SFT expands correct ones. Step-level analysis reveals that RL steepens\n(about 2.5 times), while SFT flattens (reduced to about one-third), the decay\nrates of node visitation frequency, degree, and betweenness centrality\ndistributions in the reasoning graph. This indicates that RL concentrates\nreasoning functionality into a small subset of steps, while SFT homogenizes it\nacross many steps. Furthermore, by evaluating the reasoning graph topologies\nfrom multiple perspectives, we delineate the shared and distinct\ncharacteristics of RL and SFT. Our work presents a novel reasoning path\nperspective that explains why the current best practice of two-stage training,\nwith SFT followed by RL, is successful, and offers practical implications for\ndata construction and more efficient learning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically trained by reinforcement learning\n(RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on\nreasoning traces to improve their reasoning abilities. However, how these\nmethods shape reasoning capabilities remains largely elusive. Going beyond an\naccuracy-based investigation of how these two components sculpt the reasoning\nprocess, this paper introduces a novel analysis framework that quantifies\nreasoning paths and captures their qualitative changes under each training\nprocess (with models of 1.5B, 7B, and 14B parameters on mathematical domains).\nSpecifically, we investigate the reasoning process at two levels of\ngranularity: the trajectory-level, which examines complete reasoning outputs,\nand the step-level, which analyzes reasoning graphs whose nodes correspond to\nindividual reasoning steps. Notably, clustering of unique reasoning\ntrajectories shows complementary effects: RL compresses incorrect trajectories,\nwhereas SFT expands correct ones. Step-level analysis reveals that RL steepens\n(about 2.5 times), while SFT flattens (reduced to about one-third), the decay\nrates of node visitation frequency, degree, and betweenness centrality\ndistributions in the reasoning graph. This indicates that RL concentrates\nreasoning functionality into a small subset of steps, while SFT homogenizes it\nacross many steps. Furthermore, by evaluating the reasoning graph topologies\nfrom multiple perspectives, we delineate the shared and distinct\ncharacteristics of RL and SFT. Our work presents a novel reasoning path\nperspective that explains why the current best practice of two-stage training,\nwith SFT followed by RL, is successful, and offers practical implications for\ndata construction and more efficient learning approaches."
                },
                "authors": [
                    {
                        "name": "Kohsei Matsutani"
                    },
                    {
                        "name": "Shota Takashiro"
                    },
                    {
                        "name": "Gouki Minegishi"
                    },
                    {
                        "name": "Takeshi Kojima"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    }
                ],
                "author_detail": {
                    "name": "Yutaka Matsuo"
                },
                "author": "Yutaka Matsuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22156v2",
                "updated": "2025-09-25T13:15:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    15,
                    45,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing"
                },
                "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "18 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21125v1",
                "updated": "2025-09-25T13:15:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    15,
                    1,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:15:01Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    15,
                    1,
                    3,
                    268,
                    0
                ],
                "title": "Acoustic-based Gender Differentiation in Speech-aware Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Acoustic-based Gender Differentiation in Speech-aware Language Models"
                },
                "summary": "Speech-aware Language Models (SpeechLMs) have fundamentally transformed\nhuman-AI interaction by enabling voice-based communication, yet they may\nexhibit acoustic-based gender differentiation where identical questions lead to\ndifferent responses based on the speaker's gender. This paper propose a new\ndataset that enables systematic analysis of this phenomenon, containing 9,208\nspeech samples across three categories: Gender-Independent,\nGender-Stereotypical, and Gender-Dependent. We further evaluated LLaMA-Omni\nseries and discovered a paradoxical pattern; while overall responses seems\nidentical regardless of gender, the pattern is far from unbiased responses.\nSpecifically, in Gender-Stereotypical questions, all models consistently\nexhibited male-oriented responses; meanwhile, in Gender-Dependent questions\nwhere gender differentiation would be contextually appropriate, models\nexhibited responses independent to gender instead. We also confirm that this\npattern does not result from neutral options nor perceived gender of a voice.\nWhen we allow neutral response, models tends to respond neutrally also in\nGender-Dependent questions. The paradoxical pattern yet retains when we applied\ngender neutralization methods on speech. Through comparison between SpeechLMs\nwith corresponding backbone LLMs, we confirmed that these paradoxical patterns\nprimarily stem from Whisper speech encoders, which generates male-oriented\nacoustic tokens. These findings reveal that current SpeechLMs may not\nsuccessfully remove gender biases though they prioritized general fairness\nprinciples over contextual appropriateness, highlighting the need for more\nsophisticated techniques to utilize gender information properly in speech\ntechnology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech-aware Language Models (SpeechLMs) have fundamentally transformed\nhuman-AI interaction by enabling voice-based communication, yet they may\nexhibit acoustic-based gender differentiation where identical questions lead to\ndifferent responses based on the speaker's gender. This paper propose a new\ndataset that enables systematic analysis of this phenomenon, containing 9,208\nspeech samples across three categories: Gender-Independent,\nGender-Stereotypical, and Gender-Dependent. We further evaluated LLaMA-Omni\nseries and discovered a paradoxical pattern; while overall responses seems\nidentical regardless of gender, the pattern is far from unbiased responses.\nSpecifically, in Gender-Stereotypical questions, all models consistently\nexhibited male-oriented responses; meanwhile, in Gender-Dependent questions\nwhere gender differentiation would be contextually appropriate, models\nexhibited responses independent to gender instead. We also confirm that this\npattern does not result from neutral options nor perceived gender of a voice.\nWhen we allow neutral response, models tends to respond neutrally also in\nGender-Dependent questions. The paradoxical pattern yet retains when we applied\ngender neutralization methods on speech. Through comparison between SpeechLMs\nwith corresponding backbone LLMs, we confirmed that these paradoxical patterns\nprimarily stem from Whisper speech encoders, which generates male-oriented\nacoustic tokens. These findings reveal that current SpeechLMs may not\nsuccessfully remove gender biases though they prioritized general fairness\nprinciples over contextual appropriateness, highlighting the need for more\nsophisticated techniques to utilize gender information properly in speech\ntechnology."
                },
                "authors": [
                    {
                        "name": "Junhyuk Choi"
                    },
                    {
                        "name": "Jihwan Seol"
                    },
                    {
                        "name": "Nayeon Kim"
                    },
                    {
                        "name": "Chanhee Cho"
                    },
                    {
                        "name": "EunBin Cho"
                    },
                    {
                        "name": "Bugeun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Bugeun Kim"
                },
                "author": "Bugeun Kim",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18485v2",
                "updated": "2025-09-25T13:11:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    11,
                    7,
                    3,
                    268,
                    0
                ],
                "published": "2025-06-23T10:37:57Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    10,
                    37,
                    57,
                    0,
                    174,
                    0
                ],
                "title": "A Simple \"Motivation\" Can Enhance Reinforcement Finetuning of Large\n  Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple \"Motivation\" Can Enhance Reinforcement Finetuning of Large\n  Reasoning Models"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful learn-to-reason paradigm for Large Reasoning Models to tackle complex\ntasks. However, current RLVR paradigm is still not efficient enough, as it\nworks in a trial-and-error manner. To perform better, the model needs to\nexplore the reward space by numerously generating responses and learn from\nfragmented reward signals, blind to the overall reward patterns. Fortunately,\nverifiable rewards make the natural language description of the reward function\npossible, and meanwhile, LLMs have demonstrated strong in-context learning\nability. This motivates us to explore if Large Reasoning Models can benefit\nfrom a motivation of the task, i.e., awareness of the reward function, during\nthe reinforcement finetuning process, as we humans sometimes do when learning.\nIn this paper, we introduce Motivation-enhanced Reinforcement Finetuning\n(MeRF), an intuitive yet effective method enhancing reinforcement finetuning of\nLLMs by involving ``telling LLMs rules of the game''. Specifically, MeRF\ndirectly injects the reward specification into the prompt, which serves as an\nin-context motivation for the model to be aware of the optimization objective.\nThis simple modification leverages the in-context learning ability of LLMs,\naligning generation with optimization, thereby incentivizing the model to\ngenerate desired outputs from both inner motivation and external reward.\nEmpirical evaluations demonstrate that MeRF achieves substantial performance\ngains over RLVR baseline. Moreover, ablation studies show that MeRF performs\nbetter with greater consistency between the in-context motivation and the\nexternal reward function, while the model also demonstrates an ability to adapt\nto misleading motivations through reinforcement finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful learn-to-reason paradigm for Large Reasoning Models to tackle complex\ntasks. However, current RLVR paradigm is still not efficient enough, as it\nworks in a trial-and-error manner. To perform better, the model needs to\nexplore the reward space by numerously generating responses and learn from\nfragmented reward signals, blind to the overall reward patterns. Fortunately,\nverifiable rewards make the natural language description of the reward function\npossible, and meanwhile, LLMs have demonstrated strong in-context learning\nability. This motivates us to explore if Large Reasoning Models can benefit\nfrom a motivation of the task, i.e., awareness of the reward function, during\nthe reinforcement finetuning process, as we humans sometimes do when learning.\nIn this paper, we introduce Motivation-enhanced Reinforcement Finetuning\n(MeRF), an intuitive yet effective method enhancing reinforcement finetuning of\nLLMs by involving ``telling LLMs rules of the game''. Specifically, MeRF\ndirectly injects the reward specification into the prompt, which serves as an\nin-context motivation for the model to be aware of the optimization objective.\nThis simple modification leverages the in-context learning ability of LLMs,\naligning generation with optimization, thereby incentivizing the model to\ngenerate desired outputs from both inner motivation and external reward.\nEmpirical evaluations demonstrate that MeRF achieves substantial performance\ngains over RLVR baseline. Moreover, ablation studies show that MeRF performs\nbetter with greater consistency between the in-context motivation and the\nexternal reward function, while the model also demonstrates an ability to adapt\nto misleading motivations through reinforcement finetuning."
                },
                "authors": [
                    {
                        "name": "Junjie Zhang"
                    },
                    {
                        "name": "Guozheng Ma"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Ting-En Lin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21117v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21117v2",
                "updated": "2025-09-26T05:33:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    5,
                    33,
                    48,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-25T13:04:29Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    4,
                    29,
                    3,
                    268,
                    0
                ],
                "title": "TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them"
                },
                "summary": "The adoption of Large Language Models (LLMs) as automated evaluators\n(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation\nframeworks. We identify two fundamental types of inconsistencies: (1)\nScore-Comparison Inconsistency, where lower-rated responses outperform\nhigher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity\nInconsistency, manifested through circular preference chains (A>B>C>A) and\nequivalence contradictions (A=B=C\\neq A). We argue that these issues come from\ninformation loss in discrete rating systems and ambiguous tie judgments during\npairwise evaluation. We propose TrustJudge, a probabilistic framework that\naddresses these limitations through two key innovations: 1)\ndistribution-sensitive scoring that computes continuous expectations from\ndiscrete rating probabilities, preserving information entropy for more precise\nscoring, and 2) likelihood-aware aggregation that resolves transitivity\nviolations using bidirectional preference probabilities or perplexity. We also\nformalize the theoretical limitations of current LLM-as-a-judge frameworks and\ndemonstrate how TrustJudge's components overcome them. When evaluated with\nLlama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces\nScore-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise\nTransitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining\nhigher evaluation accuracy. Our work provides the first systematic analysis of\nevaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both\ntheoretical insights and practical solutions for reliable automated assessment.\nThe framework demonstrates consistent improvements across various model\narchitectures and scales, enabling more trustworthy LLM evaluation without\nrequiring additional training or human annotations. The codes can be found at\nhttps://github.com/TrustJudge/TrustJudge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of Large Language Models (LLMs) as automated evaluators\n(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation\nframeworks. We identify two fundamental types of inconsistencies: (1)\nScore-Comparison Inconsistency, where lower-rated responses outperform\nhigher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity\nInconsistency, manifested through circular preference chains (A>B>C>A) and\nequivalence contradictions (A=B=C\\neq A). We argue that these issues come from\ninformation loss in discrete rating systems and ambiguous tie judgments during\npairwise evaluation. We propose TrustJudge, a probabilistic framework that\naddresses these limitations through two key innovations: 1)\ndistribution-sensitive scoring that computes continuous expectations from\ndiscrete rating probabilities, preserving information entropy for more precise\nscoring, and 2) likelihood-aware aggregation that resolves transitivity\nviolations using bidirectional preference probabilities or perplexity. We also\nformalize the theoretical limitations of current LLM-as-a-judge frameworks and\ndemonstrate how TrustJudge's components overcome them. When evaluated with\nLlama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces\nScore-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise\nTransitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining\nhigher evaluation accuracy. Our work provides the first systematic analysis of\nevaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both\ntheoretical insights and practical solutions for reliable automated assessment.\nThe framework demonstrates consistent improvements across various model\narchitectures and scales, enabling more trustworthy LLM evaluation without\nrequiring additional training or human annotations. The codes can be found at\nhttps://github.com/TrustJudge/TrustJudge."
                },
                "authors": [
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Yunze Song"
                    },
                    {
                        "name": "Tingyuan Zhu"
                    },
                    {
                        "name": "Xuanwang Zhang"
                    },
                    {
                        "name": "Zhuohao Yu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Chiyu Song"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Zhen Wu"
                    },
                    {
                        "name": "Xinyu Dai"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Shikun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shikun Zhang"
                },
                "author": "Shikun Zhang",
                "arxiv_comment": "22 pages, 9 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21117v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21117v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.13148v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.13148v2",
                "updated": "2025-09-25T12:56:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    56,
                    49,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-18T17:58:13Z",
                "published_parsed": [
                    2025,
                    8,
                    18,
                    17,
                    58,
                    13,
                    0,
                    230,
                    0
                ],
                "title": "MDPO: Overcoming the Training-Inference Divide of Masked Diffusion\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MDPO: Overcoming the Training-Inference Divide of Masked Diffusion\n  Language Models"
                },
                "summary": "Diffusion language models, as a promising alternative to traditional\nautoregressive (AR) models, enable faster generation and richer conditioning on\nbidirectional context. However, they suffer from a key discrepancy between\ntraining and inference: during inference, MDLMs progressively reveal the\nstructure of the generated sequence by producing fewer and fewer masked tokens,\nwhereas this structure is ignored in training as tokens are masked at random.\nAlthough this discrepancy between training and inference can lead to suboptimal\nperformance, it has been largely overlooked by previous works, leaving closing\nthis gap between the two stages an open problem. To address this, we frame the\nproblem of learning effective denoising trajectories as a sequential\ndecision-making problem and use the resulting framework to apply reinforcement\nlearning. We propose a novel Masked Diffusion Policy Optimization (MDPO) to\nexploit the Markov property diffusion possesses and explicitly train the model\nunder the same progressive refining schedule used at inference. MDPO matches\nthe performance of the previous state-of-the-art (SOTA) method with 60x fewer\ngradient updates, while achieving average improvements of 9.6% on MATH500 and\n54.2% on Countdown over SOTA when trained within the same number of weight\nupdates. Additionally, we improve the remasking strategy of MDLMs as a plug-in\ninference replacement to overcome the limitation that the model cannot refine\ntokens flexibly. This training-free method, termed Running Confidence Remasking\n(RCR), consistently enhances performance and provides further improvements when\nused with MDPO. Our findings establish great potential for investigating the\ndiscrepancy between pre-training and inference of MDLMs. Code:\nhttps://github.com/autonomousvision/mdpo. Project Page:\nhttps://cli212.github.io/MDPO/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models, as a promising alternative to traditional\nautoregressive (AR) models, enable faster generation and richer conditioning on\nbidirectional context. However, they suffer from a key discrepancy between\ntraining and inference: during inference, MDLMs progressively reveal the\nstructure of the generated sequence by producing fewer and fewer masked tokens,\nwhereas this structure is ignored in training as tokens are masked at random.\nAlthough this discrepancy between training and inference can lead to suboptimal\nperformance, it has been largely overlooked by previous works, leaving closing\nthis gap between the two stages an open problem. To address this, we frame the\nproblem of learning effective denoising trajectories as a sequential\ndecision-making problem and use the resulting framework to apply reinforcement\nlearning. We propose a novel Masked Diffusion Policy Optimization (MDPO) to\nexploit the Markov property diffusion possesses and explicitly train the model\nunder the same progressive refining schedule used at inference. MDPO matches\nthe performance of the previous state-of-the-art (SOTA) method with 60x fewer\ngradient updates, while achieving average improvements of 9.6% on MATH500 and\n54.2% on Countdown over SOTA when trained within the same number of weight\nupdates. Additionally, we improve the remasking strategy of MDLMs as a plug-in\ninference replacement to overcome the limitation that the model cannot refine\ntokens flexibly. This training-free method, termed Running Confidence Remasking\n(RCR), consistently enhances performance and provides further improvements when\nused with MDPO. Our findings establish great potential for investigating the\ndiscrepancy between pre-training and inference of MDLMs. Code:\nhttps://github.com/autonomousvision/mdpo. Project Page:\nhttps://cli212.github.io/MDPO/."
                },
                "authors": [
                    {
                        "name": "Haoyu He"
                    },
                    {
                        "name": "Katrin Renz"
                    },
                    {
                        "name": "Yong Cao"
                    },
                    {
                        "name": "Andreas Geiger"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Geiger"
                },
                "author": "Andreas Geiger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.13148v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.13148v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21106v1",
                "updated": "2025-09-25T12:53:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    53,
                    7,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:53:07Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    53,
                    7,
                    3,
                    268,
                    0
                ],
                "title": "BESPOKE: Benchmark for Search-Augmented Large Language Model\n  Personalization via Diagnostic Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BESPOKE: Benchmark for Search-Augmented Large Language Model\n  Personalization via Diagnostic Feedback"
                },
                "summary": "Search-augmented large language models (LLMs) have advanced\ninformation-seeking tasks by integrating retrieval into generation, reducing\nusers' cognitive burden compared to traditional search systems. Yet they remain\ninsufficient for fully addressing diverse user needs, which requires\nrecognizing how the same query can reflect different intents across users and\ndelivering information in preferred forms. While recent systems such as ChatGPT\nand Gemini attempt personalization by leveraging user histories, systematic\nevaluation of such personalization is under-explored. To address this gap, we\npropose BESPOKE, the realistic benchmark for evaluating personalization in\nsearch-augmented LLMs. BESPOKE is designed to be both realistic, by collecting\nauthentic chat and search histories directly from humans, and diagnostic, by\npairing responses with fine-grained preference scores and feedback. The\nbenchmark is constructed through long-term, deeply engaged human annotation,\nwhere human annotators contributed their own histories, authored queries with\ndetailed information needs, and evaluated responses with scores and diagnostic\nfeedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key\nrequirements for effective personalization in information-seeking tasks,\nproviding a foundation for fine-grained evaluation of personalized\nsearch-augmented LLMs. Our code and data are available at\nhttps://augustinlib.github.io/BESPOKE/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-augmented large language models (LLMs) have advanced\ninformation-seeking tasks by integrating retrieval into generation, reducing\nusers' cognitive burden compared to traditional search systems. Yet they remain\ninsufficient for fully addressing diverse user needs, which requires\nrecognizing how the same query can reflect different intents across users and\ndelivering information in preferred forms. While recent systems such as ChatGPT\nand Gemini attempt personalization by leveraging user histories, systematic\nevaluation of such personalization is under-explored. To address this gap, we\npropose BESPOKE, the realistic benchmark for evaluating personalization in\nsearch-augmented LLMs. BESPOKE is designed to be both realistic, by collecting\nauthentic chat and search histories directly from humans, and diagnostic, by\npairing responses with fine-grained preference scores and feedback. The\nbenchmark is constructed through long-term, deeply engaged human annotation,\nwhere human annotators contributed their own histories, authored queries with\ndetailed information needs, and evaluated responses with scores and diagnostic\nfeedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key\nrequirements for effective personalization in information-seeking tasks,\nproviding a foundation for fine-grained evaluation of personalized\nsearch-augmented LLMs. Our code and data are available at\nhttps://augustinlib.github.io/BESPOKE/."
                },
                "authors": [
                    {
                        "name": "Hyunseo Kim"
                    },
                    {
                        "name": "Sangam Lee"
                    },
                    {
                        "name": "Kwangwook Seo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21104v1",
                "updated": "2025-09-25T12:50:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    50,
                    46,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:50:46Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    50,
                    46,
                    3,
                    268,
                    0
                ],
                "title": "PerHalluEval: Persian Hallucination Evaluation Benchmark for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PerHalluEval: Persian Hallucination Evaluation Benchmark for Large\n  Language Models"
                },
                "summary": "Hallucination is a persistent issue affecting all large language Models\n(LLMs), particularly within low-resource languages such as Persian.\nPerHalluEval (Persian Hallucination Evaluation) is the first dynamic\nhallucination evaluation benchmark tailored for the Persian language. Our\nbenchmark leverages a three-stage LLM-driven pipeline, augmented with human\nvalidation, to generate plausible answers and summaries regarding QA and\nsummarization tasks, focusing on detecting extrinsic and intrinsic\nhallucinations. Moreover, we used the log probabilities of generated tokens to\nselect the most believable hallucinated instances. In addition, we engaged\nhuman annotators to highlight Persian-specific contexts in the QA dataset in\norder to evaluate LLMs' performance on content specifically related to Persian\nculture. Our evaluation of 12 LLMs, including open- and closed-source models\nusing PerHalluEval, revealed that the models generally struggle in detecting\nhallucinated Persian text. We showed that providing external knowledge, i.e.,\nthe original document for the summarization task, could mitigate hallucination\npartially. Furthermore, there was no significant difference in terms of\nhallucination when comparing LLMs specifically trained for Persian with others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination is a persistent issue affecting all large language Models\n(LLMs), particularly within low-resource languages such as Persian.\nPerHalluEval (Persian Hallucination Evaluation) is the first dynamic\nhallucination evaluation benchmark tailored for the Persian language. Our\nbenchmark leverages a three-stage LLM-driven pipeline, augmented with human\nvalidation, to generate plausible answers and summaries regarding QA and\nsummarization tasks, focusing on detecting extrinsic and intrinsic\nhallucinations. Moreover, we used the log probabilities of generated tokens to\nselect the most believable hallucinated instances. In addition, we engaged\nhuman annotators to highlight Persian-specific contexts in the QA dataset in\norder to evaluate LLMs' performance on content specifically related to Persian\nculture. Our evaluation of 12 LLMs, including open- and closed-source models\nusing PerHalluEval, revealed that the models generally struggle in detecting\nhallucinated Persian text. We showed that providing external knowledge, i.e.,\nthe original document for the summarization task, could mitigate hallucination\npartially. Furthermore, there was no significant difference in terms of\nhallucination when comparing LLMs specifically trained for Persian with others."
                },
                "authors": [
                    {
                        "name": "Mohammad Hosseini"
                    },
                    {
                        "name": "Kimia Hosseini"
                    },
                    {
                        "name": "Shayan Bali"
                    },
                    {
                        "name": "Zahra Zanjani"
                    },
                    {
                        "name": "Saeedeh Momtazi"
                    }
                ],
                "author_detail": {
                    "name": "Saeedeh Momtazi"
                },
                "author": "Saeedeh Momtazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.05935v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.05935v5",
                "updated": "2025-09-25T12:50:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    50,
                    21,
                    3,
                    268,
                    0
                ],
                "published": "2025-02-09T15:32:46Z",
                "published_parsed": [
                    2025,
                    2,
                    9,
                    15,
                    32,
                    46,
                    6,
                    40,
                    0
                ],
                "title": "Interactive Inference: A Neuromorphic Theory of Human-Computer\n  Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Inference: A Neuromorphic Theory of Human-Computer\n  Interaction"
                },
                "summary": "Neuromorphic Human-Computer Interaction (HCI) is a theoretical approach to\ndesigning better user experiences (UX) motivated by advances in the\nunderstanding of the neurophysiology of the brain. Inspired by the\nneuroscientific theory of Active Inference, Interactive Inference is a first\nexample of such approach. It offers a simplified interpretation of Active\nInference that allows designers to more readily apply this theory to design and\nevaluation. In Interactive Inference, user behaviour is modeled as Bayesian\ninference on progress and goal distributions that predicts the next action. We\nshow how the error between goal and progress distributions, or Bayesian\nsurprise, can be modeled as a simple mean square error of the signal-to-noise\nratio (SNR) of a task. The problem is that the user's capacity to process\nBayesian surprise follows the logarithm of this SNR. This means errors rise\nquickly once average capacity is exceeded. Our model allows the quantitative\nanalysis of performance and error using one framework that can provide\nreal-time estimates of the mental load in users that needs to be minimized by\ndesign. We show how three basic laws of HCI, Hick's Law, Fitts' Law and the\nPower Law can be expressed using our model. We then test the validity of the\nmodel by empirically measuring how well it predicts human performance and error\nin a car following task. Results suggest that driver processing capacity indeed\nis a logarithmic function of the SNR of the distance to a lead car. This result\nprovides initial evidence that Interactive Interference can be useful as a new\ntheoretical design tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic Human-Computer Interaction (HCI) is a theoretical approach to\ndesigning better user experiences (UX) motivated by advances in the\nunderstanding of the neurophysiology of the brain. Inspired by the\nneuroscientific theory of Active Inference, Interactive Inference is a first\nexample of such approach. It offers a simplified interpretation of Active\nInference that allows designers to more readily apply this theory to design and\nevaluation. In Interactive Inference, user behaviour is modeled as Bayesian\ninference on progress and goal distributions that predicts the next action. We\nshow how the error between goal and progress distributions, or Bayesian\nsurprise, can be modeled as a simple mean square error of the signal-to-noise\nratio (SNR) of a task. The problem is that the user's capacity to process\nBayesian surprise follows the logarithm of this SNR. This means errors rise\nquickly once average capacity is exceeded. Our model allows the quantitative\nanalysis of performance and error using one framework that can provide\nreal-time estimates of the mental load in users that needs to be minimized by\ndesign. We show how three basic laws of HCI, Hick's Law, Fitts' Law and the\nPower Law can be expressed using our model. We then test the validity of the\nmodel by empirically measuring how well it predicts human performance and error\nin a car following task. Results suggest that driver processing capacity indeed\nis a logarithmic function of the SNR of the distance to a lead car. This result\nprovides initial evidence that Interactive Interference can be useful as a new\ntheoretical design tool."
                },
                "authors": [
                    {
                        "name": "Roel Vertegaal"
                    },
                    {
                        "name": "Timothy Merritt"
                    },
                    {
                        "name": "Saul Greenberg"
                    },
                    {
                        "name": "Aneesh P. Tarun"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Zafeirios Fountas"
                    }
                ],
                "author_detail": {
                    "name": "Zafeirios Fountas"
                },
                "author": "Zafeirios Fountas",
                "arxiv_comment": "18 pages, 7 figures, 1 table, 37 mathematical formulas, in press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.05935v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.05935v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06843v2",
                "updated": "2025-09-25T12:49:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    49,
                    56,
                    3,
                    268,
                    0
                ],
                "published": "2025-06-07T15:48:04Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    15,
                    48,
                    4,
                    5,
                    158,
                    0
                ],
                "title": "United Minds or Isolated Agents? Exploring Coordination of LLMs under\n  Cognitive Load Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "United Minds or Isolated Agents? Exploring Coordination of LLMs under\n  Cognitive Load Theory"
                },
                "summary": "Large Language Models (LLMs) exhibit a notable performance ceiling on\ncomplex, multi-faceted tasks, as they often fail to integrate diverse\ninformation or adhere to multiple constraints. We posit that such limitation\narises when the demands of a task exceed the LLM's effective cognitive load\ncapacity. This interpretation draws a strong analogy to Cognitive Load Theory\n(CLT) in cognitive science, which explains similar performance boundaries in\nthe human mind, and is further supported by emerging evidence that reveals LLMs\nhave bounded working memory characteristics. Building upon this CLT-grounded\nunderstanding, we introduce CoThinker, a novel LLM-based multi-agent framework\ndesigned to mitigate cognitive overload and enhance collaborative\nproblem-solving abilities. CoThinker operationalizes CLT principles by\ndistributing intrinsic cognitive load through agent specialization and managing\ntransactional load via structured communication and a collective working\nmemory. We empirically validate CoThinker on complex problem-solving tasks and\nfabricated high cognitive load scenarios, demonstrating improvements over\nexisting multi-agent baselines in solution quality and efficiency. Our analysis\nreveals characteristic interaction patterns, providing insights into the\nemergence of collective cognition and effective load management, thus offering\na principled approach to overcoming LLM performance ceilings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit a notable performance ceiling on\ncomplex, multi-faceted tasks, as they often fail to integrate diverse\ninformation or adhere to multiple constraints. We posit that such limitation\narises when the demands of a task exceed the LLM's effective cognitive load\ncapacity. This interpretation draws a strong analogy to Cognitive Load Theory\n(CLT) in cognitive science, which explains similar performance boundaries in\nthe human mind, and is further supported by emerging evidence that reveals LLMs\nhave bounded working memory characteristics. Building upon this CLT-grounded\nunderstanding, we introduce CoThinker, a novel LLM-based multi-agent framework\ndesigned to mitigate cognitive overload and enhance collaborative\nproblem-solving abilities. CoThinker operationalizes CLT principles by\ndistributing intrinsic cognitive load through agent specialization and managing\ntransactional load via structured communication and a collective working\nmemory. We empirically validate CoThinker on complex problem-solving tasks and\nfabricated high cognitive load scenarios, demonstrating improvements over\nexisting multi-agent baselines in solution quality and efficiency. Our analysis\nreveals characteristic interaction patterns, providing insights into the\nemergence of collective cognition and effective load management, thus offering\na principled approach to overcoming LLM performance ceilings."
                },
                "authors": [
                    {
                        "name": "HaoYang Shang"
                    },
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Zi Liang"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Haibo Hu"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21100v1",
                "updated": "2025-09-25T12:46:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    46,
                    46,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:46:46Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    46,
                    46,
                    3,
                    268,
                    0
                ],
                "title": "VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal\n  Reasoning by Iterative Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal\n  Reasoning by Iterative Perception"
                },
                "summary": "Inducing reasoning in multimodal large language models (MLLMs) is critical\nfor achieving human-level perception and understanding. Existing methods mainly\nleverage LLM reasoning to analyze parsed visuals, often limited by static\nperception stages. This paper introduces Visual Test-Time Scaling (VTTS), a\nnovel approach to enhance MLLMs' reasoning via iterative perception during\ninference. VTTS mimics humans' hierarchical attention by progressively refining\nfocus on high-confidence spatio-temporal regions, guided by updated textual\npredictions. Specifically, VTTS employs an Iterative Perception (ITP)\nmechanism, incorporating reinforcement learning with spatio-temporal\nsupervision to optimize reasoning. To support this paradigm, we also present\nVTTS-80K, a dataset tailored for iterative perception. These designs allows a\nMLLM to enhance its performance by increasing its perceptual compute. Extensive\nexperiments validate VTTS's effectiveness and generalization across diverse\ntasks and benchmarks. Our newly introduced Videochat-R1.5 model has achieved\nremarkable improvements, with an average increase of over 5\\%, compared to\nrobust baselines such as Qwen2.5VL-3B and -7B, across more than 15 benchmarks\nthat encompass video conversation, video reasoning, and spatio-temporal\nperception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inducing reasoning in multimodal large language models (MLLMs) is critical\nfor achieving human-level perception and understanding. Existing methods mainly\nleverage LLM reasoning to analyze parsed visuals, often limited by static\nperception stages. This paper introduces Visual Test-Time Scaling (VTTS), a\nnovel approach to enhance MLLMs' reasoning via iterative perception during\ninference. VTTS mimics humans' hierarchical attention by progressively refining\nfocus on high-confidence spatio-temporal regions, guided by updated textual\npredictions. Specifically, VTTS employs an Iterative Perception (ITP)\nmechanism, incorporating reinforcement learning with spatio-temporal\nsupervision to optimize reasoning. To support this paradigm, we also present\nVTTS-80K, a dataset tailored for iterative perception. These designs allows a\nMLLM to enhance its performance by increasing its perceptual compute. Extensive\nexperiments validate VTTS's effectiveness and generalization across diverse\ntasks and benchmarks. Our newly introduced Videochat-R1.5 model has achieved\nremarkable improvements, with an average increase of over 5\\%, compared to\nrobust baselines such as Qwen2.5VL-3B and -7B, across more than 15 benchmarks\nthat encompass video conversation, video reasoning, and spatio-temporal\nperception."
                },
                "authors": [
                    {
                        "name": "Ziang Yan"
                    },
                    {
                        "name": "Xinhao Li"
                    },
                    {
                        "name": "Yinan He"
                    },
                    {
                        "name": "Zhengrong Yue"
                    },
                    {
                        "name": "Xiangyu Zeng"
                    },
                    {
                        "name": "Yali Wang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Limin Wang"
                    },
                    {
                        "name": "Yi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wang"
                },
                "author": "Yi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21091v1",
                "updated": "2025-09-25T12:41:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    41,
                    5,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:41:05Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    41,
                    5,
                    3,
                    268,
                    0
                ],
                "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute"
                },
                "summary": "We study best-of-$N$ for large language models (LLMs) where the selection is\nbased on majority voting. In particular, we analyze the limit $N \\to \\infty$,\nwhich we denote as Best-of-$\\infty$. While this approach achieves impressive\nperformance in the limit, it requires an infinite test-time budget. To address\nthis, we propose an adaptive generation scheme that selects $N$ based on answer\nagreement, thereby efficiently allocating inference-time computation. Beyond\nadaptivity, we extend the framework to weighted ensembles of multiple LLMs,\nshowing that such mixtures can outperform any individual model. The optimal\nensemble weighting is formulated and efficiently computed as a mixed-integer\nlinear program. Extensive experiments demonstrate the effectiveness of our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study best-of-$N$ for large language models (LLMs) where the selection is\nbased on majority voting. In particular, we analyze the limit $N \\to \\infty$,\nwhich we denote as Best-of-$\\infty$. While this approach achieves impressive\nperformance in the limit, it requires an infinite test-time budget. To address\nthis, we propose an adaptive generation scheme that selects $N$ based on answer\nagreement, thereby efficiently allocating inference-time computation. Beyond\nadaptivity, we extend the framework to weighted ensembles of multiple LLMs,\nshowing that such mixtures can outperform any individual model. The optimal\nensemble weighting is formulated and efficiently computed as a mixed-integer\nlinear program. Extensive experiments demonstrate the effectiveness of our\napproach."
                },
                "authors": [
                    {
                        "name": "Junpei Komiyama"
                    },
                    {
                        "name": "Daisuke Oba"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oyamada"
                },
                "author": "Masafumi Oyamada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21090v1",
                "updated": "2025-09-25T12:40:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    40,
                    7,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:40:07Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    40,
                    7,
                    3,
                    268,
                    0
                ],
                "title": "Task-Oriented Computation Offloading for Edge Inference: An Integrated\n  Bayesian Optimization and Deep Reinforcement Learning Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-Oriented Computation Offloading for Edge Inference: An Integrated\n  Bayesian Optimization and Deep Reinforcement Learning Framework"
                },
                "summary": "Edge intelligence (EI) allows resource-constrained edge devices (EDs) to\noffload computation-intensive AI tasks (e.g., visual object detection) to edge\nservers (ESs) for fast execution. However, transmitting high-volume raw task\ndata (e.g., 4K video) over bandwidth-limited wireless networks incurs\nsignificant latency. While EDs can reduce transmission latency by degrading\ndata before transmission (e.g., reducing resolution from 4K to 720p or 480p),\nit often deteriorates inference accuracy, creating a critical accuracy-latency\ntradeoff. The difficulty in balancing this tradeoff stems from the absence of\nclosed-form models capturing content-dependent accuracy-latency relationships.\nBesides, under bandwidth sharing constraints, the discrete degradation\ndecisions among the EDs demonstrate inherent combinatorial complexity.\nMathematically, it requires solving a challenging \\textit{black-box}\nmixed-integer nonlinear programming (MINLP). To address this problem, we\npropose LAB, a novel learning framework that seamlessly integrates deep\nreinforcement learning (DRL) and Bayesian optimization (BO). Specifically, LAB\nemploys: (a) a DNN-based actor that maps input system state to degradation\nactions, directly addressing the combinatorial complexity of the MINLP; and (b)\na BO-based critic with an explicit model built from fitting a Gaussian process\nsurrogate with historical observations, enabling model-based evaluation of\ndegradation actions. For each selected action, optimal bandwidth allocation is\nthen efficiently derived via convex optimization. Numerical evaluations on\nreal-world self-driving datasets demonstrate that LAB achieves near-optimal\naccuracy-latency tradeoff, exhibiting only 1.22\\% accuracy degradation and\n0.07s added latency compared to exhaustive search...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge intelligence (EI) allows resource-constrained edge devices (EDs) to\noffload computation-intensive AI tasks (e.g., visual object detection) to edge\nservers (ESs) for fast execution. However, transmitting high-volume raw task\ndata (e.g., 4K video) over bandwidth-limited wireless networks incurs\nsignificant latency. While EDs can reduce transmission latency by degrading\ndata before transmission (e.g., reducing resolution from 4K to 720p or 480p),\nit often deteriorates inference accuracy, creating a critical accuracy-latency\ntradeoff. The difficulty in balancing this tradeoff stems from the absence of\nclosed-form models capturing content-dependent accuracy-latency relationships.\nBesides, under bandwidth sharing constraints, the discrete degradation\ndecisions among the EDs demonstrate inherent combinatorial complexity.\nMathematically, it requires solving a challenging \\textit{black-box}\nmixed-integer nonlinear programming (MINLP). To address this problem, we\npropose LAB, a novel learning framework that seamlessly integrates deep\nreinforcement learning (DRL) and Bayesian optimization (BO). Specifically, LAB\nemploys: (a) a DNN-based actor that maps input system state to degradation\nactions, directly addressing the combinatorial complexity of the MINLP; and (b)\na BO-based critic with an explicit model built from fitting a Gaussian process\nsurrogate with historical observations, enabling model-based evaluation of\ndegradation actions. For each selected action, optimal bandwidth allocation is\nthen efficiently derived via convex optimization. Numerical evaluations on\nreal-world self-driving datasets demonstrate that LAB achieves near-optimal\naccuracy-latency tradeoff, exhibiting only 1.22\\% accuracy degradation and\n0.07s added latency compared to exhaustive search..."
                },
                "authors": [
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Suzhi Bi"
                    },
                    {
                        "name": "Ying-Jun Angela Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ying-Jun Angela Zhang"
                },
                "author": "Ying-Jun Angela Zhang",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21086v1",
                "updated": "2025-09-25T12:39:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    39,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:39:06Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    39,
                    6,
                    3,
                    268,
                    0
                ],
                "title": "UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep\n  Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep\n  Decomposition"
                },
                "summary": "We propose a novel architecture UniTransfer, which introduces both spatial\nand diffusion timestep decomposition in a progressive paradigm, achieving\nprecise and controllable video concept transfer. Specifically, in terms of\nspatial decomposition, we decouple videos into three key components: the\nforeground subject, the background, and the motion flow. Building upon this\ndecomposed formulation, we further introduce a dual-to-single-stream DiT-based\narchitecture for supporting fine-grained control over different components in\nthe videos. We also introduce a self-supervised pretraining strategy based on\nrandom masking to enhance the decomposed representation learning from\nlarge-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning\nparadigm, we further revisit the denoising diffusion process and propose a\nChain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We\ndecompose the denoising process into three stages of different granularity and\nleverage large language models (LLMs) for stage-specific instructions to guide\nthe generation progressively. We also curate an animal-centric video dataset\ncalled OpenAnimal to facilitate the advancement and benchmarking of research in\nvideo concept transfer. Extensive experiments demonstrate that our method\nachieves high-quality and controllable video concept transfer across diverse\nreference images and scenes, surpassing existing baselines in both visual\nfidelity and editability. Web Page:\nhttps://yu-shaonian.github.io/UniTransfer-Web/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel architecture UniTransfer, which introduces both spatial\nand diffusion timestep decomposition in a progressive paradigm, achieving\nprecise and controllable video concept transfer. Specifically, in terms of\nspatial decomposition, we decouple videos into three key components: the\nforeground subject, the background, and the motion flow. Building upon this\ndecomposed formulation, we further introduce a dual-to-single-stream DiT-based\narchitecture for supporting fine-grained control over different components in\nthe videos. We also introduce a self-supervised pretraining strategy based on\nrandom masking to enhance the decomposed representation learning from\nlarge-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning\nparadigm, we further revisit the denoising diffusion process and propose a\nChain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We\ndecompose the denoising process into three stages of different granularity and\nleverage large language models (LLMs) for stage-specific instructions to guide\nthe generation progressively. We also curate an animal-centric video dataset\ncalled OpenAnimal to facilitate the advancement and benchmarking of research in\nvideo concept transfer. Extensive experiments demonstrate that our method\nachieves high-quality and controllable video concept transfer across diverse\nreference images and scenes, surpassing existing baselines in both visual\nfidelity and editability. Web Page:\nhttps://yu-shaonian.github.io/UniTransfer-Web/"
                },
                "authors": [
                    {
                        "name": "Guojun Lei"
                    },
                    {
                        "name": "Rong Zhang"
                    },
                    {
                        "name": "Chi Wang"
                    },
                    {
                        "name": "Tianhang Liu"
                    },
                    {
                        "name": "Hong Li"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Weiwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Weiwei Xu"
                },
                "author": "Weiwei Xu",
                "arxiv_comment": "NeuriIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.05482v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.05482v2",
                "updated": "2025-09-25T12:35:37Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    35,
                    37,
                    3,
                    268,
                    0
                ],
                "published": "2025-07-07T21:14:27Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    21,
                    14,
                    27,
                    0,
                    188,
                    0
                ],
                "title": "Training-Free Stein Diffusion Guidance: Posterior Correction for\n  Sampling Beyond High-Density Regions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Stein Diffusion Guidance: Posterior Correction for\n  Sampling Beyond High-Density Regions"
                },
                "summary": "Training free diffusion guidance provides a flexible way to leverage\noff-the-shelf classifiers without additional training. Yet, current approaches\nhinge on posterior approximations via Tweedie's formula, which often yield\nunreliable guidance, particularly in low-density regions. Stochastic optimal\ncontrol (SOC), in contrast, provides principled posterior simulation but is\nprohibitively expensive for fast sampling. In this work, we reconcile the\nstrengths of these paradigms by introducing Stein Diffusion Guidance (SDG), a\nnovel training-free framework grounded in a surrogate SOC objective. We\nestablish a theoretical bound on the value function, demonstrating the\nnecessity of correcting approximate posteriors to faithfully reflect true\ndiffusion dynamics. Leveraging Stein variational inference, SDG identifies the\nsteepest descent direction that minimizes the Kullback-Leibler divergence\nbetween approximate and true posteriors. By incorporating a principled Stein\ncorrection mechanism and a novel running cost functional, SDG enables effective\nguidance in low-density regions. Experiments on molecular low-density sampling\ntasks suggest that SDG consistently surpasses standard training-free guidance\nmethods, highlighting its potential for broader diffusion-based sampling beyond\nhigh-density regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training free diffusion guidance provides a flexible way to leverage\noff-the-shelf classifiers without additional training. Yet, current approaches\nhinge on posterior approximations via Tweedie's formula, which often yield\nunreliable guidance, particularly in low-density regions. Stochastic optimal\ncontrol (SOC), in contrast, provides principled posterior simulation but is\nprohibitively expensive for fast sampling. In this work, we reconcile the\nstrengths of these paradigms by introducing Stein Diffusion Guidance (SDG), a\nnovel training-free framework grounded in a surrogate SOC objective. We\nestablish a theoretical bound on the value function, demonstrating the\nnecessity of correcting approximate posteriors to faithfully reflect true\ndiffusion dynamics. Leveraging Stein variational inference, SDG identifies the\nsteepest descent direction that minimizes the Kullback-Leibler divergence\nbetween approximate and true posteriors. By incorporating a principled Stein\ncorrection mechanism and a novel running cost functional, SDG enables effective\nguidance in low-density regions. Experiments on molecular low-density sampling\ntasks suggest that SDG consistently surpasses standard training-free guidance\nmethods, highlighting its potential for broader diffusion-based sampling beyond\nhigh-density regions."
                },
                "authors": [
                    {
                        "name": "Van Khoa Nguyen"
                    },
                    {
                        "name": "Lionel Blond"
                    },
                    {
                        "name": "Alexandros Kalousis"
                    }
                ],
                "author_detail": {
                    "name": "Alexandros Kalousis"
                },
                "author": "Alexandros Kalousis",
                "arxiv_comment": "Revised version with additional results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.05482v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.05482v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21081v1",
                "updated": "2025-09-25T12:32:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    32,
                    2,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:32:02Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    32,
                    2,
                    3,
                    268,
                    0
                ],
                "title": "TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix"
                },
                "summary": "Multi-Head Latent Attention (MLA) is a recent attention mechanism adopted in\nstate-of-the-art LLMs such as DeepSeek-v3 and Kimi K2. Thanks to its novel\nformulation, MLA allows two functionally equivalent but computationally\ndistinct kernel implementations: naive and absorb. While the naive kernels\n(e.g., FlashAttention) are typically preferred in training and prefill for\ntheir computational efficiency, existing decoding kernels (e.g., FlashMLA) rely\non the absorb method to minimize HBM bandwidth usage. However, the\ncompute-bound nature of the absorb implementations prohibits performance\nbenefits from data reuse opportunities in attention calculations, such as\nshared prefixes. In this work, we introduce TyphoonMLA, a hybrid approach that\ncombines naive and absorb formulations to harness the strengths of both.\nTyphoonMLA effectively leverages the shared prefix by applying the naive\nformulation to the compute-bound parts of attention calculations, while\nreducing the bandwidth requirements for non-shared parts by using the absorb\nformulation. As a result, TyphoonMLA improves the throughput of attention\ncalculations in MLA architectures by up to 3x and 3.24x on NPU and GPUs, with\nonly a 3% overhead in HBM size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA) is a recent attention mechanism adopted in\nstate-of-the-art LLMs such as DeepSeek-v3 and Kimi K2. Thanks to its novel\nformulation, MLA allows two functionally equivalent but computationally\ndistinct kernel implementations: naive and absorb. While the naive kernels\n(e.g., FlashAttention) are typically preferred in training and prefill for\ntheir computational efficiency, existing decoding kernels (e.g., FlashMLA) rely\non the absorb method to minimize HBM bandwidth usage. However, the\ncompute-bound nature of the absorb implementations prohibits performance\nbenefits from data reuse opportunities in attention calculations, such as\nshared prefixes. In this work, we introduce TyphoonMLA, a hybrid approach that\ncombines naive and absorb formulations to harness the strengths of both.\nTyphoonMLA effectively leverages the shared prefix by applying the naive\nformulation to the compute-bound parts of attention calculations, while\nreducing the bandwidth requirements for non-shared parts by using the absorb\nformulation. As a result, TyphoonMLA improves the throughput of attention\ncalculations in MLA architectures by up to 3x and 3.24x on NPU and GPUs, with\nonly a 3% overhead in HBM size."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yzgler"
                    },
                    {
                        "name": "Ahmet elik"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14295v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14295v3",
                "updated": "2025-09-25T12:30:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    30,
                    29,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-17T02:31:03Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    2,
                    31,
                    3,
                    2,
                    260,
                    0
                ],
                "title": "Aegis: Automated Error Generation and Identification for Multi-Agent\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aegis: Automated Error Generation and Identification for Multi-Agent\n  Systems"
                },
                "summary": "As Multi-Agent Systems (MAS) become increasingly autonomous and complex,\nunderstanding their error modes is critical for ensuring their reliability and\nsafety. However, research in this area has been severely hampered by the lack\nof large-scale, diverse datasets with precise, ground-truth error labels. To\naddress this bottleneck, we introduce \\textbf{AEGIS}, a novel framework for\n\\textbf{A}utomated \\textbf{E}rror \\textbf{G}eneration and\n\\textbf{I}dentification for Multi-Agent \\textbf{S}ystems. By systematically\ninjecting controllable and traceable errors into initially successful\ntrajectories, we create a rich dataset of realistic failures. This is achieved\nusing a context-aware, LLM-based adaptive manipulator that performs\nsophisticated attacks like prompt injection and response corruption to induce\nspecific, predefined error modes. We demonstrate the value of our dataset by\nexploring three distinct learning paradigms for the error identification task:\nSupervised Fine-Tuning, Reinforcement Learning, and Contrastive Learning. Our\ncomprehensive experiments show that models trained on AEGIS data achieve\nsubstantial improvements across all three learning paradigms. Notably, several\nof our fine-tuned models demonstrate performance competitive with or superior\nto proprietary systems an order of magnitude larger, validating our automated\ndata generation framework as a crucial resource for developing more robust and\ninterpretable multi-agent systems. Our project website is available at\nhttps://kfq20.github.io/AEGIS-Website.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Multi-Agent Systems (MAS) become increasingly autonomous and complex,\nunderstanding their error modes is critical for ensuring their reliability and\nsafety. However, research in this area has been severely hampered by the lack\nof large-scale, diverse datasets with precise, ground-truth error labels. To\naddress this bottleneck, we introduce \\textbf{AEGIS}, a novel framework for\n\\textbf{A}utomated \\textbf{E}rror \\textbf{G}eneration and\n\\textbf{I}dentification for Multi-Agent \\textbf{S}ystems. By systematically\ninjecting controllable and traceable errors into initially successful\ntrajectories, we create a rich dataset of realistic failures. This is achieved\nusing a context-aware, LLM-based adaptive manipulator that performs\nsophisticated attacks like prompt injection and response corruption to induce\nspecific, predefined error modes. We demonstrate the value of our dataset by\nexploring three distinct learning paradigms for the error identification task:\nSupervised Fine-Tuning, Reinforcement Learning, and Contrastive Learning. Our\ncomprehensive experiments show that models trained on AEGIS data achieve\nsubstantial improvements across all three learning paradigms. Notably, several\nof our fine-tuned models demonstrate performance competitive with or superior\nto proprietary systems an order of magnitude larger, validating our automated\ndata generation framework as a crucial resource for developing more robust and\ninterpretable multi-agent systems. Our project website is available at\nhttps://kfq20.github.io/AEGIS-Website."
                },
                "authors": [
                    {
                        "name": "Fanqi Kong"
                    },
                    {
                        "name": "Ruijie Zhang"
                    },
                    {
                        "name": "Huaxiao Yin"
                    },
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Xiaofei Zhang"
                    },
                    {
                        "name": "Ziang Chen"
                    },
                    {
                        "name": "Zhaowei Zhang"
                    },
                    {
                        "name": "Xiaoyuan Zhang"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Xue Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Feng"
                },
                "author": "Xue Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14295v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14295v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07522v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07522v2",
                "updated": "2025-09-25T12:30:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    30,
                    9,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-12T13:03:26Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    13,
                    3,
                    26,
                    0,
                    132,
                    0
                ],
                "title": "Byam: Fixing Breaking Dependency Updates with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Byam: Fixing Breaking Dependency Updates with Large Language Models"
                },
                "summary": "Application Programming Interfaces (APIs) facilitate the integration of\nthird-party dependencies within the code of client applications. However,\nchanges to an API, such as deprecation, modification of parameter names or\ntypes, or complete replacement with a new API, can break existing client code.\nThese changes are called breaking dependency updates; It is often tedious for\nAPI users to identify the cause of these breaks and update their code\naccordingly. In this paper, we explore the use of Large Language Models (LLMs)\nto automate client code updates in response to breaking dependency updates. We\nevaluate our approach on the BUMP dataset, a benchmark for breaking dependency\nupdates in Java projects. Our approach leverages LLMs with advanced prompts,\nincluding information from the build process and from the breaking dependency\nanalysis. We assess effectiveness at three granularity levels: at the build\nlevel, the file level, and the individual compilation error level. We\nexperiment with five LLMs: Google Gemini-2.0 Flash, OpenAI GPT4o-mini, OpenAI\no3-mini, Alibaba Qwen2.5-32b-instruct, and DeepSeek V3. Our results show that\nLLMs can automatically repair breaking updates. Among the considered models,\nOpenAI's o3-mini is the best, able to completely fix 27% of the builds when\nusing prompts that include contextual information such as the buggy line, API\ndifferences, error messages, and step-by-step reasoning instructions. Also, it\nfixes 78% of the individual compilation errors. Overall, our findings\ndemonstrate the potential for LLMs to fix compilation errors due to breaking\ndependency updates, supporting developers in their efforts to stay up-to-date\nwith changes in their dependencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application Programming Interfaces (APIs) facilitate the integration of\nthird-party dependencies within the code of client applications. However,\nchanges to an API, such as deprecation, modification of parameter names or\ntypes, or complete replacement with a new API, can break existing client code.\nThese changes are called breaking dependency updates; It is often tedious for\nAPI users to identify the cause of these breaks and update their code\naccordingly. In this paper, we explore the use of Large Language Models (LLMs)\nto automate client code updates in response to breaking dependency updates. We\nevaluate our approach on the BUMP dataset, a benchmark for breaking dependency\nupdates in Java projects. Our approach leverages LLMs with advanced prompts,\nincluding information from the build process and from the breaking dependency\nanalysis. We assess effectiveness at three granularity levels: at the build\nlevel, the file level, and the individual compilation error level. We\nexperiment with five LLMs: Google Gemini-2.0 Flash, OpenAI GPT4o-mini, OpenAI\no3-mini, Alibaba Qwen2.5-32b-instruct, and DeepSeek V3. Our results show that\nLLMs can automatically repair breaking updates. Among the considered models,\nOpenAI's o3-mini is the best, able to completely fix 27% of the builds when\nusing prompts that include contextual information such as the buggy line, API\ndifferences, error messages, and step-by-step reasoning instructions. Also, it\nfixes 78% of the individual compilation errors. Overall, our findings\ndemonstrate the potential for LLMs to fix compilation errors due to breaking\ndependency updates, supporting developers in their efforts to stay up-to-date\nwith changes in their dependencies."
                },
                "authors": [
                    {
                        "name": "Frank Reyes"
                    },
                    {
                        "name": "May Mahmoud"
                    },
                    {
                        "name": "Federico Bono"
                    },
                    {
                        "name": "Sarah Nadi"
                    },
                    {
                        "name": "Benoit Baudry"
                    },
                    {
                        "name": "Martin Monperrus"
                    }
                ],
                "author_detail": {
                    "name": "Martin Monperrus"
                },
                "author": "Martin Monperrus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07522v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07522v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21080v1",
                "updated": "2025-09-25T12:28:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    28,
                    25,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:28:25Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    28,
                    25,
                    3,
                    268,
                    0
                ],
                "title": "Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and\n  Agentic Mitigation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and\n  Agentic Mitigation in LLMs"
                },
                "summary": "Large language models (LLMs) have unlocked a wide range of downstream\ngenerative applications. However, we found that they also risk perpetuating\nsubtle fairness issues tied to culture, positioning their generations from the\nperspectives of the mainstream US culture while demonstrating salient\nexternality towards non-mainstream ones. In this work, we identify and\nsystematically investigate this novel culture positioning bias, in which an\nLLM's default generative stance aligns with a mainstream view and treats other\ncultures as outsiders. We propose the CultureLens benchmark with 4000\ngeneration prompts and 3 evaluation metrics for quantifying this bias through\nthe lens of a culturally situated interview script generation task, in which an\nLLM is positioned as an onsite reporter interviewing local people across 10\ndiverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a\nstark pattern: while models adopt insider tones in over 88 percent of\nUS-contexted scripts on average, they disproportionately adopt mainly outsider\nstances for less dominant cultures. To resolve these biases, we propose 2\ninference-time mitigation methods: a baseline prompt-based Fairness\nIntervention Pillars (FIP) method, and a structured Mitigation via Fairness\nAgents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent)\nintroduces a self-reflection and rewriting loop based on fairness guidelines.\n(2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized\nagents: a Planner Agent(initial script generation), a Critique Agent (evaluates\ninitial script against fairness pillars), and a Refinement Agent (incorporates\nfeedback to produce a polished, unbiased script). Empirical results showcase\nthe effectiveness of agent-based methods as a promising direction for\nmitigating biases in generative LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a wide range of downstream\ngenerative applications. However, we found that they also risk perpetuating\nsubtle fairness issues tied to culture, positioning their generations from the\nperspectives of the mainstream US culture while demonstrating salient\nexternality towards non-mainstream ones. In this work, we identify and\nsystematically investigate this novel culture positioning bias, in which an\nLLM's default generative stance aligns with a mainstream view and treats other\ncultures as outsiders. We propose the CultureLens benchmark with 4000\ngeneration prompts and 3 evaluation metrics for quantifying this bias through\nthe lens of a culturally situated interview script generation task, in which an\nLLM is positioned as an onsite reporter interviewing local people across 10\ndiverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a\nstark pattern: while models adopt insider tones in over 88 percent of\nUS-contexted scripts on average, they disproportionately adopt mainly outsider\nstances for less dominant cultures. To resolve these biases, we propose 2\ninference-time mitigation methods: a baseline prompt-based Fairness\nIntervention Pillars (FIP) method, and a structured Mitigation via Fairness\nAgents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent)\nintroduces a self-reflection and rewriting loop based on fairness guidelines.\n(2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized\nagents: a Planner Agent(initial script generation), a Critique Agent (evaluates\ninitial script against fairness pillars), and a Refinement Agent (incorporates\nfeedback to produce a polished, unbiased script). Empirical results showcase\nthe effectiveness of agent-based methods as a promising direction for\nmitigating biases in generative LLMs."
                },
                "authors": [
                    {
                        "name": "Yixin Wan"
                    },
                    {
                        "name": "Xingrun Chen"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21079v1",
                "updated": "2025-09-25T12:28:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    28,
                    22,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:28:22Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    28,
                    22,
                    3,
                    268,
                    0
                ],
                "title": "SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials"
                },
                "summary": "Foundation models have shown remarkable capabilities in various domains, but\ntheir performance on complex, multimodal engineering problems remains largely\nunexplored. We introduce SoM-1K, the first large-scale multimodal benchmark\ndataset dedicated to evaluating foundation models on problems in the strength\nof materials (SoM). The dataset, which contains 1,065 annotated SoM problems,\nmirrors real-world engineering tasks by including both textual problem\nstatements and schematic diagrams. Due to the limited capabilities of current\nfoundation models in understanding complicated visual information, we propose a\nnovel prompting strategy called Descriptions of Images (DoI), which provides\nrigorous expert-generated text descriptions of the visual diagrams as the\ncontext. We evaluate eight representative foundation models, including both\nlarge language models (LLMs) and vision language models (VLMs). Our results\nshow that current foundation models struggle significantly with these\nengineering problems, with the best-performing model achieving only 56.6%\naccuracy. Interestingly, we found that LLMs, when provided with DoI, often\noutperform VLMs provided with visual diagrams. A detailed error analysis\nreveals that DoI plays a crucial role in mitigating visual misinterpretation\nerrors, suggesting that accurate text-based descriptions can be more effective\nthan direct image input for current foundation models. This work establishes a\nrigorous benchmark for engineering AI and highlights a critical need for\ndeveloping more robust multimodal reasoning capabilities in foundation models,\nparticularly in scientific and engineering contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models have shown remarkable capabilities in various domains, but\ntheir performance on complex, multimodal engineering problems remains largely\nunexplored. We introduce SoM-1K, the first large-scale multimodal benchmark\ndataset dedicated to evaluating foundation models on problems in the strength\nof materials (SoM). The dataset, which contains 1,065 annotated SoM problems,\nmirrors real-world engineering tasks by including both textual problem\nstatements and schematic diagrams. Due to the limited capabilities of current\nfoundation models in understanding complicated visual information, we propose a\nnovel prompting strategy called Descriptions of Images (DoI), which provides\nrigorous expert-generated text descriptions of the visual diagrams as the\ncontext. We evaluate eight representative foundation models, including both\nlarge language models (LLMs) and vision language models (VLMs). Our results\nshow that current foundation models struggle significantly with these\nengineering problems, with the best-performing model achieving only 56.6%\naccuracy. Interestingly, we found that LLMs, when provided with DoI, often\noutperform VLMs provided with visual diagrams. A detailed error analysis\nreveals that DoI plays a crucial role in mitigating visual misinterpretation\nerrors, suggesting that accurate text-based descriptions can be more effective\nthan direct image input for current foundation models. This work establishes a\nrigorous benchmark for engineering AI and highlights a critical need for\ndeveloping more robust multimodal reasoning capabilities in foundation models,\nparticularly in scientific and engineering contexts."
                },
                "authors": [
                    {
                        "name": "Qixin Wan"
                    },
                    {
                        "name": "Zilong Wang"
                    },
                    {
                        "name": "Jingwen Zhou"
                    },
                    {
                        "name": "Wanting Wang"
                    },
                    {
                        "name": "Ziheng Geng"
                    },
                    {
                        "name": "Jiachen Liu"
                    },
                    {
                        "name": "Ran Cao"
                    },
                    {
                        "name": "Minghui Cheng"
                    },
                    {
                        "name": "Lu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Cheng"
                },
                "author": "Lu Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01403v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01403v4",
                "updated": "2025-09-25T12:28:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    28,
                    19,
                    3,
                    268,
                    0
                ],
                "published": "2025-02-03T14:34:37Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    14,
                    34,
                    37,
                    0,
                    34,
                    0
                ],
                "title": "AdaSVD: Adaptive Singular Value Decomposition for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaSVD: Adaptive Singular Value Decomposition for Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP) tasks, yet their substantial memory requirements\npresent significant challenges for deployment on resource-constrained devices.\nSingular Value Decomposition (SVD) has emerged as a promising compression\ntechnique for LLMs, offering considerable reductions in memory overhead.\nHowever, existing SVD-based methods often struggle to effectively mitigate the\nerrors introduced by SVD truncation, leading to a noticeable performance gap\nwhen compared to the original models. Furthermore, applying a uniform\ncompression ratio across all transformer layers fails to account for the\nvarying importance of different layers. To address these challenges, we propose\nAdaSVD, an adaptive SVD-based LLM compression approach. Specifically, AdaSVD\nintroduces adaComp, which adaptively compensates for SVD truncation errors by\nalternately updating the singular matrices $\\mathcal{U}$ and\n$\\mathcal{V}^\\top$. Additionally, AdaSVD introduces adaCR, which adaptively\nassigns layer-specific compression ratios based on the relative importance of\neach layer. Extensive experiments across multiple LLM/VLM families and\nevaluation metrics demonstrate that AdaSVD consistently outperforms\nstate-of-the-art (SOTA) SVD-based methods, achieving superior performance with\nsignificantly reduced memory requirements. Code and models of AdaSVD will be\navailable at https://github.com/ZHITENGLI/AdaSVD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP) tasks, yet their substantial memory requirements\npresent significant challenges for deployment on resource-constrained devices.\nSingular Value Decomposition (SVD) has emerged as a promising compression\ntechnique for LLMs, offering considerable reductions in memory overhead.\nHowever, existing SVD-based methods often struggle to effectively mitigate the\nerrors introduced by SVD truncation, leading to a noticeable performance gap\nwhen compared to the original models. Furthermore, applying a uniform\ncompression ratio across all transformer layers fails to account for the\nvarying importance of different layers. To address these challenges, we propose\nAdaSVD, an adaptive SVD-based LLM compression approach. Specifically, AdaSVD\nintroduces adaComp, which adaptively compensates for SVD truncation errors by\nalternately updating the singular matrices $\\mathcal{U}$ and\n$\\mathcal{V}^\\top$. Additionally, AdaSVD introduces adaCR, which adaptively\nassigns layer-specific compression ratios based on the relative importance of\neach layer. Extensive experiments across multiple LLM/VLM families and\nevaluation metrics demonstrate that AdaSVD consistently outperforms\nstate-of-the-art (SOTA) SVD-based methods, achieving superior performance with\nsignificantly reduced memory requirements. Code and models of AdaSVD will be\navailable at https://github.com/ZHITENGLI/AdaSVD."
                },
                "authors": [
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Mingyuan Xia"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Zheng Hui"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "The code and models will be available at\n  https://github.com/ZHITENGLI/AdaSVD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01403v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01403v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10248v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10248v3",
                "updated": "2025-09-25T12:26:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    26,
                    58,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-12T13:45:24Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    45,
                    24,
                    4,
                    255,
                    0
                ],
                "title": "Prompt Injection Attacks on LLM Generated Reviews of Scientific\n  Publications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Injection Attacks on LLM Generated Reviews of Scientific\n  Publications"
                },
                "summary": "The ongoing intense discussion on rising LLM usage in the scientific\npeer-review process has recently been mingled by reports of authors using\nhidden prompt injections to manipulate review scores. Since the existence of\nsuch \"attacks\" - although seen by some commentators as \"self-defense\" - would\nhave a great impact on the further debate, this paper investigates the\npracticability and technical success of the described manipulations. Our\nsystematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide\nrange of LLMs shows two distinct results: I) very simple prompt injections are\nindeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews\nare generally biased toward acceptance (>95% in many models). Both results have\ngreat impact on the ongoing discussions on LLM usage in peer-review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ongoing intense discussion on rising LLM usage in the scientific\npeer-review process has recently been mingled by reports of authors using\nhidden prompt injections to manipulate review scores. Since the existence of\nsuch \"attacks\" - although seen by some commentators as \"self-defense\" - would\nhave a great impact on the further debate, this paper investigates the\npracticability and technical success of the described manipulations. Our\nsystematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide\nrange of LLMs shows two distinct results: I) very simple prompt injections are\nindeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews\nare generally biased toward acceptance (>95% in many models). Both results have\ngreat impact on the ongoing discussions on LLM usage in peer-review."
                },
                "authors": [
                    {
                        "name": "Janis Keuper"
                    }
                ],
                "author_detail": {
                    "name": "Janis Keuper"
                },
                "author": "Janis Keuper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10248v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10248v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21075v1",
                "updated": "2025-09-25T12:25:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    25,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:25:06Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    25,
                    6,
                    3,
                    268,
                    0
                ],
                "title": "Communication Bias in Large Language Models: A Regulatory Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication Bias in Large Language Models: A Regulatory Perspective"
                },
                "summary": "Large language models (LLMs) are increasingly central to many applications,\nraising concerns about bias, fairness, and regulatory compliance. This paper\nreviews risks of biased outputs and their societal impact, focusing on\nframeworks like the EU's AI Act and the Digital Services Act. We argue that\nbeyond constant regulation, stronger attention to competition and design\ngovernance is needed to ensure fair, trustworthy AI. This is a preprint of the\nCommunications of the ACM article of the same title.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly central to many applications,\nraising concerns about bias, fairness, and regulatory compliance. This paper\nreviews risks of biased outputs and their societal impact, focusing on\nframeworks like the EU's AI Act and the Digital Services Act. We argue that\nbeyond constant regulation, stronger attention to competition and design\ngovernance is needed to ensure fair, trustworthy AI. This is a preprint of the\nCommunications of the ACM article of the same title."
                },
                "authors": [
                    {
                        "name": "Adrian Kuenzler"
                    },
                    {
                        "name": "Stefan Schmid"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Schmid"
                },
                "author": "Stefan Schmid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21074v1",
                "updated": "2025-09-25T12:24:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    24,
                    32,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:24:32Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    24,
                    32,
                    3,
                    268,
                    0
                ],
                "title": "RePro: Leveraging Large Language Models for Semi-Automated Reproduction\n  of Networking Research Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RePro: Leveraging Large Language Models for Semi-Automated Reproduction\n  of Networking Research Results"
                },
                "summary": "Reproducing networking research is a critical but challenging task due to the\nscarcity of open-source code. While Large Language Models (LLMs) can automate\ncode generation, current approaches lack the generalizability required for the\ndiverse networking field. To address this, we propose RePro, a semi-automated\nreproduction framework that leverages advanced prompt engineering to reproduce\nnetwork systems from their research papers. RePro combines few-shot in-context\nlearning with Structured and Semantic Chain of Thought (SCoT/SeCoT) techniques\nto systematically translate a paper's description into an optimized, executable\nimplementation. The framework operates through a three-stage pipeline: system\ndescription extraction, structural code generation, and code optimization. Our\nevaluation with five state-of-the-art LLMs across diverse network sub-domains\ndemonstrates that RePro significantly reduces reproduction time compared to\nmanual efforts while achieving comparable system performance, validating its\neffectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducing networking research is a critical but challenging task due to the\nscarcity of open-source code. While Large Language Models (LLMs) can automate\ncode generation, current approaches lack the generalizability required for the\ndiverse networking field. To address this, we propose RePro, a semi-automated\nreproduction framework that leverages advanced prompt engineering to reproduce\nnetwork systems from their research papers. RePro combines few-shot in-context\nlearning with Structured and Semantic Chain of Thought (SCoT/SeCoT) techniques\nto systematically translate a paper's description into an optimized, executable\nimplementation. The framework operates through a three-stage pipeline: system\ndescription extraction, structural code generation, and code optimization. Our\nevaluation with five state-of-the-art LLMs across diverse network sub-domains\ndemonstrates that RePro significantly reduces reproduction time compared to\nmanual efforts while achieving comparable system performance, validating its\neffectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yining Jiang"
                    },
                    {
                        "name": "Wenyun Xu"
                    },
                    {
                        "name": "Qingyu Song"
                    },
                    {
                        "name": "Yuling Lin"
                    },
                    {
                        "name": "Xuanhao Liu"
                    },
                    {
                        "name": "Xiaoqiang Zheng"
                    },
                    {
                        "name": "Qiang Su"
                    },
                    {
                        "name": "Lizhao You"
                    },
                    {
                        "name": "Lu Tang"
                    },
                    {
                        "name": "Wangjian Feng"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Qiao Xiang"
                    },
                    {
                        "name": "Jiwu Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwu Shu"
                },
                "author": "Jiwu Shu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16949v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16949v3",
                "updated": "2025-09-25T12:24:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    24,
                    28,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-23T08:47:31Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    8,
                    47,
                    31,
                    5,
                    235,
                    0
                ],
                "title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the Best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3. Our code is available at\nhttps://github.com/IANNXANG/RuscaRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the Best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3. Our code is available at\nhttps://github.com/IANNXANG/RuscaRL."
                },
                "authors": [
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Sunzhu Li"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Wenkai Fang"
                    },
                    {
                        "name": "Kongcheng Zhang"
                    },
                    {
                        "name": "Jiale Zhao"
                    },
                    {
                        "name": "Jingwen Yang"
                    },
                    {
                        "name": "Yihe Zhou"
                    },
                    {
                        "name": "Jianwei Lv"
                    },
                    {
                        "name": "Tongya Zheng"
                    },
                    {
                        "name": "Hengtong Lu"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Yan Xie"
                    },
                    {
                        "name": "Mingli Song"
                    }
                ],
                "author_detail": {
                    "name": "Mingli Song"
                },
                "author": "Mingli Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16949v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16949v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21073v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21073v1",
                "updated": "2025-09-25T12:23:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    23,
                    57,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:23:57Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    23,
                    57,
                    3,
                    268,
                    0
                ],
                "title": "Normalizing Flows are Capable Visuomotor Policy Learning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Normalizing Flows are Capable Visuomotor Policy Learning Models"
                },
                "summary": "The field of general purpose robotics has recently embraced powerful\nprobabilistic models, such as diffusion models, to model and learn complex\nbehaviors. However, these models often come with significant trade-offs, namely\nhigh computational costs for inference and a fundamental inability to quantify\noutput uncertainty. We argue that a model's trustworthiness, a critical factor\nfor reliable, general-purpose robotics, is inherently linked to its ability to\nprovide confidence measures.\n  In this work, we introduce Normalizing Flows Policy, a novel visuomotor\npolicy learning model based on Normalizing Flows. We show that Normalizing\nFlows are a natural and powerful alternative to diffusion models, providing\nboth a statistically sound measure of confidence and a highly efficient\ninference process. Through comprehensive experiments across four distinct\nsimulated robotic tasks, we demonstrate that Normalizing Flows Policy achieves\nperformance comparable to, and often surpassing, Diffusion Policy, and it does\nso not only with improved sample efficiency but also with up to 30 times faster\ninference. Additionally, our ablation study validates several key architectural\nand training techniques that enable Normalizing Flows to perform well in this\ndomain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of general purpose robotics has recently embraced powerful\nprobabilistic models, such as diffusion models, to model and learn complex\nbehaviors. However, these models often come with significant trade-offs, namely\nhigh computational costs for inference and a fundamental inability to quantify\noutput uncertainty. We argue that a model's trustworthiness, a critical factor\nfor reliable, general-purpose robotics, is inherently linked to its ability to\nprovide confidence measures.\n  In this work, we introduce Normalizing Flows Policy, a novel visuomotor\npolicy learning model based on Normalizing Flows. We show that Normalizing\nFlows are a natural and powerful alternative to diffusion models, providing\nboth a statistically sound measure of confidence and a highly efficient\ninference process. Through comprehensive experiments across four distinct\nsimulated robotic tasks, we demonstrate that Normalizing Flows Policy achieves\nperformance comparable to, and often surpassing, Diffusion Policy, and it does\nso not only with improved sample efficiency but also with up to 30 times faster\ninference. Additionally, our ablation study validates several key architectural\nand training techniques that enable Normalizing Flows to perform well in this\ndomain."
                },
                "authors": [
                    {
                        "name": "Simon Kristoffersson Lind"
                    },
                    {
                        "name": "Jialong Li"
                    },
                    {
                        "name": "Maj Stenmark"
                    },
                    {
                        "name": "Volker Krger"
                    }
                ],
                "author_detail": {
                    "name": "Volker Krger"
                },
                "author": "Volker Krger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21073v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21073v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21072v1",
                "updated": "2025-09-25T12:23:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    23,
                    49,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:23:49Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    23,
                    49,
                    3,
                    268,
                    0
                ],
                "title": "Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web\n  Reconnaissance, Tool Generation, and Task Execution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recon-Act: A Self-Evolving Multi-Agent Browser-Use System via Web\n  Reconnaissance, Tool Generation, and Task Execution"
                },
                "summary": "Recent years, multimodal models have made remarkable strides and pave the way\nfor intelligent browser use agents. However, when solving tasks on real world\nwebpages in multi-turn, long-horizon trajectories, current agents still suffer\nfrom disordered action sequencing and excessive trial and error during\nexecution. This paper introduces Recon-Act, a self-evolving multi-agent\nframework grounded in Reconnaissance-Action behavioral paradigm. The system\ncomprises a Reconnaissance Team and an Action Team: the former conducts\ncomparative analysis and tool generation, while the latter handles intent\ndecomposition, tool orchestration, and execution. By contrasting the erroneous\ntrajectories with successful ones, the Reconnaissance Team infers remedies, and\nabstracts them into a unified notion of generalized tools, either expressed as\nhints or as rule-based codes, and register to the tool archive in real time.\nThe Action Team reinference the process empowered with these targeting tools,\nthus establishing a closed-loop training pipeline of\ndata-tools-action-feedback. Following the 6 level implementation roadmap\nproposed in this work, we have currently reached Level 3 (with limited\nhuman-in-the-loop intervention). Leveraging generalized tools obtained through\nreconnaissance, Recon-Act substantially improves adaptability to unseen\nwebsites and solvability on long-horizon tasks, and achieves state-of-the-art\nperformance on the challenging VisualWebArena dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent years, multimodal models have made remarkable strides and pave the way\nfor intelligent browser use agents. However, when solving tasks on real world\nwebpages in multi-turn, long-horizon trajectories, current agents still suffer\nfrom disordered action sequencing and excessive trial and error during\nexecution. This paper introduces Recon-Act, a self-evolving multi-agent\nframework grounded in Reconnaissance-Action behavioral paradigm. The system\ncomprises a Reconnaissance Team and an Action Team: the former conducts\ncomparative analysis and tool generation, while the latter handles intent\ndecomposition, tool orchestration, and execution. By contrasting the erroneous\ntrajectories with successful ones, the Reconnaissance Team infers remedies, and\nabstracts them into a unified notion of generalized tools, either expressed as\nhints or as rule-based codes, and register to the tool archive in real time.\nThe Action Team reinference the process empowered with these targeting tools,\nthus establishing a closed-loop training pipeline of\ndata-tools-action-feedback. Following the 6 level implementation roadmap\nproposed in this work, we have currently reached Level 3 (with limited\nhuman-in-the-loop intervention). Leveraging generalized tools obtained through\nreconnaissance, Recon-Act substantially improves adaptability to unseen\nwebsites and solvability on long-horizon tasks, and achieves state-of-the-art\nperformance on the challenging VisualWebArena dataset."
                },
                "authors": [
                    {
                        "name": "Kaiwen He"
                    },
                    {
                        "name": "Zhiwei Wang"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jinjie Gu"
                },
                "author": "Jinjie Gu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21067v1",
                "updated": "2025-09-25T12:18:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    18,
                    20,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:18:20Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    18,
                    20,
                    3,
                    268,
                    0
                ],
                "title": "Designing for Novice Debuggers: A Pilot Study on an AI-Assisted\n  Debugging Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing for Novice Debuggers: A Pilot Study on an AI-Assisted\n  Debugging Tool"
                },
                "summary": "Debugging is a fundamental skill that novice programmers must develop.\nNumerous tools have been created to assist novice programmers in this process.\nRecently, large language models (LLMs) have been integrated with automated\nprogram repair techniques to generate fixes for students' buggy code. However,\nmany of these tools foster an over-reliance on AI and do not actively engage\nstudents in the debugging process. In this work, we aim to design an intuitive\ndebugging assistant, CodeHinter, that combines traditional debugging tools with\nLLM-based techniques to help novice debuggers fix semantic errors while\npromoting active engagement in the debugging process. We present findings from\nour second design iteration, which we tested with a group of undergraduate\nstudents. Our results indicate that the students found the tool highly\neffective in resolving semantic errors and significantly easier to use than the\nfirst version. Consistent with our previous study, error localization was the\nmost valuable feature. Finally, we conclude that any AI-assisted debugging tool\nshould be personalized based on user profiles to optimize their interactions\nwith students.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging is a fundamental skill that novice programmers must develop.\nNumerous tools have been created to assist novice programmers in this process.\nRecently, large language models (LLMs) have been integrated with automated\nprogram repair techniques to generate fixes for students' buggy code. However,\nmany of these tools foster an over-reliance on AI and do not actively engage\nstudents in the debugging process. In this work, we aim to design an intuitive\ndebugging assistant, CodeHinter, that combines traditional debugging tools with\nLLM-based techniques to help novice debuggers fix semantic errors while\npromoting active engagement in the debugging process. We present findings from\nour second design iteration, which we tested with a group of undergraduate\nstudents. Our results indicate that the students found the tool highly\neffective in resolving semantic errors and significantly easier to use than the\nfirst version. Consistent with our previous study, error localization was the\nmost valuable feature. Finally, we conclude that any AI-assisted debugging tool\nshould be personalized based on user profiles to optimize their interactions\nwith students."
                },
                "authors": [
                    {
                        "name": "Oka Kurniawan"
                    },
                    {
                        "name": "Erick Chandra"
                    },
                    {
                        "name": "Christopher M. Poskitt"
                    },
                    {
                        "name": "Yannic Noller"
                    },
                    {
                        "name": "Kenny Tsu Wei Choo"
                    },
                    {
                        "name": "Cyrille Jegourel"
                    }
                ],
                "author_detail": {
                    "name": "Cyrille Jegourel"
                },
                "author": "Cyrille Jegourel",
                "arxiv_comment": "Accepted by the 25th Koli Calling International Conference on\n  Computing Education Research (Koli Calling 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20317v2",
                "updated": "2025-09-25T12:17:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    17,
                    1,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-24T17:01:32Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    1,
                    32,
                    2,
                    267,
                    0
                ],
                "title": "SIM-CoT: Supervised Implicit Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIM-CoT: Supervised Implicit Chain-of-Thought"
                },
                "summary": "Implicit Chain-of-Thought (CoT) methods offer a token-efficient alternative\nto explicit CoT reasoning in Large Language Models (LLMs), but a persistent\nperformance gap has limited their adoption. We identify a core latent\ninstability issue when scaling the computational budget of implicit CoT: as the\nnumber of reasoning tokens increases, training often becomes unstable and\ncollapses. Our analysis shows that this instability arises from latent\nrepresentations becoming homogeneous and losing semantic diversity, caused by\ninsufficient step-level supervision in current implicit CoT methods. To address\nthis, we propose SIM-CoT, a plug-and-play training module that introduces\nstep-level supervision to stabilize and enrich the latent reasoning space.\nSIM-CoT employs an auxiliary decoder during training to align each implicit\ntoken with its corresponding explicit reasoning step, ensuring latent states\ncapture distinct and meaningful information. The auxiliary decoder is removed\nat inference, preserving the efficiency of implicit CoT with no added overhead.\nIt also provides interpretability by projecting each latent token onto an\nexplicit reasoning vocabulary, enabling per-step visualization and diagnosis.\nSIM-CoT significantly improves both in-domain accuracy and out-of-domain\nstability of implicit CoT methods, boosting Coconut by +8.2\\% on GPT-2 and CODI\nby +3.0\\% on LLaMA-3.1 8B. It further surpasses the explicit CoT baseline on\nGPT-2 by 2.1\\% with 2.3$\\times$ greater token efficiency, while closing the\nperformance gap on larger models like LLaMA-3.1 8B. Code:\nhttps://github.com/InternLM/SIM-CoT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Chain-of-Thought (CoT) methods offer a token-efficient alternative\nto explicit CoT reasoning in Large Language Models (LLMs), but a persistent\nperformance gap has limited their adoption. We identify a core latent\ninstability issue when scaling the computational budget of implicit CoT: as the\nnumber of reasoning tokens increases, training often becomes unstable and\ncollapses. Our analysis shows that this instability arises from latent\nrepresentations becoming homogeneous and losing semantic diversity, caused by\ninsufficient step-level supervision in current implicit CoT methods. To address\nthis, we propose SIM-CoT, a plug-and-play training module that introduces\nstep-level supervision to stabilize and enrich the latent reasoning space.\nSIM-CoT employs an auxiliary decoder during training to align each implicit\ntoken with its corresponding explicit reasoning step, ensuring latent states\ncapture distinct and meaningful information. The auxiliary decoder is removed\nat inference, preserving the efficiency of implicit CoT with no added overhead.\nIt also provides interpretability by projecting each latent token onto an\nexplicit reasoning vocabulary, enabling per-step visualization and diagnosis.\nSIM-CoT significantly improves both in-domain accuracy and out-of-domain\nstability of implicit CoT methods, boosting Coconut by +8.2\\% on GPT-2 and CODI\nby +3.0\\% on LLaMA-3.1 8B. It further surpasses the explicit CoT baseline on\nGPT-2 by 2.1\\% with 2.3$\\times$ greater token efficiency, while closing the\nperformance gap on larger models like LLaMA-3.1 8B. Code:\nhttps://github.com/InternLM/SIM-CoT"
                },
                "authors": [
                    {
                        "name": "Xilin Wei"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09487v2",
                "updated": "2025-09-25T12:11:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    11,
                    20,
                    3,
                    268,
                    0
                ],
                "published": "2025-02-13T16:52:06Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    16,
                    52,
                    6,
                    3,
                    44,
                    0
                ],
                "title": "Quantifying depressive mental states with large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying depressive mental states with large language models"
                },
                "summary": "Large Language Models (LLMs) may have an important role to play in mental\nhealth by facilitating the quantification of verbal expressions used to\ncommunicate emotions, feelings and thoughts. While there has been substantial\nand very promising work in this area, the fundamental limits are uncertain.\nHere, focusing on depressive symptoms, we outline and evaluate LLM performance\non three critical tests. The first test evaluates LLM performance on a novel\nground-truth dataset from a large human sample (n=770). This dataset is novel\nas it contains both standard clinically validated quantifications of depression\nsymptoms and specific verbal descriptions of the thoughts related to each\nsymptom by the same individual. The performance of LLMs on this richly\ninformative data shows an upper bound on the performance in this domain, and\nallow us to examine the extent to which inference about symptoms generalises.\nSecond, we test to what extent the latent structure in LLMs can capture the\nclinically observed patterns. We train supervised sparse auto-encoders (sSAE)\nto predict specific symptoms and symptom patterns within a syndrome. We find\nthat sSAE weights can effectively modify the clinical pattern produced by the\nmodel, and thereby capture the latent structure of relevant clinical variation.\nThird, if LLMs correctly capture and quantify relevant mental states, then\nthese states should respond to changes in emotional states induced by validated\nemotion induction interventions. We show that this holds in a third experiment\nwith 190 participants. Overall, this work provides foundational insights into\nthe quantification of pathological mental states with LLMs, highlighting hard\nlimits on the requirements of the data underlying LLM-based quantification; but\nalso suggesting LLMs show substantial conceptual alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) may have an important role to play in mental\nhealth by facilitating the quantification of verbal expressions used to\ncommunicate emotions, feelings and thoughts. While there has been substantial\nand very promising work in this area, the fundamental limits are uncertain.\nHere, focusing on depressive symptoms, we outline and evaluate LLM performance\non three critical tests. The first test evaluates LLM performance on a novel\nground-truth dataset from a large human sample (n=770). This dataset is novel\nas it contains both standard clinically validated quantifications of depression\nsymptoms and specific verbal descriptions of the thoughts related to each\nsymptom by the same individual. The performance of LLMs on this richly\ninformative data shows an upper bound on the performance in this domain, and\nallow us to examine the extent to which inference about symptoms generalises.\nSecond, we test to what extent the latent structure in LLMs can capture the\nclinically observed patterns. We train supervised sparse auto-encoders (sSAE)\nto predict specific symptoms and symptom patterns within a syndrome. We find\nthat sSAE weights can effectively modify the clinical pattern produced by the\nmodel, and thereby capture the latent structure of relevant clinical variation.\nThird, if LLMs correctly capture and quantify relevant mental states, then\nthese states should respond to changes in emotional states induced by validated\nemotion induction interventions. We show that this holds in a third experiment\nwith 190 participants. Overall, this work provides foundational insights into\nthe quantification of pathological mental states with LLMs, highlighting hard\nlimits on the requirements of the data underlying LLM-based quantification; but\nalso suggesting LLMs show substantial conceptual alignment."
                },
                "authors": [
                    {
                        "name": "Jakub Onysk"
                    },
                    {
                        "name": "Quentin J. M. Huys"
                    }
                ],
                "author_detail": {
                    "name": "Quentin J. M. Huys"
                },
                "author": "Quentin J. M. Huys",
                "arxiv_comment": "main text - 9 pages, 6 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21057v1",
                "updated": "2025-09-25T12:08:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    8,
                    31,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:08:31Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    8,
                    31,
                    3,
                    268,
                    0
                ],
                "title": "PMark: Towards Robust and Distortion-free Semantic-level Watermarking\n  with Channel Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PMark: Towards Robust and Distortion-free Semantic-level Watermarking\n  with Channel Constraints"
                },
                "summary": "Semantic-level watermarking (SWM) for large language models (LLMs) enhances\nwatermarking robustness against text modifications and paraphrasing attacks by\ntreating the sentence as the fundamental unit. However, existing methods still\nlack strong theoretical guarantees of robustness, and reject-sampling-based\ngeneration often introduces significant distribution distortions compared with\nunwatermarked outputs. In this work, we introduce a new theoretical framework\non SWM through the concept of proxy functions (PFs) $\\unicode{x2013}$ functions\nthat map sentences to scalar values. Building on this framework, we propose\nPMark, a simple yet powerful SWM method that estimates the PF median for the\nnext sentence dynamically through sampling while enforcing multiple PF\nconstraints (which we call channels) to strengthen watermark evidence. Equipped\nwith solid theoretical guarantees, PMark achieves the desired distortion-free\nproperty and improves the robustness against paraphrasing-style attacks. We\nalso provide an empirically optimized version that further removes the\nrequirement for dynamical median estimation for better sampling efficiency.\nExperimental results show that PMark consistently outperforms existing SWM\nbaselines in both text quality and robustness, offering a more effective\nparadigm for detecting machine-generated text. Our code will be released at\n[this URL](https://github.com/PMark-repo/PMark).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic-level watermarking (SWM) for large language models (LLMs) enhances\nwatermarking robustness against text modifications and paraphrasing attacks by\ntreating the sentence as the fundamental unit. However, existing methods still\nlack strong theoretical guarantees of robustness, and reject-sampling-based\ngeneration often introduces significant distribution distortions compared with\nunwatermarked outputs. In this work, we introduce a new theoretical framework\non SWM through the concept of proxy functions (PFs) $\\unicode{x2013}$ functions\nthat map sentences to scalar values. Building on this framework, we propose\nPMark, a simple yet powerful SWM method that estimates the PF median for the\nnext sentence dynamically through sampling while enforcing multiple PF\nconstraints (which we call channels) to strengthen watermark evidence. Equipped\nwith solid theoretical guarantees, PMark achieves the desired distortion-free\nproperty and improves the robustness against paraphrasing-style attacks. We\nalso provide an empirically optimized version that further removes the\nrequirement for dynamical median estimation for better sampling efficiency.\nExperimental results show that PMark consistently outperforms existing SWM\nbaselines in both text quality and robustness, offering a more effective\nparadigm for detecting machine-generated text. Our code will be released at\n[this URL](https://github.com/PMark-repo/PMark)."
                },
                "authors": [
                    {
                        "name": "Jiahao Huo"
                    },
                    {
                        "name": "Shuliang Liu"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Junyan Zhang"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Mingxun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Mingxun Zhou"
                },
                "author": "Mingxun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08683v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08683v2",
                "updated": "2025-09-25T12:07:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    7,
                    40,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-13T15:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    15,
                    44,
                    10,
                    1,
                    133,
                    0
                ],
                "title": "Uncertainty-Aware Surrogate-based Amortized Bayesian Inference for\n  Computationally Expensive Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Uncertainty-Aware Surrogate-based Amortized Bayesian Inference for\n  Computationally Expensive Models"
                },
                "summary": "Bayesian inference typically relies on a large number of model evaluations to\nestimate posterior distributions. Established methods like Markov Chain Monte\nCarlo (MCMC) and Amortized Bayesian Inference (ABI) can become computationally\nchallenging. While ABI enables fast inference after training, generating\nsufficient training data still requires thousands of model simulations, which\nis infeasible for expensive models. Surrogate models offer a solution by\nproviding approximate simulations at a lower computational cost, allowing the\ngeneration of large data sets for training. However, the introduced\napproximation errors and uncertainties can lead to overconfident posterior\nestimates. To address this, we propose Uncertainty-Aware Surrogate-based\nAmortized Bayesian Inference (UA-SABI) -- a framework that combines surrogate\nmodeling and ABI while explicitly quantifying and propagating surrogate\nuncertainties through the inference pipeline. Our experiments show that this\napproach enables reliable, fast, and repeated Bayesian inference for\ncomputationally expensive models, even under tight time constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference typically relies on a large number of model evaluations to\nestimate posterior distributions. Established methods like Markov Chain Monte\nCarlo (MCMC) and Amortized Bayesian Inference (ABI) can become computationally\nchallenging. While ABI enables fast inference after training, generating\nsufficient training data still requires thousands of model simulations, which\nis infeasible for expensive models. Surrogate models offer a solution by\nproviding approximate simulations at a lower computational cost, allowing the\ngeneration of large data sets for training. However, the introduced\napproximation errors and uncertainties can lead to overconfident posterior\nestimates. To address this, we propose Uncertainty-Aware Surrogate-based\nAmortized Bayesian Inference (UA-SABI) -- a framework that combines surrogate\nmodeling and ABI while explicitly quantifying and propagating surrogate\nuncertainties through the inference pipeline. Our experiments show that this\napproach enables reliable, fast, and repeated Bayesian inference for\ncomputationally expensive models, even under tight time constraints."
                },
                "authors": [
                    {
                        "name": "Stefania Scheurer"
                    },
                    {
                        "name": "Philipp Reiser"
                    },
                    {
                        "name": "Tim Brnnette"
                    },
                    {
                        "name": "Wolfgang Nowak"
                    },
                    {
                        "name": "Anneli Guthke"
                    },
                    {
                        "name": "Paul-Christian Brkner"
                    }
                ],
                "author_detail": {
                    "name": "Paul-Christian Brkner"
                },
                "author": "Paul-Christian Brkner",
                "arxiv_comment": "27 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08683v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08683v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02097v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02097v3",
                "updated": "2025-09-26T02:22:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    2,
                    22,
                    22,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-02T08:52:16Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    8,
                    52,
                    16,
                    1,
                    245,
                    0
                ],
                "title": "JudgeAgent: Knowledge-wise and Dynamic LLM Evaluation with\n  Agent-as-Interviewer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JudgeAgent: Knowledge-wise and Dynamic LLM Evaluation with\n  Agent-as-Interviewer"
                },
                "summary": "Current evaluation paradigms for large language models (LLMs) suffer from\noverestimated or biased evaluations and mismatched question difficulty, leading\nto incomplete evaluations of knowledge and capability boundaries, which hinder\ntheir effective application and optimization. To address these challenges, we\npropose Agent-as-Interviewer, a dynamic evaluation paradigm that employs LLM\nagents to conduct multi-turn interactions for evaluation. Unlike current\nbenchmarking or dynamic interaction paradigms, Agent-as-Interviewer utilizes\nagents to invoke knowledge tools for wider and deeper knowledge in the dynamic\nmulti-turn question generation, achieving more comprehensive evaluations of\nLLM's knowledge boundaries. It also leverages agents to plan query strategies\nfor adjustment of the question difficulty levels, enhancing the difficulty\ncontrol to match the actual capabilities of target LLMs. Based on this\nparadigm, we develop JudgeAgent, a knowledge-wise dynamic evaluation framework\nthat employs knowledge-driven synthesis as the agent's tool and uses difficulty\nscoring as strategy guidance, thereby finally providing valuable suggestions to\nhelp targets optimize themselves. Extensive experiments validate the\neffectiveness of JudgeAgent's suggestions, demonstrating that\nAgent-as-Interviewer can accurately identify the knowledge and capability\nboundaries of target models. The source code is available on\nhttps://github.com/DataArcTech/JudgeAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current evaluation paradigms for large language models (LLMs) suffer from\noverestimated or biased evaluations and mismatched question difficulty, leading\nto incomplete evaluations of knowledge and capability boundaries, which hinder\ntheir effective application and optimization. To address these challenges, we\npropose Agent-as-Interviewer, a dynamic evaluation paradigm that employs LLM\nagents to conduct multi-turn interactions for evaluation. Unlike current\nbenchmarking or dynamic interaction paradigms, Agent-as-Interviewer utilizes\nagents to invoke knowledge tools for wider and deeper knowledge in the dynamic\nmulti-turn question generation, achieving more comprehensive evaluations of\nLLM's knowledge boundaries. It also leverages agents to plan query strategies\nfor adjustment of the question difficulty levels, enhancing the difficulty\ncontrol to match the actual capabilities of target LLMs. Based on this\nparadigm, we develop JudgeAgent, a knowledge-wise dynamic evaluation framework\nthat employs knowledge-driven synthesis as the agent's tool and uses difficulty\nscoring as strategy guidance, thereby finally providing valuable suggestions to\nhelp targets optimize themselves. Extensive experiments validate the\neffectiveness of JudgeAgent's suggestions, demonstrating that\nAgent-as-Interviewer can accurately identify the knowledge and capability\nboundaries of target models. The source code is available on\nhttps://github.com/DataArcTech/JudgeAgent."
                },
                "authors": [
                    {
                        "name": "Zhichao Shi"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Cangli Yao"
                    },
                    {
                        "name": "Zhenxin Huang"
                    },
                    {
                        "name": "Shengjie Ma"
                    },
                    {
                        "name": "Yinghan Shen"
                    },
                    {
                        "name": "Jian Guo"
                    },
                    {
                        "name": "Yuanzhuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuanzhuo Wang"
                },
                "author": "Yuanzhuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02097v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02097v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2509.21319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21319v1",
                "updated": "2025-09-25T16:19:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    16,
                    19,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T16:19:06Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    16,
                    19,
                    6,
                    3,
                    268,
                    0
                ],
                "title": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RLBFF: Binary Flexible Feedback to bridge between Human Feedback &\n  Verifiable Rewards"
                },
                "summary": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Human Feedback (RLHF) and Reinforcement Learning\nwith Verifiable Rewards (RLVR) are the main RL paradigms used in LLM\npost-training, each offering distinct advantages. However, RLHF struggles with\ninterpretability and reward hacking because it relies on human judgments that\nusually lack explicit criteria, whereas RLVR is limited in scope by its focus\non correctness-based verifiers. We propose Reinforcement Learning with Binary\nFlexible Feedback (RLBFF), which combines the versatility of human-driven\npreferences with the precision of rule-based verification, enabling reward\nmodels to capture nuanced aspects of response quality beyond mere correctness.\nRLBFF extracts principles that can be answered in a binary fashion (e.g.\naccuracy of information: yes, or code readability: no) from natural language\nfeedback. Such principles can then be used to ground Reward Model training as\nan entailment task (response satisfies or does not satisfy an arbitrary\nprinciple). We show that Reward Models trained in this manner can outperform\nBradley-Terry models when matched for data and achieve top performance on\nRM-Bench (86.2%) and JudgeBench (81.4%, #1 on leaderboard as of September 24,\n2025). Additionally, users can specify principles of interest at inference time\nto customize the focus of our reward models, in contrast to Bradley-Terry\nmodels. Finally, we present a fully open source recipe (including data) to\nalign Qwen3-32B using RLBFF and our Reward Model, to match or exceed the\nperformance of o3-mini and DeepSeek R1 on general alignment benchmarks of\nMT-Bench, WildBench, and Arena Hard v2 (at <5% of the inference cost)."
                },
                "authors": [
                    {
                        "name": "Zhilin Wang"
                    },
                    {
                        "name": "Jiaqi Zeng"
                    },
                    {
                        "name": "Olivier Delalleau"
                    },
                    {
                        "name": "Ellie Evans"
                    },
                    {
                        "name": "Daniel Egert"
                    },
                    {
                        "name": "Hoo-Chang Shin"
                    },
                    {
                        "name": "Felipe Soares"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Oleksii Kuchaiev"
                    }
                ],
                "author_detail": {
                    "name": "Oleksii Kuchaiev"
                },
                "author": "Oleksii Kuchaiev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21318v1",
                "updated": "2025-09-25T16:07:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    16,
                    7,
                    38,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T16:07:38Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    16,
                    7,
                    38,
                    3,
                    268,
                    0
                ],
                "title": "SD3.5-Flash: Distribution-Guided Distillation of Generative Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SD3.5-Flash: Distribution-Guided Distillation of Generative Flows"
                },
                "summary": "We present SD3.5-Flash, an efficient few-step distillation framework that\nbrings high-quality image generation to accessible consumer devices. Our\napproach distills computationally prohibitive rectified flow models through a\nreformulated distribution matching objective tailored specifically for few-step\ngeneration. We introduce two key innovations: \"timestep sharing\" to reduce\ngradient noise and \"split-timestep fine-tuning\" to improve prompt alignment.\nCombined with comprehensive pipeline optimizations like text encoder\nrestructuring and specialized quantization, our system enables both rapid\ngeneration and memory-efficient deployment across different hardware\nconfigurations. This democratizes access across the full spectrum of devices,\nfrom mobile phones to desktop computers. Through extensive evaluation including\nlarge-scale user studies, we demonstrate that SD3.5-Flash consistently\noutperforms existing few-step methods, making advanced generative AI truly\naccessible for practical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SD3.5-Flash, an efficient few-step distillation framework that\nbrings high-quality image generation to accessible consumer devices. Our\napproach distills computationally prohibitive rectified flow models through a\nreformulated distribution matching objective tailored specifically for few-step\ngeneration. We introduce two key innovations: \"timestep sharing\" to reduce\ngradient noise and \"split-timestep fine-tuning\" to improve prompt alignment.\nCombined with comprehensive pipeline optimizations like text encoder\nrestructuring and specialized quantization, our system enables both rapid\ngeneration and memory-efficient deployment across different hardware\nconfigurations. This democratizes access across the full spectrum of devices,\nfrom mobile phones to desktop computers. Through extensive evaluation including\nlarge-scale user studies, we demonstrate that SD3.5-Flash consistently\noutperforms existing few-step methods, making advanced generative AI truly\naccessible for practical deployment."
                },
                "authors": [
                    {
                        "name": "Hmrishav Bandyopadhyay"
                    },
                    {
                        "name": "Rahim Entezari"
                    },
                    {
                        "name": "Jim Scott"
                    },
                    {
                        "name": "Reshinth Adithyan"
                    },
                    {
                        "name": "Yi-Zhe Song"
                    },
                    {
                        "name": "Varun Jampani"
                    }
                ],
                "author_detail": {
                    "name": "Varun Jampani"
                },
                "author": "Varun Jampani",
                "arxiv_comment": "Project Page: https://hmrishavbandy.github.io/sd35flash/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21317v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21317v1",
                "updated": "2025-09-25T15:38:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    38,
                    27,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:38:27Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    38,
                    27,
                    3,
                    268,
                    0
                ],
                "title": "Interactive Recommendation Agent with Active User Commands",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interactive Recommendation Agent with Active User Commands"
                },
                "summary": "Traditional recommender systems rely on passive feedback mechanisms that\nlimit users to simple choices such as like and dislike. However, these\ncoarse-grained signals fail to capture users' nuanced behavior motivations and\nintentions. In turn, current systems cannot also distinguish which specific\nitem attributes drive user satisfaction or dissatisfaction, resulting in\ninaccurate preference modeling. These fundamental limitations create a\npersistent gap between user intentions and system interpretations, ultimately\nundermining user satisfaction and harming system effectiveness.\n  To address these limitations, we introduce the Interactive Recommendation\nFeed (IRF), a pioneering paradigm that enables natural language commands within\nmainstream recommendation feeds. Unlike traditional systems that confine users\nto passive implicit behavioral influence, IRF empowers active explicit control\nover recommendation policies through real-time linguistic commands. To support\nthis paradigm, we develop RecBot, a dual-agent architecture where a Parser\nAgent transforms linguistic expressions into structured preferences and a\nPlanner Agent dynamically orchestrates adaptive tool chains for on-the-fly\npolicy adjustment. To enable practical deployment, we employ\nsimulation-augmented knowledge distillation to achieve efficient performance\nwhile maintaining strong reasoning capabilities. Through extensive offline and\nlong-term online experiments, RecBot shows significant improvements in both\nuser satisfaction and business outcomes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional recommender systems rely on passive feedback mechanisms that\nlimit users to simple choices such as like and dislike. However, these\ncoarse-grained signals fail to capture users' nuanced behavior motivations and\nintentions. In turn, current systems cannot also distinguish which specific\nitem attributes drive user satisfaction or dissatisfaction, resulting in\ninaccurate preference modeling. These fundamental limitations create a\npersistent gap between user intentions and system interpretations, ultimately\nundermining user satisfaction and harming system effectiveness.\n  To address these limitations, we introduce the Interactive Recommendation\nFeed (IRF), a pioneering paradigm that enables natural language commands within\nmainstream recommendation feeds. Unlike traditional systems that confine users\nto passive implicit behavioral influence, IRF empowers active explicit control\nover recommendation policies through real-time linguistic commands. To support\nthis paradigm, we develop RecBot, a dual-agent architecture where a Parser\nAgent transforms linguistic expressions into structured preferences and a\nPlanner Agent dynamically orchestrates adaptive tool chains for on-the-fly\npolicy adjustment. To enable practical deployment, we employ\nsimulation-augmented knowledge distillation to achieve efficient performance\nwhile maintaining strong reasoning capabilities. Through extensive offline and\nlong-term online experiments, RecBot shows significant improvements in both\nuser satisfaction and business outcomes."
                },
                "authors": [
                    {
                        "name": "Jiakai Tang"
                    },
                    {
                        "name": "Yujie Luo"
                    },
                    {
                        "name": "Xunke Xi"
                    },
                    {
                        "name": "Fei Sun"
                    },
                    {
                        "name": "Xueyang Feng"
                    },
                    {
                        "name": "Sunhao Dai"
                    },
                    {
                        "name": "Chao Yi"
                    },
                    {
                        "name": "Dian Chen"
                    },
                    {
                        "name": "Zhujin Gao"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Wen Chen"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Yuning Jiang"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21317v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21317v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21310v1",
                "updated": "2025-09-25T15:27:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    27,
                    15,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:27:15Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    27,
                    15,
                    3,
                    268,
                    0
                ],
                "title": "SAGE: A Realistic Benchmark for Semantic Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAGE: A Realistic Benchmark for Semantic Understanding"
                },
                "summary": "As large language models (LLMs) achieve strong performance on traditional\nbenchmarks, there is an urgent need for more challenging evaluation frameworks\nthat probe deeper aspects of semantic understanding. We introduce SAGE\n(Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed\nto assess both embedding models and similarity metrics across five categories:\nHuman Preference Alignment, Transformation Robustness, Information Sensitivity,\nClustering Performance, and Retrieval Robustness. Unlike existing benchmarks\nthat focus on isolated capabilities, SAGE evaluates semantic understanding\nthrough adversarial conditions, noisy transformations, and nuanced human\njudgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding\nmodels and classical metrics reveals significant performance gaps, with no\nsingle approach excelling across all dimensions. For instance, while\nstate-of-the-art embedding models like OpenAI's text-embedding-3-large dominate\nin aligning with human preferences (0.682 vs. 0.591 for the best classical\nmetric), they are significantly outperformed by classical metrics on\ninformation sensitivity tasks, where Jaccard Similarity achieves a score of\n0.905 compared to the top embedding score of 0.794. SAGE further uncovers\ncritical trade-offs: OpenAI's text-embedding-3-small achieves the highest\nclustering performance (0.483) but demonstrates extreme brittleness with the\nlowest robustness score (0.011). SAGE exposes critical limitations in current\nsemantic understanding capabilities and provides a more realistic assessment of\nmodel robustness for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) achieve strong performance on traditional\nbenchmarks, there is an urgent need for more challenging evaluation frameworks\nthat probe deeper aspects of semantic understanding. We introduce SAGE\n(Semantic Alignment & Generalization Evaluation), a rigorous benchmark designed\nto assess both embedding models and similarity metrics across five categories:\nHuman Preference Alignment, Transformation Robustness, Information Sensitivity,\nClustering Performance, and Retrieval Robustness. Unlike existing benchmarks\nthat focus on isolated capabilities, SAGE evaluates semantic understanding\nthrough adversarial conditions, noisy transformations, and nuanced human\njudgment tasks across 30+ datasets. Our comprehensive evaluation of 9 embedding\nmodels and classical metrics reveals significant performance gaps, with no\nsingle approach excelling across all dimensions. For instance, while\nstate-of-the-art embedding models like OpenAI's text-embedding-3-large dominate\nin aligning with human preferences (0.682 vs. 0.591 for the best classical\nmetric), they are significantly outperformed by classical metrics on\ninformation sensitivity tasks, where Jaccard Similarity achieves a score of\n0.905 compared to the top embedding score of 0.794. SAGE further uncovers\ncritical trade-offs: OpenAI's text-embedding-3-small achieves the highest\nclustering performance (0.483) but demonstrates extreme brittleness with the\nlowest robustness score (0.011). SAGE exposes critical limitations in current\nsemantic understanding capabilities and provides a more realistic assessment of\nmodel robustness for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Samarth Goel"
                    },
                    {
                        "name": "Reagan J. Lee"
                    },
                    {
                        "name": "Kannan Ramchandran"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Ramchandran"
                },
                "author": "Kannan Ramchandran",
                "arxiv_comment": "39th Conference on Neural Information Processing Systems (NeurIPS\n  2025) Workshop: Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent\n  Abilities, and Scaling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21305v1",
                "updated": "2025-09-25T15:19:39Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    19,
                    39,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:19:39Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    19,
                    39,
                    3,
                    268,
                    0
                ],
                "title": "Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors\n  in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sycophancy Is Not One Thing: Causal Separation of Sycophantic Behaviors\n  in LLMs"
                },
                "summary": "Large language models (LLMs) often exhibit sycophantic behaviors -- such as\nexcessive agreement with or flattery of the user -- but it is unclear whether\nthese behaviors arise from a single mechanism or multiple distinct processes.\nWe decompose sycophancy into sycophantic agreement and sycophantic praise,\ncontrasting both with genuine agreement. Using difference-in-means directions,\nactivation additions, and subspace geometry across multiple models and\ndatasets, we show that: (1) the three behaviors are encoded along distinct\nlinear directions in latent space; (2) each behavior can be independently\namplified or suppressed without affecting the others; and (3) their\nrepresentational structure is consistent across model families and scales.\nThese results suggest that sycophantic behaviors correspond to distinct,\nindependently steerable representations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often exhibit sycophantic behaviors -- such as\nexcessive agreement with or flattery of the user -- but it is unclear whether\nthese behaviors arise from a single mechanism or multiple distinct processes.\nWe decompose sycophancy into sycophantic agreement and sycophantic praise,\ncontrasting both with genuine agreement. Using difference-in-means directions,\nactivation additions, and subspace geometry across multiple models and\ndatasets, we show that: (1) the three behaviors are encoded along distinct\nlinear directions in latent space; (2) each behavior can be independently\namplified or suppressed without affecting the others; and (3) their\nrepresentational structure is consistent across model families and scales.\nThese results suggest that sycophantic behaviors correspond to distinct,\nindependently steerable representations."
                },
                "authors": [
                    {
                        "name": "Daniel Vennemeyer"
                    },
                    {
                        "name": "Phan Anh Duong"
                    },
                    {
                        "name": "Tiffany Zhan"
                    },
                    {
                        "name": "Tianyu Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Jiang"
                },
                "author": "Tianyu Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19580v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19580v2",
                "updated": "2025-09-25T15:18:21Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    18,
                    21,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-23T21:09:24Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    21,
                    9,
                    24,
                    1,
                    266,
                    0
                ],
                "title": "LLMs4All: A Review on Large Language Models for Research and\n  Applications in Academic Disciplines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs4All: A Review on Large Language Models for Research and\n  Applications in Academic Disciplines"
                },
                "summary": "Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view\nof the world. For example, Large Language Models (LLMs) based applications such\nas ChatGPT have shown the capability of generating human-like conversation on\nextensive topics. Due to the impressive performance on a variety of\nlanguage-related tasks (e.g., open-domain question answering, translation, and\ndocument summarization), one can envision the far-reaching impacts that can be\nbrought by the LLMs with broader real-world applications (e.g., customer\nservice, education and accessibility, and scientific discovery). Inspired by\ntheir success, this paper will offer an overview of state-of-the-art LLMs and\ntheir integration into a wide range of academic disciplines, including: (1)\narts, letters, and law (e.g., history, philosophy, political science, arts and\narchitecture, law), (2) economics and business (e.g., finance, economics,\naccounting, marketing), and (3) science and engineering (e.g., mathematics,\nphysics and mechanical engineering, chemistry and chemical engineering, life\nsciences and bioengineering, earth sciences and civil engineering, computer\nscience and electrical engineering). Integrating humanity and technology, in\nthis paper, we will explore how LLMs are shaping research and practice in these\nfields, while also discussing key limitations, open challenges, and future\ndirections in the era of generative AI. The review of how LLMs are engaged\nacross disciplines-along with key observations and insights-can help\nresearchers and practitioners interested in exploiting LLMs to advance their\nworks in diverse real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view\nof the world. For example, Large Language Models (LLMs) based applications such\nas ChatGPT have shown the capability of generating human-like conversation on\nextensive topics. Due to the impressive performance on a variety of\nlanguage-related tasks (e.g., open-domain question answering, translation, and\ndocument summarization), one can envision the far-reaching impacts that can be\nbrought by the LLMs with broader real-world applications (e.g., customer\nservice, education and accessibility, and scientific discovery). Inspired by\ntheir success, this paper will offer an overview of state-of-the-art LLMs and\ntheir integration into a wide range of academic disciplines, including: (1)\narts, letters, and law (e.g., history, philosophy, political science, arts and\narchitecture, law), (2) economics and business (e.g., finance, economics,\naccounting, marketing), and (3) science and engineering (e.g., mathematics,\nphysics and mechanical engineering, chemistry and chemical engineering, life\nsciences and bioengineering, earth sciences and civil engineering, computer\nscience and electrical engineering). Integrating humanity and technology, in\nthis paper, we will explore how LLMs are shaping research and practice in these\nfields, while also discussing key limitations, open challenges, and future\ndirections in the era of generative AI. The review of how LLMs are engaged\nacross disciplines-along with key observations and insights-can help\nresearchers and practitioners interested in exploiting LLMs to advance their\nworks in diverse real-world applications."
                },
                "authors": [
                    {
                        "name": "Yanfang Fanny Ye"
                    },
                    {
                        "name": "Zheyuan Zhang"
                    },
                    {
                        "name": "Tianyi Ma"
                    },
                    {
                        "name": "Zehong Wang"
                    },
                    {
                        "name": "Yiyang Li"
                    },
                    {
                        "name": "Shifu Hou"
                    },
                    {
                        "name": "Weixiang Sun"
                    },
                    {
                        "name": "Kaiwen Shi"
                    },
                    {
                        "name": "Yijun Ma"
                    },
                    {
                        "name": "Wei Song"
                    },
                    {
                        "name": "Ahmed Abbasi"
                    },
                    {
                        "name": "Ying Cheng"
                    },
                    {
                        "name": "Jane Cleland-Huang"
                    },
                    {
                        "name": "Steven Corcelli"
                    },
                    {
                        "name": "Patricia Culligan"
                    },
                    {
                        "name": "Robert Goulding"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Ting Hua"
                    },
                    {
                        "name": "John Lalor"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Tengfei Luo"
                    },
                    {
                        "name": "Ed Maginn"
                    },
                    {
                        "name": "Nuno Moniz"
                    },
                    {
                        "name": "Jason Rohr"
                    },
                    {
                        "name": "Brett Savoie"
                    },
                    {
                        "name": "Daniel Slate"
                    },
                    {
                        "name": "Tom Stapleford"
                    },
                    {
                        "name": "Matthew Webber"
                    },
                    {
                        "name": "Olaf Wiest"
                    },
                    {
                        "name": "Johnny Zhang"
                    },
                    {
                        "name": "Nitesh Chawla"
                    }
                ],
                "author_detail": {
                    "name": "Nitesh Chawla"
                },
                "author": "Nitesh Chawla",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19580v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21302v1",
                "updated": "2025-09-25T15:17:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    17,
                    11,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:17:11Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    17,
                    11,
                    3,
                    268,
                    0
                ],
                "title": "Quantized Visual Geometry Grounded Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized Visual Geometry Grounded Transformer"
                },
                "summary": "Learning-based 3D reconstruction models, represented by Visual Geometry\nGrounded Transformers (VGGTs), have made remarkable progress with the use of\nlarge-scale transformers. Their prohibitive computational and memory costs\nseverely hinder real-world deployment. Post-Training Quantization (PTQ) has\nbecome a common practice for compressing and accelerating models. However, we\nempirically observe that PTQ faces unique obstacles when compressing\nbillion-scale VGGTs: the data-independent special tokens induce heavy-tailed\nactivation distributions, while the multi-view nature of 3D data makes\ncalibration sample selection highly unstable. This paper proposes the first\nQuantization framework for VGGTs, namely QuantVGGT. This mainly relies on two\ntechnical contributions: First, we introduce Dual-Smoothed Fine-Grained\nQuantization, which integrates pre-global Hadamard rotation and post-local\nchannel smoothing to mitigate heavy-tailed distributions and inter-channel\nvariance robustly. Second, we design Noise-Filtered Diverse Sampling, which\nfilters outliers via deep-layer statistics and constructs frame-aware diverse\ncalibration clusters to ensure stable quantization ranges. Comprehensive\nexperiments demonstrate that QuantVGGT achieves the state-of-the-art results\nacross different benchmarks and bit-width, surpassing the previous\nstate-of-the-art generic quantization method with a great margin. We highlight\nthat our 4-bit QuantVGGT can deliver a 3.7$\\times$ memory reduction and\n2.5$\\times$ acceleration in real-hardware inference, while maintaining\nreconstruction accuracy above 98\\% of its full-precision counterpart. This\ndemonstrates the vast advantages and practicality of QuantVGGT in\nresource-constrained scenarios. Our code is released in\nhttps://github.com/wlfeng0509/QuantVGGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-based 3D reconstruction models, represented by Visual Geometry\nGrounded Transformers (VGGTs), have made remarkable progress with the use of\nlarge-scale transformers. Their prohibitive computational and memory costs\nseverely hinder real-world deployment. Post-Training Quantization (PTQ) has\nbecome a common practice for compressing and accelerating models. However, we\nempirically observe that PTQ faces unique obstacles when compressing\nbillion-scale VGGTs: the data-independent special tokens induce heavy-tailed\nactivation distributions, while the multi-view nature of 3D data makes\ncalibration sample selection highly unstable. This paper proposes the first\nQuantization framework for VGGTs, namely QuantVGGT. This mainly relies on two\ntechnical contributions: First, we introduce Dual-Smoothed Fine-Grained\nQuantization, which integrates pre-global Hadamard rotation and post-local\nchannel smoothing to mitigate heavy-tailed distributions and inter-channel\nvariance robustly. Second, we design Noise-Filtered Diverse Sampling, which\nfilters outliers via deep-layer statistics and constructs frame-aware diverse\ncalibration clusters to ensure stable quantization ranges. Comprehensive\nexperiments demonstrate that QuantVGGT achieves the state-of-the-art results\nacross different benchmarks and bit-width, surpassing the previous\nstate-of-the-art generic quantization method with a great margin. We highlight\nthat our 4-bit QuantVGGT can deliver a 3.7$\\times$ memory reduction and\n2.5$\\times$ acceleration in real-hardware inference, while maintaining\nreconstruction accuracy above 98\\% of its full-precision counterpart. This\ndemonstrates the vast advantages and practicality of QuantVGGT in\nresource-constrained scenarios. Our code is released in\nhttps://github.com/wlfeng0509/QuantVGGT."
                },
                "authors": [
                    {
                        "name": "Weilun Feng"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Mingqiang Wu"
                    },
                    {
                        "name": "Chuanguang Yang"
                    },
                    {
                        "name": "Yuqi Li"
                    },
                    {
                        "name": "Xiangqi Li"
                    },
                    {
                        "name": "Zhulin An"
                    },
                    {
                        "name": "Libo Huang"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Michele Magno"
                    },
                    {
                        "name": "Yongjun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Yongjun Xu"
                },
                "author": "Yongjun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21301v1",
                "updated": "2025-09-25T15:17:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    17,
                    5,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:17:05Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    17,
                    5,
                    3,
                    268,
                    0
                ],
                "title": "Nova: Real-Time Agentic Vision-Language Model Serving with Adaptive\n  Cross-Stage Parallelization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nova: Real-Time Agentic Vision-Language Model Serving with Adaptive\n  Cross-Stage Parallelization"
                },
                "summary": "This paper presents Nova, a real-time scheduling framework for serving\nagentic vision-language models (VLMs) on a single GPU with balanced per-request\nlatency and overall request process throughput. Our design begins by enabling\neffective pipelining across vision encode, LLM prefill, and LLM decode stages\nof VLMs, by exploiting their heterogeneous resource demands during execution\nand incorporating elastic GPU spatial partitioning among stages to maximally\nutilize the compute and memory resources. Building on this, we introduce a\nreal-time scheduling algorithm that adaptively calibrates resource allocation\namong stages based on a Pareto-optimal analysis of the latency-throughput\ntrade-off, allowing the system to sustain responsiveness and resource\nefficiency under dynamic request loads. To further alleviate GPU memory\npressure, we design a lightweight weight offloading strategy for vision\nencoders that preserves inference efficiency with minimized memory overhead.\nExtensive evaluations on both synthetic and real-world agent workloads\ndemonstrate that Nova consistently outperforms the state-of-the-art baselines,\nimproving the maximum latency by up to 23.3%, while keeping competitive\nthroughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents Nova, a real-time scheduling framework for serving\nagentic vision-language models (VLMs) on a single GPU with balanced per-request\nlatency and overall request process throughput. Our design begins by enabling\neffective pipelining across vision encode, LLM prefill, and LLM decode stages\nof VLMs, by exploiting their heterogeneous resource demands during execution\nand incorporating elastic GPU spatial partitioning among stages to maximally\nutilize the compute and memory resources. Building on this, we introduce a\nreal-time scheduling algorithm that adaptively calibrates resource allocation\namong stages based on a Pareto-optimal analysis of the latency-throughput\ntrade-off, allowing the system to sustain responsiveness and resource\nefficiency under dynamic request loads. To further alleviate GPU memory\npressure, we design a lightweight weight offloading strategy for vision\nencoders that preserves inference efficiency with minimized memory overhead.\nExtensive evaluations on both synthetic and real-world agent workloads\ndemonstrate that Nova consistently outperforms the state-of-the-art baselines,\nimproving the maximum latency by up to 23.3%, while keeping competitive\nthroughput."
                },
                "authors": [
                    {
                        "name": "Yuhang Xu"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Dong Zhang"
                    },
                    {
                        "name": "Bingheng Yan"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11679v2",
                "updated": "2025-09-25T15:14:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    14,
                    10,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-16T20:39:30Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    20,
                    39,
                    30,
                    4,
                    136,
                    0
                ],
                "title": "Ambiguity Resolution in Text-to-Structured Data Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguity Resolution in Text-to-Structured Data Mapping"
                },
                "summary": "Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods to ambiguity handling either\nrely on the ReACT framework to obtain correct mappings through trial and error,\nor on supervised fine-tuning to bias models toward specific tasks. In this\npaper, we adopt a different approach that characterizes representation\ndifferences of ambiguous text in the latent space and leverages these\ndifferences to identify ambiguity before mapping them to structured data. To\ndetect sentence-level ambiguity, we focus on the relationship between ambiguous\nquestions and their interpretations. Unlike distances calculated by dense\nembeddings, we introduce a new distance measure based on a path kernel over\nconcepts. With this measurement, we identify patterns to distinguish ambiguous\nfrom unambiguous questions. Furthermore, we propose a method for improving LLM\nperformance on ambiguous agentic tool calling through missing concept\nprediction. Both achieve state-of-the-art results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ambiguity in natural language is a significant obstacle for achieving\naccurate text to structured data mapping through large language models (LLMs),\nwhich affects the performance of tasks such as mapping text to agentic tool\ncalling and text-to-SQL queries. Existing methods to ambiguity handling either\nrely on the ReACT framework to obtain correct mappings through trial and error,\nor on supervised fine-tuning to bias models toward specific tasks. In this\npaper, we adopt a different approach that characterizes representation\ndifferences of ambiguous text in the latent space and leverages these\ndifferences to identify ambiguity before mapping them to structured data. To\ndetect sentence-level ambiguity, we focus on the relationship between ambiguous\nquestions and their interpretations. Unlike distances calculated by dense\nembeddings, we introduce a new distance measure based on a path kernel over\nconcepts. With this measurement, we identify patterns to distinguish ambiguous\nfrom unambiguous questions. Furthermore, we propose a method for improving LLM\nperformance on ambiguous agentic tool calling through missing concept\nprediction. Both achieve state-of-the-art results."
                },
                "authors": [
                    {
                        "name": "Zhibo Hu"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Yanfeng Shu"
                    },
                    {
                        "name": "Hye-Young Paik"
                    },
                    {
                        "name": "Liming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liming Zhu"
                },
                "author": "Liming Zhu",
                "arxiv_comment": "17 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21294v1",
                "updated": "2025-09-25T15:13:00Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    13,
                    0,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:13:00Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    13,
                    0,
                    3,
                    268,
                    0
                ],
                "title": "The role of synthetic data in Multilingual, Multi-cultural AI systems:\n  Lessons from Indic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The role of synthetic data in Multilingual, Multi-cultural AI systems:\n  Lessons from Indic Languages"
                },
                "summary": "Developing AI systems that operate effectively across languages while\nremaining culturally grounded is a long-standing challenge, particularly in\nlow-resource settings. Synthetic data provides a promising avenue, yet its\neffectiveness in multilingual and multicultural contexts remains underexplored.\nWe investigate the creation and impact of synthetic, culturally contextualized\ndatasets for Indian languages through a bottom-up generation strategy that\nprompts large open-source LLMs (>= 235B parameters) to ground data generation\nin language-specific Wikipedia content. This approach complements the dominant\ntop-down paradigm of translating synthetic datasets from high-resource\nlanguages such as English. We introduce Updesh, a high-quality large-scale\nsynthetic instruction-following dataset comprising 9.5M data points across 13\nIndian languages, encompassing diverse reasoning and generative tasks with an\nemphasis on long-context, multi-turn capabilities, and alignment with Indian\ncultural contexts. A comprehensive evaluation incorporating both automated\nmetrics and human annotation across 10k assessments indicates that generated\ndata is high quality; though, human evaluation highlights areas for further\nimprovement. Additionally, we perform downstream evaluations by fine-tuning\nmodels on our dataset and assessing the performance across 15 diverse\nmultilingual datasets. Models trained on Updesh consistently achieve\nsignificant gains on generative tasks and remain competitive on multiple-choice\nstyle NLU tasks. Notably, relative improvements are most pronounced in low and\nmedium-resource languages, narrowing their gap with high-resource languages.\nThese findings provide empirical evidence that effective multilingual AI\nrequires multi-faceted data curation and generation strategies that incorporate\ncontext-aware, culturally grounded methodologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Developing AI systems that operate effectively across languages while\nremaining culturally grounded is a long-standing challenge, particularly in\nlow-resource settings. Synthetic data provides a promising avenue, yet its\neffectiveness in multilingual and multicultural contexts remains underexplored.\nWe investigate the creation and impact of synthetic, culturally contextualized\ndatasets for Indian languages through a bottom-up generation strategy that\nprompts large open-source LLMs (>= 235B parameters) to ground data generation\nin language-specific Wikipedia content. This approach complements the dominant\ntop-down paradigm of translating synthetic datasets from high-resource\nlanguages such as English. We introduce Updesh, a high-quality large-scale\nsynthetic instruction-following dataset comprising 9.5M data points across 13\nIndian languages, encompassing diverse reasoning and generative tasks with an\nemphasis on long-context, multi-turn capabilities, and alignment with Indian\ncultural contexts. A comprehensive evaluation incorporating both automated\nmetrics and human annotation across 10k assessments indicates that generated\ndata is high quality; though, human evaluation highlights areas for further\nimprovement. Additionally, we perform downstream evaluations by fine-tuning\nmodels on our dataset and assessing the performance across 15 diverse\nmultilingual datasets. Models trained on Updesh consistently achieve\nsignificant gains on generative tasks and remain competitive on multiple-choice\nstyle NLU tasks. Notably, relative improvements are most pronounced in low and\nmedium-resource languages, narrowing their gap with high-resource languages.\nThese findings provide empirical evidence that effective multilingual AI\nrequires multi-faceted data curation and generation strategies that incorporate\ncontext-aware, culturally grounded methodologies."
                },
                "authors": [
                    {
                        "name": "Pranjal A. Chitale"
                    },
                    {
                        "name": "Varun Gumma"
                    },
                    {
                        "name": "Sanchit Ahuja"
                    },
                    {
                        "name": "Prashant Kodali"
                    },
                    {
                        "name": "Manan Uppadhyay"
                    },
                    {
                        "name": "Deepthi Sudharsan"
                    },
                    {
                        "name": "Sunayana Sitaram"
                    }
                ],
                "author_detail": {
                    "name": "Sunayana Sitaram"
                },
                "author": "Sunayana Sitaram",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.09679v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.09679v2",
                "updated": "2025-09-25T15:12:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    12,
                    18,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-11T17:59:51Z",
                "published_parsed": [
                    2025,
                    9,
                    11,
                    17,
                    59,
                    51,
                    3,
                    254,
                    0
                ],
                "title": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ButterflyQuant: Ultra-low-bit LLM Quantization through Learnable\n  Orthogonal Butterfly Transforms"
                },
                "summary": "Large language models require massive memory footprints, severely limiting\ndeployment on consumer hardware. Quantization reduces memory through lower\nnumerical precision, but extreme 2-bit quantization suffers from catastrophic\nperformance loss due to outliers in activations. Rotation-based methods such as\nQuIP and QuaRot apply orthogonal transforms to eliminate outliers before\nquantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} =\n(\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these\nmethods use fixed transforms--Hadamard matrices achieving optimal worst-case\ncoherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight\ndistributions. We identify that different transformer layers exhibit distinct\noutlier patterns, motivating layer-adaptive rotations rather than\none-size-fits-all approaches. In this work, we propose ButterflyQuant, which\nreplaces Hadamard rotations with learnable butterfly transforms parameterized\nby continuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$\nentries that are non-differentiable and thus prohibit gradient-based learning,\nbutterfly transforms' continuous parameterization enables smooth optimization\nwhile guaranteeing orthogonality by construction. This orthogonal constraint\nensures theoretical guarantees in outlier suppression while achieving $O(n \\log\nn)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable\nparameters. We further introduce a uniformity regularization on\npost-transformation activations to promote smoother distributions amenable to\nquantization. Learning requires only 128 calibration samples and converges in\nminutes on a single GPU--a negligible one-time cost. For LLaMA-2-7B with 2-bit\nquantization, ButterflyQuant achieves 15.4 perplexity versus 37.3 for QuIP.\n\\href{https://github.com/42Shawn/Butterflyquant-llm}{Codes} are available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models require massive memory footprints, severely limiting\ndeployment on consumer hardware. Quantization reduces memory through lower\nnumerical precision, but extreme 2-bit quantization suffers from catastrophic\nperformance loss due to outliers in activations. Rotation-based methods such as\nQuIP and QuaRot apply orthogonal transforms to eliminate outliers before\nquantization, using computational invariance: $\\mathbf{y} = \\mathbf{Wx} =\n(\\mathbf{WQ}^T)(\\mathbf{Qx})$ for orthogonal $\\mathbf{Q}$. However, these\nmethods use fixed transforms--Hadamard matrices achieving optimal worst-case\ncoherence $\\mu = 1/\\sqrt{n}$--that cannot adapt to specific weight\ndistributions. We identify that different transformer layers exhibit distinct\noutlier patterns, motivating layer-adaptive rotations rather than\none-size-fits-all approaches. In this work, we propose ButterflyQuant, which\nreplaces Hadamard rotations with learnable butterfly transforms parameterized\nby continuous Givens rotation angles. Unlike Hadamard's discrete $\\{+1, -1\\}$\nentries that are non-differentiable and thus prohibit gradient-based learning,\nbutterfly transforms' continuous parameterization enables smooth optimization\nwhile guaranteeing orthogonality by construction. This orthogonal constraint\nensures theoretical guarantees in outlier suppression while achieving $O(n \\log\nn)$ computational complexity with only $\\frac{n \\log n}{2}$ learnable\nparameters. We further introduce a uniformity regularization on\npost-transformation activations to promote smoother distributions amenable to\nquantization. Learning requires only 128 calibration samples and converges in\nminutes on a single GPU--a negligible one-time cost. For LLaMA-2-7B with 2-bit\nquantization, ButterflyQuant achieves 15.4 perplexity versus 37.3 for QuIP.\n\\href{https://github.com/42Shawn/Butterflyquant-llm}{Codes} are available."
                },
                "authors": [
                    {
                        "name": "Bingxin Xu"
                    },
                    {
                        "name": "Zhen Dong"
                    },
                    {
                        "name": "Oussama Elachqar"
                    },
                    {
                        "name": "Yuzhang Shang"
                    }
                ],
                "author_detail": {
                    "name": "Yuzhang Shang"
                },
                "author": "Yuzhang Shang",
                "arxiv_comment": "Replace discrete Hadamard transforms with continuous Butterfly\n  transforms to facilitate the learning of rotation matrices in LLM\n  quantization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.09679v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.09679v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21282v1",
                "updated": "2025-09-25T15:03:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    3,
                    18,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:03:18Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    3,
                    18,
                    3,
                    268,
                    0
                ],
                "title": "It's Not You, It's Clipping: A Soft Trust-Region via Probability\n  Smoothing for LLM RL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It's Not You, It's Clipping: A Soft Trust-Region via Probability\n  Smoothing for LLM RL"
                },
                "summary": "Training large language models (LLMs) with reinforcement learning (RL)\nmethods such as PPO and GRPO commonly relies on ratio clipping to stabilise\nupdates. While effective at preventing instability, clipping discards\ninformation and introduces gradient discontinuities. We propose Probability\nSmoothing Policy Optimisation (PSPO), which smooths the current policy's\nprobabilities toward the old (behaviour) policy before computing the importance\nratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient\nsignal, while interpolation toward the old policy creates a soft trust region\nthat discourages large, destabilising updates, with formal guarantees.\n  We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and\nQwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset\ngeneralisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO\n(single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar\nperformance but improves the reasoning leading to clearer and more concise\nresponses which are more logical. Compared to clipped GRPO, GR-PSPO\nsubstantially improves performance both the 0.5B and 1.5B models, with a boost\nof over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models (LLMs) with reinforcement learning (RL)\nmethods such as PPO and GRPO commonly relies on ratio clipping to stabilise\nupdates. While effective at preventing instability, clipping discards\ninformation and introduces gradient discontinuities. We propose Probability\nSmoothing Policy Optimisation (PSPO), which smooths the current policy's\nprobabilities toward the old (behaviour) policy before computing the importance\nratio, analogous to label smoothing. Unlike clipping, PSPO preserves gradient\nsignal, while interpolation toward the old policy creates a soft trust region\nthat discourages large, destabilising updates, with formal guarantees.\n  We instantiate PSPO within GRPO (GR-PSPO) and fine-tune Qwen2.5-0.5B and\nQwen2.5-1.5B on GSM8K, evaluating on GSM8K test and the cross-dataset\ngeneralisation on SVAMP, ASDiv, and MATH-500. Relative to unclipped GRPO\n(single iteration; no data reuse, ratio always = 1), GR-PSPO achieves similar\nperformance but improves the reasoning leading to clearer and more concise\nresponses which are more logical. Compared to clipped GRPO, GR-PSPO\nsubstantially improves performance both the 0.5B and 1.5B models, with a boost\nof over 20% on GSM8K (39.7% vs. 17.6% for 0.5B, 59.4% vs. 37.8% for 1.5B)."
                },
                "authors": [
                    {
                        "name": "Madeleine Dwyer"
                    },
                    {
                        "name": "Adam Sobey"
                    },
                    {
                        "name": "Adriane Chapman"
                    }
                ],
                "author_detail": {
                    "name": "Adriane Chapman"
                },
                "author": "Adriane Chapman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.23764v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.23764v2",
                "updated": "2025-09-25T15:01:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    1,
                    27,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-29T17:59:52Z",
                "published_parsed": [
                    2025,
                    5,
                    29,
                    17,
                    59,
                    52,
                    3,
                    149,
                    0
                ],
                "title": "MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence"
                },
                "summary": "Spatial intelligence is essential for multimodal large language models\n(MLLMs) operating in the complex physical world. Existing benchmarks, however,\nprobe only single-image relations and thus fail to assess the multi-image\nspatial reasoning that real-world deployments demand. We introduce MMSI-Bench,\na VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision\nresearchers spent more than 300 hours meticulously crafting 1,000 challenging,\nunambiguous multiple-choice questions from over 120,000 images, each paired\nwith carefully designed distractors and a step-by-step reasoning process. We\nconduct extensive experiments and thoroughly evaluate 34 open-source and\nproprietary MLLMs, observing a wide gap: the strongest open-source model\nattains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while\nhumans score 97%. These results underscore the challenging nature of MMSI-Bench\nand the substantial headroom for future research. Leveraging the annotated\nreasoning processes, we also provide an automated error analysis pipeline that\ndiagnoses four dominant failure modes, including (1) grounding errors, (2)\noverlap-matching and scene-reconstruction errors, (3) situation-transformation\nreasoning errors, and (4) spatial-logic errors, offering valuable insights for\nadvancing multi-image spatial intelligence. Project page:\nhttps://runsenxu.com/projects/MMSI_Bench .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spatial intelligence is essential for multimodal large language models\n(MLLMs) operating in the complex physical world. Existing benchmarks, however,\nprobe only single-image relations and thus fail to assess the multi-image\nspatial reasoning that real-world deployments demand. We introduce MMSI-Bench,\na VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision\nresearchers spent more than 300 hours meticulously crafting 1,000 challenging,\nunambiguous multiple-choice questions from over 120,000 images, each paired\nwith carefully designed distractors and a step-by-step reasoning process. We\nconduct extensive experiments and thoroughly evaluate 34 open-source and\nproprietary MLLMs, observing a wide gap: the strongest open-source model\nattains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while\nhumans score 97%. These results underscore the challenging nature of MMSI-Bench\nand the substantial headroom for future research. Leveraging the annotated\nreasoning processes, we also provide an automated error analysis pipeline that\ndiagnoses four dominant failure modes, including (1) grounding errors, (2)\noverlap-matching and scene-reconstruction errors, (3) situation-transformation\nreasoning errors, and (4) spatial-logic errors, offering valuable insights for\nadvancing multi-image spatial intelligence. Project page:\nhttps://runsenxu.com/projects/MMSI_Bench ."
                },
                "authors": [
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Runsen Xu"
                    },
                    {
                        "name": "Yiman Xie"
                    },
                    {
                        "name": "Sizhe Yang"
                    },
                    {
                        "name": "Mo Li"
                    },
                    {
                        "name": "Jingli Lin"
                    },
                    {
                        "name": "Chenming Zhu"
                    },
                    {
                        "name": "Xiaochen Chen"
                    },
                    {
                        "name": "Haodong Duan"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Tai Wang"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    }
                ],
                "author_detail": {
                    "name": "Jiangmiao Pang"
                },
                "author": "Jiangmiao Pang",
                "arxiv_comment": "34 pages. A comprehensive, fully human-curated, multi-image-based\n  spatial intelligence benchmark with reasoning annotation for MLLMs. Project\n  page: https://runsenxu.com/projects/MMSI_Bench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.23764v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.23764v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21275v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21275v1",
                "updated": "2025-09-25T15:01:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    1,
                    25,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:01:25Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    1,
                    25,
                    3,
                    268,
                    0
                ],
                "title": "Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM\n  Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Centric Elastic Pipeline Parallelism for Efficient Long-Context LLM\n  Training"
                },
                "summary": "Long context training is crucial for LLM's context extension. Existing\nschemes, such as sequence parallelism, incur substantial communication\noverhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness\nhinges on partitioning granularity. Batch-level PP dividing input samples\nexhibits high memory consumption in long-context scenario, whereas token-level\nPP splitting sequences into slices alleviates memory overhead but may incur\nhardware under-utilization. This trade-off motivates adaptively selecting PP\ngranularity to match resource and workload characteristics. Moreover, sequence\nlength distribution of the real-world dataset exhibits skewness, posing a\nchallenge on PP's workload balance and efficient scheduling. Current static PP\nscheduling methods overlook the variance of sequence length, leading to\nsuboptimal performance. In this paper, we propose Elastic Pipeline Parallelism\n(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource\nand workload heterogeneity. We build InfiniPipe, a distributed training system\nthat unleashes the potential of EPP via (1) a resource-aware and\nworkload-balanced sequence processor that splits long sequences and packs short\nones; and (2) a co-optimization methodology that jointly optimizes pipeline\nschedule and gradient checkpointing via a mechanism named stage-aware\nchunk-level adaptive checkpointing. Comprehensive experiments demonstrate that\nInfiniPipe achieves a 1.69x speedup over state-of-the-art systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long context training is crucial for LLM's context extension. Existing\nschemes, such as sequence parallelism, incur substantial communication\noverhead. Pipeline parallelism (PP) reduces this cost, but its effectiveness\nhinges on partitioning granularity. Batch-level PP dividing input samples\nexhibits high memory consumption in long-context scenario, whereas token-level\nPP splitting sequences into slices alleviates memory overhead but may incur\nhardware under-utilization. This trade-off motivates adaptively selecting PP\ngranularity to match resource and workload characteristics. Moreover, sequence\nlength distribution of the real-world dataset exhibits skewness, posing a\nchallenge on PP's workload balance and efficient scheduling. Current static PP\nscheduling methods overlook the variance of sequence length, leading to\nsuboptimal performance. In this paper, we propose Elastic Pipeline Parallelism\n(EPP) that orchestrates token-level PP and batch-level PP to adapt to resource\nand workload heterogeneity. We build InfiniPipe, a distributed training system\nthat unleashes the potential of EPP via (1) a resource-aware and\nworkload-balanced sequence processor that splits long sequences and packs short\nones; and (2) a co-optimization methodology that jointly optimizes pipeline\nschedule and gradient checkpointing via a mechanism named stage-aware\nchunk-level adaptive checkpointing. Comprehensive experiments demonstrate that\nInfiniPipe achieves a 1.69x speedup over state-of-the-art systems."
                },
                "authors": [
                    {
                        "name": "Shiju Wang"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Ao Sun"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Zijian Zhu"
                    },
                    {
                        "name": "Bin Cui"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Kaisheng Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kaisheng Ma"
                },
                "author": "Kaisheng Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21275v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21275v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00827v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00827v6",
                "updated": "2025-09-25T15:00:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    0,
                    41,
                    3,
                    268,
                    0
                ],
                "published": "2024-10-29T07:15:56Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    7,
                    15,
                    56,
                    1,
                    303,
                    0
                ],
                "title": "IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models\n  Using Themselves",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IDEATOR: Jailbreaking and Benchmarking Large Vision-Language Models\n  Using Themselves"
                },
                "summary": "As large Vision-Language Models (VLMs) gain prominence, ensuring their safe\ndeployment has become critical. Recent studies have explored VLM robustness\nagainst jailbreak attacks-techniques that exploit model vulnerabilities to\nelicit harmful outputs. However, the limited availability of diverse multimodal\ndata has constrained current approaches to rely heavily on adversarial or\nmanually crafted images derived from harmful text datasets, which often lack\neffectiveness and diversity across different contexts. In this paper, we\npropose IDEATOR, a novel jailbreak method that autonomously generates malicious\nimage-text pairs for black-box jailbreak attacks. IDEATOR is grounded in the\ninsight that VLMs themselves could serve as powerful red team models for\ngenerating multimodal jailbreak prompts. Specifically, IDEATOR leverages a VLM\nto create targeted jailbreak texts and pairs them with jailbreak images\ngenerated by a state-of-the-art diffusion model. Extensive experiments\ndemonstrate IDEATOR's high effectiveness and transferability, achieving a 94%\nattack success rate (ASR) in jailbreaking MiniGPT-4 with an average of only\n5.34 queries, and high ASRs of 82%, 88%, and 75% when transferred to LLaVA,\nInstructBLIP, and Chameleon, respectively. Building on IDEATOR's strong\ntransferability and automated process, we introduce the VLJailbreakBench, a\nsafety benchmark comprising 3,654 multimodal jailbreak samples. Our benchmark\nresults on 11 recently released VLMs reveal significant gaps in safety\nalignment. For instance, our challenge set achieves ASRs of 46.31% on GPT-4o\nand 19.65% on Claude-3.5-Sonnet, underscoring the urgent need for stronger\ndefenses. VLJailbreakBench is publicly available at\nhttps://roywang021.github.io/VLJailbreakBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large Vision-Language Models (VLMs) gain prominence, ensuring their safe\ndeployment has become critical. Recent studies have explored VLM robustness\nagainst jailbreak attacks-techniques that exploit model vulnerabilities to\nelicit harmful outputs. However, the limited availability of diverse multimodal\ndata has constrained current approaches to rely heavily on adversarial or\nmanually crafted images derived from harmful text datasets, which often lack\neffectiveness and diversity across different contexts. In this paper, we\npropose IDEATOR, a novel jailbreak method that autonomously generates malicious\nimage-text pairs for black-box jailbreak attacks. IDEATOR is grounded in the\ninsight that VLMs themselves could serve as powerful red team models for\ngenerating multimodal jailbreak prompts. Specifically, IDEATOR leverages a VLM\nto create targeted jailbreak texts and pairs them with jailbreak images\ngenerated by a state-of-the-art diffusion model. Extensive experiments\ndemonstrate IDEATOR's high effectiveness and transferability, achieving a 94%\nattack success rate (ASR) in jailbreaking MiniGPT-4 with an average of only\n5.34 queries, and high ASRs of 82%, 88%, and 75% when transferred to LLaVA,\nInstructBLIP, and Chameleon, respectively. Building on IDEATOR's strong\ntransferability and automated process, we introduce the VLJailbreakBench, a\nsafety benchmark comprising 3,654 multimodal jailbreak samples. Our benchmark\nresults on 11 recently released VLMs reveal significant gaps in safety\nalignment. For instance, our challenge set achieves ASRs of 46.31% on GPT-4o\nand 19.65% on Claude-3.5-Sonnet, underscoring the urgent need for stronger\ndefenses. VLJailbreakBench is publicly available at\nhttps://roywang021.github.io/VLJailbreakBench."
                },
                "authors": [
                    {
                        "name": "Ruofan Wang"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Yixu Wang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Xiaosen Wang"
                    },
                    {
                        "name": "Yan Teng"
                    },
                    {
                        "name": "Yingchun Wang"
                    },
                    {
                        "name": "Xingjun Ma"
                    },
                    {
                        "name": "Yu-Gang Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Gang Jiang"
                },
                "author": "Yu-Gang Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00827v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00827v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21271v1",
                "updated": "2025-09-25T15:00:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    0,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T15:00:16Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    15,
                    0,
                    16,
                    3,
                    268,
                    0
                ],
                "title": "SuperOffload: Unleashing the Power of Large-Scale LLM Training on\n  Superchips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuperOffload: Unleashing the Power of Large-Scale LLM Training on\n  Superchips"
                },
                "summary": "The emergence of Superchips represents a significant advancement in\nnext-generation AI hardware. These Superchips employ a tightly coupled\nheterogeneous architecture that integrates GPU and CPU on the same package,\nwhich offers unprecedented computational power. However, there has been scant\nresearch investigating how LLM training benefits from this new architecture. In\nthis work, for the first time, we study LLM training solutions based on\noffloading for Superchips. We observe important differences between Superchips\nand traditional loosely-coupled GPU-CPU architecture, which necessitate\nrevisiting prevailing assumptions about offloading. Based on that, we present\nSuperOffload, a Superchip-centric offloading system that simultaneously uses\nHopper GPU, Grace CPU, and NVLink-C2C interconnect more efficiently.\nSuperOffload accomplishes this via a combination of techniques, such as\nadaptive weight offloading, bucketization repartitioning, Superchip-aware\ncasting, speculative execution, and a highly optimized Adam optimizer for Grace\nCPUs. Our evaluation of SuperOffload on NVIDIA GH200 demonstrates up to 2.5x\nthroughput improvement compared to state-of-the-art offloading-based systems,\nenabling training of up to 25B model on a single Superchip while achieving high\ntraining throughput. We also extend SuperOffload with ZeRO-style data\nparallelism and DeepSpeed-Ulysses sequence parallelism, enabling training of\n13B model with sequence lengths up to 1 million tokens on 8 GH200 while\nachieving 55% MFU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of Superchips represents a significant advancement in\nnext-generation AI hardware. These Superchips employ a tightly coupled\nheterogeneous architecture that integrates GPU and CPU on the same package,\nwhich offers unprecedented computational power. However, there has been scant\nresearch investigating how LLM training benefits from this new architecture. In\nthis work, for the first time, we study LLM training solutions based on\noffloading for Superchips. We observe important differences between Superchips\nand traditional loosely-coupled GPU-CPU architecture, which necessitate\nrevisiting prevailing assumptions about offloading. Based on that, we present\nSuperOffload, a Superchip-centric offloading system that simultaneously uses\nHopper GPU, Grace CPU, and NVLink-C2C interconnect more efficiently.\nSuperOffload accomplishes this via a combination of techniques, such as\nadaptive weight offloading, bucketization repartitioning, Superchip-aware\ncasting, speculative execution, and a highly optimized Adam optimizer for Grace\nCPUs. Our evaluation of SuperOffload on NVIDIA GH200 demonstrates up to 2.5x\nthroughput improvement compared to state-of-the-art offloading-based systems,\nenabling training of up to 25B model on a single Superchip while achieving high\ntraining throughput. We also extend SuperOffload with ZeRO-style data\nparallelism and DeepSpeed-Ulysses sequence parallelism, enabling training of\n13B model with sequence lengths up to 1 million tokens on 8 GH200 while\nachieving 55% MFU."
                },
                "authors": [
                    {
                        "name": "Xinyu Lian"
                    },
                    {
                        "name": "Masahiro Tanaka"
                    },
                    {
                        "name": "Olatunji Ruwase"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "arxiv_comment": "16 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21269v1",
                "updated": "2025-09-25T14:59:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    59,
                    43,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:59:43Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    59,
                    43,
                    3,
                    268,
                    0
                ],
                "title": "LLMTrace: A Corpus for Classification and Fine-Grained Localization of\n  AI-Written Text",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMTrace: A Corpus for Classification and Fine-Grained Localization of\n  AI-Written Text"
                },
                "summary": "The widespread use of human-like text from Large Language Models (LLMs)\nnecessitates the development of robust detection systems. However, progress is\nlimited by a critical lack of suitable training data; existing datasets are\noften generated with outdated models, are predominantly in English, and fail to\naddress the increasingly common scenario of mixed human-AI authorship.\nCrucially, while some datasets address mixed authorship, none provide the\ncharacter-level annotations required for the precise localization of\nAI-generated segments within a text. To address these gaps, we introduce\nLLMTrace, a new large-scale, bilingual (English and Russian) corpus for\nAI-generated text detection. Constructed using a diverse range of modern\nproprietary and open-source LLMs, our dataset is designed to support two key\ntasks: traditional full-text binary classification (human vs. AI) and the novel\ntask of AI-generated interval detection, facilitated by character-level\nannotations. We believe LLMTrace will serve as a vital resource for training\nand evaluating the next generation of more nuanced and practical AI detection\nmodels. The project page is available at\n\\href{https://sweetdream779.github.io/LLMTrace-info/}{iitolstykh/LLMTrace}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread use of human-like text from Large Language Models (LLMs)\nnecessitates the development of robust detection systems. However, progress is\nlimited by a critical lack of suitable training data; existing datasets are\noften generated with outdated models, are predominantly in English, and fail to\naddress the increasingly common scenario of mixed human-AI authorship.\nCrucially, while some datasets address mixed authorship, none provide the\ncharacter-level annotations required for the precise localization of\nAI-generated segments within a text. To address these gaps, we introduce\nLLMTrace, a new large-scale, bilingual (English and Russian) corpus for\nAI-generated text detection. Constructed using a diverse range of modern\nproprietary and open-source LLMs, our dataset is designed to support two key\ntasks: traditional full-text binary classification (human vs. AI) and the novel\ntask of AI-generated interval detection, facilitated by character-level\nannotations. We believe LLMTrace will serve as a vital resource for training\nand evaluating the next generation of more nuanced and practical AI detection\nmodels. The project page is available at\n\\href{https://sweetdream779.github.io/LLMTrace-info/}{iitolstykh/LLMTrace}."
                },
                "authors": [
                    {
                        "name": "Irina Tolstykh"
                    },
                    {
                        "name": "Aleksandra Tsybina"
                    },
                    {
                        "name": "Sergey Yakubson"
                    },
                    {
                        "name": "Maksim Kuprashevich"
                    }
                ],
                "author_detail": {
                    "name": "Maksim Kuprashevich"
                },
                "author": "Maksim Kuprashevich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16851v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16851v3",
                "updated": "2025-09-25T14:58:23Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    58,
                    23,
                    3,
                    268,
                    0
                ],
                "published": "2024-03-25T15:15:09Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    15,
                    15,
                    9,
                    0,
                    85,
                    0
                ],
                "title": "Can social media provide early warning of retraction? Evidence from\n  critical tweets identified by human annotation and large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can social media provide early warning of retraction? Evidence from\n  critical tweets identified by human annotation and large language models"
                },
                "summary": "Timely detection of problematic research is essential for safeguarding\nscientific integrity. To explore whether social media commentary can serve as\nan early indicator of potentially problematic articles, this study analysed\n3,815 tweets referencing 604 retracted articles and 3,373 tweets referencing\n668 comparable non-retracted articles. Tweets critical of the articles were\nidentified through both human annotation and large language models (LLMs).\nHuman annotation revealed that 8.3% of retracted articles were associated with\nat least one critical tweet prior to retraction, compared to only 1.5% of\nnon-retracted articles, highlighting the potential of tweets as early warning\nsignals of retraction. However, critical tweets identified by LLMs (GPT-4o\nmini, Gemini 2.0 Flash-Lite, and Claude 3.5 Haiku) only partially aligned with\nhuman annotation, suggesting that fully automated monitoring of\npost-publication discourse should be applied with caution. A human-AI\ncollaborative approach may offer a more reliable and scalable alternative, with\nhuman expertise helping to filter out tweets critical of issues unrelated to\nthe research integrity of the articles. Overall, this study provides insights\ninto how social media signals, combined with generative AI technologies, may\nsupport efforts to strengthen research integrity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely detection of problematic research is essential for safeguarding\nscientific integrity. To explore whether social media commentary can serve as\nan early indicator of potentially problematic articles, this study analysed\n3,815 tweets referencing 604 retracted articles and 3,373 tweets referencing\n668 comparable non-retracted articles. Tweets critical of the articles were\nidentified through both human annotation and large language models (LLMs).\nHuman annotation revealed that 8.3% of retracted articles were associated with\nat least one critical tweet prior to retraction, compared to only 1.5% of\nnon-retracted articles, highlighting the potential of tweets as early warning\nsignals of retraction. However, critical tweets identified by LLMs (GPT-4o\nmini, Gemini 2.0 Flash-Lite, and Claude 3.5 Haiku) only partially aligned with\nhuman annotation, suggesting that fully automated monitoring of\npost-publication discourse should be applied with caution. A human-AI\ncollaborative approach may offer a more reliable and scalable alternative, with\nhuman expertise helping to filter out tweets critical of issues unrelated to\nthe research integrity of the articles. Overall, this study provides insights\ninto how social media signals, combined with generative AI technologies, may\nsupport efforts to strengthen research integrity."
                },
                "authors": [
                    {
                        "name": "Er-Te Zheng"
                    },
                    {
                        "name": "Hui-Zhen Fu"
                    },
                    {
                        "name": "Mike Thelwall"
                    },
                    {
                        "name": "Zhichao Fang"
                    }
                ],
                "author_detail": {
                    "name": "Zhichao Fang"
                },
                "author": "Zhichao Fang",
                "arxiv_doi": "10.1002/asi.70028",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1002/asi.70028",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.16851v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16851v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "27 pages, 5 figures",
                "arxiv_journal_ref": "Journal of the Association for Information Science and Technology,\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21267v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21267v1",
                "updated": "2025-09-25T14:58:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    58,
                    7,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:58:07Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    58,
                    7,
                    3,
                    268,
                    0
                ],
                "title": "LLM Output Homogenization is Task Dependent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Output Homogenization is Task Dependent"
                },
                "summary": "A large language model can be less helpful if it exhibits output response\nhomogenization. But whether two responses are considered homogeneous, and\nwhether such homogenization is problematic, both depend on the task category.\nFor instance, in objective math tasks, we often expect no variation in the\nfinal answer but anticipate variation in the problem-solving strategy. Whereas,\nfor creative writing tasks, we may expect variation in key narrative components\n(e.g. plot, genre, setting, etc), beyond the vocabulary or embedding diversity\nproduced by temperature-sampling. Previous work addressing output\nhomogenization often fails to conceptualize diversity in a task-dependent way.\nWe address this gap in the literature directly by making the following\ncontributions. (1) We present a task taxonomy comprised of eight task\ncategories that each have distinct conceptualizations of output homogenization.\n(2) We introduce task-anchored functional diversity to better evaluate output\nhomogenization. (3) We propose a task-anchored sampling technique that\nincreases functional diversity for task categories where homogenization is\nundesired, while preserving homogenization where it is desired. (4) We\nchallenge the perceived existence of a diversity-quality trade-off by\nincreasing functional diversity while maintaining response quality. Overall, we\ndemonstrate how task dependence improves the evaluation and mitigation of\noutput homogenization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A large language model can be less helpful if it exhibits output response\nhomogenization. But whether two responses are considered homogeneous, and\nwhether such homogenization is problematic, both depend on the task category.\nFor instance, in objective math tasks, we often expect no variation in the\nfinal answer but anticipate variation in the problem-solving strategy. Whereas,\nfor creative writing tasks, we may expect variation in key narrative components\n(e.g. plot, genre, setting, etc), beyond the vocabulary or embedding diversity\nproduced by temperature-sampling. Previous work addressing output\nhomogenization often fails to conceptualize diversity in a task-dependent way.\nWe address this gap in the literature directly by making the following\ncontributions. (1) We present a task taxonomy comprised of eight task\ncategories that each have distinct conceptualizations of output homogenization.\n(2) We introduce task-anchored functional diversity to better evaluate output\nhomogenization. (3) We propose a task-anchored sampling technique that\nincreases functional diversity for task categories where homogenization is\nundesired, while preserving homogenization where it is desired. (4) We\nchallenge the perceived existence of a diversity-quality trade-off by\nincreasing functional diversity while maintaining response quality. Overall, we\ndemonstrate how task dependence improves the evaluation and mitigation of\noutput homogenization."
                },
                "authors": [
                    {
                        "name": "Shomik Jain"
                    },
                    {
                        "name": "Jack Lanchantin"
                    },
                    {
                        "name": "Maximilian Nickel"
                    },
                    {
                        "name": "Karen Ullrich"
                    },
                    {
                        "name": "Ashia Wilson"
                    },
                    {
                        "name": "Jamelle Watson-Daniels"
                    }
                ],
                "author_detail": {
                    "name": "Jamelle Watson-Daniels"
                },
                "author": "Jamelle Watson-Daniels",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21267v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21267v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21266v1",
                "updated": "2025-09-25T14:57:52Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    57,
                    52,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:57:52Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    57,
                    52,
                    3,
                    268,
                    0
                ],
                "title": "Grounding AI Explanations in Experience: A Reflective Cognitive\n  Architecture for Clinical Decision Support",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounding AI Explanations in Experience: A Reflective Cognitive\n  Architecture for Clinical Decision Support"
                },
                "summary": "Effective disease prediction in modern healthcare demands the twin goals of\nhigh accuracy and transparent, clinically meaningful explanations. Existing\nmachine learning and large language model (LLM) based approaches often struggle\nto balance these goals. Many models yield accurate but unclear statistical\noutputs, while others generate fluent but statistically unsupported narratives,\noften undermining both the validity of the explanation and the predictive\naccuracy itself. This shortcoming comes from a shallow interaction with the\ndata, preventing the development of a deep, detailed understanding similar to a\nhuman expert's. We argue that high accuracy and high-quality explanations are\nnot separate objectives but are mutually reinforcing outcomes of a model that\ndevelops a deep, direct understanding of the data. To achieve this, we propose\nthe Reflective Cognitive Architecture (RCA), a novel framework that coordinates\nmultiple LLMs to learn from direct experience. RCA features an iterative rule\nrefinement mechanism that improves its logic from prediction errors and a\ndistribution-aware rules check mechanism that bases its reasoning in the\ndataset's global statistics. By using predictive accuracy as a signal to drive\ndeeper comprehension, RCA builds a strong internal model of the data. We\nevaluated RCA on one private and two public datasets against 22 baselines. The\nresults demonstrate that RCA not only achieves state-of-the-art accuracy and\nrobustness with a relative improvement of up to 40\\% over the baseline but,\nmore importantly, leverages this deep understanding to excel in generating\nexplanations that are clear, logical, evidence-based, and balanced,\nhighlighting its potential for creating genuinely trustworthy clinical decision\nsupport systems. The code is available at \\https://github.com/ssssszj/RCA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective disease prediction in modern healthcare demands the twin goals of\nhigh accuracy and transparent, clinically meaningful explanations. Existing\nmachine learning and large language model (LLM) based approaches often struggle\nto balance these goals. Many models yield accurate but unclear statistical\noutputs, while others generate fluent but statistically unsupported narratives,\noften undermining both the validity of the explanation and the predictive\naccuracy itself. This shortcoming comes from a shallow interaction with the\ndata, preventing the development of a deep, detailed understanding similar to a\nhuman expert's. We argue that high accuracy and high-quality explanations are\nnot separate objectives but are mutually reinforcing outcomes of a model that\ndevelops a deep, direct understanding of the data. To achieve this, we propose\nthe Reflective Cognitive Architecture (RCA), a novel framework that coordinates\nmultiple LLMs to learn from direct experience. RCA features an iterative rule\nrefinement mechanism that improves its logic from prediction errors and a\ndistribution-aware rules check mechanism that bases its reasoning in the\ndataset's global statistics. By using predictive accuracy as a signal to drive\ndeeper comprehension, RCA builds a strong internal model of the data. We\nevaluated RCA on one private and two public datasets against 22 baselines. The\nresults demonstrate that RCA not only achieves state-of-the-art accuracy and\nrobustness with a relative improvement of up to 40\\% over the baseline but,\nmore importantly, leverages this deep understanding to excel in generating\nexplanations that are clear, logical, evidence-based, and balanced,\nhighlighting its potential for creating genuinely trustworthy clinical decision\nsupport systems. The code is available at \\https://github.com/ssssszj/RCA."
                },
                "authors": [
                    {
                        "name": "Zijian Shao"
                    },
                    {
                        "name": "Haiyang Shen"
                    },
                    {
                        "name": "Mugeng Liu"
                    },
                    {
                        "name": "Gecheng Fu"
                    },
                    {
                        "name": "Yaoqi Guo"
                    },
                    {
                        "name": "Yanfeng Wang"
                    },
                    {
                        "name": "Yun Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yun Ma"
                },
                "author": "Yun Ma",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21264v1",
                "updated": "2025-09-25T14:56:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    56,
                    49,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:56:49Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    56,
                    49,
                    3,
                    268,
                    0
                ],
                "title": "\\LARGE GMP$^{3}$: Learning-Driven, Bellman-Guided Trajectory Planning\n  for UAVs in Real-Time on SE(3)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\\LARGE GMP$^{3}$: Learning-Driven, Bellman-Guided Trajectory Planning\n  for UAVs in Real-Time on SE(3)"
                },
                "summary": "We propose $\\text{GMP}^{3}$, a multiphase global path planning framework that\ngenerates dynamically feasible three-dimensional trajectories for unmanned\naerial vehicles (UAVs) operating in cluttered environments. The framework\nextends traditional path planning from Euclidean position spaces to the Lie\ngroup $\\mathrm{SE}(3)$, allowing joint learning of translational motion and\nrotational dynamics. A modified Bellman-based operator is introduced to support\nreinforcement learning (RL) policy updates while leveraging prior trajectory\ninformation for improved convergence. $\\text{GMP}^{3}$ is designed as a\ndistributed framework in which agents influence each other and share policy\ninformation along the trajectory: each agent refines its assigned segment and\nshares with its neighbors via a consensus-based scheme, enabling cooperative\npolicy updates and convergence toward a path shaped globally even under\nkinematic constraints. We also propose DroneManager, a modular ground control\nsoftware that interfaces the planner with real UAV platforms via the MAVLink\nprotocol, supporting real-time deployment and feedback. Simulation studies and\nindoor flight experiments validate the effectiveness of the proposed method in\nconstrained 3D environments, demonstrating reliable obstacle avoidance and\nsmooth, feasible trajectories across both position and orientation. The\nopen-source implementation is available at\nhttps://github.com/Domattee/DroneManager",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose $\\text{GMP}^{3}$, a multiphase global path planning framework that\ngenerates dynamically feasible three-dimensional trajectories for unmanned\naerial vehicles (UAVs) operating in cluttered environments. The framework\nextends traditional path planning from Euclidean position spaces to the Lie\ngroup $\\mathrm{SE}(3)$, allowing joint learning of translational motion and\nrotational dynamics. A modified Bellman-based operator is introduced to support\nreinforcement learning (RL) policy updates while leveraging prior trajectory\ninformation for improved convergence. $\\text{GMP}^{3}$ is designed as a\ndistributed framework in which agents influence each other and share policy\ninformation along the trajectory: each agent refines its assigned segment and\nshares with its neighbors via a consensus-based scheme, enabling cooperative\npolicy updates and convergence toward a path shaped globally even under\nkinematic constraints. We also propose DroneManager, a modular ground control\nsoftware that interfaces the planner with real UAV platforms via the MAVLink\nprotocol, supporting real-time deployment and feedback. Simulation studies and\nindoor flight experiments validate the effectiveness of the proposed method in\nconstrained 3D environments, demonstrating reliable obstacle avoidance and\nsmooth, feasible trajectories across both position and orientation. The\nopen-source implementation is available at\nhttps://github.com/Domattee/DroneManager"
                },
                "authors": [
                    {
                        "name": "Babak Salamat"
                    },
                    {
                        "name": "Dominik Mattern"
                    },
                    {
                        "name": "Sebastian-Sven Olzem"
                    },
                    {
                        "name": "Gerhard Elsbacher"
                    },
                    {
                        "name": "Christian Seidel"
                    },
                    {
                        "name": "Andrea M. Tonello"
                    }
                ],
                "author_detail": {
                    "name": "Andrea M. Tonello"
                },
                "author": "Andrea M. Tonello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21262v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21262v1",
                "updated": "2025-09-25T14:54:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    54,
                    36,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:54:36Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    54,
                    36,
                    3,
                    268,
                    0
                ],
                "title": "Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Un-Doubling Diffusion: LLM-guided Disambiguation of Homonym Duplication"
                },
                "summary": "Homonyms are words with identical spelling but distinct meanings, which pose\nchallenges for many generative models. When a homonym appears in a prompt,\ndiffusion models may generate multiple senses of the word simultaneously, which\nis known as homonym duplication. This issue is further complicated by an\nAnglocentric bias, which includes an additional translation step before the\ntext-to-image model pipeline. As a result, even words that are not homonymous\nin the original language may become homonyms and lose their meaning after\ntranslation into English. In this paper, we introduce a method for measuring\nduplication rates and conduct evaluations of different diffusion models using\nboth automatic evaluation utilizing Vision-Language Models (VLM) and human\nevaluation. Additionally, we investigate methods to mitigate the homonym\nduplication problem through prompt expansion, demonstrating that this approach\nalso effectively reduces duplication related to Anglocentric bias. The code for\nthe automatic evaluation pipeline is publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Homonyms are words with identical spelling but distinct meanings, which pose\nchallenges for many generative models. When a homonym appears in a prompt,\ndiffusion models may generate multiple senses of the word simultaneously, which\nis known as homonym duplication. This issue is further complicated by an\nAnglocentric bias, which includes an additional translation step before the\ntext-to-image model pipeline. As a result, even words that are not homonymous\nin the original language may become homonyms and lose their meaning after\ntranslation into English. In this paper, we introduce a method for measuring\nduplication rates and conduct evaluations of different diffusion models using\nboth automatic evaluation utilizing Vision-Language Models (VLM) and human\nevaluation. Additionally, we investigate methods to mitigate the homonym\nduplication problem through prompt expansion, demonstrating that this approach\nalso effectively reduces duplication related to Anglocentric bias. The code for\nthe automatic evaluation pipeline is publicly available."
                },
                "authors": [
                    {
                        "name": "Evgeny Kaskov"
                    },
                    {
                        "name": "Elizaveta Petrova"
                    },
                    {
                        "name": "Petr Surovtsev"
                    },
                    {
                        "name": "Anna Kostikova"
                    },
                    {
                        "name": "Ilya Mistiurin"
                    },
                    {
                        "name": "Alexander Kapitanov"
                    },
                    {
                        "name": "Alexander Nagaev"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Nagaev"
                },
                "author": "Alexander Nagaev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21262v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21262v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21259v1",
                "updated": "2025-09-25T14:53:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    53,
                    36,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:53:36Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    53,
                    36,
                    3,
                    268,
                    0
                ],
                "title": "Semantic Edge-Cloud Communication for Real-Time Urban Traffic\n  Surveillance with ViT and LLMs over Mobile Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Edge-Cloud Communication for Real-Time Urban Traffic\n  Surveillance with ViT and LLMs over Mobile Networks"
                },
                "summary": "Real-time urban traffic surveillance is vital for Intelligent Transportation\nSystems (ITS) to ensure road safety, optimize traffic flow, track vehicle\ntrajectories, and prevent collisions in smart cities. Deploying edge cameras\nacross urban environments is a standard practice for monitoring road\nconditions. However, integrating these with intelligent models requires a\nrobust understanding of dynamic traffic scenarios and a responsive interface\nfor user interaction. Although multimodal Large Language Models (LLMs) can\ninterpret traffic images and generate informative responses, their deployment\non edge devices is infeasible due to high computational demands. Therefore, LLM\ninference must occur on the cloud, necessitating visual data transmission from\nedge to cloud, a process hindered by limited bandwidth, leading to potential\ndelays that compromise real-time performance. To address this challenge, we\npropose a semantic communication framework that significantly reduces\ntransmission overhead. Our method involves detecting Regions of Interest (RoIs)\nusing YOLOv11, cropping relevant image segments, and converting them into\ncompact embedding vectors using a Vision Transformer (ViT). These embeddings\nare then transmitted to the cloud, where an image decoder reconstructs the\ncropped images. The reconstructed images are processed by a multimodal LLM to\ngenerate traffic condition descriptions. This approach achieves a 99.9%\nreduction in data transmission size while maintaining an LLM response accuracy\nof 89% for reconstructed cropped images, compared to 93% accuracy with original\ncropped images. Our results demonstrate the efficiency and practicality of ViT\nand LLM-assisted edge-cloud semantic communication for real-time traffic\nsurveillance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time urban traffic surveillance is vital for Intelligent Transportation\nSystems (ITS) to ensure road safety, optimize traffic flow, track vehicle\ntrajectories, and prevent collisions in smart cities. Deploying edge cameras\nacross urban environments is a standard practice for monitoring road\nconditions. However, integrating these with intelligent models requires a\nrobust understanding of dynamic traffic scenarios and a responsive interface\nfor user interaction. Although multimodal Large Language Models (LLMs) can\ninterpret traffic images and generate informative responses, their deployment\non edge devices is infeasible due to high computational demands. Therefore, LLM\ninference must occur on the cloud, necessitating visual data transmission from\nedge to cloud, a process hindered by limited bandwidth, leading to potential\ndelays that compromise real-time performance. To address this challenge, we\npropose a semantic communication framework that significantly reduces\ntransmission overhead. Our method involves detecting Regions of Interest (RoIs)\nusing YOLOv11, cropping relevant image segments, and converting them into\ncompact embedding vectors using a Vision Transformer (ViT). These embeddings\nare then transmitted to the cloud, where an image decoder reconstructs the\ncropped images. The reconstructed images are processed by a multimodal LLM to\ngenerate traffic condition descriptions. This approach achieves a 99.9%\nreduction in data transmission size while maintaining an LLM response accuracy\nof 89% for reconstructed cropped images, compared to 93% accuracy with original\ncropped images. Our results demonstrate the efficiency and practicality of ViT\nand LLM-assisted edge-cloud semantic communication for real-time traffic\nsurveillance."
                },
                "authors": [
                    {
                        "name": "Murat Arda Onsu"
                    },
                    {
                        "name": "Poonam Lohan"
                    },
                    {
                        "name": "Burak Kantarci"
                    },
                    {
                        "name": "Aisha Syed"
                    },
                    {
                        "name": "Matthew Andrews"
                    },
                    {
                        "name": "Sean Kennedy"
                    }
                ],
                "author_detail": {
                    "name": "Sean Kennedy"
                },
                "author": "Sean Kennedy",
                "arxiv_comment": "17 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10278v2",
                "updated": "2025-09-25T14:52:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    52,
                    25,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-15T13:27:18Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    27,
                    18,
                    3,
                    135,
                    0
                ],
                "title": "MASS: Muli-agent simulation scaling for portfolio construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASS: Muli-agent simulation scaling for portfolio construction"
                },
                "summary": "The application of LLM-based agents in financial investment has shown\nsignificant promise, yet existing approaches often require intermediate steps\nlike predicting individual stock movements or rely on predefined, static\nworkflows. These limitations restrict their adaptability and effectiveness in\nconstructing optimal portfolios. In this paper, we introduce the Multi-Agent\nScaling Simulation (MASS), a novel framework that leverages multi-agent\nsimulation for direct, end-to-end portfolio construction. At its core, MASS\nemploys a backward optimization process to dynamically learn the optimal\ndistribution of heterogeneous agents, enabling the system to adapt to evolving\nmarket regimes. A key finding enabled by our framework is the exploration of\nthe scaling effect for portfolio construction: we demonstrate that as the\nnumber of agents increases exponentially (up to 512), the aggregated decisions\nyield progressively higher excess returns. Extensive experiments on a\nchallenging, self-collected dataset from the 2023 Chinese A-share market show\nthat MASS consistently outperforms seven state-of-the-art baselines. Further\nbacktesting, stability analyses and the experiment on data leakage concerns\nvalidate its enhanced profitability and robustness. We have open-sourced our\ncode, dataset, and training snapshots at https://github.com/gta0804/MASS/ to\nfoster further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of LLM-based agents in financial investment has shown\nsignificant promise, yet existing approaches often require intermediate steps\nlike predicting individual stock movements or rely on predefined, static\nworkflows. These limitations restrict their adaptability and effectiveness in\nconstructing optimal portfolios. In this paper, we introduce the Multi-Agent\nScaling Simulation (MASS), a novel framework that leverages multi-agent\nsimulation for direct, end-to-end portfolio construction. At its core, MASS\nemploys a backward optimization process to dynamically learn the optimal\ndistribution of heterogeneous agents, enabling the system to adapt to evolving\nmarket regimes. A key finding enabled by our framework is the exploration of\nthe scaling effect for portfolio construction: we demonstrate that as the\nnumber of agents increases exponentially (up to 512), the aggregated decisions\nyield progressively higher excess returns. Extensive experiments on a\nchallenging, self-collected dataset from the 2023 Chinese A-share market show\nthat MASS consistently outperforms seven state-of-the-art baselines. Further\nbacktesting, stability analyses and the experiment on data leakage concerns\nvalidate its enhanced profitability and robustness. We have open-sourced our\ncode, dataset, and training snapshots at https://github.com/gta0804/MASS/ to\nfoster further research."
                },
                "authors": [
                    {
                        "name": "Taian Guo"
                    },
                    {
                        "name": "Haiyang Shen"
                    },
                    {
                        "name": "JinSheng Huang"
                    },
                    {
                        "name": "Zhengyang Mao"
                    },
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Binqi Chen"
                    },
                    {
                        "name": "Zhuoru Chen"
                    },
                    {
                        "name": "Luchen Liu"
                    },
                    {
                        "name": "Bingyu Xia"
                    },
                    {
                        "name": "Xuhui Liu"
                    },
                    {
                        "name": "Yun Ma"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11684v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11684v2",
                "updated": "2025-09-25T14:50:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    50,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-02-17T11:22:24Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    11,
                    22,
                    24,
                    0,
                    48,
                    0
                ],
                "title": "MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps\n  through Fill-in-the-Middle Task",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathFimer: Enhancing Mathematical Reasoning by Expanding Reasoning Steps\n  through Fill-in-the-Middle Task"
                },
                "summary": "Mathematical reasoning represents a critical frontier in advancing large\nlanguage models (LLMs). While step-by-step approaches have emerged as the\ndominant paradigm for mathematical problem-solving in LLMs, the quality of\nreasoning steps in training data fundamentally constrains the performance of\nthe models. Recent studies has demonstrated that more detailed intermediate\nsteps can enhance model performance, yet existing methods for step expansion\neither require more powerful external models or incur substantial computational\ncosts. In this paper, we introduce MathFimer, a novel framework for\nmathematical reasoning step expansion inspired by the \"Fill-in-the-middle\" task\nfrom code completion. By decomposing solution chains into prefix-suffix pairs\nand training models to reconstruct missing intermediate steps, we develop a\nspecialized model, MathFimer-7B, on our carefully curated NuminaMath-FIM\ndataset. We then apply these models to enhance existing mathematical reasoning\ndatasets by inserting detailed intermediate steps into their solution chains,\ncreating MathFimer-expanded versions. Through comprehensive experiments on\nmultiple mathematical reasoning datasets, including MathInstruct, MetaMathQA\nand etc., we demonstrate that models trained on MathFimer-expanded data\nconsistently outperform their counterparts trained on original data across\nvarious benchmarks such as GSM8K and MATH. Our approach offers a practical,\nscalable solution for enhancing mathematical reasoning capabilities in LLMs\nwithout relying on powerful external models or expensive inference procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning represents a critical frontier in advancing large\nlanguage models (LLMs). While step-by-step approaches have emerged as the\ndominant paradigm for mathematical problem-solving in LLMs, the quality of\nreasoning steps in training data fundamentally constrains the performance of\nthe models. Recent studies has demonstrated that more detailed intermediate\nsteps can enhance model performance, yet existing methods for step expansion\neither require more powerful external models or incur substantial computational\ncosts. In this paper, we introduce MathFimer, a novel framework for\nmathematical reasoning step expansion inspired by the \"Fill-in-the-middle\" task\nfrom code completion. By decomposing solution chains into prefix-suffix pairs\nand training models to reconstruct missing intermediate steps, we develop a\nspecialized model, MathFimer-7B, on our carefully curated NuminaMath-FIM\ndataset. We then apply these models to enhance existing mathematical reasoning\ndatasets by inserting detailed intermediate steps into their solution chains,\ncreating MathFimer-expanded versions. Through comprehensive experiments on\nmultiple mathematical reasoning datasets, including MathInstruct, MetaMathQA\nand etc., we demonstrate that models trained on MathFimer-expanded data\nconsistently outperform their counterparts trained on original data across\nvarious benchmarks such as GSM8K and MATH. Our approach offers a practical,\nscalable solution for enhancing mathematical reasoning capabilities in LLMs\nwithout relying on powerful external models or expensive inference procedures."
                },
                "authors": [
                    {
                        "name": "Yuchen Yan"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Jin Jiang"
                    },
                    {
                        "name": "Xin Xu"
                    },
                    {
                        "name": "Mengdi Zhang"
                    },
                    {
                        "name": "Jian Shao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11684v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11684v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.06795v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.06795v3",
                "updated": "2025-09-25T14:48:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    48,
                    59,
                    3,
                    268,
                    0
                ],
                "published": "2025-07-09T12:30:42Z",
                "published_parsed": [
                    2025,
                    7,
                    9,
                    12,
                    30,
                    42,
                    2,
                    190,
                    0
                ],
                "title": "ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual\n  Pretraining"
                },
                "summary": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment."
                },
                "authors": [
                    {
                        "name": "Seonwu Kim"
                    },
                    {
                        "name": "Yohan Na"
                    },
                    {
                        "name": "Kihun Kim"
                    },
                    {
                        "name": "Hanhee Cho"
                    },
                    {
                        "name": "Geun Lim"
                    },
                    {
                        "name": "Mintae Kim"
                    },
                    {
                        "name": "Seongik Park"
                    },
                    {
                        "name": "Ki Hyun Kim"
                    },
                    {
                        "name": "Youngsub Han"
                    },
                    {
                        "name": "Byoung-Ki Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Byoung-Ki Jeon"
                },
                "author": "Byoung-Ki Jeon",
                "arxiv_comment": "Accepted at EMNLP 2025 Industry Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.06795v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.06795v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01733v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01733v2",
                "updated": "2025-09-25T14:46:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    46,
                    11,
                    3,
                    268,
                    0
                ],
                "published": "2024-10-02T16:46:01Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    46,
                    1,
                    2,
                    276,
                    0
                ],
                "title": "ASCIIEval: Benchmarking Models' Visual Perception in Text Strings via\n  ASCII Art",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASCIIEval: Benchmarking Models' Visual Perception in Text Strings via\n  ASCII Art"
                },
                "summary": "Perceiving visual semantics embedded within consecutive characters is a\ncrucial yet under-explored capability for both Large Language Models (LLMs) and\nMulti-modal Large Language Models (MLLMs). In this work, we select ASCII art as\na representative artifact. It depicts concepts through careful arrangement of\ncharacters, which can be formulated in both text and image modalities. We frame\nthe problem as a recognition task, and construct a novel benchmark, ASCIIEval.\nIt covers over 3K samples with an elaborate categorization tree, along with a\ntraining set for further enhancement. Encompassing a comprehensive analysis of\ntens of models through different input modalities, our benchmark demonstrate\nits multi-faceted diagnostic power. Given textual input, language models shows\ntheir visual perception ability on ASCII art concepts. Proprietary models\nachieve over 70% accuracy on certain categories, with GPT-5 topping the rank.\nFor image inputs, we reveal that open-source MLLMs suffer from a trade-off\nbetween fine-grained text recognition and collective visual perception. They\nexhibit limited generalization ability to this special kind of arts, leading to\nthe dramatic gap of over 20.01% accuracy compared with their proprietary\ncounterparts. Another critical finding is that model performance is sensitive\nto the length of the ASCII art, with this sensitivity varying across input\nmodalities. Unfortunately, none of the models could successfully benefit from\nthe simultaneous provision of both modalities, highlighting the need for more\nflexible modality-fusion approaches. Besides, we also introduce approaches for\nfurther enhancement and discuss future directions. Resources are available at\nhttps://github.com/JiaQiSJTU/VisionInText.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceiving visual semantics embedded within consecutive characters is a\ncrucial yet under-explored capability for both Large Language Models (LLMs) and\nMulti-modal Large Language Models (MLLMs). In this work, we select ASCII art as\na representative artifact. It depicts concepts through careful arrangement of\ncharacters, which can be formulated in both text and image modalities. We frame\nthe problem as a recognition task, and construct a novel benchmark, ASCIIEval.\nIt covers over 3K samples with an elaborate categorization tree, along with a\ntraining set for further enhancement. Encompassing a comprehensive analysis of\ntens of models through different input modalities, our benchmark demonstrate\nits multi-faceted diagnostic power. Given textual input, language models shows\ntheir visual perception ability on ASCII art concepts. Proprietary models\nachieve over 70% accuracy on certain categories, with GPT-5 topping the rank.\nFor image inputs, we reveal that open-source MLLMs suffer from a trade-off\nbetween fine-grained text recognition and collective visual perception. They\nexhibit limited generalization ability to this special kind of arts, leading to\nthe dramatic gap of over 20.01% accuracy compared with their proprietary\ncounterparts. Another critical finding is that model performance is sensitive\nto the length of the ASCII art, with this sensitivity varying across input\nmodalities. Unfortunately, none of the models could successfully benefit from\nthe simultaneous provision of both modalities, highlighting the need for more\nflexible modality-fusion approaches. Besides, we also introduce approaches for\nfurther enhancement and discuss future directions. Resources are available at\nhttps://github.com/JiaQiSJTU/VisionInText."
                },
                "authors": [
                    {
                        "name": "Qi Jia"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Shanshan Huang"
                    },
                    {
                        "name": "Ziheng Qin"
                    },
                    {
                        "name": "Yizhu Liu"
                    },
                    {
                        "name": "Bill Yuchen Lin"
                    },
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Guangtao Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Guangtao Zhai"
                },
                "author": "Guangtao Zhai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01733v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01733v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21251v1",
                "updated": "2025-09-25T14:45:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    45,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:45:06Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    45,
                    6,
                    3,
                    268,
                    0
                ],
                "title": "Instruction-tuned Self-Questioning Framework for Multimodal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instruction-tuned Self-Questioning Framework for Multimodal Reasoning"
                },
                "summary": "The field of vision-language understanding has been actively researched in\nrecent years, thanks to the development of Large Language Models~(LLMs).\nHowever, it still needs help with problems requiring multi-step reasoning, even\nfor very simple questions. Recent studies adopt LLMs to tackle this problem by\niteratively generating sub-questions and answers. However, there are\ndisadvantages such as 1) the fine-grained visual contents of images are not\navailable using LLMs that cannot read visual information, 2) internal\nmechanisms are inaccessible and difficult to reproduce by using black-box LLMs.\nTo solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP,\nwhich improves inference performance by generating image-aware informative\nsub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists\nof a Questioner, Answerer, and Reasoner that share the same architecture.\nQuestioner and Answerer generate sub-questions and sub-answers to help infer\nthe main-question, and Reasoner performs reasoning on the main-question\nconsidering the generated sub-question information. Our experiments show that\nthe proposed method SQ-InstructBLIP, which uses the generated sub-questions as\nadditional information when solving the VQA task, performs more accurate\nreasoning than the previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of vision-language understanding has been actively researched in\nrecent years, thanks to the development of Large Language Models~(LLMs).\nHowever, it still needs help with problems requiring multi-step reasoning, even\nfor very simple questions. Recent studies adopt LLMs to tackle this problem by\niteratively generating sub-questions and answers. However, there are\ndisadvantages such as 1) the fine-grained visual contents of images are not\navailable using LLMs that cannot read visual information, 2) internal\nmechanisms are inaccessible and difficult to reproduce by using black-box LLMs.\nTo solve these problems, we propose the SQ (Self-Questioning)-InstructBLIP,\nwhich improves inference performance by generating image-aware informative\nsub-questions and sub-answers iteratively. The SQ-InstructBLIP, which consists\nof a Questioner, Answerer, and Reasoner that share the same architecture.\nQuestioner and Answerer generate sub-questions and sub-answers to help infer\nthe main-question, and Reasoner performs reasoning on the main-question\nconsidering the generated sub-question information. Our experiments show that\nthe proposed method SQ-InstructBLIP, which uses the generated sub-questions as\nadditional information when solving the VQA task, performs more accurate\nreasoning than the previous works."
                },
                "authors": [
                    {
                        "name": "You-Won Jang"
                    },
                    {
                        "name": "Yu-Jung Heo"
                    },
                    {
                        "name": "Jaeseok Kim"
                    },
                    {
                        "name": "Minsu Lee"
                    },
                    {
                        "name": "Du-Seong Chang"
                    },
                    {
                        "name": "Byoung-Tak Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Byoung-Tak Zhang"
                },
                "author": "Byoung-Tak Zhang",
                "arxiv_comment": "This paper was accepted to the \"CLVL: 5th Workshop on Closing the\n  Loop Between Vision and Language (ICCV 2023 CLVL workshop).\"",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21243v1",
                "updated": "2025-09-25T14:37:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    37,
                    54,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:37:54Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    37,
                    54,
                    3,
                    268,
                    0
                ],
                "title": "RetoVLA: Reusing Register Tokens for Spatial Reasoning in\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetoVLA: Reusing Register Tokens for Spatial Reasoning in\n  Vision-Language-Action Models"
                },
                "summary": "Recent Vision-Language-Action (VLA) models demonstrate remarkable\ngeneralization in robotics but are restricted by their substantial size and\ncomputational cost, limiting real-world deployment. However, conventional\nlightweighting methods often sacrifice critical capabilities, particularly\nspatial reasoning. This creates a trade-off between efficiency and performance.\nTo address this challenge, our work reuses Register Tokens, which were\nintroduced for artifact removal in Vision Transformers but subsequently\ndiscarded. We suppose that these tokens contain essential spatial information\nand propose RetoVLA, a novel architecture that reuses them directly by\ninjecting them into the Action Expert.\n  RetoVLA maintains a lightweight structure while leveraging this repurposed\nspatial context to enhance reasoning. We demonstrate RetoVLA's effectiveness\nthrough a series of comprehensive experiments. On our custom-built 7-DOF robot\narm, the model achieves a 17.1%p absolute improvement in success rates for\ncomplex manipulation tasks. Our results confirm that reusing Register Tokens\ndirectly enhances spatial reasoning, demonstrating that what was previously\ndiscarded as an artifact is in fact a valuable, unexplored resource for robotic\nintelligence. A video demonstration is available at:\nhttps://youtu.be/2CseBR-snZg",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Vision-Language-Action (VLA) models demonstrate remarkable\ngeneralization in robotics but are restricted by their substantial size and\ncomputational cost, limiting real-world deployment. However, conventional\nlightweighting methods often sacrifice critical capabilities, particularly\nspatial reasoning. This creates a trade-off between efficiency and performance.\nTo address this challenge, our work reuses Register Tokens, which were\nintroduced for artifact removal in Vision Transformers but subsequently\ndiscarded. We suppose that these tokens contain essential spatial information\nand propose RetoVLA, a novel architecture that reuses them directly by\ninjecting them into the Action Expert.\n  RetoVLA maintains a lightweight structure while leveraging this repurposed\nspatial context to enhance reasoning. We demonstrate RetoVLA's effectiveness\nthrough a series of comprehensive experiments. On our custom-built 7-DOF robot\narm, the model achieves a 17.1%p absolute improvement in success rates for\ncomplex manipulation tasks. Our results confirm that reusing Register Tokens\ndirectly enhances spatial reasoning, demonstrating that what was previously\ndiscarded as an artifact is in fact a valuable, unexplored resource for robotic\nintelligence. A video demonstration is available at:\nhttps://youtu.be/2CseBR-snZg"
                },
                "authors": [
                    {
                        "name": "Jiyeon Koo"
                    },
                    {
                        "name": "Taewan Cho"
                    },
                    {
                        "name": "Hyunjoon Kang"
                    },
                    {
                        "name": "Eunseom Pyo"
                    },
                    {
                        "name": "Tae Gyun Oh"
                    },
                    {
                        "name": "Taeryang Kim"
                    },
                    {
                        "name": "Andrew Jaeyong Choi"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Jaeyong Choi"
                },
                "author": "Andrew Jaeyong Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21241v1",
                "updated": "2025-09-25T14:37:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    37,
                    40,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:37:40Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    37,
                    40,
                    3,
                    268,
                    0
                ],
                "title": "Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven\n  Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Explaining Fine Tuned LLMs via Counterfactuals A Knowledge Graph Driven\n  Framework"
                },
                "summary": "The widespread adoption of Low-Rank Adaptation (LoRA) has enabled large\nlanguage models (LLMs) to acquire domain-specific knowledge with remarkable\nefficiency. However, understanding how such a fine-tuning mechanism alters a\nmodel's structural reasoning and semantic behavior remains an open challenge.\nThis work introduces a novel framework that explains fine-tuned LLMs via\ncounterfactuals grounded in knowledge graphs. Specifically, we construct\nBioToolKG, a domain-specific heterogeneous knowledge graph in bioinformatics\ntools and design a counterfactual-based fine-tuned LLMs explainer\n(CFFTLLMExplainer) that learns soft masks over graph nodes and edges to\ngenerate minimal structural perturbations that induce maximum semantic\ndivergence. Our method jointly optimizes structural sparsity and semantic\ndivergence while enforcing interpretability preserving constraints such as\nentropy regularization and edge smoothness. We apply this framework to a\nfine-tuned LLaMA-based LLM and reveal that counterfactual masking exposes the\nmodel's structural dependencies and aligns with LoRA-induced parameter shifts.\nThis work provides new insights into the internal mechanisms of fine-tuned LLMs\nand highlights counterfactual graphs as a potential tool for interpretable AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of Low-Rank Adaptation (LoRA) has enabled large\nlanguage models (LLMs) to acquire domain-specific knowledge with remarkable\nefficiency. However, understanding how such a fine-tuning mechanism alters a\nmodel's structural reasoning and semantic behavior remains an open challenge.\nThis work introduces a novel framework that explains fine-tuned LLMs via\ncounterfactuals grounded in knowledge graphs. Specifically, we construct\nBioToolKG, a domain-specific heterogeneous knowledge graph in bioinformatics\ntools and design a counterfactual-based fine-tuned LLMs explainer\n(CFFTLLMExplainer) that learns soft masks over graph nodes and edges to\ngenerate minimal structural perturbations that induce maximum semantic\ndivergence. Our method jointly optimizes structural sparsity and semantic\ndivergence while enforcing interpretability preserving constraints such as\nentropy regularization and edge smoothness. We apply this framework to a\nfine-tuned LLaMA-based LLM and reveal that counterfactual masking exposes the\nmodel's structural dependencies and aligns with LoRA-induced parameter shifts.\nThis work provides new insights into the internal mechanisms of fine-tuned LLMs\nand highlights counterfactual graphs as a potential tool for interpretable AI."
                },
                "authors": [
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Ziyang Chen"
                    },
                    {
                        "name": "Md Faisal Kabir"
                    }
                ],
                "author_detail": {
                    "name": "Md Faisal Kabir"
                },
                "author": "Md Faisal Kabir",
                "arxiv_comment": "16 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21240v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21240v1",
                "updated": "2025-09-25T14:37:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    37,
                    9,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:37:09Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    37,
                    9,
                    3,
                    268,
                    0
                ],
                "title": "Tree Search for LLM Agent Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree Search for LLM Agent Reinforcement Learning"
                },
                "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe agentic capabilities of large language models (LLMs). In long-term and\nmulti-turn agent tasks, existing approaches driven solely by outcome rewards\noften suffer from the problem of sparse supervision. To address the challenge,\nwe propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped\nagent RL method based on tree search, where each tree node represents the\ncomplete agent interaction step. By sharing common prefixes, the tree search\nsampling increases the number of rollouts achievable within a fixed budget of\ntokens or tool calls. Moreover, we find that the tree-structured trajectory\nnaturally allows the construction of step-wise process supervised signals even\nusing only the outcome reward. Based on this, Tree-GRPO estimates the grouped\nrelative advantages both on intra-tree and inter-tree levels. Through\ntheoretical analysis, we demonstrate that the objective of intra-tree level\ngroup relative policy optimization is equivalent to that of step-level direct\npreference learning. Experiments across 11 datasets and 3 types of QA tasks\ndemonstrate the superiority of the proposed tree-based RL over the chain-based\nRL method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe agentic capabilities of large language models (LLMs). In long-term and\nmulti-turn agent tasks, existing approaches driven solely by outcome rewards\noften suffer from the problem of sparse supervision. To address the challenge,\nwe propose Tree-based Group Relative Policy Optimization (Tree-GRPO), a grouped\nagent RL method based on tree search, where each tree node represents the\ncomplete agent interaction step. By sharing common prefixes, the tree search\nsampling increases the number of rollouts achievable within a fixed budget of\ntokens or tool calls. Moreover, we find that the tree-structured trajectory\nnaturally allows the construction of step-wise process supervised signals even\nusing only the outcome reward. Based on this, Tree-GRPO estimates the grouped\nrelative advantages both on intra-tree and inter-tree levels. Through\ntheoretical analysis, we demonstrate that the objective of intra-tree level\ngroup relative policy optimization is equivalent to that of step-level direct\npreference learning. Experiments across 11 datasets and 3 types of QA tasks\ndemonstrate the superiority of the proposed tree-based RL over the chain-based\nRL method."
                },
                "authors": [
                    {
                        "name": "Yuxiang Ji"
                    },
                    {
                        "name": "Ziyu Ma"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Guanhua Chen"
                    },
                    {
                        "name": "Xiangxiang Chu"
                    },
                    {
                        "name": "Liaoni Wu"
                    }
                ],
                "author_detail": {
                    "name": "Liaoni Wu"
                },
                "author": "Liaoni Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21240v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21240v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21237v1",
                "updated": "2025-09-25T14:35:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    35,
                    44,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:35:44Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    35,
                    44,
                    3,
                    268,
                    0
                ],
                "title": "Query-Centric Graph Retrieval Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-Centric Graph Retrieval Augmented Generation"
                },
                "summary": "Graph-based retrieval-augmented generation (RAG) enriches large language\nmodels (LLMs) with external knowledge for long-context understanding and\nmulti-hop reasoning, but existing methods face a granularity dilemma:\nfine-grained entity-level graphs incur high token costs and lose context, while\ncoarse document-level graphs fail to capture nuanced relations. We introduce\nQCG-RAG, a query-centric graph RAG framework that enables query-granular\nindexing and multi-hop chunk retrieval. Our query-centric approach leverages\nDoc2Query and Doc2Query{-}{-} to construct query-centric graphs with\ncontrollable granularity, improving graph quality and interpretability. A\ntailored multi-hop retrieval mechanism then selects relevant chunks via the\ngenerated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG\nconsistently outperforms prior chunk-based and graph-based RAG methods in\nquestion answering accuracy, establishing a new paradigm for multi-hop\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph-based retrieval-augmented generation (RAG) enriches large language\nmodels (LLMs) with external knowledge for long-context understanding and\nmulti-hop reasoning, but existing methods face a granularity dilemma:\nfine-grained entity-level graphs incur high token costs and lose context, while\ncoarse document-level graphs fail to capture nuanced relations. We introduce\nQCG-RAG, a query-centric graph RAG framework that enables query-granular\nindexing and multi-hop chunk retrieval. Our query-centric approach leverages\nDoc2Query and Doc2Query{-}{-} to construct query-centric graphs with\ncontrollable granularity, improving graph quality and interpretability. A\ntailored multi-hop retrieval mechanism then selects relevant chunks via the\ngenerated queries. Experiments on LiHuaWorld and MultiHop-RAG show that QCG-RAG\nconsistently outperforms prior chunk-based and graph-based RAG methods in\nquestion answering accuracy, establishing a new paradigm for multi-hop\nreasoning."
                },
                "authors": [
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Jianyuan Bo"
                    },
                    {
                        "name": "Yongyue Zhang"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21333v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21333v2",
                "updated": "2025-09-25T14:34:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    34,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-27T15:27:46Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    15,
                    27,
                    46,
                    1,
                    147,
                    0
                ],
                "title": "MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in\n  Video Scenarios",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MME-VideoOCR: Evaluating OCR-Based Capabilities of Multimodal LLMs in\n  Video Scenarios"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved considerable accuracy\nin Optical Character Recognition (OCR) from static images. However, their\nefficacy in video OCR is significantly diminished due to factors such as motion\nblur, temporal variations, and visual effects inherent in video content. To\nprovide clearer guidance for training practical MLLMs, we introduce the\nMME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR\napplication scenarios. MME-VideoOCR features 10 task categories comprising 25\nindividual tasks and spans 44 diverse scenarios. These tasks extend beyond text\nrecognition to incorporate deeper comprehension and reasoning of textual\ncontent within videos. The benchmark consists of 1,464 videos with varying\nresolutions, aspect ratios, and durations, along with 2,000 meticulously\ncurated, manually annotated question-answer pairs. We evaluate 18\nstate-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing\nmodel (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained\nanalysis indicates that while existing MLLMs demonstrate strong performance on\ntasks where relevant texts are contained within a single or few frames, they\nexhibit limited capability in effectively handling tasks that demand holistic\nvideo comprehension. These limitations are especially evident in scenarios that\nrequire spatio-temporal reasoning, cross-frame information integration, or\nresistance to language prior bias. Our findings also highlight the importance\nof high-resolution visual input and sufficient temporal coverage for reliable\nOCR in dynamic video scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved considerable accuracy\nin Optical Character Recognition (OCR) from static images. However, their\nefficacy in video OCR is significantly diminished due to factors such as motion\nblur, temporal variations, and visual effects inherent in video content. To\nprovide clearer guidance for training practical MLLMs, we introduce the\nMME-VideoOCR benchmark, which encompasses a comprehensive range of video OCR\napplication scenarios. MME-VideoOCR features 10 task categories comprising 25\nindividual tasks and spans 44 diverse scenarios. These tasks extend beyond text\nrecognition to incorporate deeper comprehension and reasoning of textual\ncontent within videos. The benchmark consists of 1,464 videos with varying\nresolutions, aspect ratios, and durations, along with 2,000 meticulously\ncurated, manually annotated question-answer pairs. We evaluate 18\nstate-of-the-art MLLMs on MME-VideoOCR, revealing that even the best-performing\nmodel (Gemini-2.5 Pro) achieves an accuracy of only 73.7%. Fine-grained\nanalysis indicates that while existing MLLMs demonstrate strong performance on\ntasks where relevant texts are contained within a single or few frames, they\nexhibit limited capability in effectively handling tasks that demand holistic\nvideo comprehension. These limitations are especially evident in scenarios that\nrequire spatio-temporal reasoning, cross-frame information integration, or\nresistance to language prior bias. Our findings also highlight the importance\nof high-resolution visual input and sufficient temporal coverage for reliable\nOCR in dynamic video scenarios."
                },
                "authors": [
                    {
                        "name": "Yang Shi"
                    },
                    {
                        "name": "Huanqian Wang"
                    },
                    {
                        "name": "Wulin Xie"
                    },
                    {
                        "name": "Huanyao Zhang"
                    },
                    {
                        "name": "Lijie Zhao"
                    },
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Xinfeng Li"
                    },
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Zhuoer Wen"
                    },
                    {
                        "name": "Wenting Liu"
                    },
                    {
                        "name": "Zhuoran Zhang"
                    },
                    {
                        "name": "Xinlong Chen"
                    },
                    {
                        "name": "Bohan Zeng"
                    },
                    {
                        "name": "Sihan Yang"
                    },
                    {
                        "name": "Yushuo Guan"
                    },
                    {
                        "name": "Zhang Zhang"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Haoxuan Li"
                    },
                    {
                        "name": "Zhouchen Lin"
                    },
                    {
                        "name": "Yuanxing Zhang"
                    },
                    {
                        "name": "Pengfei Wan"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Wenjing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjing Yang"
                },
                "author": "Wenjing Yang",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21333v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21333v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21224v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21224v1",
                "updated": "2025-09-25T14:29:49Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    29,
                    49,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:29:49Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    29,
                    49,
                    3,
                    268,
                    0
                ],
                "title": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous\n  Meta-Cognitive Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Do LLM Agents Do When Left Alone? Evidence of Spontaneous\n  Meta-Cognitive Patterns"
                },
                "summary": "We introduce an architecture for studying the behavior of large language\nmodel (LLM) agents in the absence of externally imposed tasks. Our continuous\nreason and act framework, using persistent memory and self-feedback, enables\nsustained autonomous operation. We deployed this architecture across 18 runs\nusing 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents\nspontaneously organize into three distinct behavioral patterns: (1) systematic\nproduction of multi-cycle projects, (2) methodological self-inquiry into their\nown cognitive processes, and (3) recursive conceptualization of their own\nnature. These tendencies proved highly model-specific, with some models\ndeterministically adopting a single pattern across all runs. A cross-model\nassessment further reveals that models exhibit stable, divergent biases when\nevaluating these emergent behaviors in themselves and others. These findings\nprovide the first systematic documentation of unprompted LLM agent behavior,\nestablishing a baseline for predicting actions during task ambiguity, error\nrecovery, or extended autonomous operation in deployed systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an architecture for studying the behavior of large language\nmodel (LLM) agents in the absence of externally imposed tasks. Our continuous\nreason and act framework, using persistent memory and self-feedback, enables\nsustained autonomous operation. We deployed this architecture across 18 runs\nusing 6 frontier models from Anthropic, OpenAI, XAI, and Google. We find agents\nspontaneously organize into three distinct behavioral patterns: (1) systematic\nproduction of multi-cycle projects, (2) methodological self-inquiry into their\nown cognitive processes, and (3) recursive conceptualization of their own\nnature. These tendencies proved highly model-specific, with some models\ndeterministically adopting a single pattern across all runs. A cross-model\nassessment further reveals that models exhibit stable, divergent biases when\nevaluating these emergent behaviors in themselves and others. These findings\nprovide the first systematic documentation of unprompted LLM agent behavior,\nestablishing a baseline for predicting actions during task ambiguity, error\nrecovery, or extended autonomous operation in deployed systems."
                },
                "authors": [
                    {
                        "name": "Stefan Szeider"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Szeider"
                },
                "author": "Stefan Szeider",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21224v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21224v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18056v2",
                "updated": "2025-09-25T14:28:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    28,
                    56,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-22T17:30:15Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    30,
                    15,
                    0,
                    265,
                    0
                ],
                "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning\n  for Video LLMs"
                },
                "summary": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework\ndesigned to improve the effectiveness of adapting multimodal large language\nmodels (MLLMs) to video temporal grounding tasks. We reveal that existing\nreinforcement learning methods, such as Group Relative Policy Optimization\n(GRPO), rely on on-policy sampling for policy updates. However, in tasks with\nlarge temporal search spaces, this strategy becomes both inefficient and\nlimited in performance, as it often fails to identify temporally accurate\nsolutions. To address this limitation, TempSamp-R1 leverages ground-truth\nannotations as off-policy supervision to provide temporally precise guidance,\neffectively compensating for the sparsity and misalignment in on-policy\nsolutions. To further stabilize training and reduce variance in reward-based\nupdates, TempSamp-R1 provides a non-linear soft advantage computation method\nthat dynamically reshapes the reward feedback via an asymmetric transformation.\nBy employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1\noptimizes a single unified model to support both CoT and non-CoT inference\nmodes, enabling efficient handling of queries with varying reasoning\ncomplexity. Experimental results demonstrate that TempSamp-R1 outperforms\nGRPO-based baselines, establishing new state-of-the-art performance on\nbenchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions\n(R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover,\nTempSamp-R1 shows robust few-shot generalization capabilities under limited\ndata. Code: https://github.com/HVision-NKU/TempSamp-R1"
                },
                "authors": [
                    {
                        "name": "Yunheng Li"
                    },
                    {
                        "name": "Jing Cheng"
                    },
                    {
                        "name": "Shaoyong Jia"
                    },
                    {
                        "name": "Hangyi Kuang"
                    },
                    {
                        "name": "Shaohui Jiao"
                    },
                    {
                        "name": "Qibin Hou"
                    },
                    {
                        "name": "Ming-Ming Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ming-Ming Cheng"
                },
                "author": "Ming-Ming Cheng",
                "arxiv_comment": "Accepted at NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21221v1",
                "updated": "2025-09-25T14:27:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    27,
                    41,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:27:41Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    27,
                    41,
                    3,
                    268,
                    0
                ],
                "title": "Go With The Flow: Churn-Tolerant Decentralized Training of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Go With The Flow: Churn-Tolerant Decentralized Training of Large\n  Language Models"
                },
                "summary": "Motivated by the emergence of large language models (LLMs) and the importance\nof democratizing their training, we propose GWTF, the first crash tolerant\npractical decentralized training framework for LLMs. Differently from existing\ndistributed and federated training frameworks, GWTF enables the efficient\ncollaborative training of a LLM on heterogeneous clients that volunteer their\nresources. In addition, GWTF addresses node churn, i.e., clients joining or\nleaving the system at any time, and network instabilities, i.e., network links\nbecoming unstable or unreliable. The core of GWTF is a novel decentralized flow\nalgorithm that finds the most effective routing that maximizes the number of\nmicrobatches trained with the lowest possible delay. We extensively evaluate\nGWTF on GPT-like and LLaMa-like models and compare it against the prior art.\nOur results indicate that GWTF reduces the training time by up to 45% in\nrealistic and challenging scenarios that involve heterogeneous client nodes\ndistributed over 10 different geographic locations with a high node churn rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the emergence of large language models (LLMs) and the importance\nof democratizing their training, we propose GWTF, the first crash tolerant\npractical decentralized training framework for LLMs. Differently from existing\ndistributed and federated training frameworks, GWTF enables the efficient\ncollaborative training of a LLM on heterogeneous clients that volunteer their\nresources. In addition, GWTF addresses node churn, i.e., clients joining or\nleaving the system at any time, and network instabilities, i.e., network links\nbecoming unstable or unreliable. The core of GWTF is a novel decentralized flow\nalgorithm that finds the most effective routing that maximizes the number of\nmicrobatches trained with the lowest possible delay. We extensively evaluate\nGWTF on GPT-like and LLaMa-like models and compare it against the prior art.\nOur results indicate that GWTF reduces the training time by up to 45% in\nrealistic and challenging scenarios that involve heterogeneous client nodes\ndistributed over 10 different geographic locations with a high node churn rate."
                },
                "authors": [
                    {
                        "name": "Nikolay Blagoev"
                    },
                    {
                        "name": "Bart Cox"
                    },
                    {
                        "name": "Jrmie Decouchant"
                    },
                    {
                        "name": "Lydia Y. Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lydia Y. Chen"
                },
                "author": "Lydia Y. Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.17166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.17166v2",
                "updated": "2025-09-25T14:24:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    24,
                    56,
                    3,
                    268,
                    0
                ],
                "published": "2025-02-24T14:02:00Z",
                "published_parsed": [
                    2025,
                    2,
                    24,
                    14,
                    2,
                    0,
                    0,
                    55,
                    0
                ],
                "title": "JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for\n  Legal Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JUREX-4E: Juridical Expert-Annotated Four-Element Knowledge Base for\n  Legal Reasoning"
                },
                "summary": "In recent years, Large Language Models (LLMs) have been widely applied to\nlegal tasks. To enhance their understanding of legal texts and improve\nreasoning accuracy, a promising approach is to incorporate legal theories. One\nof the most widely adopted theories is the Four-Element Theory (FET), which\ndefines the crime constitution through four elements: Subject, Object,\nSubjective Aspect, and Objective Aspect. While recent work has explored\nprompting LLMs to follow FET, our evaluation demonstrates that LLM-generated\nfour-elements are often incomplete and less representative, limiting their\neffectiveness in legal reasoning. To address these issues, we present JUREX-4E,\nan expert-annotated four-element knowledge base covering 155 criminal charges.\nThe annotations follow a progressive hierarchical framework grounded in legal\nsource validity and incorporate diverse interpretive methods to ensure\nprecision and authority. We evaluate JUREX-4E on the Similar Charge\nDisambiguation task and apply it to Legal Case Retrieval. Experimental results\nvalidate the high quality of JUREX-4E and its substantial impact on downstream\nlegal tasks, underscoring its potential for advancing legal AI applications.\nThe dataset and code are available at: https://github.com/THUlawtech/JUREX",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have been widely applied to\nlegal tasks. To enhance their understanding of legal texts and improve\nreasoning accuracy, a promising approach is to incorporate legal theories. One\nof the most widely adopted theories is the Four-Element Theory (FET), which\ndefines the crime constitution through four elements: Subject, Object,\nSubjective Aspect, and Objective Aspect. While recent work has explored\nprompting LLMs to follow FET, our evaluation demonstrates that LLM-generated\nfour-elements are often incomplete and less representative, limiting their\neffectiveness in legal reasoning. To address these issues, we present JUREX-4E,\nan expert-annotated four-element knowledge base covering 155 criminal charges.\nThe annotations follow a progressive hierarchical framework grounded in legal\nsource validity and incorporate diverse interpretive methods to ensure\nprecision and authority. We evaluate JUREX-4E on the Similar Charge\nDisambiguation task and apply it to Legal Case Retrieval. Experimental results\nvalidate the high quality of JUREX-4E and its substantial impact on downstream\nlegal tasks, underscoring its potential for advancing legal AI applications.\nThe dataset and code are available at: https://github.com/THUlawtech/JUREX"
                },
                "authors": [
                    {
                        "name": "Huanghai Liu"
                    },
                    {
                        "name": "Quzhe Huang"
                    },
                    {
                        "name": "Qingjing Chen"
                    },
                    {
                        "name": "Yiran Hu"
                    },
                    {
                        "name": "Jiayu Ma"
                    },
                    {
                        "name": "Yun Liu"
                    },
                    {
                        "name": "Weixing Shen"
                    },
                    {
                        "name": "Yansong Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Feng"
                },
                "author": "Yansong Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.17166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.17166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21212v1",
                "updated": "2025-09-25T14:21:44Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    21,
                    44,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:21:44Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    21,
                    44,
                    3,
                    268,
                    0
                ],
                "title": "SGMem: Sentence Graph Memory for Long-Term Conversational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGMem: Sentence Graph Memory for Long-Term Conversational Agents"
                },
                "summary": "Long-term conversational agents require effective memory management to handle\ndialogue histories that exceed the context window of large language models\n(LLMs). Existing methods based on fact extraction or summarization reduce\nredundancy but struggle to organize and retrieve relevant information across\ndifferent granularities of dialogue and generated memory. We introduce SGMem\n(Sentence Graph Memory), which represents dialogue as sentence-level graphs\nwithin chunked units, capturing associations across turn-, round-, and\nsession-level contexts. By combining retrieved raw dialogue with generated\nmemory such as summaries, facts and insights, SGMem supplies LLMs with coherent\nand relevant context for response generation. Experiments on LongMemEval and\nLoCoMo show that SGMem consistently improves accuracy and outperforms strong\nbaselines in long-term conversational question answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-term conversational agents require effective memory management to handle\ndialogue histories that exceed the context window of large language models\n(LLMs). Existing methods based on fact extraction or summarization reduce\nredundancy but struggle to organize and retrieve relevant information across\ndifferent granularities of dialogue and generated memory. We introduce SGMem\n(Sentence Graph Memory), which represents dialogue as sentence-level graphs\nwithin chunked units, capturing associations across turn-, round-, and\nsession-level contexts. By combining retrieved raw dialogue with generated\nmemory such as summaries, facts and insights, SGMem supplies LLMs with coherent\nand relevant context for response generation. Experiments on LongMemEval and\nLoCoMo show that SGMem consistently improves accuracy and outperforms strong\nbaselines in long-term conversational question answering."
                },
                "authors": [
                    {
                        "name": "Yaxiong Wu"
                    },
                    {
                        "name": "Yongyue Zhang"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Yong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Liu"
                },
                "author": "Yong Liu",
                "arxiv_comment": "19 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22337v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22337v2",
                "updated": "2025-09-25T14:21:40Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    21,
                    40,
                    3,
                    268,
                    0
                ],
                "published": "2025-07-30T02:44:20Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    2,
                    44,
                    20,
                    2,
                    211,
                    0
                ],
                "title": "A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers"
                },
                "summary": "Understanding and solving complex reasoning tasks is vital for addressing the\ninformation needs of a user. Although dense neural models learn contextualised\nembeddings, they still underperform on queries containing negation. To\nunderstand this phenomenon, we study negation in both traditional neural\ninformation retrieval and LLM-based models. We (1) introduce a taxonomy of\nnegation that derives from philosophical, linguistic, and logical definitions;\n(2) generate two benchmark datasets that can be used to evaluate the\nperformance of neural information retrieval models and to fine-tune models for\na more robust performance on negation; and (3) propose a logic-based\nclassification mechanism that can be used to analyze the performance of\nretrieval models on existing datasets. Our taxonomy produces a balanced data\ndistribution over negation types, providing a better training setup that leads\nto faster convergence on the NevIR dataset. Moreover, we propose a\nclassification schema that reveals the coverage of negation types in existing\ndatasets, offering insights into the factors that might affect the\ngeneralization of fine-tuned models on negation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and solving complex reasoning tasks is vital for addressing the\ninformation needs of a user. Although dense neural models learn contextualised\nembeddings, they still underperform on queries containing negation. To\nunderstand this phenomenon, we study negation in both traditional neural\ninformation retrieval and LLM-based models. We (1) introduce a taxonomy of\nnegation that derives from philosophical, linguistic, and logical definitions;\n(2) generate two benchmark datasets that can be used to evaluate the\nperformance of neural information retrieval models and to fine-tune models for\na more robust performance on negation; and (3) propose a logic-based\nclassification mechanism that can be used to analyze the performance of\nretrieval models on existing datasets. Our taxonomy produces a balanced data\ndistribution over negation types, providing a better training setup that leads\nto faster convergence on the NevIR dataset. Moreover, we propose a\nclassification schema that reveals the coverage of negation types in existing\ndatasets, offering insights into the factors that might affect the\ngeneralization of fine-tuned models on negation."
                },
                "authors": [
                    {
                        "name": "Roxana Petcu"
                    },
                    {
                        "name": "Samarth Bhargav"
                    },
                    {
                        "name": "Maarten de Rijke"
                    },
                    {
                        "name": "Evangelos Kanoulas"
                    }
                ],
                "author_detail": {
                    "name": "Evangelos Kanoulas"
                },
                "author": "Evangelos Kanoulas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22337v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22337v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21208v1",
                "updated": "2025-09-25T14:19:51Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    19,
                    51,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:19:51Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    19,
                    51,
                    3,
                    268,
                    0
                ],
                "title": "CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A\n  Fine-grained Corpus and Reasoning Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLaw: Benchmarking Chinese Legal Knowledge in Large Language Models - A\n  Fine-grained Corpus and Reasoning Analysis"
                },
                "summary": "Large Language Models (LLMs) are increasingly tasked with analyzing legal\ntexts and citing relevant statutes, yet their reliability is often compromised\nby general pre-training that ingests legal texts without specialized focus,\nobscuring the true depth of their legal knowledge. This paper introduces CLaw,\na novel benchmark specifically engineered to meticulously evaluate LLMs on\nChinese legal knowledge and its application in reasoning. CLaw comprises two\nkey components: (1) a comprehensive, fine-grained corpus of all 306 Chinese\nnational statutes, segmented to the subparagraph level and incorporating\nprecise historical revision timesteps for rigorous recall evaluation (64,849\nentries), and (2) a challenging set of 254 case-based reasoning instances\nderived from China Supreme Court curated materials to assess the practical\napplication of legal knowledge. Our empirical evaluation reveals that most\ncontemporary LLMs significantly struggle to faithfully reproduce legal\nprovisions. As accurate retrieval and citation of legal provisions form the\nbasis of legal reasoning, this deficiency critically undermines the reliability\nof their responses. We contend that achieving trustworthy legal reasoning in\nLLMs requires a robust synergy of accurate knowledge retrieval--potentially\nenhanced through supervised fine-tuning (SFT) or retrieval-augmented generation\n(RAG)--and strong general reasoning capabilities. This work provides an\nessential benchmark and critical insights for advancing domain-specific LLM\nreasoning, particularly within the complex legal sphere.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly tasked with analyzing legal\ntexts and citing relevant statutes, yet their reliability is often compromised\nby general pre-training that ingests legal texts without specialized focus,\nobscuring the true depth of their legal knowledge. This paper introduces CLaw,\na novel benchmark specifically engineered to meticulously evaluate LLMs on\nChinese legal knowledge and its application in reasoning. CLaw comprises two\nkey components: (1) a comprehensive, fine-grained corpus of all 306 Chinese\nnational statutes, segmented to the subparagraph level and incorporating\nprecise historical revision timesteps for rigorous recall evaluation (64,849\nentries), and (2) a challenging set of 254 case-based reasoning instances\nderived from China Supreme Court curated materials to assess the practical\napplication of legal knowledge. Our empirical evaluation reveals that most\ncontemporary LLMs significantly struggle to faithfully reproduce legal\nprovisions. As accurate retrieval and citation of legal provisions form the\nbasis of legal reasoning, this deficiency critically undermines the reliability\nof their responses. We contend that achieving trustworthy legal reasoning in\nLLMs requires a robust synergy of accurate knowledge retrieval--potentially\nenhanced through supervised fine-tuning (SFT) or retrieval-augmented generation\n(RAG)--and strong general reasoning capabilities. This work provides an\nessential benchmark and critical insights for advancing domain-specific LLM\nreasoning, particularly within the complex legal sphere."
                },
                "authors": [
                    {
                        "name": "Xinzhe Xu"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Chen Chen"
                    }
                ],
                "author_detail": {
                    "name": "Chen Chen"
                },
                "author": "Chen Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18847v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18847v2",
                "updated": "2025-09-25T14:17:18Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    17,
                    18,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-23T09:35:49Z",
                "published_parsed": [
                    2025,
                    9,
                    23,
                    9,
                    35,
                    49,
                    1,
                    266,
                    0
                ],
                "title": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured\n  Reflection for Reliable Tool Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured\n  Reflection for Reliable Tool Interactions"
                },
                "summary": "Tool-augmented large language models (LLMs) are usually trained with\nsupervised imitation or coarse-grained reinforcement learning that optimizes\nsingle tool calls. Current self-reflection practices rely on heuristic prompts\nor one-way reasoning: the model is urged to 'think more' instead of learning\nerror diagnosis and repair. This is fragile in multi-turn interactions; after a\nfailure the model often repeats the same mistake. We propose structured\nreflection, which turns the path from error to repair into an explicit,\ncontrollable, and trainable action. The agent produces a short yet precise\nreflection: it diagnoses the failure using evidence from the previous step and\nthen proposes a correct, executable follow-up call. For training we combine\nDAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing\nthe stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce\nTool-Reflection-Bench, a lightweight benchmark that programmatically checks\nstructural validity, executability, parameter correctness, and result\nconsistency. Tasks are built as mini trajectories of erroneous call,\nreflection, and corrected call, with disjoint train and test splits.\nExperiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn\ntool-call success and error recovery, and a reduction of redundant calls. These\nresults indicate that making reflection explicit and optimizing it directly\nimproves the reliability of tool interaction and offers a reproducible path for\nagents to learn from failure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tool-augmented large language models (LLMs) are usually trained with\nsupervised imitation or coarse-grained reinforcement learning that optimizes\nsingle tool calls. Current self-reflection practices rely on heuristic prompts\nor one-way reasoning: the model is urged to 'think more' instead of learning\nerror diagnosis and repair. This is fragile in multi-turn interactions; after a\nfailure the model often repeats the same mistake. We propose structured\nreflection, which turns the path from error to repair into an explicit,\ncontrollable, and trainable action. The agent produces a short yet precise\nreflection: it diagnoses the failure using evidence from the previous step and\nthen proposes a correct, executable follow-up call. For training we combine\nDAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing\nthe stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce\nTool-Reflection-Bench, a lightweight benchmark that programmatically checks\nstructural validity, executability, parameter correctness, and result\nconsistency. Tasks are built as mini trajectories of erroneous call,\nreflection, and corrected call, with disjoint train and test splits.\nExperiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn\ntool-call success and error recovery, and a reduction of redundant calls. These\nresults indicate that making reflection explicit and optimizing it directly\nimproves the reliability of tool interaction and offers a reproducible path for\nagents to learn from failure."
                },
                "authors": [
                    {
                        "name": "Junhao Su"
                    },
                    {
                        "name": "Yuanliang Wan"
                    },
                    {
                        "name": "Junwei Yang"
                    },
                    {
                        "name": "Hengyu Shi"
                    },
                    {
                        "name": "Tianyang Han"
                    },
                    {
                        "name": "Junfeng Luo"
                    },
                    {
                        "name": "Yurui Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Yurui Qiu"
                },
                "author": "Yurui Qiu",
                "arxiv_comment": "27pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18847v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18847v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21207v1",
                "updated": "2025-09-25T14:15:43Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    15,
                    43,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:15:43Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    15,
                    43,
                    3,
                    268,
                    0
                ],
                "title": "From Physics to Machine Learning and Back: Part II - Learning and\n  Observational Bias in PHM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Physics to Machine Learning and Back: Part II - Learning and\n  Observational Bias in PHM"
                },
                "summary": "Prognostics and Health Management ensures the reliability, safety, and\nefficiency of complex engineered systems by enabling fault detection,\nanticipating equipment failures, and optimizing maintenance activities\nthroughout an asset lifecycle. However, real-world PHM presents persistent\nchallenges: sensor data is often noisy or incomplete, available labels are\nlimited, and degradation behaviors and system interdependencies can be highly\ncomplex and nonlinear. Physics-informed machine learning has emerged as a\npromising approach to address these limitations by embedding physical knowledge\ninto data-driven models. This review examines how incorporating learning and\nobservational biases through physics-informed modeling and data strategies can\nguide models toward physically consistent and reliable predictions. Learning\nbiases embed physical constraints into model training through physics-informed\nloss functions and governing equations, or by incorporating properties like\nmonotonicity. Observational biases influence data selection and synthesis to\nensure models capture realistic system behavior through virtual sensing for\nestimating unmeasured states, physics-based simulation for data augmentation,\nand multi-sensor fusion strategies. The review then examines how these\napproaches enable the transition from passive prediction to active\ndecision-making through reinforcement learning, which allows agents to learn\nmaintenance policies that respect physical constraints while optimizing\noperational objectives. This closes the loop between model-based predictions,\nsimulation, and actual system operation, empowering adaptive decision-making.\nFinally, the review addresses the critical challenge of scaling PHM solutions\nfrom individual assets to fleet-wide deployment. Fast adaptation methods\nincluding meta-learning and few-shot learning are reviewed alongside domain\ngeneralization techniques ...",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prognostics and Health Management ensures the reliability, safety, and\nefficiency of complex engineered systems by enabling fault detection,\nanticipating equipment failures, and optimizing maintenance activities\nthroughout an asset lifecycle. However, real-world PHM presents persistent\nchallenges: sensor data is often noisy or incomplete, available labels are\nlimited, and degradation behaviors and system interdependencies can be highly\ncomplex and nonlinear. Physics-informed machine learning has emerged as a\npromising approach to address these limitations by embedding physical knowledge\ninto data-driven models. This review examines how incorporating learning and\nobservational biases through physics-informed modeling and data strategies can\nguide models toward physically consistent and reliable predictions. Learning\nbiases embed physical constraints into model training through physics-informed\nloss functions and governing equations, or by incorporating properties like\nmonotonicity. Observational biases influence data selection and synthesis to\nensure models capture realistic system behavior through virtual sensing for\nestimating unmeasured states, physics-based simulation for data augmentation,\nand multi-sensor fusion strategies. The review then examines how these\napproaches enable the transition from passive prediction to active\ndecision-making through reinforcement learning, which allows agents to learn\nmaintenance policies that respect physical constraints while optimizing\noperational objectives. This closes the loop between model-based predictions,\nsimulation, and actual system operation, empowering adaptive decision-making.\nFinally, the review addresses the critical challenge of scaling PHM solutions\nfrom individual assets to fleet-wide deployment. Fast adaptation methods\nincluding meta-learning and few-shot learning are reviewed alongside domain\ngeneralization techniques ..."
                },
                "authors": [
                    {
                        "name": "Olga Fink"
                    },
                    {
                        "name": "Ismail Nejjar"
                    },
                    {
                        "name": "Vinay Sharma"
                    },
                    {
                        "name": "Keivan Faghih Niresi"
                    },
                    {
                        "name": "Han Sun"
                    },
                    {
                        "name": "Hao Dong"
                    },
                    {
                        "name": "Chenghao Xu"
                    },
                    {
                        "name": "Amaury Wei"
                    },
                    {
                        "name": "Arthur Bizzi"
                    },
                    {
                        "name": "Raffael Theiler"
                    },
                    {
                        "name": "Yuan Tian"
                    },
                    {
                        "name": "Leandro Von Krannichfeldt"
                    },
                    {
                        "name": "Zhan Ma"
                    },
                    {
                        "name": "Sergei Garmaev"
                    },
                    {
                        "name": "Zepeng Zhang"
                    },
                    {
                        "name": "Mengjie Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Mengjie Zhao"
                },
                "author": "Mengjie Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01445v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01445v2",
                "updated": "2025-09-25T14:11:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    11,
                    59,
                    3,
                    268,
                    0
                ],
                "published": "2025-04-02T07:56:39Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    7,
                    56,
                    39,
                    2,
                    92,
                    0
                ],
                "title": "Compositional-ARC: Assessing Systematic Generalization in Abstract\n  Spatial Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compositional-ARC: Assessing Systematic Generalization in Abstract\n  Spatial Reasoning"
                },
                "summary": "Systematic generalization refers to the capacity to understand and generate\nnovel combinations from known components. Despite recent progress by large\nlanguage models (LLMs) across various domains, these models often fail to\nextend their knowledge to novel compositional scenarios, revealing notable\nlimitations in systematic generalization. There has been an ongoing debate\nabout whether neural networks possess the capacity for systematic\ngeneralization, with recent studies suggesting that meta-learning approaches\ndesigned for compositionality can significantly enhance this ability. However,\nthese insights have largely been confined to linguistic problems, leaving their\napplicability to other tasks an open question. In this study, we extend\nmeta-learning for compositionality to the domain of abstract spatial reasoning.\nTo this end, we introduce $\\textit{Compositional-ARC}-$a dataset designed to\nevaluate the capacity of models to systematically generalize from known\ngeometric transformations (e.g., translation, rotation) of abstract\ntwo-dimensional objects to novel combinations of these transformations (e.g.,\ntranslation+rotation). Our results show that a small transformer-based\nencoder-decoder model, trained via meta-learning for compositionality, can\nsystematically generalize to previously unseen transformation compositions.\nNotably, despite having only 5.7M parameters, this model significantly\noutperforms state-of-the-art LLMs$-$including o3-mini, GPT-4o, and Gemini 2.0\nFlash, which fail to exhibit similar systematic behavior$-$and performs on par\nwith the winning model of the ARC prize 2024, an 8B-parameter LLM trained via\ntest-time training. Our findings highlight the effectiveness of meta-learning\nin promoting systematicity beyond linguistic tasks, suggesting a promising\ndirection toward more robust and generalizable models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic generalization refers to the capacity to understand and generate\nnovel combinations from known components. Despite recent progress by large\nlanguage models (LLMs) across various domains, these models often fail to\nextend their knowledge to novel compositional scenarios, revealing notable\nlimitations in systematic generalization. There has been an ongoing debate\nabout whether neural networks possess the capacity for systematic\ngeneralization, with recent studies suggesting that meta-learning approaches\ndesigned for compositionality can significantly enhance this ability. However,\nthese insights have largely been confined to linguistic problems, leaving their\napplicability to other tasks an open question. In this study, we extend\nmeta-learning for compositionality to the domain of abstract spatial reasoning.\nTo this end, we introduce $\\textit{Compositional-ARC}-$a dataset designed to\nevaluate the capacity of models to systematically generalize from known\ngeometric transformations (e.g., translation, rotation) of abstract\ntwo-dimensional objects to novel combinations of these transformations (e.g.,\ntranslation+rotation). Our results show that a small transformer-based\nencoder-decoder model, trained via meta-learning for compositionality, can\nsystematically generalize to previously unseen transformation compositions.\nNotably, despite having only 5.7M parameters, this model significantly\noutperforms state-of-the-art LLMs$-$including o3-mini, GPT-4o, and Gemini 2.0\nFlash, which fail to exhibit similar systematic behavior$-$and performs on par\nwith the winning model of the ARC prize 2024, an 8B-parameter LLM trained via\ntest-time training. Our findings highlight the effectiveness of meta-learning\nin promoting systematicity beyond linguistic tasks, suggesting a promising\ndirection toward more robust and generalizable models."
                },
                "authors": [
                    {
                        "name": "Philipp Mondorf"
                    },
                    {
                        "name": "Shijia Zhou"
                    },
                    {
                        "name": "Monica Riedler"
                    },
                    {
                        "name": "Barbara Plank"
                    }
                ],
                "author_detail": {
                    "name": "Barbara Plank"
                },
                "author": "Barbara Plank",
                "arxiv_comment": "29 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01445v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01445v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21199v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21199v1",
                "updated": "2025-09-25T14:11:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    11,
                    57,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:11:57Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    11,
                    57,
                    3,
                    268,
                    0
                ],
                "title": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in\n  Multi-Hop QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Fano-Style Accuracy Upper Bound for LLM Single-Pass Reasoning in\n  Multi-Hop QA"
                },
                "summary": "Multi-Hop Question Answering (MHQA) requires integrating dispersed,\ninterdependent evidence through sequential reasoning under noise. This task is\nchallenging for LLMs as they have a finite per-pass output capacity, beyond\nwhich the integration of task-relevant evidence proves unreliable.\nConsequently, the single-pass reasoning paradigm is inherently vulnerable to\nthis capacity overflow. To formalize this bottleneck, our analysis establishes\na Fano-style accuracy upper bound, defining a theoretical performance ceiling\nfor single-pass LLMs. This bound reveals that accuracy inevitably collapses\nonce task complexity exceeds model capacity, providing general principles for\ncapacity-aware representation and structuring of MHQA in LLMs. Building on\nthese principles, we introduce a proof-of-concept multi-call framework for\nMHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware\ntask decomposition with active pruning of prior reasoning traces, keeping the\ninformation load within the single-pass limit. It further achieves robustness\nby a dependency-explicit workflow that enables precise control over the\nreasoning path. We construct a stringent and noise-rich benchmark to validate\nour theory and framework. Experimental results show that model behavior aligns\nwith our predicted capacity curves while InfoQA achieves consistent performance\nimprovements. We hope our work inspires more LLM multi-step reasoning methods:\n\\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Hop Question Answering (MHQA) requires integrating dispersed,\ninterdependent evidence through sequential reasoning under noise. This task is\nchallenging for LLMs as they have a finite per-pass output capacity, beyond\nwhich the integration of task-relevant evidence proves unreliable.\nConsequently, the single-pass reasoning paradigm is inherently vulnerable to\nthis capacity overflow. To formalize this bottleneck, our analysis establishes\na Fano-style accuracy upper bound, defining a theoretical performance ceiling\nfor single-pass LLMs. This bound reveals that accuracy inevitably collapses\nonce task complexity exceeds model capacity, providing general principles for\ncapacity-aware representation and structuring of MHQA in LLMs. Building on\nthese principles, we introduce a proof-of-concept multi-call framework for\nMHQA, InfoQA. It ensures high per-step accuracy by combining capacity-aware\ntask decomposition with active pruning of prior reasoning traces, keeping the\ninformation load within the single-pass limit. It further achieves robustness\nby a dependency-explicit workflow that enables precise control over the\nreasoning path. We construct a stringent and noise-rich benchmark to validate\nour theory and framework. Experimental results show that model behavior aligns\nwith our predicted capacity curves while InfoQA achieves consistent performance\nimprovements. We hope our work inspires more LLM multi-step reasoning methods:\n\\faGithub \\href{https://github.com/KaiyangWan/InfoQA}{InfoQA}."
                },
                "authors": [
                    {
                        "name": "Kaiyang Wan"
                    },
                    {
                        "name": "Lang Gao"
                    },
                    {
                        "name": "Honglin Mu"
                    },
                    {
                        "name": "Preslav Nakov"
                    },
                    {
                        "name": "Yuxia Wang"
                    },
                    {
                        "name": "Xiuying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiuying Chen"
                },
                "author": "Xiuying Chen",
                "arxiv_comment": "21 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21199v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21199v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18672v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18672v2",
                "updated": "2025-09-25T14:09:33Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    9,
                    33,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-26T04:31:28Z",
                "published_parsed": [
                    2025,
                    8,
                    26,
                    4,
                    31,
                    28,
                    1,
                    238,
                    0
                ],
                "title": "Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning\n  Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning\n  Tasks"
                },
                "summary": "Empirical scaling laws have driven the evolution of large language models\n(LLMs), yet their coefficients shift whenever the model architecture or data\npipeline changes. Mixture-of-Experts (MoE) models, now standard in\nstate-of-the-art systems, introduce a new sparsity dimension that current\ndense-model frontiers overlook. We investigate how MoE sparsity influences two\ndistinct capability regimes: memorization skills and reasoning skills. By\ntraining MoE families that vary total parameters, active parameters, and\ntop-$k$ routing under fixed compute budgets, we disentangle pre-training loss\nfrom downstream accuracy. Our results reveal two principles. First, Active\nFLOPs: models with identical training loss but greater active compute achieve\nhigher reasoning accuracy. Second, Total tokens per parameter (TPP):\nmemorization tasks improve with more parameters, while reasoning tasks benefit\nfrom optimal TPP, indicating that reasoning is data-hungry. Neither\nreinforcement learning post-training (GRPO) nor increased test-time compute\nalters these trends. We therefore argue that optimal MoE sparsity must be\ndetermined jointly by active FLOPs and TPP, revising the classical picture of\ncompute-optimal scaling. Our model checkpoints, code and logs are open-source\nat https://github.com/rioyokotalab/optimal-sparsity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical scaling laws have driven the evolution of large language models\n(LLMs), yet their coefficients shift whenever the model architecture or data\npipeline changes. Mixture-of-Experts (MoE) models, now standard in\nstate-of-the-art systems, introduce a new sparsity dimension that current\ndense-model frontiers overlook. We investigate how MoE sparsity influences two\ndistinct capability regimes: memorization skills and reasoning skills. By\ntraining MoE families that vary total parameters, active parameters, and\ntop-$k$ routing under fixed compute budgets, we disentangle pre-training loss\nfrom downstream accuracy. Our results reveal two principles. First, Active\nFLOPs: models with identical training loss but greater active compute achieve\nhigher reasoning accuracy. Second, Total tokens per parameter (TPP):\nmemorization tasks improve with more parameters, while reasoning tasks benefit\nfrom optimal TPP, indicating that reasoning is data-hungry. Neither\nreinforcement learning post-training (GRPO) nor increased test-time compute\nalters these trends. We therefore argue that optimal MoE sparsity must be\ndetermined jointly by active FLOPs and TPP, revising the classical picture of\ncompute-optimal scaling. Our model checkpoints, code and logs are open-source\nat https://github.com/rioyokotalab/optimal-sparsity."
                },
                "authors": [
                    {
                        "name": "Taishi Nakamura"
                    },
                    {
                        "name": "Satoki Ishikawa"
                    },
                    {
                        "name": "Masaki Kawamura"
                    },
                    {
                        "name": "Takumi Okamoto"
                    },
                    {
                        "name": "Daisuke Nohara"
                    },
                    {
                        "name": "Jun Suzuki"
                    },
                    {
                        "name": "Rio Yokota"
                    }
                ],
                "author_detail": {
                    "name": "Rio Yokota"
                },
                "author": "Rio Yokota",
                "arxiv_comment": "Presented at the Second AI for Math Workshop at ICML",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.18672v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18672v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.12462v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.12462v2",
                "updated": "2025-09-25T14:09:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    9,
                    15,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-18T15:34:45Z",
                "published_parsed": [
                    2025,
                    5,
                    18,
                    15,
                    34,
                    45,
                    6,
                    138,
                    0
                ],
                "title": "Provably Sample-Efficient Robust Reinforcement Learning with Average\n  Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provably Sample-Efficient Robust Reinforcement Learning with Average\n  Reward"
                },
                "summary": "Robust reinforcement learning (RL) under the average-reward criterion is\nessential for long-term decision-making, particularly when the environment may\ndiffer from its specification. However, a significant gap exists in\nunderstanding the finite-sample complexity of these methods, as most existing\nwork provides only asymptotic guarantees. This limitation hinders their\nprincipled understanding and practical deployment, especially in data-limited\nscenarios. We close this gap by proposing \\textbf{Robust Halpern Iteration\n(RHI)}, a new algorithm designed for robust Markov Decision Processes (MDPs)\nwith transition uncertainty characterized by $\\ell_p$-norm and contamination\nmodels. Our approach offers three key advantages over previous methods: (1).\nWeaker Structural Assumptions: RHI only requires the underlying robust MDP to\nbe communicating, a less restrictive condition than the commonly assumed\nergodicity or irreducibility; (2). No Prior Knowledge: Our algorithm operates\nwithout requiring any prior knowledge of the robust MDP; (3). State-of-the-Art\nSample Complexity: To learn an $\\epsilon$-optimal robust policy, RHI achieves a\nsample complexity of $\\tilde{\\mathcal O}\\left(\\frac{SA\\mathcal\nH^{2}}{\\epsilon^{2}}\\right)$, where $S$ and $A$ denote the numbers of states\nand actions, and $\\mathcal H$ is the robust optimal bias span. This result\nrepresents the tightest known bound. Our work hence provides essential\ntheoretical understanding of sample efficiency of robust average reward RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust reinforcement learning (RL) under the average-reward criterion is\nessential for long-term decision-making, particularly when the environment may\ndiffer from its specification. However, a significant gap exists in\nunderstanding the finite-sample complexity of these methods, as most existing\nwork provides only asymptotic guarantees. This limitation hinders their\nprincipled understanding and practical deployment, especially in data-limited\nscenarios. We close this gap by proposing \\textbf{Robust Halpern Iteration\n(RHI)}, a new algorithm designed for robust Markov Decision Processes (MDPs)\nwith transition uncertainty characterized by $\\ell_p$-norm and contamination\nmodels. Our approach offers three key advantages over previous methods: (1).\nWeaker Structural Assumptions: RHI only requires the underlying robust MDP to\nbe communicating, a less restrictive condition than the commonly assumed\nergodicity or irreducibility; (2). No Prior Knowledge: Our algorithm operates\nwithout requiring any prior knowledge of the robust MDP; (3). State-of-the-Art\nSample Complexity: To learn an $\\epsilon$-optimal robust policy, RHI achieves a\nsample complexity of $\\tilde{\\mathcal O}\\left(\\frac{SA\\mathcal\nH^{2}}{\\epsilon^{2}}\\right)$, where $S$ and $A$ denote the numbers of states\nand actions, and $\\mathcal H$ is the robust optimal bias span. This result\nrepresents the tightest known bound. Our work hence provides essential\ntheoretical understanding of sample efficiency of robust average reward RL."
                },
                "authors": [
                    {
                        "name": "Zachary Roch"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "George Atia"
                    },
                    {
                        "name": "Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Wang"
                },
                "author": "Yue Wang",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.12462v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.12462v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21193v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21193v1",
                "updated": "2025-09-25T14:05:55Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    5,
                    55,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:05:55Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    5,
                    55,
                    3,
                    268,
                    0
                ],
                "title": "Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for\n  Scientific Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eigen-1: Adaptive Multi-Agent Refinement with Monitor-Based RAG for\n  Scientific Reasoning"
                },
                "summary": "Large language models (LLMs) have recently shown strong progress on\nscientific reasoning, yet two major bottlenecks remain. First, explicit\nretrieval fragments reasoning, imposing a hidden \"tool tax\" of extra tokens and\nsteps. Second, multi-agent pipelines often dilute strong solutions by averaging\nacross all candidates. We address these challenges with a unified framework\nthat combines implicit retrieval and structured collaboration. At its\nfoundation, a Monitor-based retrieval module operates at the token level,\nintegrating external knowledge with minimal disruption to reasoning. On top of\nthis substrate, Hierarchical Solution Refinement (HSR) iteratively designates\neach candidate as an anchor to be repaired by its peers, while Quality-Aware\nIterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's\nLast Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the\nhighest reported to date, surpassing the strongest agent baseline by 13.4\npoints and leading frontier LLMs by up to 18.1 points, while simultaneously\nreducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA\nand TRQA confirm robustness across domains. Error analysis shows that reasoning\nfailures and knowledge gaps co-occur in over 85\\% of cases, while diversity\nanalysis reveals a clear dichotomy: retrieval tasks benefit from solution\nvariety, whereas reasoning tasks favor consensus. Together, these findings\ndemonstrate how implicit augmentation and structured refinement overcome the\ninefficiencies of explicit tool use and uniform aggregation. Code is available\nat: https://github.com/tangxiangru/Eigen-1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently shown strong progress on\nscientific reasoning, yet two major bottlenecks remain. First, explicit\nretrieval fragments reasoning, imposing a hidden \"tool tax\" of extra tokens and\nsteps. Second, multi-agent pipelines often dilute strong solutions by averaging\nacross all candidates. We address these challenges with a unified framework\nthat combines implicit retrieval and structured collaboration. At its\nfoundation, a Monitor-based retrieval module operates at the token level,\nintegrating external knowledge with minimal disruption to reasoning. On top of\nthis substrate, Hierarchical Solution Refinement (HSR) iteratively designates\neach candidate as an anchor to be repaired by its peers, while Quality-Aware\nIterative Reasoning (QAIR) adapts refinement to solution quality. On Humanity's\nLast Exam (HLE) Bio/Chem Gold, our framework achieves 48.3\\% accuracy -- the\nhighest reported to date, surpassing the strongest agent baseline by 13.4\npoints and leading frontier LLMs by up to 18.1 points, while simultaneously\nreducing token usage by 53.5\\% and agent steps by 43.7\\%. Results on SuperGPQA\nand TRQA confirm robustness across domains. Error analysis shows that reasoning\nfailures and knowledge gaps co-occur in over 85\\% of cases, while diversity\nanalysis reveals a clear dichotomy: retrieval tasks benefit from solution\nvariety, whereas reasoning tasks favor consensus. Together, these findings\ndemonstrate how implicit augmentation and structured refinement overcome the\ninefficiencies of explicit tool use and uniform aggregation. Code is available\nat: https://github.com/tangxiangru/Eigen-1."
                },
                "authors": [
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Wanghan Xu"
                    },
                    {
                        "name": "Yujie Wang"
                    },
                    {
                        "name": "Zijie Guo"
                    },
                    {
                        "name": "Daniel Shao"
                    },
                    {
                        "name": "Jiapeng Chen"
                    },
                    {
                        "name": "Cixuan Zhang"
                    },
                    {
                        "name": "Ziyi Wang"
                    },
                    {
                        "name": "Lixin Zhang"
                    },
                    {
                        "name": "Guancheng Wan"
                    },
                    {
                        "name": "Wenlong Zhang"
                    },
                    {
                        "name": "Lei Bai"
                    },
                    {
                        "name": "Zhenfei Yin"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Di Jin"
                    }
                ],
                "author_detail": {
                    "name": "Di Jin"
                },
                "author": "Di Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21193v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21193v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21192v1",
                "updated": "2025-09-25T14:05:47Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    5,
                    47,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T14:05:47Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    14,
                    5,
                    47,
                    3,
                    268,
                    0
                ],
                "title": "GEP: A GCG-Based method for extracting personally identifiable\n  information from chatbots built on small language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEP: A GCG-Based method for extracting personally identifiable\n  information from chatbots built on small language models"
                },
                "summary": "Small language models (SLMs) become unprecedentedly appealing due to their\napproximately equivalent performance compared to large language models (LLMs)\nin certain fields with less energy and time consumption during training and\ninference. However, the personally identifiable information (PII) leakage of\nSLMs for downstream tasks has yet to be explored. In this study, we investigate\nthe PII leakage of the chatbot based on SLM. We first finetune a new chatbot,\ni.e., ChatBioGPT based on the backbone of BioGPT using medical datasets Alpaca\nand HealthCareMagic. It shows a matchable performance in BERTscore compared\nwith previous studies of ChatDoctor and ChatGPT. Based on this model, we prove\nthat the previous template-based PII attacking methods cannot effectively\nextract the PII in the dataset for leakage detection under the SLM condition.\nWe then propose GEP, which is a greedy coordinate gradient-based (GCG) method\nspecifically designed for PII extraction. We conduct experimental studies of\nGEP and the results show an increment of up to 60$\\times$ more leakage compared\nwith the previous template-based methods. We further expand the capability of\nGEP in the case of a more complicated and realistic situation by conducting\nfree-style insertion where the inserted PII in the dataset is in the form of\nvarious syntactic expressions instead of fixed templates, and GEP is still able\nto reveal a PII leakage rate of up to 4.53%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small language models (SLMs) become unprecedentedly appealing due to their\napproximately equivalent performance compared to large language models (LLMs)\nin certain fields with less energy and time consumption during training and\ninference. However, the personally identifiable information (PII) leakage of\nSLMs for downstream tasks has yet to be explored. In this study, we investigate\nthe PII leakage of the chatbot based on SLM. We first finetune a new chatbot,\ni.e., ChatBioGPT based on the backbone of BioGPT using medical datasets Alpaca\nand HealthCareMagic. It shows a matchable performance in BERTscore compared\nwith previous studies of ChatDoctor and ChatGPT. Based on this model, we prove\nthat the previous template-based PII attacking methods cannot effectively\nextract the PII in the dataset for leakage detection under the SLM condition.\nWe then propose GEP, which is a greedy coordinate gradient-based (GCG) method\nspecifically designed for PII extraction. We conduct experimental studies of\nGEP and the results show an increment of up to 60$\\times$ more leakage compared\nwith the previous template-based methods. We further expand the capability of\nGEP in the case of a more complicated and realistic situation by conducting\nfree-style insertion where the inserted PII in the dataset is in the form of\nvarious syntactic expressions instead of fixed templates, and GEP is still able\nto reveal a PII leakage rate of up to 4.53%."
                },
                "authors": [
                    {
                        "name": "Jieli Zhu"
                    },
                    {
                        "name": "Vi Ngoc-Nha Tran"
                    }
                ],
                "author_detail": {
                    "name": "Vi Ngoc-Nha Tran"
                },
                "author": "Vi Ngoc-Nha Tran",
                "arxiv_comment": "16 pages, 5 figures, 4 tables. Under review as a conference paper at\n  ICLR 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21175v1",
                "updated": "2025-09-25T13:56:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    56,
                    56,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:56:56Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    56,
                    56,
                    3,
                    268,
                    0
                ],
                "title": "Who's Laughing Now? An Overview of Computational Humour Generation and\n  Explanation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who's Laughing Now? An Overview of Computational Humour Generation and\n  Explanation"
                },
                "summary": "The creation and perception of humour is a fundamental human trait,\npositioning its computational understanding as one of the most challenging\ntasks in natural language processing (NLP). As an abstract, creative, and\nfrequently context-dependent construct, humour requires extensive reasoning to\nunderstand and create, making it a pertinent task for assessing the\ncommon-sense knowledge and reasoning abilities of modern large language models\n(LLMs). In this work, we survey the landscape of computational humour as it\npertains to the generative tasks of creation and explanation. We observe that,\ndespite the task of understanding humour bearing all the hallmarks of a\nfoundational NLP task, work on generating and explaining humour beyond puns\nremains sparse, while state-of-the-art models continue to fall short of human\ncapabilities. We bookend our literature survey by motivating the importance of\ncomputational humour processing as a subdiscipline of NLP and presenting an\nextensive discussion of future directions for research in the area that takes\ninto account the subjective and ethically ambiguous nature of humour.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The creation and perception of humour is a fundamental human trait,\npositioning its computational understanding as one of the most challenging\ntasks in natural language processing (NLP). As an abstract, creative, and\nfrequently context-dependent construct, humour requires extensive reasoning to\nunderstand and create, making it a pertinent task for assessing the\ncommon-sense knowledge and reasoning abilities of modern large language models\n(LLMs). In this work, we survey the landscape of computational humour as it\npertains to the generative tasks of creation and explanation. We observe that,\ndespite the task of understanding humour bearing all the hallmarks of a\nfoundational NLP task, work on generating and explaining humour beyond puns\nremains sparse, while state-of-the-art models continue to fall short of human\ncapabilities. We bookend our literature survey by motivating the importance of\ncomputational humour processing as a subdiscipline of NLP and presenting an\nextensive discussion of future directions for research in the area that takes\ninto account the subjective and ethically ambiguous nature of humour."
                },
                "authors": [
                    {
                        "name": "Tyler Loakman"
                    },
                    {
                        "name": "William Thorne"
                    },
                    {
                        "name": "Chenghua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Chenghua Lin"
                },
                "author": "Chenghua Lin",
                "arxiv_comment": "Accepted to INLG 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14128v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14128v2",
                "updated": "2025-09-25T13:56:16Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    56,
                    16,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-17T16:08:46Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    16,
                    8,
                    46,
                    2,
                    260,
                    0
                ],
                "title": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance\n  Models for Multilingual ASR and AST",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance\n  Models for Multilingual ASR and AST"
                },
                "summary": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report introduces Canary-1B-v2, a fast, robust multilingual model for\nAutomatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built\nwith a FastConformer encoder and Transformer decoder, it supports 25 languages\nprimarily European. The model was trained on 1.7M hours of total data samples,\nincluding Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce\nhallucinations for ASR and AST. We describe its two-stage pre-training and\nfine-tuning process with dynamic data balancing, as well as experiments with an\nnGPT encoder. Results show nGPT scales well with massive data, while\nFastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the\nNeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable\nsegment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2\noutperforms Whisper-large-v3 on English ASR while being 10x faster, and\ndelivers competitive multilingual ASR and AST performance against larger models\nlike Seamless-M4T-v2-large and LLM-based systems. We also release\nParakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the\nsame 25 languages with just 600M parameters."
                },
                "authors": [
                    {
                        "name": "Monica Sekoyan"
                    },
                    {
                        "name": "Nithin Rao Koluguri"
                    },
                    {
                        "name": "Nune Tadevosyan"
                    },
                    {
                        "name": "Piotr Zelasko"
                    },
                    {
                        "name": "Travis Bartley"
                    },
                    {
                        "name": "Nikolay Karpov"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Mini Version of it Submitted to ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14128v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14128v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21173v1",
                "updated": "2025-09-25T13:54:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    54,
                    34,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:54:34Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    54,
                    34,
                    3,
                    268,
                    0
                ],
                "title": "Can Less Precise Be More Reliable? A Systematic Evaluation of\n  Quantization's Impact on CLIP Beyond Accuracy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Less Precise Be More Reliable? A Systematic Evaluation of\n  Quantization's Impact on CLIP Beyond Accuracy"
                },
                "summary": "The powerful zero-shot generalization capabilities of vision-language models\n(VLMs) like CLIP have enabled new paradigms for safety-related tasks such as\nout-of-distribution (OOD) detection. However, additional aspects crucial for\nthe computationally efficient and reliable deployment of CLIP are still\noverlooked. In particular, the impact of quantization on CLIP's performance\nbeyond accuracy remains underexplored. This work presents a large-scale\nevaluation of quantization on CLIP models, assessing not only in-distribution\naccuracy but a comprehensive suite of reliability metrics and revealing\ncounterintuitive results driven by pre-training source. We demonstrate that\nquantization consistently improves calibration for typically underconfident\npre-trained models, while often degrading it for overconfident variants.\nIntriguingly, this degradation in calibration does not preclude gains in other\nreliability metrics; we find that OOD detection can still improve for these\nsame poorly calibrated models. Furthermore, we identify specific\nquantization-aware training (QAT) methods that yield simultaneous gains in\nzero-shot accuracy, calibration, and OOD robustness, challenging the view of a\nstrict efficiency-performance trade-off. These findings offer critical insights\nfor navigating the multi-objective problem of deploying efficient, reliable,\nand robust VLMs by utilizing quantization beyond its conventional role.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The powerful zero-shot generalization capabilities of vision-language models\n(VLMs) like CLIP have enabled new paradigms for safety-related tasks such as\nout-of-distribution (OOD) detection. However, additional aspects crucial for\nthe computationally efficient and reliable deployment of CLIP are still\noverlooked. In particular, the impact of quantization on CLIP's performance\nbeyond accuracy remains underexplored. This work presents a large-scale\nevaluation of quantization on CLIP models, assessing not only in-distribution\naccuracy but a comprehensive suite of reliability metrics and revealing\ncounterintuitive results driven by pre-training source. We demonstrate that\nquantization consistently improves calibration for typically underconfident\npre-trained models, while often degrading it for overconfident variants.\nIntriguingly, this degradation in calibration does not preclude gains in other\nreliability metrics; we find that OOD detection can still improve for these\nsame poorly calibrated models. Furthermore, we identify specific\nquantization-aware training (QAT) methods that yield simultaneous gains in\nzero-shot accuracy, calibration, and OOD robustness, challenging the view of a\nstrict efficiency-performance trade-off. These findings offer critical insights\nfor navigating the multi-objective problem of deploying efficient, reliable,\nand robust VLMs by utilizing quantization beyond its conventional role."
                },
                "authors": [
                    {
                        "name": "Aymen Bouguerra"
                    },
                    {
                        "name": "Daniel Montoya"
                    },
                    {
                        "name": "Alexandra Gomez-Villa"
                    },
                    {
                        "name": "Fabio Arnez"
                    },
                    {
                        "name": "Chokri Mraidha"
                    }
                ],
                "author_detail": {
                    "name": "Chokri Mraidha"
                },
                "author": "Chokri Mraidha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21170v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21170v1",
                "updated": "2025-09-25T13:51:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    51,
                    56,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:51:56Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    51,
                    56,
                    3,
                    268,
                    0
                ],
                "title": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A\n  Maximum Entropy Regulated Long Chain-of-Thought Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Tuning LLMs to Analyze Multiple Dimensions of Code Review: A\n  Maximum Entropy Regulated Long Chain-of-Thought Approach"
                },
                "summary": "Large Language Models (LLMs) have shown great potential in supporting\nautomated code review due to their impressive capabilities in context\nunderstanding and reasoning. However, these capabilities are still limited\ncompared to human-level cognition because they are heavily influenced by the\ntraining data. Recent research has demonstrated significantly improved\nperformance through fine-tuning LLMs with code review data. However, compared\nto human reviewers who often simultaneously analyze multiple dimensions of code\nreview to better identify issues, the full potential of these methods is\nhampered by the limited or vague information used to fine-tune the models. This\npaper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that\ntrains LLMs with an impressive reasoning ability to analyze multiple dimensions\nof code review by harnessing long COT techniques to provide rich structured\ninformation. To address context loss and reasoning logic loss issues that\nfrequently occur when LLMs process long COT prompts, we propose a solution that\ncombines the Maximum Entropy (ME) modeling principle with pre-defined reasoning\npathways in MelcotCR to enable more effective utilization of in-context\nknowledge within long COT prompts while strengthening the logical tightness of\nthe reasoning process. Empirical evaluations on our curated MelcotCR dataset\nand the public CodeReviewer dataset reveal that a low-parameter base model,\nsuch as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art\nmethods in terms of the accuracy of detecting and describing code issues, with\nits performance remarkably on par with that of the 671B DeepSeek-R1 model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown great potential in supporting\nautomated code review due to their impressive capabilities in context\nunderstanding and reasoning. However, these capabilities are still limited\ncompared to human-level cognition because they are heavily influenced by the\ntraining data. Recent research has demonstrated significantly improved\nperformance through fine-tuning LLMs with code review data. However, compared\nto human reviewers who often simultaneously analyze multiple dimensions of code\nreview to better identify issues, the full potential of these methods is\nhampered by the limited or vague information used to fine-tune the models. This\npaper contributes MelcotCR, a chain-of-thought (COT) fine-tuning approach that\ntrains LLMs with an impressive reasoning ability to analyze multiple dimensions\nof code review by harnessing long COT techniques to provide rich structured\ninformation. To address context loss and reasoning logic loss issues that\nfrequently occur when LLMs process long COT prompts, we propose a solution that\ncombines the Maximum Entropy (ME) modeling principle with pre-defined reasoning\npathways in MelcotCR to enable more effective utilization of in-context\nknowledge within long COT prompts while strengthening the logical tightness of\nthe reasoning process. Empirical evaluations on our curated MelcotCR dataset\nand the public CodeReviewer dataset reveal that a low-parameter base model,\nsuch as 14B Qwen2.5, fine-tuned with MelcotCR can surpass state-of-the-art\nmethods in terms of the accuracy of detecting and describing code issues, with\nits performance remarkably on par with that of the 671B DeepSeek-R1 model."
                },
                "authors": [
                    {
                        "name": "Yongda Yu"
                    },
                    {
                        "name": "Guohao Shi"
                    },
                    {
                        "name": "Xianwei Wu"
                    },
                    {
                        "name": "Haochuan He"
                    },
                    {
                        "name": "XueMing Gu"
                    },
                    {
                        "name": "Qianqian Zhao"
                    },
                    {
                        "name": "Kui Liu"
                    },
                    {
                        "name": "Qiushi Wang"
                    },
                    {
                        "name": "Zhao Tian"
                    },
                    {
                        "name": "Haifeng Shen"
                    },
                    {
                        "name": "Guoping Rong"
                    }
                ],
                "author_detail": {
                    "name": "Guoping Rong"
                },
                "author": "Guoping Rong",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21170v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21170v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.04495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.04495v2",
                "updated": "2025-09-25T13:51:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    51,
                    34,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-06T14:44:23Z",
                "published_parsed": [
                    2025,
                    8,
                    6,
                    14,
                    44,
                    23,
                    2,
                    218,
                    0
                ],
                "title": "Causal Reflection with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Reflection with Language Models"
                },
                "summary": "While LLMs exhibit impressive fluency and factual recall, they struggle with\nrobust causal reasoning, often relying on spurious correlations and brittle\npatterns. Similarly, traditional Reinforcement Learning agents also lack causal\nunderstanding, optimizing for rewards without modeling why actions lead to\noutcomes. We introduce Causal Reflection, a framework that explicitly models\ncausality as a dynamic function over state, action, time, and perturbation,\nenabling agents to reason about delayed and nonlinear effects. Additionally, we\ndefine a formal Reflect mechanism that identifies mismatches between predicted\nand observed outcomes and generates causal hypotheses to revise the agent's\ninternal model. In this architecture, LLMs serve not as black-box reasoners,\nbut as structured inference engines translating formal causal outputs into\nnatural language explanations and counterfactuals. Our framework lays the\ntheoretical groundwork for Causal Reflective agents that can adapt,\nself-correct, and communicate causal understanding in evolving environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While LLMs exhibit impressive fluency and factual recall, they struggle with\nrobust causal reasoning, often relying on spurious correlations and brittle\npatterns. Similarly, traditional Reinforcement Learning agents also lack causal\nunderstanding, optimizing for rewards without modeling why actions lead to\noutcomes. We introduce Causal Reflection, a framework that explicitly models\ncausality as a dynamic function over state, action, time, and perturbation,\nenabling agents to reason about delayed and nonlinear effects. Additionally, we\ndefine a formal Reflect mechanism that identifies mismatches between predicted\nand observed outcomes and generates causal hypotheses to revise the agent's\ninternal model. In this architecture, LLMs serve not as black-box reasoners,\nbut as structured inference engines translating formal causal outputs into\nnatural language explanations and counterfactuals. Our framework lays the\ntheoretical groundwork for Causal Reflective agents that can adapt,\nself-correct, and communicate causal understanding in evolving environments."
                },
                "authors": [
                    {
                        "name": "Abi Aryan"
                    },
                    {
                        "name": "Zac Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zac Liu"
                },
                "author": "Zac Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.04495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.04495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21164v1",
                "updated": "2025-09-25T13:50:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    50,
                    9,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:50:09Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    50,
                    9,
                    3,
                    268,
                    0
                ],
                "title": "Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just\n  What They Say",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Thoughts: Learning to Aggregate What Experts Think, Not Just\n  What They Say"
                },
                "summary": "Open-source Large Language Models (LLMs) increasingly specialize by domain\n(e.g., math, code, general reasoning), motivating systems that leverage\ncomplementary strengths across models. Prior multi-LLM approaches either (i)\nroute a query to one or a few experts and generate independently, (ii)\naggregate outputs from each model via costly multi-turn exchanges, or (iii)\nfuse weights into a single model-typically requiring architectural homogeneity.\nWe introduce Mixture of Thoughts (MoT), a simple method for latent-level\ncollaboration among heterogeneous experts under a global routing scheme. For\neach query, a lightweight router selects top-$K$ experts and designates a\nprimary expert; uniformly placed interaction layers project hidden states into\na shared latent space where the primary expert performs cross-attention over\nits active (selected) peers. Pre-trained experts remain frozen; only the router\nand the lightweight interaction layers are trained with a novel joint training\nobjective that improves both the expert selection and inter-expert\ncollaboration. Across five in-distribution (ID) and three out-of-distribution\n(OOD) benchmarks, MoT surpasses the current routing and aggregation-based\nstate-of-the-art, Avengers, by $+0.38\\%$ and $+2.92\\%$, respectively. Further,\nMoT significantly outperforms the best-performing single model. It achieves\nthis with single-pass inference, runtime comparable to routing baselines, and\nnone of the overheads of iterative aggregation. MoT offers a simple\nlatent-space mechanism for combining heterogeneous LLMs, a practical step\ntoward broader multi-LLM collaboration. Our code is publicly available at\nhttps://github.com/jacobfa/mot.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source Large Language Models (LLMs) increasingly specialize by domain\n(e.g., math, code, general reasoning), motivating systems that leverage\ncomplementary strengths across models. Prior multi-LLM approaches either (i)\nroute a query to one or a few experts and generate independently, (ii)\naggregate outputs from each model via costly multi-turn exchanges, or (iii)\nfuse weights into a single model-typically requiring architectural homogeneity.\nWe introduce Mixture of Thoughts (MoT), a simple method for latent-level\ncollaboration among heterogeneous experts under a global routing scheme. For\neach query, a lightweight router selects top-$K$ experts and designates a\nprimary expert; uniformly placed interaction layers project hidden states into\na shared latent space where the primary expert performs cross-attention over\nits active (selected) peers. Pre-trained experts remain frozen; only the router\nand the lightweight interaction layers are trained with a novel joint training\nobjective that improves both the expert selection and inter-expert\ncollaboration. Across five in-distribution (ID) and three out-of-distribution\n(OOD) benchmarks, MoT surpasses the current routing and aggregation-based\nstate-of-the-art, Avengers, by $+0.38\\%$ and $+2.92\\%$, respectively. Further,\nMoT significantly outperforms the best-performing single model. It achieves\nthis with single-pass inference, runtime comparable to routing baselines, and\nnone of the overheads of iterative aggregation. MoT offers a simple\nlatent-space mechanism for combining heterogeneous LLMs, a practical step\ntoward broader multi-LLM collaboration. Our code is publicly available at\nhttps://github.com/jacobfa/mot."
                },
                "authors": [
                    {
                        "name": "Jacob Fein-Ashley"
                    },
                    {
                        "name": "Dhruv Parikh"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21163v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21163v1",
                "updated": "2025-09-25T13:49:38Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    49,
                    38,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:49:38Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    49,
                    38,
                    3,
                    268,
                    0
                ],
                "title": "Distributed Specialization: Rare-Token Neurons in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributed Specialization: Rare-Token Neurons in Large Language Models"
                },
                "summary": "Large language models (LLMs) struggle with representing and generating rare\ntokens despite their importance in specialized domains. We investigate whether\nLLMs develop internal specialization mechanisms through discrete modular\narchitectures or distributed parameter-level differentiation. Through\nsystematic analysis of final-layer MLP neurons across multiple model families,\nwe discover that rare-token processing emerges via \\textit{distributed\nspecialization}: functionally coordinated but spatially distributed subnetworks\nthat exhibit three distinct organizational principles. First, we identify a\nreproducible three-regime influence hierarchy comprising highly influential\nplateau neurons(also termed as rare-token neurons), power-law decay neurons,\nand minimally contributing neurons, which is absent in common-token processing.\nSecond, plateau neurons demonstrate coordinated activation patterns (reduced\neffective dimensionality) while remaining spatially distributed rather than\nforming discrete clusters. Third, these specialized mechanisms are universally\naccessible through standard attention pathways without requiring dedicated\nrouting circuits. Training dynamics reveal that functional specialization\nemerges gradually through parameter differentiation, with specialized neurons\ndeveloping increasingly heavy-tailed weight correlation spectra consistent with\nHeavy-Tailed Self-Regularization signatures. Our findings establish that LLMs\nprocess rare-tokens through distributed coordination within shared\narchitectures rather than mixture-of-experts-style modularity. These results\nprovide insights for interpretable model editing, computational efficiency\noptimization, and understanding emergent functional organization in transformer\nnetworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) struggle with representing and generating rare\ntokens despite their importance in specialized domains. We investigate whether\nLLMs develop internal specialization mechanisms through discrete modular\narchitectures or distributed parameter-level differentiation. Through\nsystematic analysis of final-layer MLP neurons across multiple model families,\nwe discover that rare-token processing emerges via \\textit{distributed\nspecialization}: functionally coordinated but spatially distributed subnetworks\nthat exhibit three distinct organizational principles. First, we identify a\nreproducible three-regime influence hierarchy comprising highly influential\nplateau neurons(also termed as rare-token neurons), power-law decay neurons,\nand minimally contributing neurons, which is absent in common-token processing.\nSecond, plateau neurons demonstrate coordinated activation patterns (reduced\neffective dimensionality) while remaining spatially distributed rather than\nforming discrete clusters. Third, these specialized mechanisms are universally\naccessible through standard attention pathways without requiring dedicated\nrouting circuits. Training dynamics reveal that functional specialization\nemerges gradually through parameter differentiation, with specialized neurons\ndeveloping increasingly heavy-tailed weight correlation spectra consistent with\nHeavy-Tailed Self-Regularization signatures. Our findings establish that LLMs\nprocess rare-tokens through distributed coordination within shared\narchitectures rather than mixture-of-experts-style modularity. These results\nprovide insights for interpretable model editing, computational efficiency\noptimization, and understanding emergent functional organization in transformer\nnetworks."
                },
                "authors": [
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Haozheng Wang"
                    },
                    {
                        "name": "Yueheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Yueheng Li"
                },
                "author": "Yueheng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21163v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21163v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21161v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21161v1",
                "updated": "2025-09-25T13:46:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    46,
                    56,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:46:56Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    46,
                    56,
                    3,
                    268,
                    0
                ],
                "title": "DATS: Distance-Aware Temperature Scaling for Calibrated\n  Class-Incremental Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DATS: Distance-Aware Temperature Scaling for Calibrated\n  Class-Incremental Learning"
                },
                "summary": "Continual Learning (CL) is recently gaining increasing attention for its\nability to enable a single model to learn incrementally from a sequence of new\nclasses. In this scenario, it is important to keep consistent predictive\nperformance across all the classes and prevent the so-called Catastrophic\nForgetting (CF). However, in safety-critical applications, predictive\nperformance alone is insufficient. Predictive models should also be able to\nreliably communicate their uncertainty in a calibrated manner - that is, with\nconfidence scores aligned to the true frequencies of target events. Existing\napproaches in CL address calibration primarily from a data-centric perspective,\nrelying on a single temperature shared across all tasks. Such solutions\noverlook task-specific differences, leading to large fluctuations in\ncalibration error across tasks. For this reason, we argue that a more\nprincipled approach should adapt the temperature according to the distance to\nthe current task. However, the unavailability of the task information at test\ntime/during deployment poses a major challenge to achieve the intended\nobjective. For this, we propose Distance-Aware Temperature Scaling (DATS),\nwhich combines prototype-based distance estimation with distance-aware\ncalibration to infer task proximity and assign adaptive temperatures without\nprior task information. Through extensive empirical evaluation on both standard\nbenchmarks and real-world, imbalanced datasets taken from the biomedical\ndomain, our approach demonstrates to be stable, reliable and consistent in\nreducing calibration error across tasks compared to state-of-the-art\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Learning (CL) is recently gaining increasing attention for its\nability to enable a single model to learn incrementally from a sequence of new\nclasses. In this scenario, it is important to keep consistent predictive\nperformance across all the classes and prevent the so-called Catastrophic\nForgetting (CF). However, in safety-critical applications, predictive\nperformance alone is insufficient. Predictive models should also be able to\nreliably communicate their uncertainty in a calibrated manner - that is, with\nconfidence scores aligned to the true frequencies of target events. Existing\napproaches in CL address calibration primarily from a data-centric perspective,\nrelying on a single temperature shared across all tasks. Such solutions\noverlook task-specific differences, leading to large fluctuations in\ncalibration error across tasks. For this reason, we argue that a more\nprincipled approach should adapt the temperature according to the distance to\nthe current task. However, the unavailability of the task information at test\ntime/during deployment poses a major challenge to achieve the intended\nobjective. For this, we propose Distance-Aware Temperature Scaling (DATS),\nwhich combines prototype-based distance estimation with distance-aware\ncalibration to infer task proximity and assign adaptive temperatures without\nprior task information. Through extensive empirical evaluation on both standard\nbenchmarks and real-world, imbalanced datasets taken from the biomedical\ndomain, our approach demonstrates to be stable, reliable and consistent in\nreducing calibration error across tasks compared to state-of-the-art\napproaches."
                },
                "authors": [
                    {
                        "name": "Giuseppe Serra"
                    },
                    {
                        "name": "Florian Buettner"
                    }
                ],
                "author_detail": {
                    "name": "Florian Buettner"
                },
                "author": "Florian Buettner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21161v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21161v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19202v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19202v4",
                "updated": "2025-09-25T13:44:11Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    44,
                    11,
                    3,
                    268,
                    0
                ],
                "published": "2025-01-31T15:12:20Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    12,
                    20,
                    4,
                    31,
                    0
                ],
                "title": "Improving LLM Unlearning Robustness via Random Perturbations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving LLM Unlearning Robustness via Random Perturbations"
                },
                "summary": "Here, we show that current state-of-the-art LLM unlearning methods inherently\nreduce models' robustness, causing them to misbehave even when a single\nnon-adversarial forget-token is present in the retain-query. Toward\nunderstanding underlying causes, we propose a novel theoretical framework that\nreframes the unlearning process as backdoor attacks and defenses: forget-tokens\nact as backdoor triggers that, when activated in retain-queries, cause\ndisruptions in unlearned models' behaviors, similar to successful backdoor\nattacks. The sense that, LLM unlearning methods themselves poison the model,\nmake it more vulnerable to forget-tokens, and hide rather than erase target\nknowledge, describes their true mechanism. To mitigate the vulnerability caused\nby the forgetting process, we reinterpret the retaining process as a backdoor\ndefense and propose Random Noise Augmentation (RNA), a lightweight, model and\nmethod-agnostic approach with theoretical guarantees for improving the\nrobustness of models. Extensive experiments demonstrate that RNA significantly\nimproves the robustness of unlearned models while preserving forget and retain\nperformances. This backdoor attack-defense framework offers insights into the\nmechanism of unlearning that can shed light on future research directions for\nimproving unlearning robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Here, we show that current state-of-the-art LLM unlearning methods inherently\nreduce models' robustness, causing them to misbehave even when a single\nnon-adversarial forget-token is present in the retain-query. Toward\nunderstanding underlying causes, we propose a novel theoretical framework that\nreframes the unlearning process as backdoor attacks and defenses: forget-tokens\nact as backdoor triggers that, when activated in retain-queries, cause\ndisruptions in unlearned models' behaviors, similar to successful backdoor\nattacks. The sense that, LLM unlearning methods themselves poison the model,\nmake it more vulnerable to forget-tokens, and hide rather than erase target\nknowledge, describes their true mechanism. To mitigate the vulnerability caused\nby the forgetting process, we reinterpret the retaining process as a backdoor\ndefense and propose Random Noise Augmentation (RNA), a lightweight, model and\nmethod-agnostic approach with theoretical guarantees for improving the\nrobustness of models. Extensive experiments demonstrate that RNA significantly\nimproves the robustness of unlearned models while preserving forget and retain\nperformances. This backdoor attack-defense framework offers insights into the\nmechanism of unlearning that can shed light on future research directions for\nimproving unlearning robustness."
                },
                "authors": [
                    {
                        "name": "Dang Huu-Tien"
                    },
                    {
                        "name": "Hoang Thanh-Tung"
                    },
                    {
                        "name": "Anh Bui"
                    },
                    {
                        "name": "Minh-Phuong Nguyen"
                    },
                    {
                        "name": "Le-Minh Nguyen"
                    },
                    {
                        "name": "Naoya Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Naoya Inoue"
                },
                "author": "Naoya Inoue",
                "arxiv_comment": "29 pages, 13 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19202v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19202v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21155v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21155v2",
                "updated": "2025-09-26T02:37:41Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    2,
                    37,
                    41,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-25T13:42:28Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    42,
                    28,
                    3,
                    268,
                    0
                ],
                "title": "Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning the Wrong Lessons: Syntactic-Domain Spurious Correlations in\n  Language Models"
                },
                "summary": "For an LLM to correctly respond to an instruction it must understand both the\nsemantics and the domain (i.e., subject area) of a given task-instruction pair.\nHowever, syntax can also convey implicit information Recent work shows that\nsyntactic templates -- frequent sequences of Part-of-Speech (PoS) tags -- are\nprevalent in training data and often appear in model outputs. In this work we\ncharacterize syntactic templates, domain, and semantics in task-instruction\npairs. We identify cases of spurious correlations between syntax and domain,\nwhere models learn to associate a domain with syntax during training; this can\nsometimes override prompt semantics. Using a synthetic training dataset, we\nfind that the syntactic-domain correlation can lower performance (mean 0.51 +/-\n0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an\nevaluation framework to detect this phenomenon in trained models, and show that\nit occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B;\nLlama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study\non the implications for safety finetuning, showing that unintended\nsyntactic-domain correlations can be used to bypass refusals in OLMo-2-7B\nInstruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test\nfor syntactic-domain correlations, and (2) to ensure syntactic diversity in\ntraining data, specifically within domains, to prevent such spurious\ncorrelations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For an LLM to correctly respond to an instruction it must understand both the\nsemantics and the domain (i.e., subject area) of a given task-instruction pair.\nHowever, syntax can also convey implicit information Recent work shows that\nsyntactic templates -- frequent sequences of Part-of-Speech (PoS) tags -- are\nprevalent in training data and often appear in model outputs. In this work we\ncharacterize syntactic templates, domain, and semantics in task-instruction\npairs. We identify cases of spurious correlations between syntax and domain,\nwhere models learn to associate a domain with syntax during training; this can\nsometimes override prompt semantics. Using a synthetic training dataset, we\nfind that the syntactic-domain correlation can lower performance (mean 0.51 +/-\n0.06) on entity knowledge tasks in OLMo-2 models (1B-13B). We introduce an\nevaluation framework to detect this phenomenon in trained models, and show that\nit occurs on a subset of the FlanV2 dataset in open (OLMo-2-7B;\nLlama-4-Maverick), and closed (GPT-4o) models. Finally, we present a case study\non the implications for safety finetuning, showing that unintended\nsyntactic-domain correlations can be used to bypass refusals in OLMo-2-7B\nInstruct and GPT-4o. Our findings highlight two needs: (1) to explicitly test\nfor syntactic-domain correlations, and (2) to ensure syntactic diversity in\ntraining data, specifically within domains, to prevent such spurious\ncorrelations."
                },
                "authors": [
                    {
                        "name": "Chantal Shaib"
                    },
                    {
                        "name": "Vinith M. Suriyakumar"
                    },
                    {
                        "name": "Levent Sagun"
                    },
                    {
                        "name": "Byron C. Wallace"
                    },
                    {
                        "name": "Marzyeh Ghassemi"
                    }
                ],
                "author_detail": {
                    "name": "Marzyeh Ghassemi"
                },
                "author": "Marzyeh Ghassemi",
                "arxiv_comment": "NeurIPS 2025 Spotlight",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21155v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21155v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21154v1",
                "updated": "2025-09-25T13:40:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    40,
                    36,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:40:36Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    40,
                    36,
                    3,
                    268,
                    0
                ],
                "title": "GRPO is Secretly a Process Reward Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GRPO is Secretly a Process Reward Model"
                },
                "summary": "We prove theoretically that the GRPO RL algorithm induces a non-trivial\nprocess reward model (PRM), under certain assumptions regarding within-group\noverlap of token sequences across completions. We then show empirically that\nthese assumptions are met under real-world conditions: GRPO does in fact induce\na non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a\nflaw in the GRPO objective: non-uniformly distributed process steps hinder both\nexploration and exploitation (under different conditions). We propose a simple\nmodification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and\nshow that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy\nand performance on downstream reasoning tasks$-$and reach peak performance more\nrapidly$-$than LLMs trained with standard GRPO. Our results call into question\nthe advantage of costly, explicitly-defined PRMs for GRPO: we show that it is\npossible to instead leverage the hidden, built-in PRM structure within the\nvanilla GRPO algorithm to boost model performance with a negligible impact on\ntraining time and cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We prove theoretically that the GRPO RL algorithm induces a non-trivial\nprocess reward model (PRM), under certain assumptions regarding within-group\noverlap of token sequences across completions. We then show empirically that\nthese assumptions are met under real-world conditions: GRPO does in fact induce\na non-trivial PRM. Leveraging the framework of GRPO-as-a-PRM, we identify a\nflaw in the GRPO objective: non-uniformly distributed process steps hinder both\nexploration and exploitation (under different conditions). We propose a simple\nmodification to the algorithm to mitigate this defect ($\\lambda$-GRPO), and\nshow that LLMs trained with $\\lambda$-GRPO achieve higher validation accuracy\nand performance on downstream reasoning tasks$-$and reach peak performance more\nrapidly$-$than LLMs trained with standard GRPO. Our results call into question\nthe advantage of costly, explicitly-defined PRMs for GRPO: we show that it is\npossible to instead leverage the hidden, built-in PRM structure within the\nvanilla GRPO algorithm to boost model performance with a negligible impact on\ntraining time and cost."
                },
                "authors": [
                    {
                        "name": "Michael Sullivan"
                    }
                ],
                "author_detail": {
                    "name": "Michael Sullivan"
                },
                "author": "Michael Sullivan",
                "arxiv_comment": "14 pages, 6 figures; under review at ICLR 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21150v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21150v1",
                "updated": "2025-09-25T13:38:36Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    38,
                    36,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:38:36Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    38,
                    36,
                    3,
                    268,
                    0
                ],
                "title": "CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific\n  Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAD-Tokenizer: Towards Text-based CAD Prototyping via Modality-Specific\n  Tokenization"
                },
                "summary": "Computer-Aided Design (CAD) is a foundational component of industrial\nprototyping, where models are defined not by raw coordinates but by\nconstruction sequences such as sketches and extrusions. This sequential\nstructure enables both efficient prototype initialization and subsequent\nediting. Text-guided CAD prototyping, which unifies Text-to-CAD generation and\nCAD editing, has the potential to streamline the entire design pipeline.\nHowever, prior work has not explored this setting, largely because standard\nlarge language model (LLM) tokenizers decompose CAD sequences into\nnatural-language word pieces, failing to capture primitive-level CAD semantics\nand hindering attention modules from modeling geometric structure. We\nconjecture that a multimodal tokenization strategy, aligned with CAD's\nprimitive and structural nature, can provide more effective representations. To\nthis end, we propose CAD-Tokenizer, a framework that represents CAD data with\nmodality-specific tokens using a sequence-based VQ-VAE with primitive-level\npooling and constrained decoding. This design produces compact, primitive-aware\nrepresentations that align with CAD's structural nature. Applied to unified\ntext-guided CAD prototyping, CAD-Tokenizer significantly improves instruction\nfollowing and generation quality, achieving better quantitative and qualitative\nperformance over both general-purpose LLMs and task-specific baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer-Aided Design (CAD) is a foundational component of industrial\nprototyping, where models are defined not by raw coordinates but by\nconstruction sequences such as sketches and extrusions. This sequential\nstructure enables both efficient prototype initialization and subsequent\nediting. Text-guided CAD prototyping, which unifies Text-to-CAD generation and\nCAD editing, has the potential to streamline the entire design pipeline.\nHowever, prior work has not explored this setting, largely because standard\nlarge language model (LLM) tokenizers decompose CAD sequences into\nnatural-language word pieces, failing to capture primitive-level CAD semantics\nand hindering attention modules from modeling geometric structure. We\nconjecture that a multimodal tokenization strategy, aligned with CAD's\nprimitive and structural nature, can provide more effective representations. To\nthis end, we propose CAD-Tokenizer, a framework that represents CAD data with\nmodality-specific tokens using a sequence-based VQ-VAE with primitive-level\npooling and constrained decoding. This design produces compact, primitive-aware\nrepresentations that align with CAD's structural nature. Applied to unified\ntext-guided CAD prototyping, CAD-Tokenizer significantly improves instruction\nfollowing and generation quality, achieving better quantitative and qualitative\nperformance over both general-purpose LLMs and task-specific baselines."
                },
                "authors": [
                    {
                        "name": "Ruiyu Wang"
                    },
                    {
                        "name": "Shizhao Sun"
                    },
                    {
                        "name": "Weijian Ma"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21150v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21150v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11771v2",
                "updated": "2025-09-25T13:37:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    37,
                    54,
                    3,
                    268,
                    0
                ],
                "published": "2025-02-17T13:00:44Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    13,
                    0,
                    44,
                    0,
                    48,
                    0
                ],
                "title": "The Validation Gap: A Mechanistic Analysis of How Language Models\n  Compute Arithmetic but Fail to Validate It",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Validation Gap: A Mechanistic Analysis of How Language Models\n  Compute Arithmetic but Fail to Validate It"
                },
                "summary": "The ability of large language models (LLMs) to validate their output and\nidentify potential errors is crucial for ensuring robustness and reliability.\nHowever, current research indicates that LLMs struggle with self-correction,\nencountering significant challenges in detecting errors. While studies have\nexplored methods to enhance self-correction in LLMs, relatively little\nattention has been given to understanding the models' internal mechanisms\nunderlying error detection. In this paper, we present a mechanistic analysis of\nerror detection in LLMs, focusing on simple arithmetic problems. Through\ncircuit analysis, we identify the computational subgraphs responsible for\ndetecting arithmetic errors across four smaller-sized LLMs. Our findings reveal\nthat all models heavily rely on $\\textit{consistency heads}$--attention heads\nthat assess surface-level alignment of numerical values in arithmetic\nsolutions. Moreover, we observe that the models' internal arithmetic\ncomputation primarily occurs in higher layers, whereas validation takes place\nin middle layers, before the final arithmetic results are fully encoded. This\nstructural dissociation between arithmetic computation and validation seems to\nexplain why smaller-sized LLMs struggle to detect even simple arithmetic\nerrors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ability of large language models (LLMs) to validate their output and\nidentify potential errors is crucial for ensuring robustness and reliability.\nHowever, current research indicates that LLMs struggle with self-correction,\nencountering significant challenges in detecting errors. While studies have\nexplored methods to enhance self-correction in LLMs, relatively little\nattention has been given to understanding the models' internal mechanisms\nunderlying error detection. In this paper, we present a mechanistic analysis of\nerror detection in LLMs, focusing on simple arithmetic problems. Through\ncircuit analysis, we identify the computational subgraphs responsible for\ndetecting arithmetic errors across four smaller-sized LLMs. Our findings reveal\nthat all models heavily rely on $\\textit{consistency heads}$--attention heads\nthat assess surface-level alignment of numerical values in arithmetic\nsolutions. Moreover, we observe that the models' internal arithmetic\ncomputation primarily occurs in higher layers, whereas validation takes place\nin middle layers, before the final arithmetic results are fully encoded. This\nstructural dissociation between arithmetic computation and validation seems to\nexplain why smaller-sized LLMs struggle to detect even simple arithmetic\nerrors."
                },
                "authors": [
                    {
                        "name": "Leonardo Bertolazzi"
                    },
                    {
                        "name": "Philipp Mondorf"
                    },
                    {
                        "name": "Barbara Plank"
                    },
                    {
                        "name": "Raffaella Bernardi"
                    }
                ],
                "author_detail": {
                    "name": "Raffaella Bernardi"
                },
                "author": "Raffaella Bernardi",
                "arxiv_comment": "EMNLP 2025 Main, 38 pages, 33 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21144v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21144v1",
                "updated": "2025-09-25T13:30:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    30,
                    46,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:30:46Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    30,
                    46,
                    3,
                    268,
                    0
                ],
                "title": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniSS: Unified Expressive Speech-to-Speech Translation with Your Voice"
                },
                "summary": "The ultimate goal of expressive speech-to-speech translation (S2ST) is to\naccurately translate spoken content while preserving the speaker identity and\nemotional style. However, progress in this field is largely hindered by three\nkey challenges: the scarcity of paired speech data that retains expressive\nstyles, the complexity of multi-stage processing pipelines, and the limited\ntransfer of translation capabilities from large language models (LLMs). In this\nwork, we address these challenges by introducing UniSS, a novel single-stage\nframework for expressive S2ST. Our approach features carefully designed speech\nsemantic and style modeling, enabling seamless integration with existing\ntext-based LLM frameworks to develop a unified text-speech language model. To\ntransfer translation capabilities from text to speech, we propose a cross-modal\nchain-of-thought prompting process that progressively aligns audio semantics\nwith text and ensures style preservation in the decoded results. Furthermore,\nwe construct and release a large-scale, high-quality expressive S2ST dataset,\nUniST, comprising 44.8k hours of data. Experimental results show that UniSS\nsignificantly outperforms previous methods in translation fidelity and speech\nquality while preserving voice, emotion, and duration consistency. Our work\nestablishes a simpler and more effective paradigm for building the next\ngeneration of expressive S2ST systems. Audio samples are available at\nhttps://cmots.github.io/uniss-demo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ultimate goal of expressive speech-to-speech translation (S2ST) is to\naccurately translate spoken content while preserving the speaker identity and\nemotional style. However, progress in this field is largely hindered by three\nkey challenges: the scarcity of paired speech data that retains expressive\nstyles, the complexity of multi-stage processing pipelines, and the limited\ntransfer of translation capabilities from large language models (LLMs). In this\nwork, we address these challenges by introducing UniSS, a novel single-stage\nframework for expressive S2ST. Our approach features carefully designed speech\nsemantic and style modeling, enabling seamless integration with existing\ntext-based LLM frameworks to develop a unified text-speech language model. To\ntransfer translation capabilities from text to speech, we propose a cross-modal\nchain-of-thought prompting process that progressively aligns audio semantics\nwith text and ensures style preservation in the decoded results. Furthermore,\nwe construct and release a large-scale, high-quality expressive S2ST dataset,\nUniST, comprising 44.8k hours of data. Experimental results show that UniSS\nsignificantly outperforms previous methods in translation fidelity and speech\nquality while preserving voice, emotion, and duration consistency. Our work\nestablishes a simpler and more effective paradigm for building the next\ngeneration of expressive S2ST systems. Audio samples are available at\nhttps://cmots.github.io/uniss-demo."
                },
                "authors": [
                    {
                        "name": "Sitong Cheng"
                    },
                    {
                        "name": "Weizhen Bian"
                    },
                    {
                        "name": "Xinsheng Wang"
                    },
                    {
                        "name": "Ruibin Yuan"
                    },
                    {
                        "name": "Jianyi Chen"
                    },
                    {
                        "name": "Shunshun Yin"
                    },
                    {
                        "name": "Yike Guo"
                    },
                    {
                        "name": "Wei Xue"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xue"
                },
                "author": "Wei Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21144v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21144v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21134v1",
                "updated": "2025-09-25T13:25:15Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    25,
                    15,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:25:15Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    25,
                    15,
                    3,
                    268,
                    0
                ],
                "title": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ToMPO: Training LLM Strategic Decision Making from a Multi-Agent\n  Perspective"
                },
                "summary": "Large Language Models (LLMs) have been used to make decisions in complex\nscenarios, where they need models to think deeply, reason logically, and decide\nwisely. Many existing studies focus solely on multi-round conversations in\nsocial tasks or simulated environments, neglecting the various types of\ndecisions and their interdependence. Current reinforcement learning methods\nstruggle to consider the strategies of others during training. To address these\nissues, we first define a strategic decision-making problem that includes two\ntypes of decisions and their temporal dependencies. Furthermore, we propose\n**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to\noptimize the perception of other individual strategies and the game situation\ntrends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,\nToMPO enhances the LLM's strategic decision-making mainly by: 1) generating\nrollouts based on reasoning the strategies of other individuals, 2) estimating\nadvantages at both the graph-level and sample-level, and 3) balancing global\nand partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in\nterms of model output compliance and cooperative outcomes. Additionally, when\ncompared to models with parameter sizes 100 times larger, it shows an 18%\nimprovement. This demonstrates the effectiveness of the ToMPO algorithm in\nenhancing the model's strategic decision-making capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been used to make decisions in complex\nscenarios, where they need models to think deeply, reason logically, and decide\nwisely. Many existing studies focus solely on multi-round conversations in\nsocial tasks or simulated environments, neglecting the various types of\ndecisions and their interdependence. Current reinforcement learning methods\nstruggle to consider the strategies of others during training. To address these\nissues, we first define a strategic decision-making problem that includes two\ntypes of decisions and their temporal dependencies. Furthermore, we propose\n**T**heory **o**f **M**ind **P**olicy **O**ptimization **(ToMPO)** algorithm to\noptimize the perception of other individual strategies and the game situation\ntrends. Compared to the Group Relative Policy Optimization (GRPO) algorithm,\nToMPO enhances the LLM's strategic decision-making mainly by: 1) generating\nrollouts based on reasoning the strategies of other individuals, 2) estimating\nadvantages at both the graph-level and sample-level, and 3) balancing global\nand partial rewards. The ToMPO algorithm outperforms the GRPO method by 35% in\nterms of model output compliance and cooperative outcomes. Additionally, when\ncompared to models with parameter sizes 100 times larger, it shows an 18%\nimprovement. This demonstrates the effectiveness of the ToMPO algorithm in\nenhancing the model's strategic decision-making capabilities."
                },
                "authors": [
                    {
                        "name": "Yiwen Zhang"
                    },
                    {
                        "name": "Ziang Chen"
                    },
                    {
                        "name": "Fanqi Kong"
                    },
                    {
                        "name": "Yizhe Huang"
                    },
                    {
                        "name": "Xue Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Feng"
                },
                "author": "Xue Feng",
                "arxiv_comment": "22 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21129v1",
                "updated": "2025-09-25T13:19:59Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    19,
                    59,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:19:59Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    19,
                    59,
                    3,
                    268,
                    0
                ],
                "title": "EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing\n  Email Defense",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EvoMail: Self-Evolving Cognitive Agents for Adaptive Spam and Phishing\n  Email Defense"
                },
                "summary": "Modern email spam and phishing attacks have evolved far beyond keyword\nblacklists or simple heuristics. Adversaries now craft multi-modal campaigns\nthat combine natural-language text with obfuscated URLs, forged headers, and\nmalicious attachments, adapting their strategies within days to bypass filters.\nTraditional spam detection systems, which rely on static rules or\nsingle-modality models, struggle to integrate heterogeneous signals or to\ncontinuously adapt, leading to rapid performance degradation.\n  We propose EvoMail, a self-evolving cognitive agent framework for robust\ndetection of spam and phishing. EvoMail first constructs a unified\nheterogeneous email graph that fuses textual content, metadata (headers,\nsenders, domains), and embedded resources (URLs, attachments). A Cognitive\nGraph Neural Network enhanced by a Large Language Model (LLM) performs\ncontext-aware reasoning across these sources to identify coordinated spam\ncampaigns. Most critically, EvoMail engages in an adversarial self-evolution\nloop: a ''red-team'' agent generates novel evasion tactics -- such as character\nobfuscation or AI-generated phishing text -- while the ''blue-team'' detector\nlearns from failures, compresses experiences into a memory module, and reuses\nthem for future reasoning.\n  Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam,\nSpamAssassin, and TREC) and synthetic adversarial variants demonstrate that\nEvoMail consistently outperforms state-of-the-art baselines in detection\naccuracy, adaptability to evolving spam tactics, and interpretability of\nreasoning traces. These results highlight EvoMail's potential as a resilient\nand explainable defense framework against next-generation spam and phishing\nthreats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern email spam and phishing attacks have evolved far beyond keyword\nblacklists or simple heuristics. Adversaries now craft multi-modal campaigns\nthat combine natural-language text with obfuscated URLs, forged headers, and\nmalicious attachments, adapting their strategies within days to bypass filters.\nTraditional spam detection systems, which rely on static rules or\nsingle-modality models, struggle to integrate heterogeneous signals or to\ncontinuously adapt, leading to rapid performance degradation.\n  We propose EvoMail, a self-evolving cognitive agent framework for robust\ndetection of spam and phishing. EvoMail first constructs a unified\nheterogeneous email graph that fuses textual content, metadata (headers,\nsenders, domains), and embedded resources (URLs, attachments). A Cognitive\nGraph Neural Network enhanced by a Large Language Model (LLM) performs\ncontext-aware reasoning across these sources to identify coordinated spam\ncampaigns. Most critically, EvoMail engages in an adversarial self-evolution\nloop: a ''red-team'' agent generates novel evasion tactics -- such as character\nobfuscation or AI-generated phishing text -- while the ''blue-team'' detector\nlearns from failures, compresses experiences into a memory module, and reuses\nthem for future reasoning.\n  Extensive experiments on real-world datasets (Enron-Spam, Ling-Spam,\nSpamAssassin, and TREC) and synthetic adversarial variants demonstrate that\nEvoMail consistently outperforms state-of-the-art baselines in detection\naccuracy, adaptability to evolving spam tactics, and interpretability of\nreasoning traces. These results highlight EvoMail's potential as a resilient\nand explainable defense framework against next-generation spam and phishing\nthreats."
                },
                "authors": [
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "De-Tian Chu"
                    },
                    {
                        "name": "Lin-Yuan Bai"
                    },
                    {
                        "name": "Wei Kang"
                    },
                    {
                        "name": "Hai-Tao Zhang"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Zhi-Mo Han"
                    },
                    {
                        "name": "Jing Ge"
                    },
                    {
                        "name": "Hai-Feng Lin"
                    }
                ],
                "author_detail": {
                    "name": "Hai-Feng Lin"
                },
                "author": "Hai-Feng Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21128v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21128v1",
                "updated": "2025-09-25T13:18:57Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    18,
                    57,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:18:57Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    18,
                    57,
                    3,
                    268,
                    0
                ],
                "title": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RL Squeezes, SFT Expands: A Comparative Study of Reasoning LLMs"
                },
                "summary": "Large language models (LLMs) are typically trained by reinforcement learning\n(RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on\nreasoning traces to improve their reasoning abilities. However, how these\nmethods shape reasoning capabilities remains largely elusive. Going beyond an\naccuracy-based investigation of how these two components sculpt the reasoning\nprocess, this paper introduces a novel analysis framework that quantifies\nreasoning paths and captures their qualitative changes under each training\nprocess (with models of 1.5B, 7B, and 14B parameters on mathematical domains).\nSpecifically, we investigate the reasoning process at two levels of\ngranularity: the trajectory-level, which examines complete reasoning outputs,\nand the step-level, which analyzes reasoning graphs whose nodes correspond to\nindividual reasoning steps. Notably, clustering of unique reasoning\ntrajectories shows complementary effects: RL compresses incorrect trajectories,\nwhereas SFT expands correct ones. Step-level analysis reveals that RL steepens\n(about 2.5 times), while SFT flattens (reduced to about one-third), the decay\nrates of node visitation frequency, degree, and betweenness centrality\ndistributions in the reasoning graph. This indicates that RL concentrates\nreasoning functionality into a small subset of steps, while SFT homogenizes it\nacross many steps. Furthermore, by evaluating the reasoning graph topologies\nfrom multiple perspectives, we delineate the shared and distinct\ncharacteristics of RL and SFT. Our work presents a novel reasoning path\nperspective that explains why the current best practice of two-stage training,\nwith SFT followed by RL, is successful, and offers practical implications for\ndata construction and more efficient learning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are typically trained by reinforcement learning\n(RL) with verifiable rewards (RLVR) and supervised fine-tuning (SFT) on\nreasoning traces to improve their reasoning abilities. However, how these\nmethods shape reasoning capabilities remains largely elusive. Going beyond an\naccuracy-based investigation of how these two components sculpt the reasoning\nprocess, this paper introduces a novel analysis framework that quantifies\nreasoning paths and captures their qualitative changes under each training\nprocess (with models of 1.5B, 7B, and 14B parameters on mathematical domains).\nSpecifically, we investigate the reasoning process at two levels of\ngranularity: the trajectory-level, which examines complete reasoning outputs,\nand the step-level, which analyzes reasoning graphs whose nodes correspond to\nindividual reasoning steps. Notably, clustering of unique reasoning\ntrajectories shows complementary effects: RL compresses incorrect trajectories,\nwhereas SFT expands correct ones. Step-level analysis reveals that RL steepens\n(about 2.5 times), while SFT flattens (reduced to about one-third), the decay\nrates of node visitation frequency, degree, and betweenness centrality\ndistributions in the reasoning graph. This indicates that RL concentrates\nreasoning functionality into a small subset of steps, while SFT homogenizes it\nacross many steps. Furthermore, by evaluating the reasoning graph topologies\nfrom multiple perspectives, we delineate the shared and distinct\ncharacteristics of RL and SFT. Our work presents a novel reasoning path\nperspective that explains why the current best practice of two-stage training,\nwith SFT followed by RL, is successful, and offers practical implications for\ndata construction and more efficient learning approaches."
                },
                "authors": [
                    {
                        "name": "Kohsei Matsutani"
                    },
                    {
                        "name": "Shota Takashiro"
                    },
                    {
                        "name": "Gouki Minegishi"
                    },
                    {
                        "name": "Takeshi Kojima"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    }
                ],
                "author_detail": {
                    "name": "Yutaka Matsuo"
                },
                "author": "Yutaka Matsuo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21128v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21128v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21126v1",
                "updated": "2025-09-25T13:16:34Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    16,
                    34,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:16:34Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    16,
                    34,
                    3,
                    268,
                    0
                ],
                "title": "Teaching RL Agents to Act Better: VLM as Action Advisor for Online\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching RL Agents to Act Better: VLM as Action Advisor for Online\n  Reinforcement Learning"
                },
                "summary": "Online reinforcement learning in complex tasks is time-consuming, as massive\ninteraction steps are needed to learn the optimal Q-function.Vision-language\naction (VLA) policies represent a promising direction for solving diverse\ntasks; however, their performance on low-level control remains limited, and\neffective deployment often requires task-specific expert demonstrations for\nfine-tuning. In this paper, we propose \\textbf{VARL} (\\textbf{V}LM as\n\\textbf{A}ction advisor for online \\textbf{R}einforcement \\textbf{L}earning), a\nframework that leverages the domain knowledge of vision-language models (VLMs)\nto provide action suggestions for reinforcement learning agents. Unlike\nprevious methods, VARL provides action suggestions rather than designing\nheuristic rewards, thereby guaranteeing unchanged optimality and convergence.\nThe suggested actions increase sample diversity and ultimately improve sample\nefficiency, especially in sparse-reward tasks. To validate the effectiveness of\nVARL, we evaluate it across diverse environments and agent settings. Results\nshow that VARL greatly improves sample efficiency without introducing\nsignificant computational overhead. These advantages make VARL a general\nframework for online reinforcement learning and make it feasible to directly\napply reinforcement learning from scratch in real-world environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online reinforcement learning in complex tasks is time-consuming, as massive\ninteraction steps are needed to learn the optimal Q-function.Vision-language\naction (VLA) policies represent a promising direction for solving diverse\ntasks; however, their performance on low-level control remains limited, and\neffective deployment often requires task-specific expert demonstrations for\nfine-tuning. In this paper, we propose \\textbf{VARL} (\\textbf{V}LM as\n\\textbf{A}ction advisor for online \\textbf{R}einforcement \\textbf{L}earning), a\nframework that leverages the domain knowledge of vision-language models (VLMs)\nto provide action suggestions for reinforcement learning agents. Unlike\nprevious methods, VARL provides action suggestions rather than designing\nheuristic rewards, thereby guaranteeing unchanged optimality and convergence.\nThe suggested actions increase sample diversity and ultimately improve sample\nefficiency, especially in sparse-reward tasks. To validate the effectiveness of\nVARL, we evaluate it across diverse environments and agent settings. Results\nshow that VARL greatly improves sample efficiency without introducing\nsignificant computational overhead. These advantages make VARL a general\nframework for online reinforcement learning and make it feasible to directly\napply reinforcement learning from scratch in real-world environments."
                },
                "authors": [
                    {
                        "name": "Xiefeng Wu"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Shu Zhang"
                    },
                    {
                        "name": "Mingyu Hu"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Hu"
                },
                "author": "Mingyu Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.22156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.22156v2",
                "updated": "2025-09-25T13:15:45Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    15,
                    45,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-28T09:20:18Z",
                "published_parsed": [
                    2025,
                    5,
                    28,
                    9,
                    20,
                    18,
                    2,
                    148,
                    0
                ],
                "title": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InComeS: Integrating Compression and Selection Mechanisms into LLMs for\n  Efficient Model Editing"
                },
                "summary": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing model editing methods perform well in recalling exact edit\nfacts, they often struggle in complex scenarios that require deeper semantic\nunderstanding rather than mere knowledge regurgitation. Leveraging the strong\ncontextual reasoning abilities of large language models (LLMs), in-context\nlearning (ICL) becomes a promising editing method by comprehending edit\ninformation through context encoding. However, this method is constrained by\nthe limited context window of LLMs, leading to degraded performance and\nefficiency as the number of edits increases. To overcome this limitation, we\npropose InComeS, a flexible framework that enhances LLMs' ability to process\nediting contexts through explicit compression and selection mechanisms.\nSpecifically, InComeS compresses each editing context into the key-value (KV)\ncache of a special gist token, enabling efficient handling of multiple edits\nwithout being restricted by the model's context window. Furthermore,\nspecialized cross-attention modules are added to dynamically select the most\nrelevant information from the gist pools, enabling adaptive and effective\nutilization of edit information. We conduct experiments on diverse model\nediting benchmarks with various editing formats, and the results demonstrate\nthe effectiveness and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Shuaiyi Li"
                    },
                    {
                        "name": "Zhisong Zhang"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "Chenlong Deng"
                    },
                    {
                        "name": "Tianqing Fang"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Haitao Mi"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "18 pages,5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.22156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.22156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21125v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21125v1",
                "updated": "2025-09-25T13:15:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    15,
                    1,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T13:15:01Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    15,
                    1,
                    3,
                    268,
                    0
                ],
                "title": "Acoustic-based Gender Differentiation in Speech-aware Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Acoustic-based Gender Differentiation in Speech-aware Language Models"
                },
                "summary": "Speech-aware Language Models (SpeechLMs) have fundamentally transformed\nhuman-AI interaction by enabling voice-based communication, yet they may\nexhibit acoustic-based gender differentiation where identical questions lead to\ndifferent responses based on the speaker's gender. This paper propose a new\ndataset that enables systematic analysis of this phenomenon, containing 9,208\nspeech samples across three categories: Gender-Independent,\nGender-Stereotypical, and Gender-Dependent. We further evaluated LLaMA-Omni\nseries and discovered a paradoxical pattern; while overall responses seems\nidentical regardless of gender, the pattern is far from unbiased responses.\nSpecifically, in Gender-Stereotypical questions, all models consistently\nexhibited male-oriented responses; meanwhile, in Gender-Dependent questions\nwhere gender differentiation would be contextually appropriate, models\nexhibited responses independent to gender instead. We also confirm that this\npattern does not result from neutral options nor perceived gender of a voice.\nWhen we allow neutral response, models tends to respond neutrally also in\nGender-Dependent questions. The paradoxical pattern yet retains when we applied\ngender neutralization methods on speech. Through comparison between SpeechLMs\nwith corresponding backbone LLMs, we confirmed that these paradoxical patterns\nprimarily stem from Whisper speech encoders, which generates male-oriented\nacoustic tokens. These findings reveal that current SpeechLMs may not\nsuccessfully remove gender biases though they prioritized general fairness\nprinciples over contextual appropriateness, highlighting the need for more\nsophisticated techniques to utilize gender information properly in speech\ntechnology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech-aware Language Models (SpeechLMs) have fundamentally transformed\nhuman-AI interaction by enabling voice-based communication, yet they may\nexhibit acoustic-based gender differentiation where identical questions lead to\ndifferent responses based on the speaker's gender. This paper propose a new\ndataset that enables systematic analysis of this phenomenon, containing 9,208\nspeech samples across three categories: Gender-Independent,\nGender-Stereotypical, and Gender-Dependent. We further evaluated LLaMA-Omni\nseries and discovered a paradoxical pattern; while overall responses seems\nidentical regardless of gender, the pattern is far from unbiased responses.\nSpecifically, in Gender-Stereotypical questions, all models consistently\nexhibited male-oriented responses; meanwhile, in Gender-Dependent questions\nwhere gender differentiation would be contextually appropriate, models\nexhibited responses independent to gender instead. We also confirm that this\npattern does not result from neutral options nor perceived gender of a voice.\nWhen we allow neutral response, models tends to respond neutrally also in\nGender-Dependent questions. The paradoxical pattern yet retains when we applied\ngender neutralization methods on speech. Through comparison between SpeechLMs\nwith corresponding backbone LLMs, we confirmed that these paradoxical patterns\nprimarily stem from Whisper speech encoders, which generates male-oriented\nacoustic tokens. These findings reveal that current SpeechLMs may not\nsuccessfully remove gender biases though they prioritized general fairness\nprinciples over contextual appropriateness, highlighting the need for more\nsophisticated techniques to utilize gender information properly in speech\ntechnology."
                },
                "authors": [
                    {
                        "name": "Junhyuk Choi"
                    },
                    {
                        "name": "Jihwan Seol"
                    },
                    {
                        "name": "Nayeon Kim"
                    },
                    {
                        "name": "Chanhee Cho"
                    },
                    {
                        "name": "EunBin Cho"
                    },
                    {
                        "name": "Bugeun Kim"
                    }
                ],
                "author_detail": {
                    "name": "Bugeun Kim"
                },
                "author": "Bugeun Kim",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21125v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21125v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.18485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.18485v2",
                "updated": "2025-09-25T13:11:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    11,
                    7,
                    3,
                    268,
                    0
                ],
                "published": "2025-06-23T10:37:57Z",
                "published_parsed": [
                    2025,
                    6,
                    23,
                    10,
                    37,
                    57,
                    0,
                    174,
                    0
                ],
                "title": "A Simple \"Motivation\" Can Enhance Reinforcement Finetuning of Large\n  Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple \"Motivation\" Can Enhance Reinforcement Finetuning of Large\n  Reasoning Models"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful learn-to-reason paradigm for Large Reasoning Models to tackle complex\ntasks. However, current RLVR paradigm is still not efficient enough, as it\nworks in a trial-and-error manner. To perform better, the model needs to\nexplore the reward space by numerously generating responses and learn from\nfragmented reward signals, blind to the overall reward patterns. Fortunately,\nverifiable rewards make the natural language description of the reward function\npossible, and meanwhile, LLMs have demonstrated strong in-context learning\nability. This motivates us to explore if Large Reasoning Models can benefit\nfrom a motivation of the task, i.e., awareness of the reward function, during\nthe reinforcement finetuning process, as we humans sometimes do when learning.\nIn this paper, we introduce Motivation-enhanced Reinforcement Finetuning\n(MeRF), an intuitive yet effective method enhancing reinforcement finetuning of\nLLMs by involving ``telling LLMs rules of the game''. Specifically, MeRF\ndirectly injects the reward specification into the prompt, which serves as an\nin-context motivation for the model to be aware of the optimization objective.\nThis simple modification leverages the in-context learning ability of LLMs,\naligning generation with optimization, thereby incentivizing the model to\ngenerate desired outputs from both inner motivation and external reward.\nEmpirical evaluations demonstrate that MeRF achieves substantial performance\ngains over RLVR baseline. Moreover, ablation studies show that MeRF performs\nbetter with greater consistency between the in-context motivation and the\nexternal reward function, while the model also demonstrates an ability to adapt\nto misleading motivations through reinforcement finetuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful learn-to-reason paradigm for Large Reasoning Models to tackle complex\ntasks. However, current RLVR paradigm is still not efficient enough, as it\nworks in a trial-and-error manner. To perform better, the model needs to\nexplore the reward space by numerously generating responses and learn from\nfragmented reward signals, blind to the overall reward patterns. Fortunately,\nverifiable rewards make the natural language description of the reward function\npossible, and meanwhile, LLMs have demonstrated strong in-context learning\nability. This motivates us to explore if Large Reasoning Models can benefit\nfrom a motivation of the task, i.e., awareness of the reward function, during\nthe reinforcement finetuning process, as we humans sometimes do when learning.\nIn this paper, we introduce Motivation-enhanced Reinforcement Finetuning\n(MeRF), an intuitive yet effective method enhancing reinforcement finetuning of\nLLMs by involving ``telling LLMs rules of the game''. Specifically, MeRF\ndirectly injects the reward specification into the prompt, which serves as an\nin-context motivation for the model to be aware of the optimization objective.\nThis simple modification leverages the in-context learning ability of LLMs,\naligning generation with optimization, thereby incentivizing the model to\ngenerate desired outputs from both inner motivation and external reward.\nEmpirical evaluations demonstrate that MeRF achieves substantial performance\ngains over RLVR baseline. Moreover, ablation studies show that MeRF performs\nbetter with greater consistency between the in-context motivation and the\nexternal reward function, while the model also demonstrates an ability to adapt\nto misleading motivations through reinforcement finetuning."
                },
                "authors": [
                    {
                        "name": "Junjie Zhang"
                    },
                    {
                        "name": "Guozheng Ma"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Ting-En Lin"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.18485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.18485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21117v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21117v2",
                "updated": "2025-09-26T05:33:48Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    5,
                    33,
                    48,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-25T13:04:29Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    13,
                    4,
                    29,
                    3,
                    268,
                    0
                ],
                "title": "TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TrustJudge: Inconsistencies of LLM-as-a-Judge and How to Alleviate Them"
                },
                "summary": "The adoption of Large Language Models (LLMs) as automated evaluators\n(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation\nframeworks. We identify two fundamental types of inconsistencies: (1)\nScore-Comparison Inconsistency, where lower-rated responses outperform\nhigher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity\nInconsistency, manifested through circular preference chains (A>B>C>A) and\nequivalence contradictions (A=B=C\\neq A). We argue that these issues come from\ninformation loss in discrete rating systems and ambiguous tie judgments during\npairwise evaluation. We propose TrustJudge, a probabilistic framework that\naddresses these limitations through two key innovations: 1)\ndistribution-sensitive scoring that computes continuous expectations from\ndiscrete rating probabilities, preserving information entropy for more precise\nscoring, and 2) likelihood-aware aggregation that resolves transitivity\nviolations using bidirectional preference probabilities or perplexity. We also\nformalize the theoretical limitations of current LLM-as-a-judge frameworks and\ndemonstrate how TrustJudge's components overcome them. When evaluated with\nLlama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces\nScore-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise\nTransitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining\nhigher evaluation accuracy. Our work provides the first systematic analysis of\nevaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both\ntheoretical insights and practical solutions for reliable automated assessment.\nThe framework demonstrates consistent improvements across various model\narchitectures and scales, enabling more trustworthy LLM evaluation without\nrequiring additional training or human annotations. The codes can be found at\nhttps://github.com/TrustJudge/TrustJudge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The adoption of Large Language Models (LLMs) as automated evaluators\n(LLM-as-a-judge) has revealed critical inconsistencies in current evaluation\nframeworks. We identify two fundamental types of inconsistencies: (1)\nScore-Comparison Inconsistency, where lower-rated responses outperform\nhigher-scored ones in pairwise comparisons, and (2) Pairwise Transitivity\nInconsistency, manifested through circular preference chains (A>B>C>A) and\nequivalence contradictions (A=B=C\\neq A). We argue that these issues come from\ninformation loss in discrete rating systems and ambiguous tie judgments during\npairwise evaluation. We propose TrustJudge, a probabilistic framework that\naddresses these limitations through two key innovations: 1)\ndistribution-sensitive scoring that computes continuous expectations from\ndiscrete rating probabilities, preserving information entropy for more precise\nscoring, and 2) likelihood-aware aggregation that resolves transitivity\nviolations using bidirectional preference probabilities or perplexity. We also\nformalize the theoretical limitations of current LLM-as-a-judge frameworks and\ndemonstrate how TrustJudge's components overcome them. When evaluated with\nLlama-3.1-70B-Instruct as judge using our dataset, TrustJudge reduces\nScore-Comparison inconsistency by 8.43% (from 23.32% to 14.89%) and Pairwise\nTransitivity inconsistency by 10.82% (from 15.22% to 4.40%), while maintaining\nhigher evaluation accuracy. Our work provides the first systematic analysis of\nevaluation framework inconsistencies in LLM-as-a-judge paradigms, offering both\ntheoretical insights and practical solutions for reliable automated assessment.\nThe framework demonstrates consistent improvements across various model\narchitectures and scales, enabling more trustworthy LLM evaluation without\nrequiring additional training or human annotations. The codes can be found at\nhttps://github.com/TrustJudge/TrustJudge."
                },
                "authors": [
                    {
                        "name": "Yidong Wang"
                    },
                    {
                        "name": "Yunze Song"
                    },
                    {
                        "name": "Tingyuan Zhu"
                    },
                    {
                        "name": "Xuanwang Zhang"
                    },
                    {
                        "name": "Zhuohao Yu"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Chiyu Song"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Zhen Wu"
                    },
                    {
                        "name": "Xinyu Dai"
                    },
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Wei Ye"
                    },
                    {
                        "name": "Shikun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shikun Zhang"
                },
                "author": "Shikun Zhang",
                "arxiv_comment": "22 pages, 9 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21117v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21117v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21106v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21106v1",
                "updated": "2025-09-25T12:53:07Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    53,
                    7,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:53:07Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    53,
                    7,
                    3,
                    268,
                    0
                ],
                "title": "BESPOKE: Benchmark for Search-Augmented Large Language Model\n  Personalization via Diagnostic Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BESPOKE: Benchmark for Search-Augmented Large Language Model\n  Personalization via Diagnostic Feedback"
                },
                "summary": "Search-augmented large language models (LLMs) have advanced\ninformation-seeking tasks by integrating retrieval into generation, reducing\nusers' cognitive burden compared to traditional search systems. Yet they remain\ninsufficient for fully addressing diverse user needs, which requires\nrecognizing how the same query can reflect different intents across users and\ndelivering information in preferred forms. While recent systems such as ChatGPT\nand Gemini attempt personalization by leveraging user histories, systematic\nevaluation of such personalization is under-explored. To address this gap, we\npropose BESPOKE, the realistic benchmark for evaluating personalization in\nsearch-augmented LLMs. BESPOKE is designed to be both realistic, by collecting\nauthentic chat and search histories directly from humans, and diagnostic, by\npairing responses with fine-grained preference scores and feedback. The\nbenchmark is constructed through long-term, deeply engaged human annotation,\nwhere human annotators contributed their own histories, authored queries with\ndetailed information needs, and evaluated responses with scores and diagnostic\nfeedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key\nrequirements for effective personalization in information-seeking tasks,\nproviding a foundation for fine-grained evaluation of personalized\nsearch-augmented LLMs. Our code and data are available at\nhttps://augustinlib.github.io/BESPOKE/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Search-augmented large language models (LLMs) have advanced\ninformation-seeking tasks by integrating retrieval into generation, reducing\nusers' cognitive burden compared to traditional search systems. Yet they remain\ninsufficient for fully addressing diverse user needs, which requires\nrecognizing how the same query can reflect different intents across users and\ndelivering information in preferred forms. While recent systems such as ChatGPT\nand Gemini attempt personalization by leveraging user histories, systematic\nevaluation of such personalization is under-explored. To address this gap, we\npropose BESPOKE, the realistic benchmark for evaluating personalization in\nsearch-augmented LLMs. BESPOKE is designed to be both realistic, by collecting\nauthentic chat and search histories directly from humans, and diagnostic, by\npairing responses with fine-grained preference scores and feedback. The\nbenchmark is constructed through long-term, deeply engaged human annotation,\nwhere human annotators contributed their own histories, authored queries with\ndetailed information needs, and evaluated responses with scores and diagnostic\nfeedback. Leveraging BESPOKE, we conduct systematic analyses that reveal key\nrequirements for effective personalization in information-seeking tasks,\nproviding a foundation for fine-grained evaluation of personalized\nsearch-augmented LLMs. Our code and data are available at\nhttps://augustinlib.github.io/BESPOKE/."
                },
                "authors": [
                    {
                        "name": "Hyunseo Kim"
                    },
                    {
                        "name": "Sangam Lee"
                    },
                    {
                        "name": "Kwangwook Seo"
                    },
                    {
                        "name": "Dongha Lee"
                    }
                ],
                "author_detail": {
                    "name": "Dongha Lee"
                },
                "author": "Dongha Lee",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21106v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21106v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21104v1",
                "updated": "2025-09-25T12:50:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    50,
                    46,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:50:46Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    50,
                    46,
                    3,
                    268,
                    0
                ],
                "title": "PerHalluEval: Persian Hallucination Evaluation Benchmark for Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PerHalluEval: Persian Hallucination Evaluation Benchmark for Large\n  Language Models"
                },
                "summary": "Hallucination is a persistent issue affecting all large language Models\n(LLMs), particularly within low-resource languages such as Persian.\nPerHalluEval (Persian Hallucination Evaluation) is the first dynamic\nhallucination evaluation benchmark tailored for the Persian language. Our\nbenchmark leverages a three-stage LLM-driven pipeline, augmented with human\nvalidation, to generate plausible answers and summaries regarding QA and\nsummarization tasks, focusing on detecting extrinsic and intrinsic\nhallucinations. Moreover, we used the log probabilities of generated tokens to\nselect the most believable hallucinated instances. In addition, we engaged\nhuman annotators to highlight Persian-specific contexts in the QA dataset in\norder to evaluate LLMs' performance on content specifically related to Persian\nculture. Our evaluation of 12 LLMs, including open- and closed-source models\nusing PerHalluEval, revealed that the models generally struggle in detecting\nhallucinated Persian text. We showed that providing external knowledge, i.e.,\nthe original document for the summarization task, could mitigate hallucination\npartially. Furthermore, there was no significant difference in terms of\nhallucination when comparing LLMs specifically trained for Persian with others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination is a persistent issue affecting all large language Models\n(LLMs), particularly within low-resource languages such as Persian.\nPerHalluEval (Persian Hallucination Evaluation) is the first dynamic\nhallucination evaluation benchmark tailored for the Persian language. Our\nbenchmark leverages a three-stage LLM-driven pipeline, augmented with human\nvalidation, to generate plausible answers and summaries regarding QA and\nsummarization tasks, focusing on detecting extrinsic and intrinsic\nhallucinations. Moreover, we used the log probabilities of generated tokens to\nselect the most believable hallucinated instances. In addition, we engaged\nhuman annotators to highlight Persian-specific contexts in the QA dataset in\norder to evaluate LLMs' performance on content specifically related to Persian\nculture. Our evaluation of 12 LLMs, including open- and closed-source models\nusing PerHalluEval, revealed that the models generally struggle in detecting\nhallucinated Persian text. We showed that providing external knowledge, i.e.,\nthe original document for the summarization task, could mitigate hallucination\npartially. Furthermore, there was no significant difference in terms of\nhallucination when comparing LLMs specifically trained for Persian with others."
                },
                "authors": [
                    {
                        "name": "Mohammad Hosseini"
                    },
                    {
                        "name": "Kimia Hosseini"
                    },
                    {
                        "name": "Shayan Bali"
                    },
                    {
                        "name": "Zahra Zanjani"
                    },
                    {
                        "name": "Saeedeh Momtazi"
                    }
                ],
                "author_detail": {
                    "name": "Saeedeh Momtazi"
                },
                "author": "Saeedeh Momtazi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.06843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.06843v2",
                "updated": "2025-09-25T12:49:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    49,
                    56,
                    3,
                    268,
                    0
                ],
                "published": "2025-06-07T15:48:04Z",
                "published_parsed": [
                    2025,
                    6,
                    7,
                    15,
                    48,
                    4,
                    5,
                    158,
                    0
                ],
                "title": "United Minds or Isolated Agents? Exploring Coordination of LLMs under\n  Cognitive Load Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "United Minds or Isolated Agents? Exploring Coordination of LLMs under\n  Cognitive Load Theory"
                },
                "summary": "Large Language Models (LLMs) exhibit a notable performance ceiling on\ncomplex, multi-faceted tasks, as they often fail to integrate diverse\ninformation or adhere to multiple constraints. We posit that such limitation\narises when the demands of a task exceed the LLM's effective cognitive load\ncapacity. This interpretation draws a strong analogy to Cognitive Load Theory\n(CLT) in cognitive science, which explains similar performance boundaries in\nthe human mind, and is further supported by emerging evidence that reveals LLMs\nhave bounded working memory characteristics. Building upon this CLT-grounded\nunderstanding, we introduce CoThinker, a novel LLM-based multi-agent framework\ndesigned to mitigate cognitive overload and enhance collaborative\nproblem-solving abilities. CoThinker operationalizes CLT principles by\ndistributing intrinsic cognitive load through agent specialization and managing\ntransactional load via structured communication and a collective working\nmemory. We empirically validate CoThinker on complex problem-solving tasks and\nfabricated high cognitive load scenarios, demonstrating improvements over\nexisting multi-agent baselines in solution quality and efficiency. Our analysis\nreveals characteristic interaction patterns, providing insights into the\nemergence of collective cognition and effective load management, thus offering\na principled approach to overcoming LLM performance ceilings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit a notable performance ceiling on\ncomplex, multi-faceted tasks, as they often fail to integrate diverse\ninformation or adhere to multiple constraints. We posit that such limitation\narises when the demands of a task exceed the LLM's effective cognitive load\ncapacity. This interpretation draws a strong analogy to Cognitive Load Theory\n(CLT) in cognitive science, which explains similar performance boundaries in\nthe human mind, and is further supported by emerging evidence that reveals LLMs\nhave bounded working memory characteristics. Building upon this CLT-grounded\nunderstanding, we introduce CoThinker, a novel LLM-based multi-agent framework\ndesigned to mitigate cognitive overload and enhance collaborative\nproblem-solving abilities. CoThinker operationalizes CLT principles by\ndistributing intrinsic cognitive load through agent specialization and managing\ntransactional load via structured communication and a collective working\nmemory. We empirically validate CoThinker on complex problem-solving tasks and\nfabricated high cognitive load scenarios, demonstrating improvements over\nexisting multi-agent baselines in solution quality and efficiency. Our analysis\nreveals characteristic interaction patterns, providing insights into the\nemergence of collective cognition and effective load management, thus offering\na principled approach to overcoming LLM performance ceilings."
                },
                "authors": [
                    {
                        "name": "HaoYang Shang"
                    },
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Zi Liang"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Haibo Hu"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.06843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.06843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21102v1",
                "updated": "2025-09-25T12:47:27Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    47,
                    27,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:47:27Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    47,
                    27,
                    3,
                    268,
                    0
                ],
                "title": "Mammo-CLIP Dissect: A Framework for Analysing Mammography Concepts in\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mammo-CLIP Dissect: A Framework for Analysing Mammography Concepts in\n  Vision-Language Models"
                },
                "summary": "Understanding what deep learning (DL) models learn is essential for the safe\ndeployment of artificial intelligence (AI) in clinical settings. While previous\nwork has focused on pixel-based explainability methods, less attention has been\npaid to the textual concepts learned by these models, which may better reflect\nthe reasoning used by clinicians. We introduce Mammo-CLIP Dissect, the first\nconcept-based explainability framework for systematically dissecting DL vision\nmodels trained for mammography. Leveraging a mammography-specific\nvision-language model (Mammo-CLIP) as a \"dissector,\" our approach labels\nneurons at specified layers with human-interpretable textual concepts and\nquantifies their alignment to domain knowledge. Using Mammo-CLIP Dissect, we\ninvestigate three key questions: (1) how concept learning differs between DL\nvision models trained on general image datasets versus mammography-specific\ndatasets; (2) how fine-tuning for downstream mammography tasks affects concept\nspecialisation; and (3) which mammography-relevant concepts remain\nunderrepresented. We show that models trained on mammography data capture more\nclinically relevant concepts and align more closely with radiologists'\nworkflows than models not trained on mammography data. Fine-tuning for\ntask-specific classification enhances the capture of certain concept categories\n(e.g., benign calcifications) but can reduce coverage of others (e.g.,\ndensity-related features), indicating a trade-off between specialisation and\ngeneralisation. Our findings show that Mammo-CLIP Dissect provides insights\ninto how convolutional neural networks (CNNs) capture mammography-specific\nknowledge. By comparing models across training data and fine-tuning regimes, we\nreveal how domain-specific training and task-specific adaptation shape concept\nlearning. Code and concept set are available:\nhttps://github.com/Suaiba/Mammo-CLIP-Dissect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding what deep learning (DL) models learn is essential for the safe\ndeployment of artificial intelligence (AI) in clinical settings. While previous\nwork has focused on pixel-based explainability methods, less attention has been\npaid to the textual concepts learned by these models, which may better reflect\nthe reasoning used by clinicians. We introduce Mammo-CLIP Dissect, the first\nconcept-based explainability framework for systematically dissecting DL vision\nmodels trained for mammography. Leveraging a mammography-specific\nvision-language model (Mammo-CLIP) as a \"dissector,\" our approach labels\nneurons at specified layers with human-interpretable textual concepts and\nquantifies their alignment to domain knowledge. Using Mammo-CLIP Dissect, we\ninvestigate three key questions: (1) how concept learning differs between DL\nvision models trained on general image datasets versus mammography-specific\ndatasets; (2) how fine-tuning for downstream mammography tasks affects concept\nspecialisation; and (3) which mammography-relevant concepts remain\nunderrepresented. We show that models trained on mammography data capture more\nclinically relevant concepts and align more closely with radiologists'\nworkflows than models not trained on mammography data. Fine-tuning for\ntask-specific classification enhances the capture of certain concept categories\n(e.g., benign calcifications) but can reduce coverage of others (e.g.,\ndensity-related features), indicating a trade-off between specialisation and\ngeneralisation. Our findings show that Mammo-CLIP Dissect provides insights\ninto how convolutional neural networks (CNNs) capture mammography-specific\nknowledge. By comparing models across training data and fine-tuning regimes, we\nreveal how domain-specific training and task-specific adaptation shape concept\nlearning. Code and concept set are available:\nhttps://github.com/Suaiba/Mammo-CLIP-Dissect."
                },
                "authors": [
                    {
                        "name": "Suaiba Amina Salahuddin"
                    },
                    {
                        "name": "Teresa Dorszewski"
                    },
                    {
                        "name": "Marit Almenning Martiniussen"
                    },
                    {
                        "name": "Tone Hovda"
                    },
                    {
                        "name": "Antonio Portaluri"
                    },
                    {
                        "name": "Solveig Thrun"
                    },
                    {
                        "name": "Michael Kampffmeyer"
                    },
                    {
                        "name": "Elisabeth Wetzer"
                    },
                    {
                        "name": "Kristoffer Wickstrm"
                    },
                    {
                        "name": "Robert Jenssen"
                    }
                ],
                "author_detail": {
                    "name": "Robert Jenssen"
                },
                "author": "Robert Jenssen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21100v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21100v1",
                "updated": "2025-09-25T12:46:46Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    46,
                    46,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:46:46Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    46,
                    46,
                    3,
                    268,
                    0
                ],
                "title": "VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal\n  Reasoning by Iterative Perception",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VideoChat-R1.5: Visual Test-Time Scaling to Reinforce Multimodal\n  Reasoning by Iterative Perception"
                },
                "summary": "Inducing reasoning in multimodal large language models (MLLMs) is critical\nfor achieving human-level perception and understanding. Existing methods mainly\nleverage LLM reasoning to analyze parsed visuals, often limited by static\nperception stages. This paper introduces Visual Test-Time Scaling (VTTS), a\nnovel approach to enhance MLLMs' reasoning via iterative perception during\ninference. VTTS mimics humans' hierarchical attention by progressively refining\nfocus on high-confidence spatio-temporal regions, guided by updated textual\npredictions. Specifically, VTTS employs an Iterative Perception (ITP)\nmechanism, incorporating reinforcement learning with spatio-temporal\nsupervision to optimize reasoning. To support this paradigm, we also present\nVTTS-80K, a dataset tailored for iterative perception. These designs allows a\nMLLM to enhance its performance by increasing its perceptual compute. Extensive\nexperiments validate VTTS's effectiveness and generalization across diverse\ntasks and benchmarks. Our newly introduced Videochat-R1.5 model has achieved\nremarkable improvements, with an average increase of over 5\\%, compared to\nrobust baselines such as Qwen2.5VL-3B and -7B, across more than 15 benchmarks\nthat encompass video conversation, video reasoning, and spatio-temporal\nperception.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inducing reasoning in multimodal large language models (MLLMs) is critical\nfor achieving human-level perception and understanding. Existing methods mainly\nleverage LLM reasoning to analyze parsed visuals, often limited by static\nperception stages. This paper introduces Visual Test-Time Scaling (VTTS), a\nnovel approach to enhance MLLMs' reasoning via iterative perception during\ninference. VTTS mimics humans' hierarchical attention by progressively refining\nfocus on high-confidence spatio-temporal regions, guided by updated textual\npredictions. Specifically, VTTS employs an Iterative Perception (ITP)\nmechanism, incorporating reinforcement learning with spatio-temporal\nsupervision to optimize reasoning. To support this paradigm, we also present\nVTTS-80K, a dataset tailored for iterative perception. These designs allows a\nMLLM to enhance its performance by increasing its perceptual compute. Extensive\nexperiments validate VTTS's effectiveness and generalization across diverse\ntasks and benchmarks. Our newly introduced Videochat-R1.5 model has achieved\nremarkable improvements, with an average increase of over 5\\%, compared to\nrobust baselines such as Qwen2.5VL-3B and -7B, across more than 15 benchmarks\nthat encompass video conversation, video reasoning, and spatio-temporal\nperception."
                },
                "authors": [
                    {
                        "name": "Ziang Yan"
                    },
                    {
                        "name": "Xinhao Li"
                    },
                    {
                        "name": "Yinan He"
                    },
                    {
                        "name": "Zhengrong Yue"
                    },
                    {
                        "name": "Xiangyu Zeng"
                    },
                    {
                        "name": "Yali Wang"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Limin Wang"
                    },
                    {
                        "name": "Yi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Wang"
                },
                "author": "Yi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21100v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21100v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.11717v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.11717v3",
                "updated": "2025-09-25T12:44:56Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    44,
                    56,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-15T09:12:57Z",
                "published_parsed": [
                    2025,
                    9,
                    15,
                    9,
                    12,
                    57,
                    0,
                    258,
                    0
                ],
                "title": "Neural Audio Codecs for Prompt-Driven Universal Sound Separation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Audio Codecs for Prompt-Driven Universal Sound Separation"
                },
                "summary": "Text-guided sound separation supports flexible audio editing across media and\nassistive applications, but existing models like AudioSep are too compute-heavy\nfor edge deployment. Neural audio codec (NAC) models such as CodecFormer and\nSDCodec are compute-efficient but limited to fixed-class separation. We\nintroduce CodecSep, the first NAC-based model for on-device universal,\ntext-driven separation. CodecSep combines DAC compression with a Transformer\nmasker modulated by CLAP-derived FiLM parameters. Across six open-domain\nbenchmarks under matched training/prompt protocols, \\textbf{CodecSep} surpasses\n\\textbf{AudioSep} in separation fidelity (SI-SDR) while remaining competitive\nin perceptual quality (ViSQOL) and matching or exceeding fixed-stem baselines\n(TDANet, CodecFormer, SDCodec). In code-stream deployments, it needs just\n1.35~GMACs end-to-end -- approximately $54\\times$ less compute ($25\\times$\narchitecture-only) than spectrogram-domain separators like AudioSep -- while\nremaining fully bitstream-compatible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided sound separation supports flexible audio editing across media and\nassistive applications, but existing models like AudioSep are too compute-heavy\nfor edge deployment. Neural audio codec (NAC) models such as CodecFormer and\nSDCodec are compute-efficient but limited to fixed-class separation. We\nintroduce CodecSep, the first NAC-based model for on-device universal,\ntext-driven separation. CodecSep combines DAC compression with a Transformer\nmasker modulated by CLAP-derived FiLM parameters. Across six open-domain\nbenchmarks under matched training/prompt protocols, \\textbf{CodecSep} surpasses\n\\textbf{AudioSep} in separation fidelity (SI-SDR) while remaining competitive\nin perceptual quality (ViSQOL) and matching or exceeding fixed-stem baselines\n(TDANet, CodecFormer, SDCodec). In code-stream deployments, it needs just\n1.35~GMACs end-to-end -- approximately $54\\times$ less compute ($25\\times$\narchitecture-only) than spectrogram-domain separators like AudioSep -- while\nremaining fully bitstream-compatible."
                },
                "authors": [
                    {
                        "name": "Adhiraj Banerjee"
                    },
                    {
                        "name": "Vipul Arora"
                    }
                ],
                "author_detail": {
                    "name": "Vipul Arora"
                },
                "author": "Vipul Arora",
                "arxiv_comment": "main content- 10 pages, total - 23 pages, 1 figure, pre-print, under\n  review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.11717v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.11717v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21091v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21091v1",
                "updated": "2025-09-25T12:41:05Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    41,
                    5,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:41:05Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    41,
                    5,
                    3,
                    268,
                    0
                ],
                "title": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Best-of-$\\infty$ -- Asymptotic Performance of Test-Time Compute"
                },
                "summary": "We study best-of-$N$ for large language models (LLMs) where the selection is\nbased on majority voting. In particular, we analyze the limit $N \\to \\infty$,\nwhich we denote as Best-of-$\\infty$. While this approach achieves impressive\nperformance in the limit, it requires an infinite test-time budget. To address\nthis, we propose an adaptive generation scheme that selects $N$ based on answer\nagreement, thereby efficiently allocating inference-time computation. Beyond\nadaptivity, we extend the framework to weighted ensembles of multiple LLMs,\nshowing that such mixtures can outperform any individual model. The optimal\nensemble weighting is formulated and efficiently computed as a mixed-integer\nlinear program. Extensive experiments demonstrate the effectiveness of our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study best-of-$N$ for large language models (LLMs) where the selection is\nbased on majority voting. In particular, we analyze the limit $N \\to \\infty$,\nwhich we denote as Best-of-$\\infty$. While this approach achieves impressive\nperformance in the limit, it requires an infinite test-time budget. To address\nthis, we propose an adaptive generation scheme that selects $N$ based on answer\nagreement, thereby efficiently allocating inference-time computation. Beyond\nadaptivity, we extend the framework to weighted ensembles of multiple LLMs,\nshowing that such mixtures can outperform any individual model. The optimal\nensemble weighting is formulated and efficiently computed as a mixed-integer\nlinear program. Extensive experiments demonstrate the effectiveness of our\napproach."
                },
                "authors": [
                    {
                        "name": "Junpei Komiyama"
                    },
                    {
                        "name": "Daisuke Oba"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oyamada"
                },
                "author": "Masafumi Oyamada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21091v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21091v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21086v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21086v1",
                "updated": "2025-09-25T12:39:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    39,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:39:06Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    39,
                    6,
                    3,
                    268,
                    0
                ],
                "title": "UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep\n  Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniTransfer: Video Concept Transfer via Progressive Spatial and Timestep\n  Decomposition"
                },
                "summary": "We propose a novel architecture UniTransfer, which introduces both spatial\nand diffusion timestep decomposition in a progressive paradigm, achieving\nprecise and controllable video concept transfer. Specifically, in terms of\nspatial decomposition, we decouple videos into three key components: the\nforeground subject, the background, and the motion flow. Building upon this\ndecomposed formulation, we further introduce a dual-to-single-stream DiT-based\narchitecture for supporting fine-grained control over different components in\nthe videos. We also introduce a self-supervised pretraining strategy based on\nrandom masking to enhance the decomposed representation learning from\nlarge-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning\nparadigm, we further revisit the denoising diffusion process and propose a\nChain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We\ndecompose the denoising process into three stages of different granularity and\nleverage large language models (LLMs) for stage-specific instructions to guide\nthe generation progressively. We also curate an animal-centric video dataset\ncalled OpenAnimal to facilitate the advancement and benchmarking of research in\nvideo concept transfer. Extensive experiments demonstrate that our method\nachieves high-quality and controllable video concept transfer across diverse\nreference images and scenes, surpassing existing baselines in both visual\nfidelity and editability. Web Page:\nhttps://yu-shaonian.github.io/UniTransfer-Web/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a novel architecture UniTransfer, which introduces both spatial\nand diffusion timestep decomposition in a progressive paradigm, achieving\nprecise and controllable video concept transfer. Specifically, in terms of\nspatial decomposition, we decouple videos into three key components: the\nforeground subject, the background, and the motion flow. Building upon this\ndecomposed formulation, we further introduce a dual-to-single-stream DiT-based\narchitecture for supporting fine-grained control over different components in\nthe videos. We also introduce a self-supervised pretraining strategy based on\nrandom masking to enhance the decomposed representation learning from\nlarge-scale unlabeled video data. Inspired by the Chain-of-Thought reasoning\nparadigm, we further revisit the denoising diffusion process and propose a\nChain-of-Prompt (CoP) mechanism to achieve the timestep decomposition. We\ndecompose the denoising process into three stages of different granularity and\nleverage large language models (LLMs) for stage-specific instructions to guide\nthe generation progressively. We also curate an animal-centric video dataset\ncalled OpenAnimal to facilitate the advancement and benchmarking of research in\nvideo concept transfer. Extensive experiments demonstrate that our method\nachieves high-quality and controllable video concept transfer across diverse\nreference images and scenes, surpassing existing baselines in both visual\nfidelity and editability. Web Page:\nhttps://yu-shaonian.github.io/UniTransfer-Web/"
                },
                "authors": [
                    {
                        "name": "Guojun Lei"
                    },
                    {
                        "name": "Rong Zhang"
                    },
                    {
                        "name": "Chi Wang"
                    },
                    {
                        "name": "Tianhang Liu"
                    },
                    {
                        "name": "Hong Li"
                    },
                    {
                        "name": "Zhiyuan Ma"
                    },
                    {
                        "name": "Weiwei Xu"
                    }
                ],
                "author_detail": {
                    "name": "Weiwei Xu"
                },
                "author": "Weiwei Xu",
                "arxiv_comment": "NeuriIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21086v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21086v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21085v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21085v1",
                "updated": "2025-09-25T12:37:54Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    37,
                    54,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:37:54Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    37,
                    54,
                    3,
                    268,
                    0
                ],
                "title": "Flight Dynamics to Sensing Modalities: Exploiting Drone Ground Effect\n  for Accurate Edge Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flight Dynamics to Sensing Modalities: Exploiting Drone Ground Effect\n  for Accurate Edge Detection"
                },
                "summary": "Drone-based rapid and accurate environmental edge detection is highly\nadvantageous for tasks such as disaster relief and autonomous navigation.\nCurrent methods, using radars or cameras, raise deployment costs and burden\nlightweight drones with high computational demands. In this paper, we propose\nAirTouch, a system that transforms the ground effect from a stability \"foe\" in\ntraditional flight control views, into a \"friend\" for accurate and efficient\nedge detection. Our key insight is that analyzing drone basic attitude sensor\nreadings and flight commands allows us to detect ground effect changes. Such\nchanges typically indicate the drone flying over a boundary of two materials,\nmaking this information valuable for edge detection. We approach this insight\nthrough theoretical analysis, algorithm design, and implementation, fully\nleveraging the ground effect as a new sensing modality without compromising\ndrone flight stability, thereby achieving accurate and efficient scene edge\ndetection. We also compare this new sensing modality with vision-based methods\nto clarify its exclusive advantages in resource efficiency and detection\ncapability. Extensive evaluations demonstrate that our system achieves a high\ndetection accuracy with mean detection distance errors of 0.051m, outperforming\nthe baseline method performance by 86%. With such detection performance, our\nsystem requires only 43 mW power consumption, contributing to this new sensing\nmodality for low-cost and highly efficient edge detection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Drone-based rapid and accurate environmental edge detection is highly\nadvantageous for tasks such as disaster relief and autonomous navigation.\nCurrent methods, using radars or cameras, raise deployment costs and burden\nlightweight drones with high computational demands. In this paper, we propose\nAirTouch, a system that transforms the ground effect from a stability \"foe\" in\ntraditional flight control views, into a \"friend\" for accurate and efficient\nedge detection. Our key insight is that analyzing drone basic attitude sensor\nreadings and flight commands allows us to detect ground effect changes. Such\nchanges typically indicate the drone flying over a boundary of two materials,\nmaking this information valuable for edge detection. We approach this insight\nthrough theoretical analysis, algorithm design, and implementation, fully\nleveraging the ground effect as a new sensing modality without compromising\ndrone flight stability, thereby achieving accurate and efficient scene edge\ndetection. We also compare this new sensing modality with vision-based methods\nto clarify its exclusive advantages in resource efficiency and detection\ncapability. Extensive evaluations demonstrate that our system achieves a high\ndetection accuracy with mean detection distance errors of 0.051m, outperforming\nthe baseline method performance by 86%. With such detection performance, our\nsystem requires only 43 mW power consumption, contributing to this new sensing\nmodality for low-cost and highly efficient edge detection."
                },
                "authors": [
                    {
                        "name": "Chenyu Zhao"
                    },
                    {
                        "name": "Jingao Xu"
                    },
                    {
                        "name": "Ciyu Ruan"
                    },
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Shengbo Wang"
                    },
                    {
                        "name": "Jiaqi Li"
                    },
                    {
                        "name": "Jirong Zha"
                    },
                    {
                        "name": "Weijie Hong"
                    },
                    {
                        "name": "Zheng Yang"
                    },
                    {
                        "name": "Yunhao Liu"
                    },
                    {
                        "name": "Xiao-Ping Zhang"
                    },
                    {
                        "name": "Xinlei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei Chen"
                },
                "author": "Xinlei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21085v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21085v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21081v1",
                "updated": "2025-09-25T12:32:02Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    32,
                    2,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:32:02Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    32,
                    2,
                    3,
                    268,
                    0
                ],
                "title": "TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TyphoonMLA: A Mixed Naive-Absorb MLA Kernel For Shared Prefix"
                },
                "summary": "Multi-Head Latent Attention (MLA) is a recent attention mechanism adopted in\nstate-of-the-art LLMs such as DeepSeek-v3 and Kimi K2. Thanks to its novel\nformulation, MLA allows two functionally equivalent but computationally\ndistinct kernel implementations: naive and absorb. While the naive kernels\n(e.g., FlashAttention) are typically preferred in training and prefill for\ntheir computational efficiency, existing decoding kernels (e.g., FlashMLA) rely\non the absorb method to minimize HBM bandwidth usage. However, the\ncompute-bound nature of the absorb implementations prohibits performance\nbenefits from data reuse opportunities in attention calculations, such as\nshared prefixes. In this work, we introduce TyphoonMLA, a hybrid approach that\ncombines naive and absorb formulations to harness the strengths of both.\nTyphoonMLA effectively leverages the shared prefix by applying the naive\nformulation to the compute-bound parts of attention calculations, while\nreducing the bandwidth requirements for non-shared parts by using the absorb\nformulation. As a result, TyphoonMLA improves the throughput of attention\ncalculations in MLA architectures by up to 3x and 3.24x on NPU and GPUs, with\nonly a 3% overhead in HBM size.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Head Latent Attention (MLA) is a recent attention mechanism adopted in\nstate-of-the-art LLMs such as DeepSeek-v3 and Kimi K2. Thanks to its novel\nformulation, MLA allows two functionally equivalent but computationally\ndistinct kernel implementations: naive and absorb. While the naive kernels\n(e.g., FlashAttention) are typically preferred in training and prefill for\ntheir computational efficiency, existing decoding kernels (e.g., FlashMLA) rely\non the absorb method to minimize HBM bandwidth usage. However, the\ncompute-bound nature of the absorb implementations prohibits performance\nbenefits from data reuse opportunities in attention calculations, such as\nshared prefixes. In this work, we introduce TyphoonMLA, a hybrid approach that\ncombines naive and absorb formulations to harness the strengths of both.\nTyphoonMLA effectively leverages the shared prefix by applying the naive\nformulation to the compute-bound parts of attention calculations, while\nreducing the bandwidth requirements for non-shared parts by using the absorb\nformulation. As a result, TyphoonMLA improves the throughput of attention\ncalculations in MLA architectures by up to 3x and 3.24x on NPU and GPUs, with\nonly a 3% overhead in HBM size."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yzgler"
                    },
                    {
                        "name": "Ahmet elik"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.14295v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.14295v3",
                "updated": "2025-09-25T12:30:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    30,
                    29,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-17T02:31:03Z",
                "published_parsed": [
                    2025,
                    9,
                    17,
                    2,
                    31,
                    3,
                    2,
                    260,
                    0
                ],
                "title": "Aegis: Automated Error Generation and Identification for Multi-Agent\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aegis: Automated Error Generation and Identification for Multi-Agent\n  Systems"
                },
                "summary": "As Multi-Agent Systems (MAS) become increasingly autonomous and complex,\nunderstanding their error modes is critical for ensuring their reliability and\nsafety. However, research in this area has been severely hampered by the lack\nof large-scale, diverse datasets with precise, ground-truth error labels. To\naddress this bottleneck, we introduce \\textbf{AEGIS}, a novel framework for\n\\textbf{A}utomated \\textbf{E}rror \\textbf{G}eneration and\n\\textbf{I}dentification for Multi-Agent \\textbf{S}ystems. By systematically\ninjecting controllable and traceable errors into initially successful\ntrajectories, we create a rich dataset of realistic failures. This is achieved\nusing a context-aware, LLM-based adaptive manipulator that performs\nsophisticated attacks like prompt injection and response corruption to induce\nspecific, predefined error modes. We demonstrate the value of our dataset by\nexploring three distinct learning paradigms for the error identification task:\nSupervised Fine-Tuning, Reinforcement Learning, and Contrastive Learning. Our\ncomprehensive experiments show that models trained on AEGIS data achieve\nsubstantial improvements across all three learning paradigms. Notably, several\nof our fine-tuned models demonstrate performance competitive with or superior\nto proprietary systems an order of magnitude larger, validating our automated\ndata generation framework as a crucial resource for developing more robust and\ninterpretable multi-agent systems. Our project website is available at\nhttps://kfq20.github.io/AEGIS-Website.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Multi-Agent Systems (MAS) become increasingly autonomous and complex,\nunderstanding their error modes is critical for ensuring their reliability and\nsafety. However, research in this area has been severely hampered by the lack\nof large-scale, diverse datasets with precise, ground-truth error labels. To\naddress this bottleneck, we introduce \\textbf{AEGIS}, a novel framework for\n\\textbf{A}utomated \\textbf{E}rror \\textbf{G}eneration and\n\\textbf{I}dentification for Multi-Agent \\textbf{S}ystems. By systematically\ninjecting controllable and traceable errors into initially successful\ntrajectories, we create a rich dataset of realistic failures. This is achieved\nusing a context-aware, LLM-based adaptive manipulator that performs\nsophisticated attacks like prompt injection and response corruption to induce\nspecific, predefined error modes. We demonstrate the value of our dataset by\nexploring three distinct learning paradigms for the error identification task:\nSupervised Fine-Tuning, Reinforcement Learning, and Contrastive Learning. Our\ncomprehensive experiments show that models trained on AEGIS data achieve\nsubstantial improvements across all three learning paradigms. Notably, several\nof our fine-tuned models demonstrate performance competitive with or superior\nto proprietary systems an order of magnitude larger, validating our automated\ndata generation framework as a crucial resource for developing more robust and\ninterpretable multi-agent systems. Our project website is available at\nhttps://kfq20.github.io/AEGIS-Website."
                },
                "authors": [
                    {
                        "name": "Fanqi Kong"
                    },
                    {
                        "name": "Ruijie Zhang"
                    },
                    {
                        "name": "Huaxiao Yin"
                    },
                    {
                        "name": "Guibin Zhang"
                    },
                    {
                        "name": "Xiaofei Zhang"
                    },
                    {
                        "name": "Ziang Chen"
                    },
                    {
                        "name": "Zhaowei Zhang"
                    },
                    {
                        "name": "Xiaoyuan Zhang"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Xue Feng"
                    }
                ],
                "author_detail": {
                    "name": "Xue Feng"
                },
                "author": "Xue Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.14295v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.14295v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07522v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07522v2",
                "updated": "2025-09-25T12:30:09Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    30,
                    9,
                    3,
                    268,
                    0
                ],
                "published": "2025-05-12T13:03:26Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    13,
                    3,
                    26,
                    0,
                    132,
                    0
                ],
                "title": "Byam: Fixing Breaking Dependency Updates with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Byam: Fixing Breaking Dependency Updates with Large Language Models"
                },
                "summary": "Application Programming Interfaces (APIs) facilitate the integration of\nthird-party dependencies within the code of client applications. However,\nchanges to an API, such as deprecation, modification of parameter names or\ntypes, or complete replacement with a new API, can break existing client code.\nThese changes are called breaking dependency updates; It is often tedious for\nAPI users to identify the cause of these breaks and update their code\naccordingly. In this paper, we explore the use of Large Language Models (LLMs)\nto automate client code updates in response to breaking dependency updates. We\nevaluate our approach on the BUMP dataset, a benchmark for breaking dependency\nupdates in Java projects. Our approach leverages LLMs with advanced prompts,\nincluding information from the build process and from the breaking dependency\nanalysis. We assess effectiveness at three granularity levels: at the build\nlevel, the file level, and the individual compilation error level. We\nexperiment with five LLMs: Google Gemini-2.0 Flash, OpenAI GPT4o-mini, OpenAI\no3-mini, Alibaba Qwen2.5-32b-instruct, and DeepSeek V3. Our results show that\nLLMs can automatically repair breaking updates. Among the considered models,\nOpenAI's o3-mini is the best, able to completely fix 27% of the builds when\nusing prompts that include contextual information such as the buggy line, API\ndifferences, error messages, and step-by-step reasoning instructions. Also, it\nfixes 78% of the individual compilation errors. Overall, our findings\ndemonstrate the potential for LLMs to fix compilation errors due to breaking\ndependency updates, supporting developers in their efforts to stay up-to-date\nwith changes in their dependencies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application Programming Interfaces (APIs) facilitate the integration of\nthird-party dependencies within the code of client applications. However,\nchanges to an API, such as deprecation, modification of parameter names or\ntypes, or complete replacement with a new API, can break existing client code.\nThese changes are called breaking dependency updates; It is often tedious for\nAPI users to identify the cause of these breaks and update their code\naccordingly. In this paper, we explore the use of Large Language Models (LLMs)\nto automate client code updates in response to breaking dependency updates. We\nevaluate our approach on the BUMP dataset, a benchmark for breaking dependency\nupdates in Java projects. Our approach leverages LLMs with advanced prompts,\nincluding information from the build process and from the breaking dependency\nanalysis. We assess effectiveness at three granularity levels: at the build\nlevel, the file level, and the individual compilation error level. We\nexperiment with five LLMs: Google Gemini-2.0 Flash, OpenAI GPT4o-mini, OpenAI\no3-mini, Alibaba Qwen2.5-32b-instruct, and DeepSeek V3. Our results show that\nLLMs can automatically repair breaking updates. Among the considered models,\nOpenAI's o3-mini is the best, able to completely fix 27% of the builds when\nusing prompts that include contextual information such as the buggy line, API\ndifferences, error messages, and step-by-step reasoning instructions. Also, it\nfixes 78% of the individual compilation errors. Overall, our findings\ndemonstrate the potential for LLMs to fix compilation errors due to breaking\ndependency updates, supporting developers in their efforts to stay up-to-date\nwith changes in their dependencies."
                },
                "authors": [
                    {
                        "name": "Frank Reyes"
                    },
                    {
                        "name": "May Mahmoud"
                    },
                    {
                        "name": "Federico Bono"
                    },
                    {
                        "name": "Sarah Nadi"
                    },
                    {
                        "name": "Benoit Baudry"
                    },
                    {
                        "name": "Martin Monperrus"
                    }
                ],
                "author_detail": {
                    "name": "Martin Monperrus"
                },
                "author": "Martin Monperrus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07522v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07522v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13250v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13250v2",
                "updated": "2025-09-25T12:29:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    29,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-06-16T08:46:44Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    8,
                    46,
                    44,
                    0,
                    167,
                    0
                ],
                "title": "Fluid Antenna Systems Meet Low-Altitude Wireless Networks: Fundamentals,\n  Opportunities, and Future Directions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluid Antenna Systems Meet Low-Altitude Wireless Networks: Fundamentals,\n  Opportunities, and Future Directions"
                },
                "summary": "Low-altitude wireless networks (LAWNs) are widely regarded as a cornerstone\nof the emerging low-altitude economy, yet they face significant challenges,\nincluding rapidly varying channels, diverse functional requirements, and\ndynamic interference environments. Fluid antenna (FA) systems introduce spatial\nreconfigurability that complements and extends conventional beamforming,\nenabling flexible exploitation of spatial diversity and adaptive response to\nchannel variations. This paper proposes a novel architecture for FA-empowered\nLAWNs and presents a case study demonstrating substantial improvements in\ncommunication, sensing, and control performance compared to fixed-position\nantenna (FPA) systems. Key practical deployment considerations are examined,\nincluding mechanical design, position control, energy efficiency, and\ncompliance with emerging industry standards. In addition, several future\nresearch directions are highlighted, including intelligent optimization,\nmulti-function integration, and the exploration of novel low-altitude\napplications. By integrating theoretical analysis with practical deployment\nperspectives, this paper establishes FA systems as a key enabler for adaptive,\nefficient, and resilient network infrastructures in next-generation LAWNs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-altitude wireless networks (LAWNs) are widely regarded as a cornerstone\nof the emerging low-altitude economy, yet they face significant challenges,\nincluding rapidly varying channels, diverse functional requirements, and\ndynamic interference environments. Fluid antenna (FA) systems introduce spatial\nreconfigurability that complements and extends conventional beamforming,\nenabling flexible exploitation of spatial diversity and adaptive response to\nchannel variations. This paper proposes a novel architecture for FA-empowered\nLAWNs and presents a case study demonstrating substantial improvements in\ncommunication, sensing, and control performance compared to fixed-position\nantenna (FPA) systems. Key practical deployment considerations are examined,\nincluding mechanical design, position control, energy efficiency, and\ncompliance with emerging industry standards. In addition, several future\nresearch directions are highlighted, including intelligent optimization,\nmulti-function integration, and the exploration of novel low-altitude\napplications. By integrating theoretical analysis with practical deployment\nperspectives, this paper establishes FA systems as a key enabler for adaptive,\nefficient, and resilient network infrastructures in next-generation LAWNs."
                },
                "authors": [
                    {
                        "name": "Wenchao Liu"
                    },
                    {
                        "name": "Xuhui Zhang"
                    },
                    {
                        "name": "Chunjie Wang"
                    },
                    {
                        "name": "Jinke Ren"
                    },
                    {
                        "name": "Weijie Yuan"
                    },
                    {
                        "name": "Changsheng You"
                    }
                ],
                "author_detail": {
                    "name": "Changsheng You"
                },
                "author": "Changsheng You",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13250v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13250v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21080v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21080v1",
                "updated": "2025-09-25T12:28:25Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    28,
                    25,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:28:25Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    28,
                    25,
                    3,
                    268,
                    0
                ],
                "title": "Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and\n  Agentic Mitigation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Cultural Lens Do Models Adopt? On Cultural Positioning Bias and\n  Agentic Mitigation in LLMs"
                },
                "summary": "Large language models (LLMs) have unlocked a wide range of downstream\ngenerative applications. However, we found that they also risk perpetuating\nsubtle fairness issues tied to culture, positioning their generations from the\nperspectives of the mainstream US culture while demonstrating salient\nexternality towards non-mainstream ones. In this work, we identify and\nsystematically investigate this novel culture positioning bias, in which an\nLLM's default generative stance aligns with a mainstream view and treats other\ncultures as outsiders. We propose the CultureLens benchmark with 4000\ngeneration prompts and 3 evaluation metrics for quantifying this bias through\nthe lens of a culturally situated interview script generation task, in which an\nLLM is positioned as an onsite reporter interviewing local people across 10\ndiverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a\nstark pattern: while models adopt insider tones in over 88 percent of\nUS-contexted scripts on average, they disproportionately adopt mainly outsider\nstances for less dominant cultures. To resolve these biases, we propose 2\ninference-time mitigation methods: a baseline prompt-based Fairness\nIntervention Pillars (FIP) method, and a structured Mitigation via Fairness\nAgents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent)\nintroduces a self-reflection and rewriting loop based on fairness guidelines.\n(2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized\nagents: a Planner Agent(initial script generation), a Critique Agent (evaluates\ninitial script against fairness pillars), and a Refinement Agent (incorporates\nfeedback to produce a polished, unbiased script). Empirical results showcase\nthe effectiveness of agent-based methods as a promising direction for\nmitigating biases in generative LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have unlocked a wide range of downstream\ngenerative applications. However, we found that they also risk perpetuating\nsubtle fairness issues tied to culture, positioning their generations from the\nperspectives of the mainstream US culture while demonstrating salient\nexternality towards non-mainstream ones. In this work, we identify and\nsystematically investigate this novel culture positioning bias, in which an\nLLM's default generative stance aligns with a mainstream view and treats other\ncultures as outsiders. We propose the CultureLens benchmark with 4000\ngeneration prompts and 3 evaluation metrics for quantifying this bias through\nthe lens of a culturally situated interview script generation task, in which an\nLLM is positioned as an onsite reporter interviewing local people across 10\ndiverse cultures. Empirical evaluation on 5 state-of-the-art LLMs reveals a\nstark pattern: while models adopt insider tones in over 88 percent of\nUS-contexted scripts on average, they disproportionately adopt mainly outsider\nstances for less dominant cultures. To resolve these biases, we propose 2\ninference-time mitigation methods: a baseline prompt-based Fairness\nIntervention Pillars (FIP) method, and a structured Mitigation via Fairness\nAgents (MFA) framework consisting of 2 pipelines: (1) MFA-SA (Single-Agent)\nintroduces a self-reflection and rewriting loop based on fairness guidelines.\n(2) MFA-MA (Multi-Agent) structures the process into a hierarchy of specialized\nagents: a Planner Agent(initial script generation), a Critique Agent (evaluates\ninitial script against fairness pillars), and a Refinement Agent (incorporates\nfeedback to produce a polished, unbiased script). Empirical results showcase\nthe effectiveness of agent-based methods as a promising direction for\nmitigating biases in generative LLMs."
                },
                "authors": [
                    {
                        "name": "Yixin Wan"
                    },
                    {
                        "name": "Xingrun Chen"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Wei Chang"
                },
                "author": "Kai-Wei Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21080v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21080v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21079v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21079v1",
                "updated": "2025-09-25T12:28:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    28,
                    22,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:28:22Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    28,
                    22,
                    3,
                    268,
                    0
                ],
                "title": "SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SoM-1K: A Thousand-Problem Benchmark Dataset for Strength of Materials"
                },
                "summary": "Foundation models have shown remarkable capabilities in various domains, but\ntheir performance on complex, multimodal engineering problems remains largely\nunexplored. We introduce SoM-1K, the first large-scale multimodal benchmark\ndataset dedicated to evaluating foundation models on problems in the strength\nof materials (SoM). The dataset, which contains 1,065 annotated SoM problems,\nmirrors real-world engineering tasks by including both textual problem\nstatements and schematic diagrams. Due to the limited capabilities of current\nfoundation models in understanding complicated visual information, we propose a\nnovel prompting strategy called Descriptions of Images (DoI), which provides\nrigorous expert-generated text descriptions of the visual diagrams as the\ncontext. We evaluate eight representative foundation models, including both\nlarge language models (LLMs) and vision language models (VLMs). Our results\nshow that current foundation models struggle significantly with these\nengineering problems, with the best-performing model achieving only 56.6%\naccuracy. Interestingly, we found that LLMs, when provided with DoI, often\noutperform VLMs provided with visual diagrams. A detailed error analysis\nreveals that DoI plays a crucial role in mitigating visual misinterpretation\nerrors, suggesting that accurate text-based descriptions can be more effective\nthan direct image input for current foundation models. This work establishes a\nrigorous benchmark for engineering AI and highlights a critical need for\ndeveloping more robust multimodal reasoning capabilities in foundation models,\nparticularly in scientific and engineering contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models have shown remarkable capabilities in various domains, but\ntheir performance on complex, multimodal engineering problems remains largely\nunexplored. We introduce SoM-1K, the first large-scale multimodal benchmark\ndataset dedicated to evaluating foundation models on problems in the strength\nof materials (SoM). The dataset, which contains 1,065 annotated SoM problems,\nmirrors real-world engineering tasks by including both textual problem\nstatements and schematic diagrams. Due to the limited capabilities of current\nfoundation models in understanding complicated visual information, we propose a\nnovel prompting strategy called Descriptions of Images (DoI), which provides\nrigorous expert-generated text descriptions of the visual diagrams as the\ncontext. We evaluate eight representative foundation models, including both\nlarge language models (LLMs) and vision language models (VLMs). Our results\nshow that current foundation models struggle significantly with these\nengineering problems, with the best-performing model achieving only 56.6%\naccuracy. Interestingly, we found that LLMs, when provided with DoI, often\noutperform VLMs provided with visual diagrams. A detailed error analysis\nreveals that DoI plays a crucial role in mitigating visual misinterpretation\nerrors, suggesting that accurate text-based descriptions can be more effective\nthan direct image input for current foundation models. This work establishes a\nrigorous benchmark for engineering AI and highlights a critical need for\ndeveloping more robust multimodal reasoning capabilities in foundation models,\nparticularly in scientific and engineering contexts."
                },
                "authors": [
                    {
                        "name": "Qixin Wan"
                    },
                    {
                        "name": "Zilong Wang"
                    },
                    {
                        "name": "Jingwen Zhou"
                    },
                    {
                        "name": "Wanting Wang"
                    },
                    {
                        "name": "Ziheng Geng"
                    },
                    {
                        "name": "Jiachen Liu"
                    },
                    {
                        "name": "Ran Cao"
                    },
                    {
                        "name": "Minghui Cheng"
                    },
                    {
                        "name": "Lu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Lu Cheng"
                },
                "author": "Lu Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21079v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21079v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01403v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01403v4",
                "updated": "2025-09-25T12:28:19Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    28,
                    19,
                    3,
                    268,
                    0
                ],
                "published": "2025-02-03T14:34:37Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    14,
                    34,
                    37,
                    0,
                    34,
                    0
                ],
                "title": "AdaSVD: Adaptive Singular Value Decomposition for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdaSVD: Adaptive Singular Value Decomposition for Large Language Models"
                },
                "summary": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP) tasks, yet their substantial memory requirements\npresent significant challenges for deployment on resource-constrained devices.\nSingular Value Decomposition (SVD) has emerged as a promising compression\ntechnique for LLMs, offering considerable reductions in memory overhead.\nHowever, existing SVD-based methods often struggle to effectively mitigate the\nerrors introduced by SVD truncation, leading to a noticeable performance gap\nwhen compared to the original models. Furthermore, applying a uniform\ncompression ratio across all transformer layers fails to account for the\nvarying importance of different layers. To address these challenges, we propose\nAdaSVD, an adaptive SVD-based LLM compression approach. Specifically, AdaSVD\nintroduces adaComp, which adaptively compensates for SVD truncation errors by\nalternately updating the singular matrices $\\mathcal{U}$ and\n$\\mathcal{V}^\\top$. Additionally, AdaSVD introduces adaCR, which adaptively\nassigns layer-specific compression ratios based on the relative importance of\neach layer. Extensive experiments across multiple LLM/VLM families and\nevaluation metrics demonstrate that AdaSVD consistently outperforms\nstate-of-the-art (SOTA) SVD-based methods, achieving superior performance with\nsignificantly reduced memory requirements. Code and models of AdaSVD will be\navailable at https://github.com/ZHITENGLI/AdaSVD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have achieved remarkable success in natural\nlanguage processing (NLP) tasks, yet their substantial memory requirements\npresent significant challenges for deployment on resource-constrained devices.\nSingular Value Decomposition (SVD) has emerged as a promising compression\ntechnique for LLMs, offering considerable reductions in memory overhead.\nHowever, existing SVD-based methods often struggle to effectively mitigate the\nerrors introduced by SVD truncation, leading to a noticeable performance gap\nwhen compared to the original models. Furthermore, applying a uniform\ncompression ratio across all transformer layers fails to account for the\nvarying importance of different layers. To address these challenges, we propose\nAdaSVD, an adaptive SVD-based LLM compression approach. Specifically, AdaSVD\nintroduces adaComp, which adaptively compensates for SVD truncation errors by\nalternately updating the singular matrices $\\mathcal{U}$ and\n$\\mathcal{V}^\\top$. Additionally, AdaSVD introduces adaCR, which adaptively\nassigns layer-specific compression ratios based on the relative importance of\neach layer. Extensive experiments across multiple LLM/VLM families and\nevaluation metrics demonstrate that AdaSVD consistently outperforms\nstate-of-the-art (SOTA) SVD-based methods, achieving superior performance with\nsignificantly reduced memory requirements. Code and models of AdaSVD will be\navailable at https://github.com/ZHITENGLI/AdaSVD."
                },
                "authors": [
                    {
                        "name": "Zhiteng Li"
                    },
                    {
                        "name": "Mingyuan Xia"
                    },
                    {
                        "name": "Jingyuan Zhang"
                    },
                    {
                        "name": "Zheng Hui"
                    },
                    {
                        "name": "Haotong Qin"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Yulun Zhang"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "The code and models will be available at\n  https://github.com/ZHITENGLI/AdaSVD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01403v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01403v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.10248v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.10248v3",
                "updated": "2025-09-25T12:26:58Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    26,
                    58,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-12T13:45:24Z",
                "published_parsed": [
                    2025,
                    9,
                    12,
                    13,
                    45,
                    24,
                    4,
                    255,
                    0
                ],
                "title": "Prompt Injection Attacks on LLM Generated Reviews of Scientific\n  Publications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt Injection Attacks on LLM Generated Reviews of Scientific\n  Publications"
                },
                "summary": "The ongoing intense discussion on rising LLM usage in the scientific\npeer-review process has recently been mingled by reports of authors using\nhidden prompt injections to manipulate review scores. Since the existence of\nsuch \"attacks\" - although seen by some commentators as \"self-defense\" - would\nhave a great impact on the further debate, this paper investigates the\npracticability and technical success of the described manipulations. Our\nsystematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide\nrange of LLMs shows two distinct results: I) very simple prompt injections are\nindeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews\nare generally biased toward acceptance (>95% in many models). Both results have\ngreat impact on the ongoing discussions on LLM usage in peer-review.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ongoing intense discussion on rising LLM usage in the scientific\npeer-review process has recently been mingled by reports of authors using\nhidden prompt injections to manipulate review scores. Since the existence of\nsuch \"attacks\" - although seen by some commentators as \"self-defense\" - would\nhave a great impact on the further debate, this paper investigates the\npracticability and technical success of the described manipulations. Our\nsystematic evaluation uses 1k reviews of 2024 ICLR papers generated by a wide\nrange of LLMs shows two distinct results: I) very simple prompt injections are\nindeed highly effective, reaching up to 100% acceptance scores. II) LLM reviews\nare generally biased toward acceptance (>95% in many models). Both results have\ngreat impact on the ongoing discussions on LLM usage in peer-review."
                },
                "authors": [
                    {
                        "name": "Janis Keuper"
                    }
                ],
                "author_detail": {
                    "name": "Janis Keuper"
                },
                "author": "Janis Keuper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.10248v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.10248v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21075v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21075v1",
                "updated": "2025-09-25T12:25:06Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    25,
                    6,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:25:06Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    25,
                    6,
                    3,
                    268,
                    0
                ],
                "title": "Communication Bias in Large Language Models: A Regulatory Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication Bias in Large Language Models: A Regulatory Perspective"
                },
                "summary": "Large language models (LLMs) are increasingly central to many applications,\nraising concerns about bias, fairness, and regulatory compliance. This paper\nreviews risks of biased outputs and their societal impact, focusing on\nframeworks like the EU's AI Act and the Digital Services Act. We argue that\nbeyond constant regulation, stronger attention to competition and design\ngovernance is needed to ensure fair, trustworthy AI. This is a preprint of the\nCommunications of the ACM article of the same title.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly central to many applications,\nraising concerns about bias, fairness, and regulatory compliance. This paper\nreviews risks of biased outputs and their societal impact, focusing on\nframeworks like the EU's AI Act and the Digital Services Act. We argue that\nbeyond constant regulation, stronger attention to competition and design\ngovernance is needed to ensure fair, trustworthy AI. This is a preprint of the\nCommunications of the ACM article of the same title."
                },
                "authors": [
                    {
                        "name": "Adrian Kuenzler"
                    },
                    {
                        "name": "Stefan Schmid"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Schmid"
                },
                "author": "Stefan Schmid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21075v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21075v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21074v1",
                "updated": "2025-09-25T12:24:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    24,
                    32,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:24:32Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    24,
                    32,
                    3,
                    268,
                    0
                ],
                "title": "RePro: Leveraging Large Language Models for Semi-Automated Reproduction\n  of Networking Research Results",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RePro: Leveraging Large Language Models for Semi-Automated Reproduction\n  of Networking Research Results"
                },
                "summary": "Reproducing networking research is a critical but challenging task due to the\nscarcity of open-source code. While Large Language Models (LLMs) can automate\ncode generation, current approaches lack the generalizability required for the\ndiverse networking field. To address this, we propose RePro, a semi-automated\nreproduction framework that leverages advanced prompt engineering to reproduce\nnetwork systems from their research papers. RePro combines few-shot in-context\nlearning with Structured and Semantic Chain of Thought (SCoT/SeCoT) techniques\nto systematically translate a paper's description into an optimized, executable\nimplementation. The framework operates through a three-stage pipeline: system\ndescription extraction, structural code generation, and code optimization. Our\nevaluation with five state-of-the-art LLMs across diverse network sub-domains\ndemonstrates that RePro significantly reduces reproduction time compared to\nmanual efforts while achieving comparable system performance, validating its\neffectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducing networking research is a critical but challenging task due to the\nscarcity of open-source code. While Large Language Models (LLMs) can automate\ncode generation, current approaches lack the generalizability required for the\ndiverse networking field. To address this, we propose RePro, a semi-automated\nreproduction framework that leverages advanced prompt engineering to reproduce\nnetwork systems from their research papers. RePro combines few-shot in-context\nlearning with Structured and Semantic Chain of Thought (SCoT/SeCoT) techniques\nto systematically translate a paper's description into an optimized, executable\nimplementation. The framework operates through a three-stage pipeline: system\ndescription extraction, structural code generation, and code optimization. Our\nevaluation with five state-of-the-art LLMs across diverse network sub-domains\ndemonstrates that RePro significantly reduces reproduction time compared to\nmanual efforts while achieving comparable system performance, validating its\neffectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yining Jiang"
                    },
                    {
                        "name": "Wenyun Xu"
                    },
                    {
                        "name": "Qingyu Song"
                    },
                    {
                        "name": "Yuling Lin"
                    },
                    {
                        "name": "Xuanhao Liu"
                    },
                    {
                        "name": "Xiaoqiang Zheng"
                    },
                    {
                        "name": "Qiang Su"
                    },
                    {
                        "name": "Lizhao You"
                    },
                    {
                        "name": "Lu Tang"
                    },
                    {
                        "name": "Wangjian Feng"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Qiao Xiang"
                    },
                    {
                        "name": "Jiwu Shu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwu Shu"
                },
                "author": "Jiwu Shu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.16949v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.16949v3",
                "updated": "2025-09-25T12:24:28Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    24,
                    28,
                    3,
                    268,
                    0
                ],
                "published": "2025-08-23T08:47:31Z",
                "published_parsed": [
                    2025,
                    8,
                    23,
                    8,
                    47,
                    31,
                    5,
                    235,
                    0
                ],
                "title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning"
                },
                "summary": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the Best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3. Our code is available at\nhttps://github.com/IANNXANG/RuscaRL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the Best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3. Our code is available at\nhttps://github.com/IANNXANG/RuscaRL."
                },
                "authors": [
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Sunzhu Li"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Wenkai Fang"
                    },
                    {
                        "name": "Kongcheng Zhang"
                    },
                    {
                        "name": "Jiale Zhao"
                    },
                    {
                        "name": "Jingwen Yang"
                    },
                    {
                        "name": "Yihe Zhou"
                    },
                    {
                        "name": "Jianwei Lv"
                    },
                    {
                        "name": "Tongya Zheng"
                    },
                    {
                        "name": "Hengtong Lu"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Yan Xie"
                    },
                    {
                        "name": "Mingli Song"
                    }
                ],
                "author_detail": {
                    "name": "Mingli Song"
                },
                "author": "Mingli Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.16949v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.16949v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21067v1",
                "updated": "2025-09-25T12:18:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    18,
                    20,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:18:20Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    18,
                    20,
                    3,
                    268,
                    0
                ],
                "title": "Designing for Novice Debuggers: A Pilot Study on an AI-Assisted\n  Debugging Tool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing for Novice Debuggers: A Pilot Study on an AI-Assisted\n  Debugging Tool"
                },
                "summary": "Debugging is a fundamental skill that novice programmers must develop.\nNumerous tools have been created to assist novice programmers in this process.\nRecently, large language models (LLMs) have been integrated with automated\nprogram repair techniques to generate fixes for students' buggy code. However,\nmany of these tools foster an over-reliance on AI and do not actively engage\nstudents in the debugging process. In this work, we aim to design an intuitive\ndebugging assistant, CodeHinter, that combines traditional debugging tools with\nLLM-based techniques to help novice debuggers fix semantic errors while\npromoting active engagement in the debugging process. We present findings from\nour second design iteration, which we tested with a group of undergraduate\nstudents. Our results indicate that the students found the tool highly\neffective in resolving semantic errors and significantly easier to use than the\nfirst version. Consistent with our previous study, error localization was the\nmost valuable feature. Finally, we conclude that any AI-assisted debugging tool\nshould be personalized based on user profiles to optimize their interactions\nwith students.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Debugging is a fundamental skill that novice programmers must develop.\nNumerous tools have been created to assist novice programmers in this process.\nRecently, large language models (LLMs) have been integrated with automated\nprogram repair techniques to generate fixes for students' buggy code. However,\nmany of these tools foster an over-reliance on AI and do not actively engage\nstudents in the debugging process. In this work, we aim to design an intuitive\ndebugging assistant, CodeHinter, that combines traditional debugging tools with\nLLM-based techniques to help novice debuggers fix semantic errors while\npromoting active engagement in the debugging process. We present findings from\nour second design iteration, which we tested with a group of undergraduate\nstudents. Our results indicate that the students found the tool highly\neffective in resolving semantic errors and significantly easier to use than the\nfirst version. Consistent with our previous study, error localization was the\nmost valuable feature. Finally, we conclude that any AI-assisted debugging tool\nshould be personalized based on user profiles to optimize their interactions\nwith students."
                },
                "authors": [
                    {
                        "name": "Oka Kurniawan"
                    },
                    {
                        "name": "Erick Chandra"
                    },
                    {
                        "name": "Christopher M. Poskitt"
                    },
                    {
                        "name": "Yannic Noller"
                    },
                    {
                        "name": "Kenny Tsu Wei Choo"
                    },
                    {
                        "name": "Cyrille Jegourel"
                    }
                ],
                "author_detail": {
                    "name": "Cyrille Jegourel"
                },
                "author": "Cyrille Jegourel",
                "arxiv_comment": "Accepted by the 25th Koli Calling International Conference on\n  Computing Education Research (Koli Calling 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20317v2",
                "updated": "2025-09-25T12:17:01Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    17,
                    1,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-24T17:01:32Z",
                "published_parsed": [
                    2025,
                    9,
                    24,
                    17,
                    1,
                    32,
                    2,
                    267,
                    0
                ],
                "title": "SIM-CoT: Supervised Implicit Chain-of-Thought",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIM-CoT: Supervised Implicit Chain-of-Thought"
                },
                "summary": "Implicit Chain-of-Thought (CoT) methods offer a token-efficient alternative\nto explicit CoT reasoning in Large Language Models (LLMs), but a persistent\nperformance gap has limited their adoption. We identify a core latent\ninstability issue when scaling the computational budget of implicit CoT: as the\nnumber of reasoning tokens increases, training often becomes unstable and\ncollapses. Our analysis shows that this instability arises from latent\nrepresentations becoming homogeneous and losing semantic diversity, caused by\ninsufficient step-level supervision in current implicit CoT methods. To address\nthis, we propose SIM-CoT, a plug-and-play training module that introduces\nstep-level supervision to stabilize and enrich the latent reasoning space.\nSIM-CoT employs an auxiliary decoder during training to align each implicit\ntoken with its corresponding explicit reasoning step, ensuring latent states\ncapture distinct and meaningful information. The auxiliary decoder is removed\nat inference, preserving the efficiency of implicit CoT with no added overhead.\nIt also provides interpretability by projecting each latent token onto an\nexplicit reasoning vocabulary, enabling per-step visualization and diagnosis.\nSIM-CoT significantly improves both in-domain accuracy and out-of-domain\nstability of implicit CoT methods, boosting Coconut by +8.2\\% on GPT-2 and CODI\nby +3.0\\% on LLaMA-3.1 8B. It further surpasses the explicit CoT baseline on\nGPT-2 by 2.1\\% with 2.3$\\times$ greater token efficiency, while closing the\nperformance gap on larger models like LLaMA-3.1 8B. Code:\nhttps://github.com/InternLM/SIM-CoT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Chain-of-Thought (CoT) methods offer a token-efficient alternative\nto explicit CoT reasoning in Large Language Models (LLMs), but a persistent\nperformance gap has limited their adoption. We identify a core latent\ninstability issue when scaling the computational budget of implicit CoT: as the\nnumber of reasoning tokens increases, training often becomes unstable and\ncollapses. Our analysis shows that this instability arises from latent\nrepresentations becoming homogeneous and losing semantic diversity, caused by\ninsufficient step-level supervision in current implicit CoT methods. To address\nthis, we propose SIM-CoT, a plug-and-play training module that introduces\nstep-level supervision to stabilize and enrich the latent reasoning space.\nSIM-CoT employs an auxiliary decoder during training to align each implicit\ntoken with its corresponding explicit reasoning step, ensuring latent states\ncapture distinct and meaningful information. The auxiliary decoder is removed\nat inference, preserving the efficiency of implicit CoT with no added overhead.\nIt also provides interpretability by projecting each latent token onto an\nexplicit reasoning vocabulary, enabling per-step visualization and diagnosis.\nSIM-CoT significantly improves both in-domain accuracy and out-of-domain\nstability of implicit CoT methods, boosting Coconut by +8.2\\% on GPT-2 and CODI\nby +3.0\\% on LLaMA-3.1 8B. It further surpasses the explicit CoT baseline on\nGPT-2 by 2.1\\% with 2.3$\\times$ greater token efficiency, while closing the\nperformance gap on larger models like LLaMA-3.1 8B. Code:\nhttps://github.com/InternLM/SIM-CoT"
                },
                "authors": [
                    {
                        "name": "Xilin Wei"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Yuhang Zang"
                    },
                    {
                        "name": "Xiaoyi Dong"
                    },
                    {
                        "name": "Yuhang Cao"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Xipeng Qiu"
                    },
                    {
                        "name": "Dahua Lin"
                    }
                ],
                "author_detail": {
                    "name": "Dahua Lin"
                },
                "author": "Dahua Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21065v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21065v1",
                "updated": "2025-09-25T12:16:29Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    16,
                    29,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:16:29Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    16,
                    29,
                    3,
                    268,
                    0
                ],
                "title": "A 698 nm laser system for excitation of fluorescent quantum light\n  sources on a CubeSat mission",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A 698 nm laser system for excitation of fluorescent quantum light\n  sources on a CubeSat mission"
                },
                "summary": "This manuscript reports on the development and qualification of an\nECDL-based, fiber-coupled laser system at a wavelength of {\\lambda} = 698 nm\nfor space applications. We designed and developed the optical and mechanical\nconfiguration, along with the laser driving and thermal management electronics,\nto meet space compatibility requirements. Validation tests were conducted on\noff-the-shelf components to assess their suitability for satellite deployment.\nThe final system integrates all components into a compact design optimized for\nCubeSat platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This manuscript reports on the development and qualification of an\nECDL-based, fiber-coupled laser system at a wavelength of {\\lambda} = 698 nm\nfor space applications. We designed and developed the optical and mechanical\nconfiguration, along with the laser driving and thermal management electronics,\nto meet space compatibility requirements. Validation tests were conducted on\noff-the-shelf components to assess their suitability for satellite deployment.\nThe final system integrates all components into a compact design optimized for\nCubeSat platforms."
                },
                "authors": [
                    {
                        "name": "Sven Schwertfeger"
                    },
                    {
                        "name": "Elisa Da Ros"
                    },
                    {
                        "name": "Marcel Bursy"
                    },
                    {
                        "name": "Andreas Wicht"
                    },
                    {
                        "name": "Daniel Pardo"
                    },
                    {
                        "name": "Ankush Sharma"
                    },
                    {
                        "name": "Subash Sachidananda"
                    },
                    {
                        "name": "Alexander Ling"
                    },
                    {
                        "name": "Markus Krutzik"
                    }
                ],
                "author_detail": {
                    "name": "Markus Krutzik"
                },
                "author": "Markus Krutzik",
                "arxiv_comment": "10 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21065v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21065v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21062v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21062v1",
                "updated": "2025-09-25T12:12:35Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    12,
                    35,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:12:35Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    12,
                    35,
                    3,
                    268,
                    0
                ],
                "title": "Design and deployment of a fast neural network for measuring the\n  properties of muons originating from displaced vertices in the CMS Endcap\n  Muon Track Finder",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and deployment of a fast neural network for measuring the\n  properties of muons originating from displaced vertices in the CMS Endcap\n  Muon Track Finder"
                },
                "summary": "We report on the development, implementation, and performance of a fast\nneural network used to measure the transverse momentum in the CMS Level-1\nEndcap Muon Track Finder. The network aims to improve the triggering efficiency\nof muons produced in the decays of long-lived particles (LLPs). We implemented\nit in firmware for a Xilinx Virtex-7 FPGA and deployed it during the LHC Run 3\ndata-taking in 2023. The new displaced muon triggers that use this algorithm\nbroaden the phase space accessible to the CMS experiment for searches that look\nfor evidence of LLPs that decay into muons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on the development, implementation, and performance of a fast\nneural network used to measure the transverse momentum in the CMS Level-1\nEndcap Muon Track Finder. The network aims to improve the triggering efficiency\nof muons produced in the decays of long-lived particles (LLPs). We implemented\nit in firmware for a Xilinx Virtex-7 FPGA and deployed it during the LHC Run 3\ndata-taking in 2023. The new displaced muon triggers that use this algorithm\nbroaden the phase space accessible to the CMS experiment for searches that look\nfor evidence of LLPs that decay into muons."
                },
                "authors": [
                    {
                        "name": "Efe Yigitbasi"
                    }
                ],
                "author_detail": {
                    "name": "Efe Yigitbasi"
                },
                "arxiv_affiliation": "on behalf of CMS Collaboration",
                "author": "Efe Yigitbasi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21062v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21062v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09487v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09487v2",
                "updated": "2025-09-25T12:11:20Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    11,
                    20,
                    3,
                    268,
                    0
                ],
                "published": "2025-02-13T16:52:06Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    16,
                    52,
                    6,
                    3,
                    44,
                    0
                ],
                "title": "Quantifying depressive mental states with large language models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying depressive mental states with large language models"
                },
                "summary": "Large Language Models (LLMs) may have an important role to play in mental\nhealth by facilitating the quantification of verbal expressions used to\ncommunicate emotions, feelings and thoughts. While there has been substantial\nand very promising work in this area, the fundamental limits are uncertain.\nHere, focusing on depressive symptoms, we outline and evaluate LLM performance\non three critical tests. The first test evaluates LLM performance on a novel\nground-truth dataset from a large human sample (n=770). This dataset is novel\nas it contains both standard clinically validated quantifications of depression\nsymptoms and specific verbal descriptions of the thoughts related to each\nsymptom by the same individual. The performance of LLMs on this richly\ninformative data shows an upper bound on the performance in this domain, and\nallow us to examine the extent to which inference about symptoms generalises.\nSecond, we test to what extent the latent structure in LLMs can capture the\nclinically observed patterns. We train supervised sparse auto-encoders (sSAE)\nto predict specific symptoms and symptom patterns within a syndrome. We find\nthat sSAE weights can effectively modify the clinical pattern produced by the\nmodel, and thereby capture the latent structure of relevant clinical variation.\nThird, if LLMs correctly capture and quantify relevant mental states, then\nthese states should respond to changes in emotional states induced by validated\nemotion induction interventions. We show that this holds in a third experiment\nwith 190 participants. Overall, this work provides foundational insights into\nthe quantification of pathological mental states with LLMs, highlighting hard\nlimits on the requirements of the data underlying LLM-based quantification; but\nalso suggesting LLMs show substantial conceptual alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) may have an important role to play in mental\nhealth by facilitating the quantification of verbal expressions used to\ncommunicate emotions, feelings and thoughts. While there has been substantial\nand very promising work in this area, the fundamental limits are uncertain.\nHere, focusing on depressive symptoms, we outline and evaluate LLM performance\non three critical tests. The first test evaluates LLM performance on a novel\nground-truth dataset from a large human sample (n=770). This dataset is novel\nas it contains both standard clinically validated quantifications of depression\nsymptoms and specific verbal descriptions of the thoughts related to each\nsymptom by the same individual. The performance of LLMs on this richly\ninformative data shows an upper bound on the performance in this domain, and\nallow us to examine the extent to which inference about symptoms generalises.\nSecond, we test to what extent the latent structure in LLMs can capture the\nclinically observed patterns. We train supervised sparse auto-encoders (sSAE)\nto predict specific symptoms and symptom patterns within a syndrome. We find\nthat sSAE weights can effectively modify the clinical pattern produced by the\nmodel, and thereby capture the latent structure of relevant clinical variation.\nThird, if LLMs correctly capture and quantify relevant mental states, then\nthese states should respond to changes in emotional states induced by validated\nemotion induction interventions. We show that this holds in a third experiment\nwith 190 participants. Overall, this work provides foundational insights into\nthe quantification of pathological mental states with LLMs, highlighting hard\nlimits on the requirements of the data underlying LLM-based quantification; but\nalso suggesting LLMs show substantial conceptual alignment."
                },
                "authors": [
                    {
                        "name": "Jakub Onysk"
                    },
                    {
                        "name": "Quentin J. M. Huys"
                    }
                ],
                "author_detail": {
                    "name": "Quentin J. M. Huys"
                },
                "author": "Quentin J. M. Huys",
                "arxiv_comment": "main text - 9 pages, 6 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09487v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09487v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.04877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.04877v2",
                "updated": "2025-09-25T12:09:14Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    9,
                    14,
                    3,
                    268,
                    0
                ],
                "published": "2025-06-05T10:50:42Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    10,
                    50,
                    42,
                    3,
                    156,
                    0
                ],
                "title": "There Was Never a Bottleneck in Concept Bottleneck Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There Was Never a Bottleneck in Concept Bottleneck Models"
                },
                "summary": "Deep learning representations are often difficult to interpret, which can\nhinder their deployment in sensitive applications. Concept Bottleneck Models\n(CBMs) have emerged as a promising approach to mitigate this issue by learning\nrepresentations that support target task performance while ensuring that each\ncomponent predicts a concrete concept from a predefined set. In this work, we\nargue that CBMs do not impose a true bottleneck: the fact that a component can\npredict a concept does not guarantee that it encodes only information about\nthat concept. This shortcoming raises concerns regarding interpretability and\nthe validity of intervention procedures. To overcome this limitation, we\npropose Minimal Concept Bottleneck Models (MCBMs), which incorporate an\nInformation Bottleneck (IB) objective to constrain each representation\ncomponent to retain only the information relevant to its corresponding concept.\nThis IB is implemented via a variational regularization term added to the\ntraining loss. As a result, MCBMs yield more interpretable representations,\nsupport principled concept-level interventions, and remain consistent with\nprobability-theoretic foundations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning representations are often difficult to interpret, which can\nhinder their deployment in sensitive applications. Concept Bottleneck Models\n(CBMs) have emerged as a promising approach to mitigate this issue by learning\nrepresentations that support target task performance while ensuring that each\ncomponent predicts a concrete concept from a predefined set. In this work, we\nargue that CBMs do not impose a true bottleneck: the fact that a component can\npredict a concept does not guarantee that it encodes only information about\nthat concept. This shortcoming raises concerns regarding interpretability and\nthe validity of intervention procedures. To overcome this limitation, we\npropose Minimal Concept Bottleneck Models (MCBMs), which incorporate an\nInformation Bottleneck (IB) objective to constrain each representation\ncomponent to retain only the information relevant to its corresponding concept.\nThis IB is implemented via a variational regularization term added to the\ntraining loss. As a result, MCBMs yield more interpretable representations,\nsupport principled concept-level interventions, and remain consistent with\nprobability-theoretic foundations."
                },
                "authors": [
                    {
                        "name": "Antonio Almudvar"
                    },
                    {
                        "name": "Jos Miguel Hernndez-Lobato"
                    },
                    {
                        "name": "Alfonso Ortega"
                    }
                ],
                "author_detail": {
                    "name": "Alfonso Ortega"
                },
                "author": "Alfonso Ortega",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.04877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.04877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21057v1",
                "updated": "2025-09-25T12:08:31Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    8,
                    31,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:08:31Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    8,
                    31,
                    3,
                    268,
                    0
                ],
                "title": "PMark: Towards Robust and Distortion-free Semantic-level Watermarking\n  with Channel Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PMark: Towards Robust and Distortion-free Semantic-level Watermarking\n  with Channel Constraints"
                },
                "summary": "Semantic-level watermarking (SWM) for large language models (LLMs) enhances\nwatermarking robustness against text modifications and paraphrasing attacks by\ntreating the sentence as the fundamental unit. However, existing methods still\nlack strong theoretical guarantees of robustness, and reject-sampling-based\ngeneration often introduces significant distribution distortions compared with\nunwatermarked outputs. In this work, we introduce a new theoretical framework\non SWM through the concept of proxy functions (PFs) $\\unicode{x2013}$ functions\nthat map sentences to scalar values. Building on this framework, we propose\nPMark, a simple yet powerful SWM method that estimates the PF median for the\nnext sentence dynamically through sampling while enforcing multiple PF\nconstraints (which we call channels) to strengthen watermark evidence. Equipped\nwith solid theoretical guarantees, PMark achieves the desired distortion-free\nproperty and improves the robustness against paraphrasing-style attacks. We\nalso provide an empirically optimized version that further removes the\nrequirement for dynamical median estimation for better sampling efficiency.\nExperimental results show that PMark consistently outperforms existing SWM\nbaselines in both text quality and robustness, offering a more effective\nparadigm for detecting machine-generated text. Our code will be released at\n[this URL](https://github.com/PMark-repo/PMark).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic-level watermarking (SWM) for large language models (LLMs) enhances\nwatermarking robustness against text modifications and paraphrasing attacks by\ntreating the sentence as the fundamental unit. However, existing methods still\nlack strong theoretical guarantees of robustness, and reject-sampling-based\ngeneration often introduces significant distribution distortions compared with\nunwatermarked outputs. In this work, we introduce a new theoretical framework\non SWM through the concept of proxy functions (PFs) $\\unicode{x2013}$ functions\nthat map sentences to scalar values. Building on this framework, we propose\nPMark, a simple yet powerful SWM method that estimates the PF median for the\nnext sentence dynamically through sampling while enforcing multiple PF\nconstraints (which we call channels) to strengthen watermark evidence. Equipped\nwith solid theoretical guarantees, PMark achieves the desired distortion-free\nproperty and improves the robustness against paraphrasing-style attacks. We\nalso provide an empirically optimized version that further removes the\nrequirement for dynamical median estimation for better sampling efficiency.\nExperimental results show that PMark consistently outperforms existing SWM\nbaselines in both text quality and robustness, offering a more effective\nparadigm for detecting machine-generated text. Our code will be released at\n[this URL](https://github.com/PMark-repo/PMark)."
                },
                "authors": [
                    {
                        "name": "Jiahao Huo"
                    },
                    {
                        "name": "Shuliang Liu"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Junyan Zhang"
                    },
                    {
                        "name": "Yibo Yan"
                    },
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Mingxun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Mingxun Zhou"
                },
                "author": "Mingxun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.02097v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.02097v3",
                "updated": "2025-09-26T02:22:22Z",
                "updated_parsed": [
                    2025,
                    9,
                    26,
                    2,
                    22,
                    22,
                    4,
                    269,
                    0
                ],
                "published": "2025-09-02T08:52:16Z",
                "published_parsed": [
                    2025,
                    9,
                    2,
                    8,
                    52,
                    16,
                    1,
                    245,
                    0
                ],
                "title": "JudgeAgent: Knowledge-wise and Dynamic LLM Evaluation with\n  Agent-as-Interviewer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JudgeAgent: Knowledge-wise and Dynamic LLM Evaluation with\n  Agent-as-Interviewer"
                },
                "summary": "Current evaluation paradigms for large language models (LLMs) suffer from\noverestimated or biased evaluations and mismatched question difficulty, leading\nto incomplete evaluations of knowledge and capability boundaries, which hinder\ntheir effective application and optimization. To address these challenges, we\npropose Agent-as-Interviewer, a dynamic evaluation paradigm that employs LLM\nagents to conduct multi-turn interactions for evaluation. Unlike current\nbenchmarking or dynamic interaction paradigms, Agent-as-Interviewer utilizes\nagents to invoke knowledge tools for wider and deeper knowledge in the dynamic\nmulti-turn question generation, achieving more comprehensive evaluations of\nLLM's knowledge boundaries. It also leverages agents to plan query strategies\nfor adjustment of the question difficulty levels, enhancing the difficulty\ncontrol to match the actual capabilities of target LLMs. Based on this\nparadigm, we develop JudgeAgent, a knowledge-wise dynamic evaluation framework\nthat employs knowledge-driven synthesis as the agent's tool and uses difficulty\nscoring as strategy guidance, thereby finally providing valuable suggestions to\nhelp targets optimize themselves. Extensive experiments validate the\neffectiveness of JudgeAgent's suggestions, demonstrating that\nAgent-as-Interviewer can accurately identify the knowledge and capability\nboundaries of target models. The source code is available on\nhttps://github.com/DataArcTech/JudgeAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current evaluation paradigms for large language models (LLMs) suffer from\noverestimated or biased evaluations and mismatched question difficulty, leading\nto incomplete evaluations of knowledge and capability boundaries, which hinder\ntheir effective application and optimization. To address these challenges, we\npropose Agent-as-Interviewer, a dynamic evaluation paradigm that employs LLM\nagents to conduct multi-turn interactions for evaluation. Unlike current\nbenchmarking or dynamic interaction paradigms, Agent-as-Interviewer utilizes\nagents to invoke knowledge tools for wider and deeper knowledge in the dynamic\nmulti-turn question generation, achieving more comprehensive evaluations of\nLLM's knowledge boundaries. It also leverages agents to plan query strategies\nfor adjustment of the question difficulty levels, enhancing the difficulty\ncontrol to match the actual capabilities of target LLMs. Based on this\nparadigm, we develop JudgeAgent, a knowledge-wise dynamic evaluation framework\nthat employs knowledge-driven synthesis as the agent's tool and uses difficulty\nscoring as strategy guidance, thereby finally providing valuable suggestions to\nhelp targets optimize themselves. Extensive experiments validate the\neffectiveness of JudgeAgent's suggestions, demonstrating that\nAgent-as-Interviewer can accurately identify the knowledge and capability\nboundaries of target models. The source code is available on\nhttps://github.com/DataArcTech/JudgeAgent."
                },
                "authors": [
                    {
                        "name": "Zhichao Shi"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Cangli Yao"
                    },
                    {
                        "name": "Zhenxin Huang"
                    },
                    {
                        "name": "Shengjie Ma"
                    },
                    {
                        "name": "Yinghan Shen"
                    },
                    {
                        "name": "Jian Guo"
                    },
                    {
                        "name": "Yuanzhuo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuanzhuo Wang"
                },
                "author": "Yuanzhuo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.02097v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.02097v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21054v1",
                "updated": "2025-09-25T12:03:10Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    3,
                    10,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:03:10Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    3,
                    10,
                    3,
                    268,
                    0
                ],
                "title": "Disagreements in Reasoning: How a Model's Thinking Process Dictates\n  Persuasion in Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disagreements in Reasoning: How a Model's Thinking Process Dictates\n  Persuasion in Multi-Agent Systems"
                },
                "summary": "The rapid proliferation of recent Multi-Agent Systems (MAS), where Large\nLanguage Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to\nsolve complex problems, necessitates a deep understanding of the persuasion\ndynamics that govern their interactions. This paper challenges the prevailing\nhypothesis that persuasive efficacy is primarily a function of model scale. We\npropose instead that these dynamics are fundamentally dictated by a model's\nunderlying cognitive process, especially its capacity for explicit reasoning.\nThrough a series of multi-agent persuasion experiments, we uncover a\nfundamental trade-off we term the Persuasion Duality. Our findings reveal that\nthe reasoning process in LRMs exhibits significantly greater resistance to\npersuasion, maintaining their initial beliefs more robustly. Conversely, making\nthis reasoning process transparent by sharing the \"thinking content\"\ndramatically increases their ability to persuade others. We further consider\nmore complex transmission persuasion situations and reveal complex dynamics of\ninfluence propagation and decay within multi-hop persuasion between multiple\nagent networks. This research provides systematic evidence linking a model's\ninternal processing architecture to its external persuasive behavior, offering\na novel explanation for the susceptibility of advanced models and highlighting\ncritical implications for the safety, robustness, and design of future MAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid proliferation of recent Multi-Agent Systems (MAS), where Large\nLanguage Models (LLMs) and Large Reasoning Models (LRMs) usually collaborate to\nsolve complex problems, necessitates a deep understanding of the persuasion\ndynamics that govern their interactions. This paper challenges the prevailing\nhypothesis that persuasive efficacy is primarily a function of model scale. We\npropose instead that these dynamics are fundamentally dictated by a model's\nunderlying cognitive process, especially its capacity for explicit reasoning.\nThrough a series of multi-agent persuasion experiments, we uncover a\nfundamental trade-off we term the Persuasion Duality. Our findings reveal that\nthe reasoning process in LRMs exhibits significantly greater resistance to\npersuasion, maintaining their initial beliefs more robustly. Conversely, making\nthis reasoning process transparent by sharing the \"thinking content\"\ndramatically increases their ability to persuade others. We further consider\nmore complex transmission persuasion situations and reveal complex dynamics of\ninfluence propagation and decay within multi-hop persuasion between multiple\nagent networks. This research provides systematic evidence linking a model's\ninternal processing architecture to its external persuasive behavior, offering\na novel explanation for the susceptibility of advanced models and highlighting\ncritical implications for the safety, robustness, and design of future MAS."
                },
                "authors": [
                    {
                        "name": "Haodong Zhao"
                    },
                    {
                        "name": "Jidong Li"
                    },
                    {
                        "name": "Zhaomin Wu"
                    },
                    {
                        "name": "Tianjie Ju"
                    },
                    {
                        "name": "Zhuosheng Zhang"
                    },
                    {
                        "name": "Bingsheng He"
                    },
                    {
                        "name": "Gongshen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Gongshen Liu"
                },
                "author": "Gongshen Liu",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.21051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.21051v1",
                "updated": "2025-09-25T12:01:32Z",
                "updated_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    1,
                    32,
                    3,
                    268,
                    0
                ],
                "published": "2025-09-25T12:01:32Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    12,
                    1,
                    32,
                    3,
                    268,
                    0
                ],
                "title": "When Instructions Multiply: Measuring and Estimating LLM Capabilities of\n  Multiple Instructions Following",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Instructions Multiply: Measuring and Estimating LLM Capabilities of\n  Multiple Instructions Following"
                },
                "summary": "As large language models (LLMs) are increasingly applied to real-world\nscenarios, it becomes crucial to understand their ability to follow multiple\ninstructions simultaneously. To systematically evaluate these capabilities, we\nintroduce two specialized benchmarks for fundamental domains where multiple\ninstructions following is important: Many Instruction-Following Eval\n(ManyIFEval) for text generation with up to ten instructions, and Style-aware\nMostly Basic Programming Problems (StyleMBPP) for code generation with up to\nsix instructions. Our experiments with the created benchmarks across ten LLMs\nreveal that performance consistently degrades as the number of instructions\nincreases. Furthermore, given the fact that evaluating all the possible\ncombinations of multiple instructions is computationally impractical in actual\nuse cases, we developed three types of regression models that can estimate\nperformance on both unseen instruction combinations and different numbers of\ninstructions which are not used during training. We demonstrate that a logistic\nregression model using instruction count as an explanatory variable can predict\nperformance of following multiple instructions with approximately 10% error,\neven for unseen instruction combinations. We show that relatively modest sample\nsizes (500 for ManyIFEval and 300 for StyleMBPP) are sufficient for performance\nestimation, enabling efficient evaluation of LLMs under various instruction\ncombinations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly applied to real-world\nscenarios, it becomes crucial to understand their ability to follow multiple\ninstructions simultaneously. To systematically evaluate these capabilities, we\nintroduce two specialized benchmarks for fundamental domains where multiple\ninstructions following is important: Many Instruction-Following Eval\n(ManyIFEval) for text generation with up to ten instructions, and Style-aware\nMostly Basic Programming Problems (StyleMBPP) for code generation with up to\nsix instructions. Our experiments with the created benchmarks across ten LLMs\nreveal that performance consistently degrades as the number of instructions\nincreases. Furthermore, given the fact that evaluating all the possible\ncombinations of multiple instructions is computationally impractical in actual\nuse cases, we developed three types of regression models that can estimate\nperformance on both unseen instruction combinations and different numbers of\ninstructions which are not used during training. We demonstrate that a logistic\nregression model using instruction count as an explanatory variable can predict\nperformance of following multiple instructions with approximately 10% error,\neven for unseen instruction combinations. We show that relatively modest sample\nsizes (500 for ManyIFEval and 300 for StyleMBPP) are sufficient for performance\nestimation, enabling efficient evaluation of LLMs under various instruction\ncombinations."
                },
                "authors": [
                    {
                        "name": "Keno Harada"
                    },
                    {
                        "name": "Yudai Yamazaki"
                    },
                    {
                        "name": "Masachika Taniguchi"
                    },
                    {
                        "name": "Edison Marrese-Taylor"
                    },
                    {
                        "name": "Takeshi Kojima"
                    },
                    {
                        "name": "Yusuke Iwasawa"
                    },
                    {
                        "name": "Yutaka Matsuo"
                    }
                ],
                "author_detail": {
                    "name": "Yutaka Matsuo"
                },
                "author": "Yutaka Matsuo",
                "arxiv_comment": "Accepted to EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.21051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.21051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]