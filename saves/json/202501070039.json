[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.01805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01805v1",
                "updated": "2025-01-03T13:32:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T13:32:57Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    32,
                    57,
                    4,
                    3,
                    0
                ],
                "title": "End-to-End Long Document Summarization using Gradient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-End Long Document Summarization using Gradient Caching"
                },
                "summary": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training transformer-based encoder-decoder models for long document\nsummarization poses a significant challenge due to the quadratic memory\nconsumption during training. Several approaches have been proposed to extend\nthe input length at test time, but training with these approaches is still\ndifficult, requiring truncation of input documents and causing a mismatch\nbetween training and test conditions. In this work, we propose CachED (Gradient\n$\\textbf{Cach}$ing for $\\textbf{E}$ncoder-$\\textbf{D}$ecoder models), an\napproach that enables end-to-end training of existing transformer-based\nencoder-decoder models, using the entire document without truncation.\nSpecifically, we apply non-overlapping sliding windows to input documents,\nfollowed by fusion in decoder. During backpropagation, the gradients are cached\nat the decoder and are passed through the encoder in chunks by re-computing the\nhidden vectors, similar to gradient checkpointing. In the experiments on long\ndocument summarization, we extend BART to CachED BART, processing more than\n500K tokens during training and achieving superior performance without using\nany additional parameters."
                },
                "authors": [
                    {
                        "name": "Rohit Saxena"
                    },
                    {
                        "name": "Hao Tang"
                    },
                    {
                        "name": "Frank Keller"
                    }
                ],
                "author_detail": {
                    "name": "Frank Keller"
                },
                "author": "Frank Keller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01424v1",
                "updated": "2025-01-02T18:59:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Object-level Visual Prompts for Compositional Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Object-level Visual Prompts for Compositional Image Generation"
                },
                "summary": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a method for composing object-level visual prompts within a\ntext-to-image diffusion model. Our approach addresses the task of generating\nsemantically coherent compositions across diverse scenes and styles, similar to\nthe versatility and expressiveness offered by text prompts. A key challenge in\nthis task is to preserve the identity of the objects depicted in the input\nvisual prompts, while also generating diverse compositions across different\nimages. To address this challenge, we introduce a new KV-mixed cross-attention\nmechanism, in which keys and values are learned from distinct visual\nrepresentations. The keys are derived from an encoder with a small bottleneck\nfor layout control, whereas the values come from a larger bottleneck encoder\nthat captures fine-grained appearance details. By mixing keys and values from\nthese complementary sources, our model preserves the identity of the visual\nprompts while supporting flexible variations in object arrangement, pose, and\ncomposition. During inference, we further propose object-level compositional\nguidance to improve the method's identity preservation and layout correctness.\nResults show that our technique produces diverse scene compositions that\npreserve the unique characteristics of each visual prompt, expanding the\ncreative potential of text-to-image generation."
                },
                "authors": [
                    {
                        "name": "Gaurav Parmar"
                    },
                    {
                        "name": "Or Patashnik"
                    },
                    {
                        "name": "Kuan-Chieh Wang"
                    },
                    {
                        "name": "Daniil Ostashev"
                    },
                    {
                        "name": "Srinivasa Narasimhan"
                    },
                    {
                        "name": "Jun-Yan Zhu"
                    },
                    {
                        "name": "Daniel Cohen-Or"
                    },
                    {
                        "name": "Kfir Aberman"
                    }
                ],
                "author_detail": {
                    "name": "Kfir Aberman"
                },
                "author": "Kfir Aberman",
                "arxiv_comment": "Project: https://snap-research.github.io/visual-composer/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01039v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01039v1",
                "updated": "2025-01-02T03:41:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T03:41:32Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    41,
                    32,
                    3,
                    2,
                    0
                ],
                "title": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MSWA: Refining Local Attention with Multi-ScaleWindow Attention"
                },
                "summary": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based LLMs have achieved exceptional performance across a wide\nrange of NLP tasks. However, the standard self-attention mechanism suffers from\nquadratic time complexity and linearly increased cache size. Sliding window\nattention (SWA) solves this problem by restricting the attention range to a\nfixed-size local context window. Nevertheless, SWA employs a uniform window\nsize for each head in each layer, making it inefficient in capturing context of\nvarying scales. To mitigate this limitation, we propose Multi-Scale Window\nAttention (MSWA) which applies diverse window sizes across heads and layers in\nthe Transformer. It not only allows for different window sizes among heads\nwithin the same layer but also progressively increases window size allocation\nfrom shallow to deep layers, thus enabling the model to capture contextual\ninformation with different lengths and distances. Experimental results on\nlanguage modeling and common-sense reasoning tasks substantiate that MSWA\noutperforms traditional local attention in both effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Yixing Xu"
                    },
                    {
                        "name": "Shivank Nag"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Lu Tian"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01039v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01039v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v2",
                "updated": "2025-01-02T03:40:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    40,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01005v1",
                "updated": "2025-01-02T02:02:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T02:02:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    2,
                    2,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashInfer: Efficient and Customizable Attention Engine for LLM\n  Inference Serving"
                },
                "summary": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers, driven by attention mechanisms, form the foundation of large\nlanguage models (LLMs). As these models scale up, efficient GPU attention\nkernels become essential for high-throughput and low-latency inference. Diverse\nLLM applications demand flexible and high-performance attention solutions. We\npresent FlashInfer: a customizable and efficient attention engine for LLM\nserving. FlashInfer tackles KV-cache storage heterogeneity using block-sparse\nformat and composable formats to optimize memory access and reduce redundancy.\nIt also offers a customizable attention template, enabling adaptation to\nvarious settings through Just-In-Time (JIT) compilation. Additionally,\nFlashInfer's load-balanced scheduling algorithm adjusts to dynamism of user\nrequests while maintaining compatibility with CUDAGraph which requires static\nconfiguration. FlashInfer have been integrated into leading LLM serving\nframeworks like SGLang, vLLM and MLC-Engine. Comprehensive kernel-level and\nend-to-end evaluations demonstrate FlashInfer's ability to significantly boost\nkernel performance across diverse inference scenarios: compared to\nstate-of-the-art LLM serving solutions, FlashInfer achieve 29-69%\ninter-token-latency reduction compared to compiler backends for LLM serving\nbenchmark, 28-30% latency reduction for long-context inference, and 13-17%\nspeedup for LLM serving with parallel generation."
                },
                "authors": [
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Lequn Chen"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Wuwei Lin"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Stephanie Wang"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Baris Kasikci"
                    },
                    {
                        "name": "Vinod Grover"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Luis Ceze"
                    }
                ],
                "author_detail": {
                    "name": "Luis Ceze"
                },
                "author": "Luis Ceze",
                "arxiv_comment": "code available at http://github.com/flashinfer-ai/flashinfer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00946v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00946v1",
                "updated": "2025-01-01T20:16:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T20:16:27Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    20,
                    16,
                    27,
                    2,
                    1,
                    0
                ],
                "title": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cached Adaptive Token Merging: Dynamic Token Reduction and Redundant\n  Computation Elimination in Diffusion Model"
                },
                "summary": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have emerged as a promising approach for generating\nhigh-quality, high-dimensional images. Nevertheless, these models are hindered\nby their high computational cost and slow inference, partly due to the\nquadratic computational complexity of the self-attention mechanisms with\nrespect to input size. Various approaches have been proposed to address this\ndrawback. One such approach focuses on reducing the number of tokens fed into\nthe self-attention, known as token merging (ToMe). In our method, which is\ncalled cached adaptive token merging(CA-ToMe), we calculate the similarity\nbetween tokens and then merge the r proportion of the most similar tokens.\nHowever, due to the repetitive patterns observed in adjacent steps and the\nvariation in the frequency of similarities, we aim to enhance this approach by\nimplementing an adaptive threshold for merging tokens and adding a caching\nmechanism that stores similar pairs across several adjacent steps. Empirical\nresults demonstrate that our method operates as a training-free acceleration\nmethod, achieving a speedup factor of 1.24 in the denoising process while\nmaintaining the same FID scores compared to existing approaches."
                },
                "authors": [
                    {
                        "name": "Omid Saghatchian"
                    },
                    {
                        "name": "Atiyeh Gh. Moghadam"
                    },
                    {
                        "name": "Ahmad Nickabadi"
                    }
                ],
                "author_detail": {
                    "name": "Ahmad Nickabadi"
                },
                "author": "Ahmad Nickabadi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00946v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00946v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00799v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00799v1",
                "updated": "2025-01-01T10:50:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "published": "2025-01-01T10:50:35Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    10,
                    50,
                    35,
                    2,
                    1,
                    0
                ],
                "title": "Follow The Sparse Approximate Leader for No-Regret Online Sparse Linear\n  Approximation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Follow The Sparse Approximate Leader for No-Regret Online Sparse Linear\n  Approximation"
                },
                "summary": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of \\textit{online sparse linear approximation}, where\none predicts the best sparse approximation of a sequence of measurements in\nterms of linear combination of columns of a given measurement matrix. Such\nonline prediction problems are ubiquitous, ranging from medical trials to web\ncaching to resource allocation. The inherent difficulty of offline recovery\nalso makes the online problem challenging. In this letter, we propose\nFollow-The-Approximate-Sparse-Leader, an efficient online meta-policy to\naddress this online problem. Through a detailed theoretical analysis, we prove\nthat under certain assumptions on the measurement sequence, the proposed policy\nenjoys a data-dependent sublinear upper bound on the static regret, which can\nrange from logarithmic to square-root. Numerical simulations are performed to\ncorroborate the theoretical findings and demonstrate the efficacy of the\nproposed online policy."
                },
                "authors": [
                    {
                        "name": "Samrat Mukhopadhyay"
                    },
                    {
                        "name": "Debasmita Mukherjee"
                    }
                ],
                "author_detail": {
                    "name": "Debasmita Mukherjee"
                },
                "author": "Debasmita Mukherjee",
                "arxiv_comment": "12 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00799v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00799v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21023v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21023v2",
                "updated": "2024-12-31T20:40:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    20,
                    40,
                    43,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-30T15:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    46,
                    53,
                    0,
                    365,
                    0
                ],
                "title": "EdgeRAG: Online-Indexed RAG for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeRAG: Online-Indexed RAG for Edge Devices"
                },
                "summary": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Retrieval Augmented Generation (RAG) on resource-constrained edge\ndevices is challenging due to limited memory and processing power. In this\nwork, we propose EdgeRAG which addresses the memory constraint by pruning\nembeddings within clusters and generating embeddings on-demand during\nretrieval. To avoid the latency of generating embeddings for large tail\nclusters, EdgeRAG pre-computes and stores embeddings for these clusters, while\nadaptively caching remaining embeddings to minimize redundant computations and\nfurther optimize latency. The result from BEIR suite shows that EdgeRAG offers\nsignificant latency reduction over the baseline IVF index, but with similar\ngeneration quality while allowing all of our evaluated datasets to fit into the\nmemory."
                },
                "authors": [
                    {
                        "name": "Korakit Seemakhupt"
                    },
                    {
                        "name": "Sihang Liu"
                    },
                    {
                        "name": "Samira Khan"
                    }
                ],
                "author_detail": {
                    "name": "Samira Khan"
                },
                "author": "Samira Khan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21023v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21023v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00375v1",
                "updated": "2024-12-31T09:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T09:56:40Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    9,
                    56,
                    40,
                    1,
                    366,
                    0
                ],
                "title": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Token Pruning for Caching Better: 9 Times Acceleration on Stable\n  Diffusion for Free"
                },
                "summary": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stable Diffusion has achieved remarkable success in the field of\ntext-to-image generation, with its powerful generative capabilities and diverse\ngeneration results making a lasting impact. However, its iterative denoising\nintroduces high computational costs and slows generation speed, limiting\nbroader adoption. The community has made numerous efforts to reduce this\ncomputational burden, with methods like feature caching attracting attention\ndue to their effectiveness and simplicity. Nonetheless, simply reusing features\ncomputed at previous timesteps causes the features across adjacent timesteps to\nbecome similar, reducing the dynamics of features over time and ultimately\ncompromising the quality of generated images. In this paper, we introduce a\ndynamics-aware token pruning (DaTo) approach that addresses the limitations of\nfeature caching. DaTo selectively prunes tokens with lower dynamics, allowing\nonly high-dynamic tokens to participate in self-attention layers, thereby\nextending feature dynamics across timesteps. DaTo combines feature caching with\ntoken pruning in a training-free manner, achieving both temporal and token-wise\ninformation reuse. Applied to Stable Diffusion on the ImageNet, our approach\ndelivered a 9$\\times$ speedup while reducing FID by 0.33, indicating enhanced\nimage quality. On the COCO-30k, we observed a 7$\\times$ acceleration coupled\nwith a notable FID reduction of 2.17."
                },
                "authors": [
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Bang Xiao"
                    },
                    {
                        "name": "Jiayi Tang"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuefei Ning"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10516v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10516v3",
                "updated": "2024-12-31T07:11:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    7,
                    11,
                    0,
                    1,
                    366,
                    0
                ],
                "published": "2024-09-16T17:59:52Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    17,
                    59,
                    52,
                    0,
                    260,
                    0
                ],
                "title": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval"
                },
                "summary": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nspeed and high GPU memory consumption for caching key-value (KV) vectors. This\npaper proposes RetrievalAttention, a training-free approach to both accelerate\nattention computation and reduce GPU memory consumption. By leveraging the\ndynamic sparsity of attention mechanism, RetrievalAttention proposes to build\napproximate nearest neighbor search (ANNS) indexes for KV vectors in CPU memory\nand retrieve the most relevant ones through vector search during generation.\nUnfortunately, we observe that the off-the-shelf ANNS indexes are often\nineffective for such retrieval tasks due to the out-of-distribution (OOD)\nbetween query vectors and key vectors in the attention mechanism.\nRetrievalAttention addresses the OOD challenge by designing an attention-aware\nvector search algorithm that can adapt to the distribution of query vectors.\nOur evaluation demonstrates that RetrievalAttention achieves near full\nattention accuracy while only requiring access to 1--3% of the data. This leads\nto a significant reduction in the inference cost of long-context LLMs, with a\nmuch lower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) to serve 128K tokens for LLMs with 8B parameters,\nwhich is capable of generating one token in 0.188 seconds."
                },
                "authors": [
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Meng Chen"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Zhenhua Han"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10516v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10516v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00279v1",
                "updated": "2024-12-31T05:24:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T05:24:30Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    5,
                    24,
                    30,
                    1,
                    366,
                    0
                ],
                "title": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performant Automatic BLAS Offloading on Unified Memory Architecture with\n  OpenMP First-Touch Style Data Movement"
                },
                "summary": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BLAS is a fundamental building block of advanced linear algebra libraries and\nmany modern scientific computing applications. GPUs are known for their strong\narithmetic computing capabilities and are highly suited for BLAS operations.\nHowever, porting code to GPUs often requires significant effort, especially for\nlarge, complex codes or legacy codes, even for BLAS-heavy applications. While\nvarious tools exist to automatically offload BLAS to GPUs, they are often\nimpractical due to the high costs associated with mandatory data transfers. The\nadvent of unified memory architectures in recent GPU designs, such as the\nNVIDIA Grace-Hopper, allows cache-coherent memory access across all types of\nmemory for both CPU and GPU, potentially eliminating the bottlenecks faced in\nconventional architectures. This breakthrough paves the way for innovative\napplication developments and porting strategies. Building on our preliminary\nwork demonstrating the potential of automatic *gemm offload, this paper extends\nthe framework to all level-3 BLAS operations and introduces SCILIB-Accel, a\nnovel tool for automatic BLAS offload. SCILIB-Accel leverages the memory\ncoherency in Grace-Hopper and introduces a Device First-Use data movement\npolicy inspired by the OpenMP First-Touch approach in multi-socket CPU\nprogramming, minimizing CPU-GPU data transfers for typical scientific computing\ncodes. Additionally, utilizing dynamic binary instrumentation, the tool\nintercepts BLAS symbols directly from a CPU binary, requiring no code\nmodifications or recompilation. SCILIB-Accel has been evaluated using multiple\nquantum physics codes on up to a few hundred GPU nodes, yielding promising\nspeedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was\nachieved on Grace-Hopper compared to Grace-Grace."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    }
                ],
                "author_detail": {
                    "name": "Junjie Li"
                },
                "author": "Junjie Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00243v1",
                "updated": "2024-12-31T03:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "published": "2024-12-31T03:19:38Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    3,
                    19,
                    38,
                    1,
                    366,
                    0
                ],
                "title": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Layer Cache Aggregation for Token Reduction in Ultra-Fine-Grained\n  Image Recognition"
                },
                "summary": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-fine-grained image recognition (UFGIR) is a challenging task that\ninvolves classifying images within a macro-category. While traditional FGIR\ndeals with classifying different species, UFGIR goes beyond by classifying\nsub-categories within a species such as cultivars of a plant. In recent times\nthe usage of Vision Transformer-based backbones has allowed methods to obtain\noutstanding recognition performances in this task but this comes at a\nsignificant cost in terms of computation specially since this task\nsignificantly benefits from incorporating higher resolution images. Therefore,\ntechniques such as token reduction have emerged to reduce the computational\ncost. However, dropping tokens leads to loss of essential information for\nfine-grained categories, specially as the token keep rate is reduced.\nTherefore, to counteract the loss of information brought by the usage of token\nreduction we propose a novel Cross-Layer Aggregation Classification Head and a\nCross-Layer Cache mechanism to recover and access information from previous\nlayers in later locations. Extensive experiments covering more than 2000 runs\nacross diverse settings including 5 datasets, 9 backbones, 7 token reduction\nmethods, 5 keep rates, and 2 image sizes demonstrate the effectiveness of the\nproposed plug-and-play modules and allow us to push the boundaries of accuracy\nvs cost for UFGIR by reducing the kept tokens to extremely low ratios of up to\n10\\% while maintaining a competitive accuracy to state-of-the-art models. Code\nis available at: \\url{https://github.com/arkel23/CLCA}"
                },
                "authors": [
                    {
                        "name": "Edwin Arkel Rios"
                    },
                    {
                        "name": "Jansen Christopher Yuanda"
                    },
                    {
                        "name": "Vincent Leon Ghanz"
                    },
                    {
                        "name": "Cheng-Wei Yu"
                    },
                    {
                        "name": "Bo-Cheng Lai"
                    },
                    {
                        "name": "Min-Chun Hu"
                    }
                ],
                "author_detail": {
                    "name": "Min-Chun Hu"
                },
                "author": "Min-Chun Hu",
                "arxiv_comment": "Accepted to ICASSP 2025. Main: 5 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.21015v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.21015v1",
                "updated": "2024-12-30T15:33:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T15:33:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    15,
                    33,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "MapQaTor: A System for Efficient Annotation of Map Query Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MapQaTor: A System for Efficient Annotation of Map Query Datasets"
                },
                "summary": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping and navigation services like Google Maps, Apple Maps, Openstreet\nMaps, are essential for accessing various location-based data, yet they often\nstruggle to handle natural language geospatial queries. Recent advancements in\nLarge Language Models (LLMs) show promise in question answering (QA), but\ncreating reliable geospatial QA datasets from map services remains challenging.\nWe introduce MapQaTor, a web application that streamlines the creation of\nreproducible, traceable map-based QA datasets. With its plug-and-play\narchitecture, MapQaTor enables seamless integration with any maps API, allowing\nusers to gather and visualize data from diverse sources with minimal setup. By\ncaching API responses, the platform ensures consistent ground truth, enhancing\nthe reliability of the data even as real-world information evolves. MapQaTor\ncentralizes data retrieval, annotation, and visualization within a single\nplatform, offering a unique opportunity to evaluate the current state of\nLLM-based geospatial reasoning while advancing their capabilities for improved\ngeospatial understanding. Evaluation metrics show that, MapQaTor speeds up the\nannotation process by at least 30 times compared to manual methods,\nunderscoring its potential for developing geospatial resources, such as complex\nmap reasoning datasets. The website is live at: https://mapqator.github.io/ and\na demo video is available at: https://youtu.be/7_aV9Wmhs6Q."
                },
                "authors": [
                    {
                        "name": "Mahir Labib Dihan"
                    },
                    {
                        "name": "Mohammed Eunus Ali"
                    },
                    {
                        "name": "Md Rizwan Parvez"
                    }
                ],
                "author_detail": {
                    "name": "Md Rizwan Parvez"
                },
                "author": "Md Rizwan Parvez",
                "arxiv_comment": "13 pages, 35 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.21015v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.21015v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12094v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12094v3",
                "updated": "2024-12-30T14:54:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    14,
                    54,
                    29,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-16T18:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    18,
                    58,
                    57,
                    0,
                    351,
                    0
                ],
                "title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SepLLM: Accelerate Large Language Models by Compressing One Segment into\n  One Separator"
                },
                "summary": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have exhibited exceptional performance across a\nspectrum of natural language processing tasks. However, their substantial sizes\npose considerable challenges, particularly in computational demands and\ninference speed, due to their quadratic complexity. In this work, we have\nidentified a key pattern: certain seemingly meaningless special tokens (i.e.,\nseparators) contribute disproportionately to attention scores compared to\nsemantically meaningful tokens. This observation suggests that information of\nthe segments between these separator tokens can be effectively condensed into\nthe separator tokens themselves without significant information loss. Guided by\nthis insight, we introduce SepLLM, a plug-and-play framework that accelerates\ninference by compressing these segments and eliminating redundant tokens.\nAdditionally, we implement efficient kernels for training acceleration.\nExperimental results across training-free, training-from-scratch, and\npost-training settings demonstrate SepLLM's effectiveness. Notably, using the\nLlama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the\nGSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in\nstreaming settings, SepLLM effectively processes sequences of up to 4 million\ntokens or more while maintaining consistent language modeling capabilities."
                },
                "authors": [
                    {
                        "name": "Guoxuan Chen"
                    },
                    {
                        "name": "Han Shi"
                    },
                    {
                        "name": "Jiawei Li"
                    },
                    {
                        "name": "Yihang Gao"
                    },
                    {
                        "name": "Xiaozhe Ren"
                    },
                    {
                        "name": "Yimeng Chen"
                    },
                    {
                        "name": "Xin Jiang"
                    },
                    {
                        "name": "Zhenguo Li"
                    },
                    {
                        "name": "Weiyang Liu"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_comment": "We have made our code publicly available at sepllm.github.io. Our\n  codebase supports efficient multi-node distributed training with accelerated\n  attention module Sep-Attention and also supports numerous existing Fusion\n  Operators to accelerate the training process, such as fused rope, etc. If you\n  find our code helpful, please kindly consider giving us a **star** on\n  GitHub^_^. Thank you very much!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12094v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12094v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20887v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20887v1",
                "updated": "2024-12-30T11:54:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T11:54:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    11,
                    54,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hidden Quantum Paraelectric Phase in SrTiO3 Induced by Terahertz Field"
                },
                "summary": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coherent manipulation of lattice vibrations using ultrafast light pulses\nenables access to nonequilibrium 'hidden' phases with designed functionalities\nin quantum materials. However, expanding the understanding of nonlinear\nlight-phonon interaction mechanisms remains crucial for developing new\nstrategies. Here, we report re-entrant ultrafast phase transitions in SrTiO3\ndriven by intense terahertz excitation. As the terahertz field increases, the\nsystem transitions from the quantum paraelectric (QPE) ground state to an\nintermediate ferroelectric phase, and then unexpectedly reverts to a QPE state\nabove ~500 kV/cm. The latter hidden QPE phase exhibits distinct lattice\ndynamics compared to the initial phases, highlighting activated\nantiferrodistortive phonon modes. Aided by first-principles dynamical\ncalculations, we identify the mechanism for these complex behaviors as a\nsuperposition of multiple coherently excited eigenstates of the polar soft\nmode. Our results reveal a previously uncharted quantum facet of SrTiO3 and\nopen pathways for harnessing high-order excitations to engineer quantum\nmaterials in the ultrafast regime."
                },
                "authors": [
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Hanbyul Kim"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Jianlin Luo"
                    },
                    {
                        "name": "Simone Latini"
                    },
                    {
                        "name": "Dongbin Shin"
                    },
                    {
                        "name": "Jun-Ming Liu"
                    },
                    {
                        "name": "Jing-Feng Li"
                    },
                    {
                        "name": "Angel Rubio"
                    },
                    {
                        "name": "Ce-Wen Nan"
                    },
                    {
                        "name": "Qian Li"
                    }
                ],
                "author_detail": {
                    "name": "Qian Li"
                },
                "author": "Qian Li",
                "arxiv_comment": "18 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20887v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20887v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08545v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08545v2",
                "updated": "2024-12-30T05:01:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    5,
                    1,
                    44,
                    0,
                    365,
                    0
                ],
                "published": "2024-08-16T06:11:21Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    6,
                    11,
                    21,
                    4,
                    229,
                    0
                ],
                "title": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SelectLLM: Query-Aware Efficient Selection Algorithm for Large Language\n  Models"
                },
                "summary": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have seen widespread adoption due to their\nremarkable performance across various applications, driving the accelerated\ndevelopment of a large number of diverse LLMs. However, these individual LLMs\nshow limitations in generalization and performance on complex tasks due to\ninherent training biases, model size constraints, and the quality or diversity\nof pre-training datasets. A promising direction is to efficiently harness the\ndiverse capabilities of LLMs to overcome these individual limitations. To\naddress these limitations, we introduce a novel LLM selection algorithm called\nSelectLLM, which efficiently directs input queries to the most suitable subset\nof LLMs from a large pool, ensuring that the selected models collectively\nprovide accurate responses. SelectLLM employs a multi-label classifier and\npolicy based on the classifier's predictions and confidence scores in selecting\nan optimal, query-aware, and lightweight subset of LLMs. Our findings indicate\nthat the proposed model outperforms existing ensemble-based baselines and\nachieves competitive performance with similarly sized top-performing LLMs while\nmaintaining efficiency. Specifically, it achieves a huge reduction in inference\nlatency on two challenging reasoning benchmarks: 13% on GSM8K and 70% on MMLU,\ncompared to the top-performing baselines. Also, we establish a theoretical\nupper bound by an oracle with LLMs and explore in-depth linguistic analysis to\nunderstand the performance gap between Oracle and SelectLLM."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08545v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08545v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v1",
                "updated": "2024-12-30T03:05:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have been shown to perform well on a variety of natural\nlanguage processing problems. However, as the model size and the input\nsequence's length increase, the rapid increase of KV Cache significantly slows\ndown inference speed. Therefore GQA model, as an alternative to MHA model, has\nbeen widely introduced into LLMs. In this work, we propose a low-cost method\nfor pruning MHA models into GQA models with any compression ratio of key-value\nheads. Our method is based on $\\mathit{L_0}$ masks to gradually remove\nredundant parameters. In addition, we apply orthogonal transformations to\nattention heads without changing the model to increase similarity between\nattention heads before pruning training, in order to further improve\nperformance of the model. Our method can be compatible with rotary position\nembedding (RoPE), which means the model after training can be fully adapted to\nthe mainstream standard GQA framework. Experiments demonstrate that our\nstrategy can compress up to 87.5% of key-value heads of the LLaMA2-7B model\nwithout too much performance degradation, just achieved through supervised\nfine-tuning."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "12 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00068v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00068v1",
                "updated": "2024-12-29T17:41:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:41:40Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    41,
                    40,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Optimization of Storage Systems Using Reinforcement Learning\n  Techniques"
                },
                "summary": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data-intensive applications has placed\nunprecedented demands on modern storage systems, necessitating dynamic and\nefficient optimization strategies. Traditional heuristics employed for storage\nperformance optimization often fail to adapt to the variability and complexity\nof contemporary workloads, leading to significant performance bottlenecks and\nresource inefficiencies. To address these challenges, this paper introduces\nRL-Storage, a novel reinforcement learning (RL)-based framework designed to\ndynamically optimize storage system configurations. RL-Storage leverages deep\nQ-learning algorithms to continuously learn from real-time I/O patterns and\npredict optimal storage parameters, such as cache size, queue depths, and\nreadahead settings[1]. The proposed framework operates within the storage\nkernel, ensuring minimal latency and low computational overhead. Through an\nadaptive feedback mechanism, RL-Storage dynamically adjusts critical\nparameters, achieving efficient resource utilization across a wide range of\nworkloads. Experimental evaluations conducted on a range of benchmarks,\nincluding RocksDB and PostgreSQL, demonstrate significant improvements, with\nthroughput gains of up to 2.6x and latency reductions of 43% compared to\nbaseline heuristics. Additionally, RL-Storage achieves these performance\nenhancements with a negligible CPU overhead of 0.11% and a memory footprint of\nonly 5 KB, making it suitable for seamless deployment in production\nenvironments. This work underscores the transformative potential of\nreinforcement learning techniques in addressing the dynamic nature of modern\nstorage systems. By autonomously adapting to workload variations in real time,\nRL-Storage provides a robust and scalable solution for optimizing storage\nperformance, paving the way for next-generation intelligent storage\ninfrastructures."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00068v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00068v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20524v1",
                "updated": "2024-12-29T17:18:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T17:18:21Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    18,
                    21,
                    6,
                    364,
                    0
                ],
                "title": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ns3 meets Sionna: Using Realistic Channels in Network Simulation"
                },
                "summary": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Network simulators are indispensable tools for the advancement of wireless\nnetwork technologies, offering a cost-effective and controlled environment to\nsimulate real-world network behavior. However, traditional simulators, such as\nthe widely used ns-3, exhibit limitations in accurately modeling indoor and\noutdoor scenarios due to their reliance on simplified statistical and\nstochastic channel propagation models, which often fail to accurately capture\nphysical phenomena like multipath signal propagation and shadowing by obstacles\nin the line-of-sight path. We present Ns3Sionna, which integrates a ray\ntracing-based channel model, implemented using the Sionna RT framework, within\nthe ns-3 network simulator. It allows to simulate environment-specific and\nphysically accurate channel realizations for a given 3D scene and wireless\ndevice positions. Additionally, a mobility model based on ray tracing was\ndeveloped to accurately represent device movements within the simulated 3D\nspace. Ns3Sionna provides more realistic path and delay loss estimates for both\nindoor and outdoor environments than existing ns-3 propagation models,\nparticularly in terms of spatial and temporal correlation. Moreover,\nfine-grained channel state information is provided, which could be used for the\ndevelopment of sensing applications. Due to the significant computational\ndemands of ray tracing, Ns3Sionna takes advantage of the parallel execution\ncapabilities of modern GPUs and multi-core CPUs by incorporating intelligent\npre-caching mechanisms that leverage the channel's coherence time to optimize\nruntime performance. This enables the efficient simulation of scenarios with a\nsmall to medium number of mobile nodes."
                },
                "authors": [
                    {
                        "name": "Anatolij Zubow"
                    },
                    {
                        "name": "Yannik Pilz"
                    },
                    {
                        "name": "Sascha Rösler"
                    },
                    {
                        "name": "Falko Dressler"
                    }
                ],
                "author_detail": {
                    "name": "Falko Dressler"
                },
                "author": "Falko Dressler",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20504v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20504v1",
                "updated": "2024-12-29T15:42:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "published": "2024-12-29T15:42:24Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    15,
                    42,
                    24,
                    6,
                    364,
                    0
                ],
                "title": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReTaKe: Reducing Temporal and Knowledge Redundancy for Long Video\n  Understanding"
                },
                "summary": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Large Language Models (VideoLLMs) have achieved remarkable progress in\nvideo understanding. However, existing VideoLLMs often inherit the limitations\nof their backbone LLMs in handling long sequences, leading to challenges for\nlong video understanding. Common solutions either simply uniformly sample\nvideos' frames or compress visual tokens, which focus primarily on low-level\ntemporal visual redundancy, overlooking high-level knowledge redundancy. This\nlimits the achievable compression rate with minimal loss. To this end. we\nintroduce a training-free method, $\\textbf{ReTaKe}$, containing two novel\nmodules DPSelect and PivotKV, to jointly model and reduce both temporal visual\nredundancy and knowledge redundancy for long video understanding. Specifically,\nDPSelect identifies keyframes with local maximum peak distance based on their\nvisual features, which are closely aligned with human video perception. PivotKV\nemploys the obtained keyframes as pivots and conducts KV-Cache compression for\nthe non-pivot tokens with low attention scores, which are derived from the\nlearned prior knowledge of LLMs. Experiments on benchmarks VideoMME, MLVU, and\nLVBench, show that ReTaKe can support 4x longer video sequences with minimal\nperformance loss (<1%) and outperform all similar-size VideoLLMs with 3%-5%,\neven surpassing or on par with much larger ones. Our code is available at\nhttps://github.com/SCZwangxiao/video-ReTaKe"
                },
                "authors": [
                    {
                        "name": "Xiao Wang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Jianlong Wu"
                    },
                    {
                        "name": "Shiyu Zhu"
                    },
                    {
                        "name": "Li Cao"
                    },
                    {
                        "name": "Liqiang Nie"
                    }
                ],
                "author_detail": {
                    "name": "Liqiang Nie"
                },
                "author": "Liqiang Nie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20504v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20504v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20221v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20221v1",
                "updated": "2024-12-28T17:17:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T17:17:03Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    17,
                    17,
                    3,
                    5,
                    363,
                    0
                ],
                "title": "Revisiting Cache Freshness for Emerging Real-Time Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revisiting Cache Freshness for Emerging Real-Time Applications"
                },
                "summary": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is widely used in industry to improve application performance by\nreducing data-access latency and taking the load off the backend\ninfrastructure. TTLs have become the de-facto mechanism used to keep cached\ndata reasonably fresh (i.e., not too out of date with the backend). However,\nthe emergence of real-time applications requires tighter data freshness, which\nis impractical to achieve with TTLs. We discuss why this is the case, and\npropose a simple yet effective adaptive policy to achieve the desired\nfreshness."
                },
                "authors": [
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Rishabh Iyer"
                    },
                    {
                        "name": "Scott Shenker"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "arxiv_doi": "10.1145/3696348.3696858",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696348.3696858",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.20221v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20221v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "HotNets '24",
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v1",
                "updated": "2024-12-28T14:38:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20105v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20105v1",
                "updated": "2024-12-28T10:17:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T10:17:29Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    10,
                    17,
                    29,
                    5,
                    363,
                    0
                ],
                "title": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ST$^3$: Accelerating Multimodal Large Language Model by Spatial-Temporal\n  Visual Token Trimming"
                },
                "summary": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) enhance their perceptual\ncapabilities by integrating visual and textual information. However, processing\nthe massive number of visual tokens incurs a significant computational cost.\nExisting analysis of the MLLM attention mechanisms remains shallow, leading to\ncoarse-grain token pruning strategies that fail to effectively balance speed\nand accuracy. In this paper, we conduct a comprehensive investigation of MLLM\nattention mechanisms with LLaVA. We find that numerous visual tokens and\npartial attention computations are redundant during the decoding process. Based\non this insight, we propose Spatial-Temporal Visual Token Trimming\n($\\textbf{ST}^{3}$), a framework designed to accelerate MLLM inference without\nretraining. $\\textbf{ST}^{3}$ consists of two primary components: 1)\nProgressive Visual Token Pruning (\\textbf{PVTP}), which eliminates inattentive\nvisual tokens across layers, and 2) Visual Token Annealing (\\textbf{VTA}),\nwhich dynamically reduces the number of visual tokens in each layer as the\ngenerated tokens grow. Together, these techniques deliver around\n$\\mathbf{2\\times}$ faster inference with only about $\\mathbf{30\\%}$ KV cache\nmemory compared to the original LLaVA, while maintaining consistent performance\nacross various datasets. Crucially, $\\textbf{ST}^{3}$ can be seamlessly\nintegrated into existing pre-trained MLLMs, providing a plug-and-play solution\nfor efficient inference."
                },
                "authors": [
                    {
                        "name": "Jiedong Zhuang"
                    },
                    {
                        "name": "Lu Lu"
                    },
                    {
                        "name": "Ming Dai"
                    },
                    {
                        "name": "Rui Hu"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Qiang Liu"
                    },
                    {
                        "name": "Haoji Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haoji Hu"
                },
                "author": "Haoji Hu",
                "arxiv_comment": "Accepted to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20105v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20105v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19991v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19991v1",
                "updated": "2024-12-28T03:28:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "published": "2024-12-28T03:28:52Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    3,
                    28,
                    52,
                    5,
                    363,
                    0
                ],
                "title": "A Robust Federated Learning Framework for Undependable Devices at Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Robust Federated Learning Framework for Undependable Devices at Scale"
                },
                "summary": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a federated learning (FL) system, many devices, such as smartphones, are\noften undependable (e.g., frequently disconnected from WiFi) during training.\nExisting FL frameworks always assume a dependable environment and exclude\nundependable devices from training, leading to poor model performance and\nresource wastage. In this paper, we propose FLUDE to effectively deal with\nundependable environments. First, FLUDE assesses the dependability of devices\nbased on the probability distribution of their historical behaviors (e.g., the\nlikelihood of successfully completing training). Based on this assessment,\nFLUDE adaptively selects devices with high dependability for training. To\nmitigate resource wastage during the training phase, FLUDE maintains a model\ncache on each device, aiming to preserve the latest training state for later\nuse in case local training on an undependable device is interrupted. Moreover,\nFLUDE proposes a staleness-aware strategy to judiciously distribute the global\nmodel to a subset of devices, thus significantly reducing resource wastage\nwhile maintaining model performance. We have implemented FLUDE on two physical\nplatforms with 120 smartphones and NVIDIA Jetson devices. Extensive\nexperimental results demonstrate that FLUDE can effectively improve model\nperformance and resource efficiency of FL training in undependable\nenvironments."
                },
                "authors": [
                    {
                        "name": "Shilong Wang"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Chunming Qiao"
                    },
                    {
                        "name": "Huarong Deng"
                    },
                    {
                        "name": "Qiuye Zheng"
                    },
                    {
                        "name": "Jiantao Gong"
                    }
                ],
                "author_detail": {
                    "name": "Jiantao Gong"
                },
                "author": "Jiantao Gong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19991v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19991v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19919v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19919v1",
                "updated": "2024-12-27T20:47:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "published": "2024-12-27T20:47:23Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    20,
                    47,
                    23,
                    4,
                    362,
                    0
                ],
                "title": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Comparison of Magnetic Penetration Depth in Kagome\n  Superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb)"
                },
                "summary": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report measurements of the local temperature-dependent penetration depth,\n$\\lambda(T)$, in the Kagome superconductors AV$_3$Sb$_5$ (A = Cs, K, Rb) using\nscanning superconducting quantum interference device (SQUID) microscopy. Our\nresults suggest that the superconducting order in all three compounds is fully\ngapped, in contrast to reports of nodal superconductivity in KV$_3$Sb$_5$ and\nRbV$_3$Sb$_5$. Analysis of the temperature-dependent superfluid density,\n$\\rho_s(T)$, shows deviations from the behavior expected for a single isotropic\ngap, but the data are well described by models incorporating either a single\nanisotropic gap or two isotropic gaps. Notably, the temperature dependences of\n$\\lambda(T)$ and $\\rho_s(T)$ in KV$_3$Sb$_5$ and RbV$_3$Sb$_5$ are\nqualitatively more similar to each other than to CsV$_3$Sb$_5$, consistent with\nthe superconducting phase reflecting features of the normal-state band\nstructure. Our findings provide a direct comparison of the superconducting\nproperties across the AV$_3$Sb$_5$ family."
                },
                "authors": [
                    {
                        "name": "Austin Kaczmarek"
                    },
                    {
                        "name": "Andrea Capa Salinas"
                    },
                    {
                        "name": "Stephen D. Wilson"
                    },
                    {
                        "name": "Katja C. Nowack"
                    }
                ],
                "author_detail": {
                    "name": "Katja C. Nowack"
                },
                "author": "Katja C. Nowack",
                "arxiv_comment": "5 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19919v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19919v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.supr-con",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v1",
                "updated": "2024-12-26T15:45:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    }
                ],
                "author_detail": {
                    "name": "Heung-Yeung Shum"
                },
                "author": "Heung-Yeung Shum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19051v1",
                "updated": "2024-12-26T04:13:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "published": "2024-12-26T04:13:52Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    4,
                    13,
                    52,
                    3,
                    361,
                    0
                ],
                "title": "Performance Characterization and Optimizations of Traditional ML\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterization and Optimizations of Traditional ML\n  Applications"
                },
                "summary": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Even in the era of Deep Learning based methods, traditional machine learning\nmethods with large data sets continue to attract significant attention.\nHowever, we find an apparent lack of a detailed performance characterization of\nthese methods in the context of large training datasets. In this work, we study\nthe system's behavior of a number of traditional ML methods as implemented in\npopular free software libraries/modules to identify critical performance\nbottlenecks experienced by these applications. The performance characterization\nstudy reveals several interesting insights on the performance of these\napplications. Then we evaluate the performance benefits of applying some\nwell-known optimizations at the levels of caches and the main memory. More\nspecifically, we test the usefulness of optimizations such as (i) software\nprefetching to improve cache performance and (ii) data layout and computation\nreordering optimizations to improve locality in DRAM accesses. These\noptimizations are implemented as modifications to the well-known scikit-learn\nlibrary, and hence can be easily leveraged by application programmers. We\nevaluate the impact of the proposed optimizations using a combination of\nsimulation and execution on a real system. The software prefetching\noptimization results in performance benefits varying from 5.2%-27.1% on\ndifferent ML applications while the data layout and computation reordering\napproaches yield 6.16%-28.0% performance improvement."
                },
                "authors": [
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "R. Govindarajan"
                    }
                ],
                "author_detail": {
                    "name": "R. Govindarajan"
                },
                "author": "R. Govindarajan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18960v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18960v1",
                "updated": "2024-12-25T18:36:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T18:36:21Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    18,
                    36,
                    21,
                    2,
                    360,
                    0
                ],
                "title": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRFlux: Virtual Reality Benchmark for Edge Caching Systems"
                },
                "summary": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a Unity based benchmark XRFlux for evaluating Virtual Reality\n(VR) delivery systems using edge-cloud caching. As VR applications and systems\nprogress, the need to meet strict latency and Quality of Experience (QoE)\nrequirements is increasingly evident. In the context of VR, traditional cloud\narchitectures (e.g., remote AWS S3 for content delivery) often struggle to meet\nthese demands, especially for users of the same application in different\nlocations. With edge computing, resources are brought closer to users in\nefforts to reduce latency and improve QoEs. However, VR's dynamic nature, with\nchanging fields of view (FoVs) and user synchronization requirements, creates\nvarious challenges for edge caching. We address the lack of suitable benchmarks\nand propose a framework that simulates multiuser VR scenarios while logging\nusers' interaction with objects within their actual and predicted FoVs. The\nbenchmark's activity log can then be played back through an edge cache to\nassess the resulting QoEs. This tool fills a gap by supporting research in the\noptimization of edge caching (and other edge-cloud functions) for VR streaming."
                },
                "authors": [
                    {
                        "name": "Nader Alfares"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18960v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18960v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18914v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18914v1",
                "updated": "2024-12-25T14:14:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:14:31Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    14,
                    31,
                    2,
                    360,
                    0
                ],
                "title": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-Range Tasks Using Short-Context LLMs: Incremental Reasoning With\n  Structured Memories"
                },
                "summary": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range tasks require reasoning over long inputs. Existing solutions\neither need large compute budgets, training data, access to model weights, or\nuse complex, task-specific approaches. We present PRISM, which alleviates these\nconcerns by processing information as a stream of chunks, maintaining a\nstructured in-context memory specified by a typed hierarchy schema. This\napproach demonstrates superior performance to baselines on diverse tasks while\nusing at least 4x smaller contexts than long-context models. Moreover, PRISM is\ntoken-efficient. By producing short outputs and efficiently leveraging\nkey-value (KV) caches, it achieves up to 54% cost reduction when compared to\nalternative short-context approaches. The method also scales down to tiny\ninformation chunks (e.g., 500 tokens) without increasing the number of tokens\nencoded or sacrificing quality. Furthermore, we show that it is possible to\ngenerate schemas to generalize our approach to new tasks with minimal effort."
                },
                "authors": [
                    {
                        "name": "Dulhan Jayalath"
                    },
                    {
                        "name": "James Bradley Wendt"
                    },
                    {
                        "name": "Nicholas Monath"
                    },
                    {
                        "name": "Sandeep Tata"
                    },
                    {
                        "name": "Beliz Gunel"
                    }
                ],
                "author_detail": {
                    "name": "Beliz Gunel"
                },
                "author": "Beliz Gunel",
                "arxiv_comment": "23 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18914v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18914v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18911v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18911v1",
                "updated": "2024-12-25T14:00:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T14:00:14Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    14,
                    0,
                    14,
                    2,
                    360,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Dual Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Dual Feature Caching"
                },
                "summary": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have become the dominant methods in image and\nvideo generation yet still suffer substantial computational costs. As an\neffective approach for DiT acceleration, feature caching methods are designed\nto cache the features of DiT in previous timesteps and reuse them in the next\ntimesteps, allowing us to skip the computation in the next timesteps. However,\non the one hand, aggressively reusing all the features cached in previous\ntimesteps leads to a severe drop in generation quality. On the other hand,\nconservatively caching only the features in the redundant layers or tokens but\nstill computing the important ones successfully preserves the generation\nquality but results in reductions in acceleration ratios. Observing such a\ntradeoff between generation quality and acceleration performance, this paper\nbegins by quantitatively studying the accumulated error from cached features.\nSurprisingly, we find that aggressive caching does not introduce significantly\nmore caching errors in the caching step, and the conservative feature caching\ncan fix the error introduced by aggressive caching. Thereby, we propose a dual\ncaching strategy that adopts aggressive and conservative caching iteratively,\nleading to significant acceleration and high generation quality at the same\ntime. Besides, we further introduce a V-caching strategy for token-wise\nconservative caching, which is compatible with flash attention and requires no\ntraining and calibration data.\n  Our codes have been released in Github: \\textbf{Code:\n\\href{https://github.com/Shenyi-Z/DuCa}{\\texttt{\\textcolor{cyan}{https://github.com/Shenyi-Z/DuCa}}}}"
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Evelyn Zhang"
                    },
                    {
                        "name": "Runlin Guo"
                    },
                    {
                        "name": "Haohang Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18911v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18911v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18885v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18885v1",
                "updated": "2024-12-25T11:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "published": "2024-12-25T11:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    11,
                    59,
                    17,
                    2,
                    360,
                    0
                ],
                "title": "Aspect-oriented Programming with Julia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-oriented Programming with Julia"
                },
                "summary": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes integrating Aspect-oriented Programming (AOP) into Julia,\na language widely used in scientific and High-Performance Computing (HPC). AOP\nenhances software modularity by encapsulating cross-cutting concerns, such as\nlogging, caching, and parallelizing, into separate, reusable aspects.\nLeveraging Julia's powerful metaprogramming and abstract syntax tree (AST)\nmanipulation capabilities, we introduce AspectJulia, an AOP framework designed\nto operate within Julia's runtime environment as a package. AspectJulia enables\ndevelopers to define and apply aspects seamlessly, leading to more modular,\nmaintainable, and adaptable code. We detail the implementation of AspectJulia\nand present diverse use cases, ranging from HPC and scientific computing to\nbusiness applications, demonstrating its effectiveness in managing\ncross-cutting concerns. This integration simplifies application development and\nimproves the adaptability of existing Julia modules and packages, paving the\nway for more efficient and maintainable software systems."
                },
                "authors": [
                    {
                        "name": "Osamu Ishimura"
                    },
                    {
                        "name": "Yoshihide Yoshimoto"
                    }
                ],
                "author_detail": {
                    "name": "Yoshihide Yoshimoto"
                },
                "author": "Yoshihide Yoshimoto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18885v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18885v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16187v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16187v2",
                "updated": "2024-12-24T13:04:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    13,
                    4,
                    45,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-13T06:00:27Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    6,
                    0,
                    27,
                    4,
                    348,
                    0
                ],
                "title": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HashEvict: A Pre-Attention KV Cache Eviction Strategy using\n  Locality-Sensitive Hashing"
                },
                "summary": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) use the key-value (KV) cache\nto significantly accelerate inference by storing the key and value embeddings\nof past tokens. However, this cache consumes significant GPU memory. In this\nwork, we introduce HashEvict, an algorithm that uses locality-sensitive hashing\n(LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache\nthat are cosine dissimilar to the current query token. This is achieved by\ncomputing the Hamming distance between binarized Gaussian projections of the\ncurrent token query and cached token keys, with a projection length much\nsmaller than the embedding dimension. We maintain a lightweight binary\nstructure in GPU memory to facilitate these calculations. Unlike existing\ncompression strategies that compute attention to determine token retention,\nHashEvict makes these decisions pre-attention, thereby reducing computational\ncosts. Additionally, HashEvict is dynamic - at every decoding step, the key and\nvalue of the current token replace the embeddings of a token expected to\nproduce the lowest attention score. We demonstrate that HashEvict can compress\nthe KV cache by 30%-70% while maintaining high performance across reasoning,\nmultiple-choice, long-context retrieval and summarization tasks."
                },
                "authors": [
                    {
                        "name": "Minghui Liu"
                    },
                    {
                        "name": "Tahseen Rabbani"
                    },
                    {
                        "name": "Tony O'Halloran"
                    },
                    {
                        "name": "Ananth Sankaralingam"
                    },
                    {
                        "name": "Mary-Anne Hartley"
                    },
                    {
                        "name": "Brian Gravelle"
                    },
                    {
                        "name": "Furong Huang"
                    },
                    {
                        "name": "Cornelia Fermüller"
                    },
                    {
                        "name": "Yiannis Aloimonos"
                    }
                ],
                "author_detail": {
                    "name": "Yiannis Aloimonos"
                },
                "author": "Yiannis Aloimonos",
                "arxiv_comment": "10 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16187v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16187v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v2",
                "updated": "2024-12-24T00:46:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    24,
                    0,
                    46,
                    0,
                    1,
                    359,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17747v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17747v1",
                "updated": "2024-12-23T18:02:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T18:02:25Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    18,
                    2,
                    25,
                    0,
                    358,
                    0
                ],
                "title": "Deliberation in Latent Space via Differentiable Cache Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deliberation in Latent Space via Differentiable Cache Augmentation"
                },
                "summary": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Techniques enabling large language models (LLMs) to \"think more\" by\ngenerating and attending to intermediate reasoning steps have shown promise in\nsolving complex problems. However, the standard approaches generate sequences\nof discrete tokens immediately before responding, and so they can incur\nsignificant latency costs and be challenging to optimize. In this work, we\ndemonstrate that a frozen LLM can be augmented with an offline coprocessor that\noperates on the model's key-value (kv) cache. This coprocessor augments the\ncache with a set of latent embeddings designed to improve the fidelity of\nsubsequent decoding. We train this coprocessor using the language modeling loss\nfrom the decoder on standard pretraining data, while keeping the decoder itself\nfrozen. This approach enables the model to learn, in an end-to-end\ndifferentiable fashion, how to distill additional computation into its\nkv-cache. Because the decoder remains unchanged, the coprocessor can operate\noffline and asynchronously, and the language model can function normally if the\ncoprocessor is unavailable or if a given cache is deemed not to require extra\ncomputation. We show experimentally that when a cache is augmented, the decoder\nachieves lower perplexity on numerous subsequent tokens. Furthermore, even\nwithout any task-specific training, our experiments demonstrate that cache\naugmentation consistently reduces perplexity and improves performance across a\nrange of reasoning-intensive tasks."
                },
                "authors": [
                    {
                        "name": "Luyang Liu"
                    },
                    {
                        "name": "Jonas Pfeiffer"
                    },
                    {
                        "name": "Jiaxing Wu"
                    },
                    {
                        "name": "Jun Xie"
                    },
                    {
                        "name": "Arthur Szlam"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Szlam"
                },
                "author": "Arthur Szlam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17747v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17747v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17685v1",
                "updated": "2024-12-23T16:11:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T16:11:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    16,
                    11,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducible Method for Mapping Electricity Transmission\n  Infrastructure for Space Weather Risk Assessment"
                },
                "summary": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Space weather impact assessment is constrained by the lack of available asset\ninformation to undertake modeling of Geomagnetically Induced Currents (GICs) in\nExtra High Voltage electricity infrastructure networks. The U.S. National Space\nWeather Strategy and Action Plan identifies underutilized data as a central\nissue for improving risk assessment, motivating this research. Accurate GIC\nprediction is generally not possible without information on the electrical\ncircuit, therefore we define a reproducible method based on open-source data,\nwhich enables risk analysts to collect their own substation component data.\nThis process converts OpenStreetMap (OSM) substation locations to\nhigh-resolution, component-level mapping of electricity transmission assets by\nutilizing an innovative web-browser platform to facilitate component\nannotation. As a case study example, we convert an initial 1,313 high-voltage\n(>115 kV) substations to 52,273 substation components via Google Earth APIs\nutilizing low-altitude, satellite, and Streetview imagery. We find that a total\nof 41,642 substation components (79.6%) connect to the highest substation\nvoltage levels (>345 kV) and are possibly susceptible to GIC, with a total of\n7,949 transformers identified. Compared to the initial OSM baseline, we provide\nnew detailed insights on voltage levels, line capacities, and substation\nconfigurations. Two validation workshops were undertaken to align the method\nand data with GIC assessment needs. The approach ensures consistency and rapid\nscalability, enabling users to quickly count components via a flexible\nweb-browser application."
                },
                "authors": [
                    {
                        "name": "Edward J. Oughton"
                    },
                    {
                        "name": "Evan Alexander Peters"
                    },
                    {
                        "name": "Dennies Bor"
                    },
                    {
                        "name": "Noah Rivera"
                    },
                    {
                        "name": "C. Trevor Gaunt"
                    },
                    {
                        "name": "Robert Weigel"
                    }
                ],
                "author_detail": {
                    "name": "Robert Weigel"
                },
                "author": "Robert Weigel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18919v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18919v2",
                "updated": "2024-12-23T14:40:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    14,
                    40,
                    26,
                    0,
                    358,
                    0
                ],
                "published": "2024-05-29T09:22:25Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    22,
                    25,
                    2,
                    150,
                    0
                ],
                "title": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inter-Satellite Link-Enhanced Transmission Scheme Towards Aviation IoT\n  in SAGIN"
                },
                "summary": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of the aviation Internet of Things (IoT) has positioned\nin-flight connectivity (IFC) as one of its critical applications.\nSpace-air-ground integrated networks (SAGIN) are essential for ensuring the\nperformance of IFC by enabling seamless and reliable connectivity. However,\nmost existing research treats satellites merely as transparent forwarding nodes\nand overlooks their potential caching capabilities to enhance IFC data rates.\nIn this article, we explore an IFC-oriented SAGIN where satellites and ground\nstations (GSs) work together to transmit content to airborne passengers,\nthereby facilitating airborne communication. By categorizing files into cached\n(instantly accessible via satellites) and non-cached files (available only\nthrough GSs), this article pioneers the integration of multiple inter-satellite\nlinks (ISLs) into the IFC framework, thus innovating the content delivery\nprocess for both types of files. To minimize the average delay of content\ndelivery, we formulate the corresponding optimization problems: 1) For cached\nfiles, we propose an exact penalty-based method to determine the satellite\nassociation scheme. 2) For non-cached files, we present an efficient algorithm\nbased on alternating optimization to jointly optimize satellite association and\nGS bandwidth allocation. Our proposed framework is low in complexity, paving\nthe way for high-speed Internet connectivity for aviation passengers. Finally,\nsimulation results are provided to demonstrate the effectiveness of our\nproposed IFC framework for SAGIN."
                },
                "authors": [
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Chenyu Wu"
                    },
                    {
                        "name": "Shuai Han"
                    },
                    {
                        "name": "Weixiao Meng"
                    },
                    {
                        "name": "Tony Q. S. Quek"
                    }
                ],
                "author_detail": {
                    "name": "Tony Q. S. Quek"
                },
                "author": "Tony Q. S. Quek",
                "arxiv_comment": "14 pages, 13 figures. This work has been accepted by IEEE Internet of\n  Things Journal. It is expanded on our previous research presented at the IEEE\n  Globecom 2024: Q. Chen, C. Wu, S. Han, W. Meng, and T. Q. Quek, \"Exploiting\n  Inter-Satellite Links for In-Flight Connectivity Scheme in Space-Air-Ground\n  Integrated Networks,\" in Proc. GLOBECOM 2024, Cape Town, South Africa, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18919v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18919v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.03408v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.03408v3",
                "updated": "2024-12-23T12:55:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    12,
                    55,
                    21,
                    0,
                    358,
                    0
                ],
                "published": "2024-02-05T15:10:42Z",
                "published_parsed": [
                    2024,
                    2,
                    5,
                    15,
                    10,
                    42,
                    0,
                    36,
                    0
                ],
                "title": "A Framework for Effective Invocation Methods of Various LLM Services",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Effective Invocation Methods of Various LLM Services"
                },
                "summary": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown impressive abilities in solving\nvarious natural language processing tasks and are now widely offered as\nservices. LLM services enable users to accomplish tasks without requiring\nspecialized knowledge, simply by paying service providers. However, numerous\nproviders offer various LLM services with variations in pricing, latency, and\nperformance. These factors are also affected by different invocation methods,\nsuch as the choice of context and the use of cache, which lead to unpredictable\nand uncontrollable service cost and quality. Consequently, utilizing various\nLLM services invocation methods to construct an effective (cost-saving,\nlow-latency and high-performance) invocation strategy that best meets task\ndemands becomes a pressing challenge. This paper provides a comprehensive\noverview of methods help LLM services to be invoked efficiently. Technically,\nwe define the problem of constructing an effective LLM services invocation\nstrategy, and based on this, propose a unified LLM service invocation\nframework. The framework classifies existing methods into four categories:\ninput abstraction, semantic cache, solution design, and output enhancement,\nwhich can be used separately or jointly during the invocation life cycle. We\ndiscuss the methods in each category and compare them to provide valuable\nguidance for researchers. Finally, we emphasize the open challenges in this\ndomain and shed light on future research."
                },
                "authors": [
                    {
                        "name": "Can Wang"
                    },
                    {
                        "name": "Dianbo Sui"
                    },
                    {
                        "name": "Bolin Zhang"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Jiabao Kang"
                    },
                    {
                        "name": "Zhidong Qiao"
                    },
                    {
                        "name": "Zhiying Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiying Tu"
                },
                "author": "Zhiying Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.03408v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.03408v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17464v1",
                "updated": "2024-12-23T10:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T10:41:18Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    10,
                    41,
                    18,
                    0,
                    358,
                    0
                ],
                "title": "CALLIC: Content Adaptive Learning for Lossless Image Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CALLIC: Content Adaptive Learning for Lossless Image Compression"
                },
                "summary": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learned lossless image compression has achieved significant advancements in\nrecent years. However, existing methods often rely on training amortized\ngenerative models on massive datasets, resulting in sub-optimal probability\ndistribution estimation for specific testing images during encoding process. To\naddress this challenge, we explore the connection between the Minimum\nDescription Length (MDL) principle and Parameter-Efficient Transfer Learning\n(PETL), leading to the development of a novel content-adaptive approach for\nlearned lossless image compression, dubbed CALLIC. Specifically, we first\npropose a content-aware autoregressive self-attention mechanism by leveraging\nconvolutional gating operations, termed Masked Gated ConvFormer (MGCF), and\npretrain MGCF on training dataset. Cache then Crop Inference (CCI) is proposed\nto accelerate the coding process. During encoding, we decompose pre-trained\nlayers, including depth-wise convolutions, using low-rank matrices and then\nadapt the incremental weights on testing image by Rate-guided Progressive\nFine-Tuning (RPFT). RPFT fine-tunes with gradually increasing patches that are\nsorted in descending order by estimated entropy, optimizing learning process\nand reducing adaptation time. Extensive experiments across diverse datasets\ndemonstrate that CALLIC sets a new state-of-the-art (SOTA) for learned lossless\nimage compression."
                },
                "authors": [
                    {
                        "name": "Daxin Li"
                    },
                    {
                        "name": "Yuanchao Bai"
                    },
                    {
                        "name": "Kai Wang"
                    },
                    {
                        "name": "Junjun Jiang"
                    },
                    {
                        "name": "Xianming Liu"
                    },
                    {
                        "name": "Wen Gao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Gao"
                },
                "author": "Wen Gao",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17246v1",
                "updated": "2024-12-23T03:38:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T03:38:46Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    3,
                    38,
                    46,
                    0,
                    358,
                    0
                ],
                "title": "Fast and Live Model Auto Scaling with O(1) Host Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Live Model Auto Scaling with O(1) Host Caching"
                },
                "summary": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model autoscaling is the key mechanism to achieve serverless\nmodel-as-a-service, but it faces a fundamental trade-off between scaling speed\nand storage/memory usage to cache parameters, and cannot meet frequent scaling\nrequirements across multiple hosts. The key problem is that data plane\nperformance is slow, and scaled instances remain stopped while parameters are\nloading. We first show that data plane can be made fast with no/O(1) caching by\nloading parameters through the compute network between GPUs because: (1) its\nspeed is comparable host cache and is underutilized; (2) scaling multiple\ninstances requires no or O(1) caching with network-optimized multicast. Second,\nautoscaling can be made live by breaking the scaling abstraction from a\ncoarse-grained instance-level to a fine-grained layer-level. This allows us to\noffload the layer computation from the overloaded serving instances to the\nscaled instance with cooperative execution, thus handles cases even when the\ncompute network is not sufficiently fast. Our system BLITZSCALE reduces the\nserving tail latencies by up to 86% without caching, and we achieve comparable\nperformance (or even better) to an optimal setup where all the parameters are\ncached at all the host for autoscaling."
                },
                "authors": [
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Haotian Wang"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v2",
                "updated": "2024-12-23T02:52:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    2,
                    52,
                    36,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17203v1",
                "updated": "2024-12-23T00:46:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "published": "2024-12-23T00:46:53Z",
                "published_parsed": [
                    2024,
                    12,
                    23,
                    0,
                    46,
                    53,
                    0,
                    358,
                    0
                ],
                "title": "Agile TLB Prefetching and Prediction Replacement Policy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agile TLB Prefetching and Prediction Replacement Policy"
                },
                "summary": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual-to-physical address translation is a critical performance bottleneck\nin paging-based virtual memory systems. The Translation Lookaside Buffer (TLB)\naccelerates address translation by caching frequently accessed mappings, but\nTLB misses lead to costly page walks. Hardware and software techniques address\nthis challenge. Hardware approaches enhance TLB reach through system-level\nsupport, while software optimizations include TLB prefetching, replacement\npolicies, superpages, and page size adjustments. Prefetching Page Table Entries\n(PTEs) for future accesses reduces bottlenecks but may incur overhead from\nincorrect predictions. Integrating an Agile TLB Prefetcher (ATP) with SBFP\noptimizes performance by leveraging page table locality and dynamically\nidentifying essential free PTEs during page walks. Predictive replacement\npolicies further improve TLB performance. Traditional LRU replacement is\nlimited to near-instant references, while advanced policies like SRRIP, GHRP,\nSHiP, SDBP, and CHiRP enhance performance by targeting specific inefficiencies.\nCHiRP, tailored for L2 TLBs, surpasses other policies by leveraging control\nflow history to detect dead blocks, utilizing L2 TLB entries for learning\ninstead of sampling. These integrated techniques collectively address key\nchallenges in virtual memory management."
                },
                "authors": [
                    {
                        "name": "Melkamu Mersha"
                    },
                    {
                        "name": "Tsion Abay"
                    },
                    {
                        "name": "Mingziem Bitewa"
                    },
                    {
                        "name": "Gedare Bloom"
                    }
                ],
                "author_detail": {
                    "name": "Gedare Bloom"
                },
                "author": "Gedare Bloom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v1",
                "updated": "2024-12-22T07:14:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17565v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17565v3",
                "updated": "2024-12-21T13:55:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    13,
                    55,
                    49,
                    5,
                    356,
                    0
                ],
                "published": "2024-06-25T14:02:08Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    14,
                    2,
                    8,
                    1,
                    177,
                    0
                ],
                "title": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemServe: Context Caching for Disaggregated LLM Serving with Elastic\n  Memory Pool"
                },
                "summary": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) serving has transformed from stateless to stateful\nsystems, utilizing techniques like context caching and disaggregated inference.\nThese optimizations extend the lifespan and domain of the KV cache,\nnecessitating a new architectural approach. We present MemServe, a unified\nsystem that integrates both inter-request and intra-request optimizations.\nMemServe introduces MemPool, an elastic memory pool managing distributed memory\nand KV caches across serving instances. Using MemPool APIs, MemServe combines\ncontext caching with disaggregated inference for the first time, supported by a\nglobal scheduler that enhances cache reuse through a global prompt tree-based\nlocality-aware policy. Tests show that MemServe significantly improves job\ncompletion time and time-to-first-time."
                },
                "authors": [
                    {
                        "name": "Cunchen Hu"
                    },
                    {
                        "name": "Heyang Huang"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Tao Xie"
                    },
                    {
                        "name": "Chenxi Wang"
                    },
                    {
                        "name": "Sa Wang"
                    },
                    {
                        "name": "Yungang Bao"
                    },
                    {
                        "name": "Ninghui Sun"
                    },
                    {
                        "name": "Yizhou Shan"
                    }
                ],
                "author_detail": {
                    "name": "Yizhou Shan"
                },
                "author": "Yizhou Shan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17565v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17565v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16585v1",
                "updated": "2024-12-21T11:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T11:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    11,
                    20,
                    26,
                    5,
                    356,
                    0
                ],
                "title": "Parameterized Complexity of Caching in Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameterized Complexity of Caching in Networks"
                },
                "summary": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The fundamental caching problem in networks asks to find an allocation of\ncontents to a network of caches with the aim of maximizing the cache hit rate.\nDespite the problem's importance to a variety of research areas -- including\nnot only content delivery, but also edge intelligence and inference -- and the\nextensive body of work on empirical aspects of caching, very little is known\nabout the exact boundaries of tractability for the problem beyond its general\nNP-hardness. We close this gap by performing a comprehensive\ncomplexity-theoretic analysis of the problem through the lens of the\nparameterized complexity paradigm, which is designed to provide more precise\nstatements regarding algorithmic tractability than classical complexity. Our\nresults include algorithmic lower and upper bounds which together establish the\nconditions under which the caching problem becomes tractable."
                },
                "authors": [
                    {
                        "name": "Robert Ganian"
                    },
                    {
                        "name": "Fionn Mc Inerney"
                    },
                    {
                        "name": "Dimitra Tsigkari"
                    }
                ],
                "author_detail": {
                    "name": "Dimitra Tsigkari"
                },
                "author": "Dimitra Tsigkari",
                "arxiv_comment": "A shorter version of this paper will appear in the proceedings of\n  AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v4",
                "updated": "2024-12-21T02:36:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    2,
                    36,
                    3,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16434v1",
                "updated": "2024-12-21T01:48:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "published": "2024-12-21T01:48:52Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    1,
                    48,
                    52,
                    5,
                    356,
                    0
                ],
                "title": "SYMPHONY: Improving Memory Management for LLM Inference Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SYMPHONY: Improving Memory Management for LLM Inference Workloads"
                },
                "summary": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly being deployed in applications\nsuch as chatbots, code editors, and conversational agents. A key feature of\nLLMs is their ability to engage in multi-turn interactions with humans or\nexternal tools, enabling a wide range of tasks. Each new request in a\nmulti-turn interaction depends on the intermediate state, specifically the\nkey-value (K,V) caches, from previous requests in the ongoing interaction.\nExisting serving engines either recompute the K,V caches or offload them to\nmain memory. Profiling reveals that recomputation can result in over 99% of\nprocessed tokens being redundant. On the other hand, offloading K,V caches from\nGPU memory makes inference serving stateful, leading to load imbalances across\nthe cluster. To address these challenges, we developed SYMPHONY. SYMPHONY\nleverages the observation that multi-turn work loads provide additional hints\nthat allow K,V caches to be migrated off the critical serving path. By\nutilizing these hints, SYMPHONY dynamically migrates K,V caches to enable\nfinegrained scheduling of inference requests. Our experiments demonstrate that\nSYMPHONY can handle over 8x the number of requests compared to state-of-the-art\nbaselines, with a similar latency profile."
                },
                "authors": [
                    {
                        "name": "Saurabh Agarwal"
                    },
                    {
                        "name": "Anyong Mao"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16001v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16001v1",
                "updated": "2024-12-20T15:51:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T15:51:42Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    51,
                    42,
                    4,
                    355,
                    0
                ],
                "title": "Multi-Strided Access Patterns to Boost Hardware Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Strided Access Patterns to Boost Hardware Prefetching"
                },
                "summary": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Important memory-bound kernels, such as linear algebra, convolutions, and\nstencils, rely on SIMD instructions as well as optimizations targeting improved\nvectorized data traversal and data re-use to attain satisfactory performance.\nOn on temporary CPU architectures, the hardware prefetcher is of key importance\nfor efficient utilization of the memory hierarchy. In this paper, we\ndemonstrate that transforming a memory access pattern consisting of a single\nstride to one that concurrently accesses multiple strides, can boost the\nutilization of the hardware prefetcher, and in turn improves the performance of\nmemory-bound kernels significantly. Using a set of micro-benchmarks, we\nestablish that accessing memory in a multi-strided manner enables more cache\nlines to be concurrently brought into the cache, resulting in improved cache\nhit ratios and higher effective memory bandwidth without the introduction of\ncostly software prefetch instructions. Subsequently, we show that multi-strided\nvariants of a collection of six memory-bound dense compute kernels outperform\nstate-of-the-art counterparts on three different micro-architectures. More\nspecifically, for kernels among which Matrix Vector Multiplication, Convolution\nStencil and kernels from PolyBench, we achieve significant speedups of up to\n12.55x over Polly, 2.99x over MKL, 1.98x over OpenBLAS, 1.08x over Halide and\n1.87x over OpenCV. The code transformation to take advantage of multi-strided\nmemory access is a natural extension of the loop unroll and loop interchange\ntechniques, allowing this method to be incorporated into compiler pipelines in\nthe future."
                },
                "authors": [
                    {
                        "name": "Miguel O. Blom"
                    },
                    {
                        "name": "Kristian F. D. Rietveld"
                    },
                    {
                        "name": "Rob V. van Nieuwpoort"
                    }
                ],
                "author_detail": {
                    "name": "Rob V. van Nieuwpoort"
                },
                "author": "Rob V. van Nieuwpoort",
                "arxiv_comment": "12 pages, 6 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16001v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16001v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14485v2",
                "updated": "2024-12-20T15:18:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    15,
                    18,
                    44,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-19T03:11:33Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    3,
                    11,
                    33,
                    3,
                    354,
                    0
                ],
                "title": "Towards Projected and Incremental Pseudo-Boolean Model Counting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Projected and Incremental Pseudo-Boolean Model Counting"
                },
                "summary": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model counting is a fundamental task that involves determining the number of\nsatisfying assignments to a logical formula, typically in conjunctive normal\nform (CNF). While CNF model counting has received extensive attention over\nrecent decades, interest in Pseudo-Boolean (PB) model counting is just emerging\npartly due to the greater flexibility of PB formulas. As such, we observed\nfeature gaps in existing PB counters such as a lack of support for projected\nand incremental settings, which could hinder adoption. In this work, our main\ncontribution is the introduction of the PB model counter PBCount2, the first\nexact PB model counter with support for projected and incremental model\ncounting. Our counter, PBCount2, uses our Least Occurrence Weighted Min Degree\n(LOW-MD) computation ordering heuristic to support projected model counting and\na cache mechanism to enable incremental model counting. In our evaluations,\nPBCount2 completed at least 1.40x the number of benchmarks of competing methods\nfor projected model counting and at least 1.18x of competing methods in\nincremental model counting."
                },
                "authors": [
                    {
                        "name": "Suwei Yang"
                    },
                    {
                        "name": "Kuldeep S. Meel"
                    }
                ],
                "author_detail": {
                    "name": "Kuldeep S. Meel"
                },
                "author": "Kuldeep S. Meel",
                "arxiv_comment": "To appear in AAAI25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15605v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15605v1",
                "updated": "2024-12-20T06:58:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "published": "2024-12-20T06:58:32Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    6,
                    58,
                    32,
                    4,
                    355,
                    0
                ],
                "title": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Do RAG: When Cache-Augmented Generation is All You Need for\n  Knowledge Tasks"
                },
                "summary": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) has gained traction as a powerful\napproach for enhancing language models by integrating external knowledge\nsources. However, RAG introduces challenges such as retrieval latency,\npotential errors in document selection, and increased system complexity. With\nthe advent of large language models (LLMs) featuring significantly extended\ncontext windows, this paper proposes an alternative paradigm, cache-augmented\ngeneration (CAG) that bypasses real-time retrieval. Our method involves\npreloading all relevant resources, especially when the documents or knowledge\nfor retrieval are of a limited and manageable size, into the LLM's extended\ncontext and caching its runtime parameters. During inference, the model\nutilizes these preloaded parameters to answer queries without additional\nretrieval steps. Comparative analyses reveal that CAG eliminates retrieval\nlatency and minimizes retrieval errors while maintaining context relevance.\nPerformance evaluations across multiple benchmarks highlight scenarios where\nlong-context LLMs either outperform or complement traditional RAG pipelines.\nThese findings suggest that, for certain applications, particularly those with\na constrained knowledge base, CAG provide a streamlined and efficient\nalternative to RAG, achieving comparable or superior results with reduced\ncomplexity."
                },
                "authors": [
                    {
                        "name": "Brian J Chan"
                    },
                    {
                        "name": "Chao-Ting Chen"
                    },
                    {
                        "name": "Jui-Hung Cheng"
                    },
                    {
                        "name": "Hen-Hsen Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hen-Hsen Huang"
                },
                "author": "Hen-Hsen Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15605v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15605v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02820v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02820v3",
                "updated": "2024-12-19T23:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    23,
                    52,
                    16,
                    3,
                    354,
                    0
                ],
                "published": "2024-11-05T05:41:41Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    5,
                    41,
                    41,
                    1,
                    310,
                    0
                ],
                "title": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DroidSpeak: KV Cache Sharing for Cross-LLM Communication and Multi-LLM\n  Serving"
                },
                "summary": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly employed in complex workflows,\nwhere different LLMs and fine-tuned variants collaboratively address complex\ntasks. However, these systems face significant inefficiencies due to redundant\ncontext processing of the shared context. We propose DroidSpeak, a framework\nthat optimizes context sharing between fine-tuned LLMs derived from the same\nfoundational model. DroidSpeak identifies critical layers in the KV cache and\nselectively recomputes them, enabling effective reuse of intermediate data\nwhile maintaining high accuracy.\n  Our approach balances computational efficiency and task fidelity,\nsignificantly reducing inference latency and throughput bottlenecks.\nExperiments on diverse datasets and model pairs demonstrate that DroidSpeak\nachieves up to 3x higher throughputs and 2.6x faster prefill times with\nnegligible accuracy loss compared to full recomputation."
                },
                "authors": [
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Zhuohan Gu"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Junchen Jiang"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Madan Musuvathi"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02820v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02820v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12592v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12592v2",
                "updated": "2024-12-19T22:34:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    22,
                    34,
                    37,
                    3,
                    354,
                    0
                ],
                "published": "2024-08-22T17:56:29Z",
                "published_parsed": [
                    2024,
                    8,
                    22,
                    17,
                    56,
                    29,
                    3,
                    235,
                    0
                ],
                "title": "Exposing Shadow Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exposing Shadow Branches"
                },
                "summary": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern processors implement a decoupled front-end in the form of Fetch\nDirected Instruction Prefetching (FDIP) to avoid front-end stalls. FDIP is\ndriven by the Branch Prediction Unit (BPU), relying on the BPU's accuracy and\nbranch target tracking structures to speculatively fetch instructions into the\nInstruction Cache (L1I). As data center applications become more complex, their\ncode footprints also grow, resulting in an increase in Branch Target Buffer\n(BTB) misses. FDIP can alleviate L1I cache misses, but when it encounters a BTB\nmiss, the BPU may not identify the current instruction as a branch to FDIP.\nThis can prevent FDIP from prefetching or cause it to speculate down the wrong\npath, further polluting the L1I cache. We observe that the vast majority, 75%,\nof BTB-missing, unidentified branches are actually present in instruction cache\nlines that FDIP has previously fetched but, these missing branches have not yet\nbeen decoded and inserted into the BTB. This is because the instruction line is\ndecoded from an entry point (which is the target of the previous taken branch)\ntill an exit point (the taken branch). Branch instructions present in the\nignored portion of the cache line we call them \"Shadow Branches\". Here we\npresent Skeia, a novel shadow branch decoding technique that identifies and\ndecodes unused bytes in cache lines fetched by FDIP, inserting them into a\nShadow Branch Buffer (SBB). The SBB is accessed in parallel with the BTB,\nallowing FDIP to speculate despite a BTB miss. With a minimal storage state of\n12.25KB, Skeia delivers a geomean speedup of ~5.7% over an 8K-entry BTB (78KB)\nand ~2% versus adding an equal amount of state to the BTB across 16 front-end\nbound applications. Since many branches stored in the SBB are unique compared\nto those in a similarly sized BTB, we consistently observe greater performance\ngains with Skeia across all examined sizes until saturation."
                },
                "authors": [
                    {
                        "name": "Chrysanthos Pepi"
                    },
                    {
                        "name": "Bhargav Reddy Godala"
                    },
                    {
                        "name": "Krishnam Tibrewala"
                    },
                    {
                        "name": "Gino Chacon"
                    },
                    {
                        "name": "Paul V. Gratz"
                    },
                    {
                        "name": "Daniel A. Jiménez"
                    },
                    {
                        "name": "Gilles A. Pokam"
                    },
                    {
                        "name": "David I. August"
                    }
                ],
                "author_detail": {
                    "name": "David I. August"
                },
                "author": "David I. August",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12592v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12592v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14838v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14838v1",
                "updated": "2024-12-19T13:28:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "published": "2024-12-19T13:28:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    13,
                    28,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context\n  LLMs"
                },
                "summary": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient KV cache management in LLMs is crucial for long-context tasks like\nRAG and summarization. Existing KV cache compression methods enforce a fixed\npattern, neglecting task-specific characteristics and reducing the retention of\nessential information. However, we observe distinct activation patterns across\nlayers in various tasks, highlighting the need for adaptive strategies tailored\nto each task's unique demands. Based on this insight, we propose DynamicKV, a\nmethod that dynamically optimizes token retention by adjusting the number of\ntokens retained at each layer to adapt to the specific task. DynamicKV\nestablishes global and per-layer maximum KV cache budgets, temporarily\nretaining the maximum budget for the current layer, and periodically updating\nthe KV cache sizes of all preceding layers during inference. Our method retains\nonly 1.7% of the KV cache size while achieving ~85% of the Full KV cache\nperformance on LongBench. Notably, even under extreme compression (0.9%),\nDynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the\nNeedle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be\nreleased."
                },
                "authors": [
                    {
                        "name": "Xiabin Zhou"
                    },
                    {
                        "name": "Wenbin Wang"
                    },
                    {
                        "name": "Minyan Zeng"
                    },
                    {
                        "name": "Jiaxian Guo"
                    },
                    {
                        "name": "Xuebo Liu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Liang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Liang Ding"
                },
                "author": "Liang Ding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14838v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14838v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05317v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05317v3",
                "updated": "2024-12-19T12:38:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    19,
                    12,
                    38,
                    23,
                    3,
                    354,
                    0
                ],
                "published": "2024-10-05T03:47:06Z",
                "published_parsed": [
                    2024,
                    10,
                    5,
                    3,
                    47,
                    6,
                    5,
                    279,
                    0
                ],
                "title": "Accelerating Diffusion Transformers with Token-wise Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformers with Token-wise Feature Caching"
                },
                "summary": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformers have shown significant effectiveness in both image and\nvideo synthesis at the expense of huge computation costs. To address this\nproblem, feature caching methods have been introduced to accelerate diffusion\ntransformers by caching the features in previous timesteps and reusing them in\nthe following timesteps. However, previous caching methods ignore that\ndifferent tokens exhibit different sensitivities to feature caching, and\nfeature caching on some tokens may lead to 10$\\times$ more destruction to the\noverall generation quality compared with other tokens. In this paper, we\nintroduce token-wise feature caching, allowing us to adaptively select the most\nsuitable tokens for caching, and further enable us to apply different caching\nratios to neural layers in different types and depths. Extensive experiments on\nPixArt-$\\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image\nand video generation with no requirements for training. For instance,\n2.36$\\times$ and 1.93$\\times$ acceleration are achieved on OpenSora and\nPixArt-$\\alpha$ with almost no drop in generation quality."
                },
                "authors": [
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Xuyang Liu"
                    },
                    {
                        "name": "Ting Liu"
                    },
                    {
                        "name": "Siteng Huang"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "In this version, we achieved a nearly lossless acceleration of 1.51\n  times for ToCa on FLUX in the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05317v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05317v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14392v1",
                "updated": "2024-12-18T22:52:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:52:12Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    52,
                    12,
                    2,
                    353,
                    0
                ],
                "title": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nemesis: Noise-randomized Encryption with Modular Efficiency and Secure\n  Integration in Machine Learning Systems"
                },
                "summary": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) systems that guarantee security and privacy often rely\non Fully Homomorphic Encryption (FHE) as a cornerstone technique, enabling\ncomputations on encrypted data without exposing sensitive information. However,\na critical limitation of FHE is its computational inefficiency, making it\nimpractical for large-scale applications. In this work, we propose\n\\textit{Nemesis}, a framework that accelerates FHE-based systems without\ncompromising accuracy or security. The design of Nemesis is inspired by Rache\n(SIGMOD'23), which introduced a caching mechanism for encrypted integers and\nscalars. Nemesis extends this idea with more advanced caching techniques and\nmathematical tools, enabling efficient operations over multi-slot FHE schemes\nand overcoming Rache's limitations to support general plaintext structures. We\nformally prove the security of Nemesis under standard cryptographic assumptions\nand evaluate its performance extensively on widely used datasets, including\nMNIST, FashionMNIST, and CIFAR-10. Experimental results show that Nemesis\nsignificantly reduces the computational overhead of FHE-based ML systems,\npaving the way for broader adoption of privacy-preserving technologies."
                },
                "authors": [
                    {
                        "name": "Dongfang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Dongfang Zhao"
                },
                "author": "Dongfang Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14363v1",
                "updated": "2024-12-18T22:01:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T22:01:55Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    22,
                    1,
                    55,
                    2,
                    353,
                    0
                ],
                "title": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ResQ: Mixed-Precision Quantization of Large Language Models with\n  Low-Rank Residuals"
                },
                "summary": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) of large language models (LLMs) holds the\npromise in reducing the prohibitive computational cost at inference time.\nQuantization of all weight, activation and key-value (KV) cache tensors to\n4-bit without significantly degrading generalizability is challenging, due to\nthe high quantization error caused by extreme outliers in activations. To\ntackle this problem, we propose ResQ, a PTQ method that pushes further the\nstate-of-the-art. By means of principal component analysis (PCA), it identifies\na low-rank subspace (in practice 1/8 of the hidden dimension) in which\nactivation variances are highest, and keep the coefficients within this\nsubspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.\nWithin each subspace, invariant random rotation is applied to further suppress\noutliers. We show that this is a provably optimal mixed precision quantization\nscheme that minimizes error. With the Llama families of models, we demonstrate\nthat ResQ outperforms recent uniform and mixed precision PTQ methods on a\nvariety of benchmarks, achieving up to 33% lower perplexity on Wikitext than\nthe next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code\nis available at https://github.com/utkarsh-dmx/project-resq."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Sayeh Sharify"
                    },
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "14 pages, 6 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v1",
                "updated": "2024-12-18T21:09:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). C3 on average achieves only 21% of ideal speedup, this is\ndue to known challenges of compute and memory interference between concurrent\nGPU kernels (that is, sharing of GPU's compute units, caches and HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build Concurrent Communication CoLlectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.16179v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.16179v4",
                "updated": "2024-12-18T17:36:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    17,
                    36,
                    36,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-21T16:44:51Z",
                "published_parsed": [
                    2024,
                    10,
                    21,
                    16,
                    44,
                    51,
                    0,
                    295,
                    0
                ],
                "title": "MagicPIG: LSH Sampling for Efficient LLM Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicPIG: LSH Sampling for Efficient LLM Generation"
                },
                "summary": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with long context windows have gained\nsignificant attention. However, the KV cache, stored to avoid re-computation,\nbecomes a bottleneck. Various dynamic sparse or TopK-based attention\napproximation methods have been proposed to leverage the common insight that\nattention is sparse. In this paper, we first show that TopK attention itself\nsuffers from quality degradation in certain downstream tasks because attention\nis not always as sparse as expected. Rather than selecting the keys and values\nwith the highest attention scores, sampling with theoretical guarantees can\nprovide a better estimation for attention output. To make the sampling-based\napproximation practical in LLM generation, we propose MagicPIG, a heterogeneous\nsystem based on Locality Sensitive Hashing (LSH). MagicPIG significantly\nreduces the workload of attention computation while preserving high accuracy\nfor diverse tasks. MagicPIG stores the LSH hash tables and runs the attention\ncomputation on the CPU, which allows it to serve longer contexts and larger\nbatch sizes with high approximation accuracy. MagicPIG can improve decoding\nthroughput by up to $5\\times$ across various GPU hardware and achieve 54ms\ndecoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a\ncontext of 96k tokens. The code is available at\nhttps://github.com/Infini-AI-Lab/MagicPIG."
                },
                "authors": [
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Zihao Ye"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Jianyu Zhang"
                    },
                    {
                        "name": "Niklas Nolte"
                    },
                    {
                        "name": "Yuandong Tian"
                    },
                    {
                        "name": "Matthijs Douze"
                    },
                    {
                        "name": "Leon Bottou"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.16179v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.16179v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v1",
                "updated": "2024-12-18T12:16:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic\n  Regularization"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13771v1",
                "updated": "2024-12-18T12:07:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T12:07:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    7,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Convergence: Harmonizing Recommender Systems via Two-Stage\n  Alignment and Behavioral Semantic Tokenization"
                },
                "summary": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs), endowed with exceptional reasoning\ncapabilities, are adept at discerning profound user interests from historical\nbehaviors, thereby presenting a promising avenue for the advancement of\nrecommendation systems. However, a notable discrepancy persists between the\nsparse collaborative semantics typically found in recommendation systems and\nthe dense token representations within LLMs. In our study, we propose a novel\nframework that harmoniously merges traditional recommendation models with the\nprowess of LLMs. We initiate this integration by transforming ItemIDs into\nsequences that align semantically with the LLMs space, through the proposed\nAlignment Tokenization module. Additionally, we design a series of specialized\nsupervised learning tasks aimed at aligning collaborative signals with the\nsubtleties of natural language semantics. To ensure practical applicability, we\noptimize online inference by pre-caching the top-K results for each user,\nreducing latency and improving effciency. Extensive experimental evidence\nindicates that our model markedly improves recall metrics and displays\nremarkable scalability of recommendation systems."
                },
                "authors": [
                    {
                        "name": "Guanghan Li"
                    },
                    {
                        "name": "Xun Zhang"
                    },
                    {
                        "name": "Yufei Zhang"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Guojun Yin"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "7 pages, 3 figures, AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v2",
                "updated": "2024-12-18T09:47:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    47,
                    25,
                    2,
                    353,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13649v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13649v1",
                "updated": "2024-12-18T09:27:33Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T09:27:33Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    9,
                    27,
                    33,
                    2,
                    353,
                    0
                ],
                "title": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Optimizing Key-Value Cache Compression in Long-context Generation"
                },
                "summary": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache has become a bottleneck of LLMs for long-context\ngeneration. Despite the numerous efforts in this area, the optimization for the\ndecoding phase is generally ignored. However, we believe such optimization is\ncrucial, especially for long-output generation tasks based on the following two\nobservations: (i) Excessive compression during the prefill phase, which\nrequires specific full context impairs the comprehension of the reasoning task;\n(ii) Deviation of heavy hitters occurs in the reasoning tasks with long\noutputs. Therefore, SCOPE, a simple yet efficient framework that separately\nperforms KV cache optimization during the prefill and decoding phases, is\nintroduced. Specifically, the KV cache during the prefill phase is preserved to\nmaintain the essential information, while a novel strategy based on sliding is\nproposed to select essential heavy hitters for the decoding phase. Memory usage\nand memory transfer are further optimized using adaptive and discontinuous\nstrategies. Extensive experiments on LongGenBench show the effectiveness and\ngeneralization of SCOPE and its compatibility as a plug-in to other\nprefill-only KV compression methods."
                },
                "authors": [
                    {
                        "name": "Jialong Wu"
                    },
                    {
                        "name": "Zhenglin Wang"
                    },
                    {
                        "name": "Linhai Zhang"
                    },
                    {
                        "name": "Yilong Lai"
                    },
                    {
                        "name": "Yulan He"
                    },
                    {
                        "name": "Deyu Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Deyu Zhou"
                },
                "author": "Deyu Zhou",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13649v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13649v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08584v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08584v2",
                "updated": "2024-12-18T07:45:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    7,
                    45,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-10-11T07:24:21Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    24,
                    21,
                    4,
                    285,
                    0
                ],
                "title": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipVL: Efficient Large Vision-Language Models with Dynamic Token\n  Sparsification"
                },
                "summary": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of large vision-language models (LVLMs) is constrained by the\ncomputational bottleneck of the attention mechanism during the prefill phase\nand the memory bottleneck of fetching the key-value (KV) cache in the decoding\nphase, particularly in scenarios involving high-resolution images or videos.\nVisual content often exhibits substantial redundancy, resulting in highly\nsparse attention maps within LVLMs. This sparsity can be leveraged to\naccelerate attention computation or compress the KV cache through various\napproaches. However, most studies focus on addressing only one of these\nbottlenecks and do not adequately support dynamic adjustment of sparsity\nconcerning distinct layers or tasks. In this paper, we present ZipVL, an\nefficient inference framework designed for LVLMs through a dynamic ratio\nallocation strategy of important tokens. This ratio is adaptively determined\nbased on the layer-specific distribution of attention scores, rather than fixed\nhyper-parameters, thereby improving efficiency for less complex tasks while\nmaintaining high performance for more challenging ones. Then we select\nimportant tokens based on their normalized attention scores and perform sparse\nattention mechanism solely on those important tokens, reducing the latency in\nthe prefill phase. Tokens deemed less important will be discarded to reduce KV\ncache size, alleviating the memory bottleneck in the decoding phase. Our\nexperiments demonstrate that ZipVL can accelerate the prefill phase by\n2.3$\\times$ and improve decoding throughput by 2.8$\\times$, with a minimal\naccuracy reduction of only 0.5\\% on VQAv2 benchmark over LLaVA-Next-13B model,\neffectively enhancing the generation efficiency of LVLMs."
                },
                "authors": [
                    {
                        "name": "Yefei He"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Hong Zhou"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    },
                    {
                        "name": "Bohan Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Bohan Zhuang"
                },
                "author": "Bohan Zhuang",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08584v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08584v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13509v1",
                "updated": "2024-12-18T05:16:11Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-18T05:16:11Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    16,
                    11,
                    2,
                    353,
                    0
                ],
                "title": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vivar: A Generative AR System for Intuitive Multi-Modal Sensor Data\n  Presentation"
                },
                "summary": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding sensor data can be challenging for non-experts because of the\ncomplexity and unique semantic meanings of sensor modalities. This calls for\nintuitive and effective methods to present sensor information. However,\ncreating intuitive sensor data visualizations presents three key challenges:\nthe variability of sensor readings, gaps in domain comprehension, and the\ndynamic nature of sensor data. To address these issues, we develop Vivar, a\nnovel AR system that integrates multi-modal sensor data and presents 3D\nvolumetric content for visualization. In particular, we introduce a cross-modal\nembedding approach that maps sensor data into a pre-trained visual embedding\nspace through barycentric interpolation. This allows for accurate and\ncontinuous integration of multi-modal sensor information. Vivar also\nincorporates sensor-aware AR scene generation using foundation models and 3D\nGaussian Splatting (3DGS) without requiring domain expertise. In addition,\nVivar leverages latent reuse and caching strategies to accelerate 2D and AR\ncontent generation. Our extensive experiments demonstrate that our system\nachieves 11$\\times$ latency reduction without compromising quality. A user\nstudy involving over 485 participants, including domain experts, demonstrates\nVivar's effectiveness in accuracy, consistency, and real-world applicability,\npaving the way for more intuitive sensor data visualization."
                },
                "authors": [
                    {
                        "name": "Yunqi Guo"
                    },
                    {
                        "name": "Kaiyuan Hou"
                    },
                    {
                        "name": "Heming Fu"
                    },
                    {
                        "name": "Hongkai Chen"
                    },
                    {
                        "name": "Zhenyu Yan"
                    },
                    {
                        "name": "Guoliang Xing"
                    },
                    {
                        "name": "Xiaofan Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofan Jiang"
                },
                "author": "Xiaofan Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12486v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12486v2",
                "updated": "2024-12-18T05:08:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    18,
                    5,
                    8,
                    39,
                    2,
                    353,
                    0
                ],
                "published": "2024-12-17T02:43:54Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    43,
                    54,
                    1,
                    352,
                    0
                ],
                "title": "Boosting Long-Context Management via Query-Guided Activation Refilling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Long-Context Management via Query-Guided Activation Refilling"
                },
                "summary": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts poses a significant challenge for large language\nmodels (LLMs) due to their inherent context-window limitations and the\ncomputational burden of extensive key-value (KV) activations, which severely\nimpact efficiency. For information-seeking tasks, full context perception is\noften unnecessary, as a query's information needs can dynamically range from\nlocalized details to a global perspective, depending on its complexity.\nHowever, existing methods struggle to adapt effectively to these dynamic\ninformation needs.\n  In the paper, we propose a method for processing long-context\ninformation-seeking tasks via query-guided Activation Refilling (ACRE). ACRE\nconstructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache\ncompactly captures global information, and the layer-2 (L2) cache provides\ndetailed and localized information. ACRE establishes a proxying relationship\nbetween the two caches, allowing the input query to attend to the L1 cache and\ndynamically refill it with relevant entries from the L2 cache. This mechanism\nintegrates global understanding with query-specific local details, thus\nimproving answer decoding. Experiments on a variety of long-context\ninformation-seeking datasets demonstrate ACRE's effectiveness, achieving\nimprovements in both performance and efficiency."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Defu Lian"
                    }
                ],
                "author_detail": {
                    "name": "Defu Lian"
                },
                "author": "Defu Lian",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12486v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12486v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v3",
                "updated": "2024-12-17T14:45:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    45,
                    12,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaosheng Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12953v1",
                "updated": "2024-12-17T14:34:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T14:34:51Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    14,
                    34,
                    51,
                    1,
                    352,
                    0
                ],
                "title": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Diffusion Transformer Policies with Mixture of Expert\n  Denoisers for Multitask Learning"
                },
                "summary": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Policies have become widely used in Imitation Learning, offering\nseveral appealing properties, such as generating multimodal and discontinuous\nbehavior. As models are becoming larger to capture more complex capabilities,\ntheir computational demands increase, as shown by recent scaling laws.\nTherefore, continuing with the current architectures will present a\ncomputational roadblock. To address this gap, we propose Mixture-of-Denoising\nExperts (MoDE) as a novel policy for Imitation Learning. MoDE surpasses current\nstate-of-the-art Transformer-based Diffusion Policies while enabling\nparameter-efficient scaling through sparse experts and noise-conditioned\nrouting, reducing both active parameters by 40% and inference costs by 90% via\nexpert caching. Our architecture combines this efficient scaling with\nnoise-conditioned self-attention mechanism, enabling more effective denoising\nacross different noise levels. MoDE achieves state-of-the-art performance on\n134 tasks in four established imitation learning benchmarks (CALVIN and\nLIBERO). Notably, by pretraining MoDE on diverse robotics data, we achieve 4.01\non CALVIN ABC and 0.95 on LIBERO-90. It surpasses both CNN-based and\nTransformer Diffusion Policies by an average of 57% across 4 benchmarks, while\nusing 90% fewer FLOPs and fewer active parameters compared to default Diffusion\nTransformer architectures. Furthermore, we conduct comprehensive ablations on\nMoDE's components, providing insights for designing efficient and scalable\nTransformer architectures for Diffusion Policies. Code and demonstrations are\navailable at https://mbreuss.github.io/MoDE_Diffusion_Policy/."
                },
                "authors": [
                    {
                        "name": "Moritz Reuss"
                    },
                    {
                        "name": "Jyothish Pari"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Rudolf Lioutikov"
                    }
                ],
                "author_detail": {
                    "name": "Rudolf Lioutikov"
                },
                "author": "Rudolf Lioutikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12798v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12798v1",
                "updated": "2024-12-17T11:00:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T11:00:56Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    11,
                    0,
                    56,
                    1,
                    352,
                    0
                ],
                "title": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZoRI: Towards Discriminative Zero-Shot Remote Sensing Instance\n  Segmentation"
                },
                "summary": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Instance segmentation algorithms in remote sensing are typically based on\nconventional methods, limiting their application to seen scenarios and\nclosed-set predictions. In this work, we propose a novel task called zero-shot\nremote sensing instance segmentation, aimed at identifying aerial objects that\nare absent from training data. Challenges arise when classifying aerial\ncategories with high inter-class similarity and intra-class variance. Besides,\nthe domain gap between vision-language models' pretraining datasets and remote\nsensing datasets hinders the zero-shot capabilities of the pretrained model\nwhen it is directly applied to remote sensing images. To address these\nchallenges, we propose a $\\textbf{Z}$ero-Sh$\\textbf{o}$t $\\textbf{R}$emote\nSensing $\\textbf{I}$nstance Segmentation framework, dubbed $\\textbf{ZoRI}$. Our\napproach features a discrimination-enhanced classifier that uses refined\ntextual embeddings to increase the awareness of class disparities. Instead of\ndirect fine-tuning, we propose a knowledge-maintained adaptation strategy that\ndecouples semantic-related information to preserve the pretrained\nvision-language alignment while adjusting features to capture remote sensing\ndomain-specific visual cues. Additionally, we introduce a prior-injected\nprediction with cache bank of aerial visual prototypes to supplement the\nsemantic richness of text embeddings and seamlessly integrate aerial\nrepresentations, adapting to the remote sensing domain. We establish new\nexperimental protocols and benchmarks, and extensive experiments convincingly\ndemonstrate that ZoRI achieves the state-of-art performance on the zero-shot\nremote sensing instance segmentation task. Our code is available at\nhttps://github.com/HuangShiqi128/ZoRI."
                },
                "authors": [
                    {
                        "name": "Shiqi Huang"
                    },
                    {
                        "name": "Shuting He"
                    },
                    {
                        "name": "Bihan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Bihan Wen"
                },
                "author": "Bihan Wen",
                "arxiv_comment": "AAAI 2025, code see https://github.com/HuangShiqi128/ZoRI",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12798v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12798v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12706v1",
                "updated": "2024-12-17T09:20:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T09:20:31Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    20,
                    31,
                    1,
                    352,
                    0
                ],
                "title": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "More Tokens, Lower Precision: Towards the Optimal Token-Precision\n  Trade-off in KV Cache Compression"
                },
                "summary": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) process increasing context windows, the\nmemory usage of KV cache has become a critical bottleneck during inference. The\nmainstream KV compression methods, including KV pruning and KV quantization,\nprimarily focus on either token or precision dimension and seldom explore the\nefficiency of their combination. In this paper, we comprehensively investigate\nthe token-precision trade-off in KV cache compression. Experiments demonstrate\nthat storing more tokens in the KV cache with lower precision, i.e., quantized\npruning, can significantly enhance the long-context performance of LLMs.\nFurthermore, in-depth analysis regarding token-precision trade-off from a\nseries of key aspects exhibit that, quantized pruning achieves substantial\nimprovements in retrieval-related tasks and consistently performs well across\nvarying input lengths. Moreover, quantized pruning demonstrates notable\nstability across different KV pruning methods, quantization strategies, and\nmodel scales. These findings provide valuable insights into the token-precision\ntrade-off in KV cache compression. We plan to release our code in the near\nfuture."
                },
                "authors": [
                    {
                        "name": "Jiebin Zhang"
                    },
                    {
                        "name": "Dawei Zhu"
                    },
                    {
                        "name": "Yifan Song"
                    },
                    {
                        "name": "Wenhao Wu"
                    },
                    {
                        "name": "Chuqiao Kuang"
                    },
                    {
                        "name": "Xiaoguang Li"
                    },
                    {
                        "name": "Lifeng Shang"
                    },
                    {
                        "name": "Qun Liu"
                    },
                    {
                        "name": "Sujian Li"
                    }
                ],
                "author_detail": {
                    "name": "Sujian Li"
                },
                "author": "Sujian Li",
                "arxiv_comment": "13pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12513v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12513v2",
                "updated": "2024-12-17T09:11:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    9,
                    11,
                    47,
                    1,
                    352,
                    0
                ],
                "published": "2024-10-16T12:45:35Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    12,
                    45,
                    35,
                    2,
                    290,
                    0
                ],
                "title": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FiRST: Finetuning Router-Selective Transformers for Input-Adaptive\n  Latency Reduction"
                },
                "summary": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive Large Language Models (LLMs) demonstrate remarkable\nperformance across different domains such as vision and language processing.\nHowever, due to sequential processing through a stack of transformer layers,\nautoregressive decoding faces significant computation/latency challenges,\nparticularly in resource-constrained environments like mobile and edge devices.\nExisting approaches in literature that aim to improve latency via skipping\nlayers have two distinct flavors - 1) Early exit, and 2) Input-agnostic\nheuristics where tokens exit at pre-determined layers irrespective of input\nsequence. Both the above strategies have limitations - the former cannot be\napplied to handle KV Caching necessary for speed-ups in modern framework and\nthe latter does not capture the variation in layer importance across tasks or\nmore generally, across input sequences. To address both limitations, we propose\nFiRST, an algorithm that reduces inference latency by using layer-specific\nrouters to select a subset of transformer layers adaptively for each input\nsequence - the prompt (during the prefill stage) decides which layers will be\nskipped during decoding. FiRST preserves compatibility with KV caching enabling\nfaster inference while being quality-aware. FiRST is model-agnostic and can be\neasily enabled on any pre-trained LLM. Our approach reveals that input\nadaptivity is critical - indeed, different task-specific middle layers play a\ncrucial role in evolving hidden representations depending on tasks. Extensive\nexperiments show that FiRST significantly reduces latency while outperforming\nother layer selection strategies in quality metics. It retains competitive\nperformance to base model (without layer skipping) and in some cases, even\nimproves upon it. FiRST is thus a promising and efficient solution for LLM\ndeployment in low-resource environments."
                },
                "authors": [
                    {
                        "name": "Akriti Jain"
                    },
                    {
                        "name": "Saransh Sharma"
                    },
                    {
                        "name": "Koyel Mukherjee"
                    },
                    {
                        "name": "Soumyabrata Pal"
                    }
                ],
                "author_detail": {
                    "name": "Soumyabrata Pal"
                },
                "author": "Soumyabrata Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12513v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12513v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v3",
                "updated": "2024-12-17T05:40:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    40,
                    9,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboAttention: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12543v1",
                "updated": "2024-12-17T05:09:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T05:09:45Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    5,
                    9,
                    45,
                    1,
                    352,
                    0
                ],
                "title": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Deep Reinforcement Learning for Heterogeneous\n  Edge Content Caching Networks"
                },
                "summary": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Proactive caching is essential for minimizing latency and improving Quality\nof Experience (QoE) in multi-server edge networks. Federated Deep Reinforcement\nLearning (FDRL) is a promising approach for developing cache policies tailored\nto dynamic content requests. However, FDRL faces challenges such as an\nexpanding caching action space due to increased content numbers and difficulty\nin adapting global information to heterogeneous edge environments. In this\npaper, we propose a Personalized Federated Deep Reinforcement Learning\nframework for Caching, called PF-DRL-Ca, with the aim to maximize system\nutility while satisfying caching capability constraints. To manage the\nexpanding action space, we employ a new DRL algorithm, Multi-head Deep\nQ-Network (MH-DQN), which reshapes the action output layers of DQN into a\nmulti-head structure where each head generates a sub-dimensional action. We\nnext integrate the proposed MH-DQN into a personalized federated training\nframework, employing a layer-wise approach for training to derive a\npersonalized model that can adapt to heterogeneous environments while\nexploiting the global information to accelerate learning convergence. Our\nextensive experimental results demonstrate the superiority of MH-DQN over\ntraditional DRL algorithms on a single server, as well as the advantages of the\npersonal federated training architecture compared to other frameworks."
                },
                "authors": [
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Tan Li"
                    },
                    {
                        "name": "Hai Liu"
                    },
                    {
                        "name": "Tse-Tin Chan"
                    }
                ],
                "author_detail": {
                    "name": "Tse-Tin Chan"
                },
                "author": "Tse-Tin Chan",
                "arxiv_comment": "8 pages, 8 figures, WiOpt 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12488v1",
                "updated": "2024-12-17T02:44:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T02:44:43Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    2,
                    44,
                    43,
                    1,
                    352,
                    0
                ],
                "title": "A System for Microserving of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A System for Microserving of LLMs"
                },
                "summary": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advances in LLMs bring a strong demand for efficient system\nsupport to improve overall serving efficiency. As LLM inference scales towards\nmultiple GPUs and even multiple compute nodes, various coordination patterns,\nsuch as prefill-decode disaggregation and context migration, arise in serving\nsystems. Most inference services today expose a coarse-grained request-level\nAPI with a pre-configured coordination strategy, limiting the ability to\ncustomize and dynamically reconfigure the coordination. In this paper, we\npropose LLM microserving, a multi-level architecture for structuring and\nprogramming LLM inference services. We introduces simple yet effective\nmicroserving APIs to support fine-grained sub-request level actions. A\nprogrammable router transforms user requests into sub-request calls, enabling\nthe dynamic reconfiguration of serving patterns. To support diverse execution\npatterns, we develop a unified KV cache interface that handles various KV\ncompute, transfer, and reuse scenarios. Our evaluation shows that LLM\nmicroserving can be reconfigured to support multiple disaggregation\norchestration strategies in a few lines of Python code while maintaining\nstate-of-the-art performance for LLM inference tasks. Additionally, it allows\nus to explore new strategy variants that reduce up to 47% of job completion\ntime compared to the existing strategies."
                },
                "authors": [
                    {
                        "name": "Hongyi Jin"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Charlie F. Ruan"
                    },
                    {
                        "name": "Yingcheng Wang"
                    },
                    {
                        "name": "Todd C. Mowry"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Tianqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianqi Chen"
                },
                "author": "Tianqi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12444v1",
                "updated": "2024-12-17T01:12:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "published": "2024-12-17T01:12:35Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    1,
                    12,
                    35,
                    1,
                    352,
                    0
                ],
                "title": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyDiT: Lazy Learning for the Acceleration of Diffusion Transformers"
                },
                "summary": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers have emerged as the preeminent models for a wide array\nof generative tasks, demonstrating superior performance and efficacy across\nvarious applications. The promising results come at the cost of slow inference,\nas each denoising step requires running the whole transformer model with a\nlarge amount of parameters. In this paper, we show that performing the full\ncomputation of the model at each diffusion step is unnecessary, as some\ncomputations can be skipped by lazily reusing the results of previous steps.\nFurthermore, we show that the lower bound of similarity between outputs at\nconsecutive steps is notably high, and this similarity can be linearly\napproximated using the inputs. To verify our demonstrations, we propose the\n\\textbf{LazyDiT}, a lazy learning framework that efficiently leverages cached\nresults from earlier steps to skip redundant computations. Specifically, we\nincorporate lazy learning layers into the model, effectively trained to\nmaximize laziness, enabling dynamic skipping of redundant computations.\nExperimental results show that LazyDiT outperforms the DDIM sampler across\nmultiple diffusion transformer models at various resolutions. Furthermore, we\nimplement our method on mobile devices, achieving better performance than DDIM\nwith similar latency."
                },
                "authors": [
                    {
                        "name": "Xuan Shen"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Yufa Zhou"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Yanyu Li"
                    },
                    {
                        "name": "Yifan Gong"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Jason Kuen"
                    },
                    {
                        "name": "Henghui Ding"
                    },
                    {
                        "name": "Zhihao Shu"
                    },
                    {
                        "name": "Wei Niu"
                    },
                    {
                        "name": "Pu Zhao"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Jiuxiang Gu"
                    }
                ],
                "author_detail": {
                    "name": "Jiuxiang Gu"
                },
                "author": "Jiuxiang Gu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v1",
                "updated": "2024-12-16T14:49:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, based on the View Selection Problem, we propose a\nunified view on these problems. We identify the root causes of the complexity\nof these selection problems and provide a detailed analysis of techniques to\ncope with them. Our survey provides a modern classification of selection\nalgorithms known in the literature, including the latest ones based on Machine\nLearning. We provide a ground for the reuse of the selection techniques between\ndifferent optimization scenarios and highlight challenges and promising\ndirections in the field."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11741v1",
                "updated": "2024-12-16T13:01:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T13:01:53Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    13,
                    1,
                    53,
                    0,
                    351,
                    0
                ],
                "title": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CSR:Achieving 1 Bit Key-Value Cache via Sparse Representation"
                },
                "summary": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of long-context text applications utilizing large language\nmodels (LLMs) has presented significant scalability challenges, particularly in\nmemory footprint. The linear growth of the Key-Value (KV) cache responsible for\nstoring attention keys and values to minimize redundant computations can lead\nto substantial increases in memory consumption, potentially causing models to\nfail to serve with limited memory resources. To address this issue, we propose\na novel approach called Cache Sparse Representation (CSR), which converts the\nKV cache by transforming the dense Key-Value cache tensor into sparse indexes\nand weights, offering a more memory-efficient representation during LLM\ninference. Furthermore, we introduce NeuralDict, a novel neural network-based\nmethod for automatically generating the dictionary used in our sparse\nrepresentation. Our extensive experiments demonstrate that CSR achieves\nperformance comparable to state-of-the-art KV cache quantization algorithms\nwhile maintaining robust functionality in memory-constrained environments."
                },
                "authors": [
                    {
                        "name": "Hongxuan Zhang"
                    },
                    {
                        "name": "Yao Zhao"
                    },
                    {
                        "name": "Jiaqi Zheng"
                    },
                    {
                        "name": "Chenyi Zhuang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11706v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11706v1",
                "updated": "2024-12-16T12:28:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T12:28:22Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    12,
                    28,
                    22,
                    0,
                    351,
                    0
                ],
                "title": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsymRnR: Video Diffusion Transformers Acceleration with Asymmetric\n  Reduction and Restoration"
                },
                "summary": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Diffusion Transformers (DiTs) have demonstrated significant potential\nfor generating high-fidelity videos but are computationally intensive. Existing\nacceleration methods include distillation, which requires costly retraining,\nand feature caching, which is highly sensitive to network architecture. Recent\ntoken reduction methods are training-free and architecture-agnostic, offering\ngreater flexibility and wider applicability. However, they enforce the same\nsequence length across different components, constraining their acceleration\npotential. We observe that intra-sequence redundancy in video DiTs varies\nacross features, blocks, and denoising timesteps. Building on this observation,\nwe propose Asymmetric Reduction and Restoration (AsymRnR), a training-free\napproach to accelerate video DiTs. It offers a flexible and adaptive strategy\nthat reduces the number of tokens based on their redundancy to enhance both\nacceleration and generation quality. We further propose matching cache to\nfacilitate faster processing. Integrated into state-of-the-art video DiTs,\nAsymRnR achieves a superior speedup without compromising the quality."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11706v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11706v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11685v1",
                "updated": "2024-12-16T11:55:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "published": "2024-12-16T11:55:26Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    11,
                    55,
                    26,
                    0,
                    351,
                    0
                ],
                "title": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultra-High-Definition Dynamic Multi-Exposure Image Fusion via Infinite\n  Pixel Learning"
                },
                "summary": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the continuous improvement of device imaging resolution, the popularity\nof Ultra-High-Definition (UHD) images is increasing. Unfortunately, existing\nmethods for fusing multi-exposure images in dynamic scenes are designed for\nlow-resolution images, which makes them inefficient for generating high-quality\nUHD images on a resource-constrained device. To alleviate the limitations of\nextremely long-sequence inputs, inspired by the Large Language Model (LLM) for\nprocessing infinitely long texts, we propose a novel learning paradigm to\nachieve UHD multi-exposure dynamic scene image fusion on a single\nconsumer-grade GPU, named Infinite Pixel Learning (IPL). The design of our\napproach comes from three key components: The first step is to slice the input\nsequences to relieve the pressure generated by the model processing the data\nstream; Second, we develop an attention cache technique, which is similar to KV\ncache for infinite data stream processing; Finally, we design a method for\nattention cache compression to alleviate the storage burden of the cache on the\ndevice. In addition, we provide a new UHD benchmark to evaluate the\neffectiveness of our method. Extensive experimental results show that our\nmethod maintains high-quality visual performance while fusing UHD dynamic\nmulti-exposure images in real-time (>40fps) on a single consumer-grade GPU."
                },
                "authors": [
                    {
                        "name": "Xingchi Chen"
                    },
                    {
                        "name": "Zhuoran Zheng"
                    },
                    {
                        "name": "Xuerui Li"
                    },
                    {
                        "name": "Yuying Chen"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Wenqi Ren"
                    }
                ],
                "author_detail": {
                    "name": "Wenqi Ren"
                },
                "author": "Wenqi Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v1",
                "updated": "2024-12-15T21:02:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.02388v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.02388v3",
                "updated": "2024-12-15T03:29:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    3,
                    29,
                    54,
                    6,
                    350,
                    0
                ],
                "published": "2023-05-03T19:07:06Z",
                "published_parsed": [
                    2023,
                    5,
                    3,
                    19,
                    7,
                    6,
                    2,
                    123,
                    0
                ],
                "title": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PULSE: Accelerating Distributed Pointer-Traversals on Disaggregated\n  Memory (Extended Version)"
                },
                "summary": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches at CPU nodes in disaggregated memory architectures amortize the high\ndata access latency over the network. However, such caches are fundamentally\nunable to improve performance for workloads requiring pointer traversals across\nlinked data structures. We argue for accelerating these pointer traversals\ncloser to disaggregated memory in a manner that preserves expressiveness for\nsupporting various linked structures, ensures energy efficiency and\nperformance, and supports distributed execution. We design PULSE, a distributed\npointer-traversal framework for rack-scale disaggregated memory to meet all the\nabove requirements. Our evaluation of PULSE shows that it enables low-latency,\nhigh-throughput, and energy-efficient execution for a wide range of pointer\ntraversal workloads on disaggregated memory that fare poorly with caching\nalone."
                },
                "authors": [
                    {
                        "name": "Yupeng Tang"
                    },
                    {
                        "name": "Seung-seob Lee"
                    },
                    {
                        "name": "Abhishek Bhattacharjee"
                    },
                    {
                        "name": "Anurag Khandelwal"
                    }
                ],
                "author_detail": {
                    "name": "Anurag Khandelwal"
                },
                "author": "Anurag Khandelwal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.02388v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.02388v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11021v1",
                "updated": "2024-12-15T02:30:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "published": "2024-12-15T02:30:09Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    2,
                    30,
                    9,
                    6,
                    350,
                    0
                ],
                "title": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SparseMap: Loop Mapping for Sparse CNNs on Streaming Coarse-grained\n  Reconfigurable Array"
                },
                "summary": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming coarse-grained reconfgurable array (CGRA) is a promising\narchitecture for data/computing-intensive applications because of its\nfexibility, high throughput and efcient memory system. However,when\naccelerating sparse CNNs, the irregular input data demands inside sparse CNNs\nwould cause excessive caching operations (COPs) and multi-cycle internal\ndependencies (MCIDs) between operations, declining the throughput of the\nstreaming CGRA. We propose a mapping method for sparse CNNs onto streaming\nCGRA, SparseMap, which incorporates an efcient I/O data management along with\noperation scheduling and binding, to reduce the COPs and MCIDs, thereby\nensuring the optimal throughput of streaming CGRA.The experimental results show\nSparseMap reduces 92.5% COPs and 46.0 % MCIDs while achieves the same or even\nsmaller initiation interval (II) compared to previous works."
                },
                "authors": [
                    {
                        "name": "Xiaobing Ni"
                    },
                    {
                        "name": "Mengke Ge"
                    },
                    {
                        "name": "Jiaheng Ruan"
                    },
                    {
                        "name": "Song Chen"
                    },
                    {
                        "name": "Yi Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Kang"
                },
                "author": "Yi Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15246v1",
                "updated": "2024-12-14T06:47:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T06:47:56Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    6,
                    47,
                    56,
                    5,
                    349,
                    0
                ],
                "title": "Accelerating Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Retrieval-Augmented Generation"
                },
                "summary": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An evolving solution to address hallucination and enhance accuracy in large\nlanguage models (LLMs) is Retrieval-Augmented Generation (RAG), which involves\naugmenting LLMs with information retrieved from an external knowledge source,\nsuch as the web. This paper profiles several RAG execution pipelines and\ndemystifies the complex interplay between their retrieval and generation\nphases. We demonstrate that while exact retrieval schemes are expensive, they\ncan reduce inference time compared to approximate retrieval variants because an\nexact retrieval model can send a smaller but more accurate list of documents to\nthe generative model while maintaining the same end-to-end accuracy. This\nobservation motivates the acceleration of the exact nearest neighbor search for\nRAG.\n  In this work, we design Intelligent Knowledge Store (IKS), a type-2 CXL\ndevice that implements a scale-out near-memory acceleration architecture with a\nnovel cache-coherent interface between the host CPU and near-memory\naccelerators. IKS offers 13.4-27.9x faster exact nearest neighbor search over a\n512GB vector database compared with executing the search on Intel Sapphire\nRapids CPUs. This higher search performance translates to 1.7-26.3x lower\nend-to-end inference time for representative RAG applications. IKS is\ninherently a memory expander; its internal DRAM can be disaggregated and used\nfor other applications running on the server to prevent DRAM, which is the most\nexpensive component in today's servers, from being stranded."
                },
                "authors": [
                    {
                        "name": "Derrick Quinn"
                    },
                    {
                        "name": "Mohammad Nouri"
                    },
                    {
                        "name": "Neel Patel"
                    },
                    {
                        "name": "John Salihu"
                    },
                    {
                        "name": "Alireza Salemi"
                    },
                    {
                        "name": "Sukhan Lee"
                    },
                    {
                        "name": "Hamed Zamani"
                    },
                    {
                        "name": "Mohammad Alian"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Alian"
                },
                "author": "Mohammad Alian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10685v1",
                "updated": "2024-12-14T05:20:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "published": "2024-12-14T05:20:50Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    5,
                    20,
                    50,
                    5,
                    349,
                    0
                ],
                "title": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RMCSA Algorithm for Congestion-Aware and Service Latency Aware Dynamic\n  Service Provisioning in Software-Defined SDM-EONs"
                },
                "summary": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The implementation of 5G and the future deployment of 6G necessitate the\nutilization of optical networks that possess substantial capacity and exhibit\nminimal latency. The dynamic arrival and departure of connection requests in\noptical networks result in particular central links experiencing more traffic\nand congestion than non-central links. The occurrence of congested links leads\nto service blocking despite the availability of resources within the network,\nrestricting the efficient utilization of network resources. The available\nalgorithms in the literature that aim to balance load among network links offer\na trade-off between blocking performance and algorithmic complexity, thus\nincreasing service provisioning time. This work proposes a dynamic\nrouting-based congestion-aware routing, modulation, core, and spectrum\nassignment (RMCSA) algorithm for space division multiplexing elastic optical\nnetworks (SDM-EONs). The algorithm finds alternative candidate paths based on\nreal-time link occupancy metrics to minimize blocking due to link congestion\nunder dynamic traffic scenarios. As a result, the algorithm reduces the\nformation of congestion hotspots in the network owing to link-betweenness\ncentrality. We have performed extensive simulations using two realistic network\ntopologies to compare the performance of the proposed algorithm with relevant\nRMCSA algorithms available in the literature. The simulation results verify the\nsuperior performance of our proposed algorithm compared to the benchmark Yen's\nK-shortest paths and K-Disjoint shortest paths RMCSA algorithms in connection\nblocking ratio and spectrum utilization efficiency. To expedite the\nroute-finding process, we present a novel caching strategy that allows the\nproposed algorithm to demonstrate a much-reduced service delay time compared to\nthe recently developed adaptive link weight-based load-balancing RMCSA\nalgorithm."
                },
                "authors": [
                    {
                        "name": "Baljinder Singh Heera"
                    },
                    {
                        "name": "Shrinivas Petale"
                    },
                    {
                        "name": "Yatindra Nath Singh"
                    },
                    {
                        "name": "Suresh Subramaniam"
                    }
                ],
                "author_detail": {
                    "name": "Suresh Subramaniam"
                },
                "author": "Suresh Subramaniam",
                "arxiv_comment": "The preliminary work was presented at ONDM 2023 conference.\n  https://doi.org/10.23919/ONDM57372.2023.10144866",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10319v1",
                "updated": "2024-12-13T17:59:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:59:52Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    59,
                    52,
                    4,
                    348,
                    0
                ],
                "title": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCBench: A KV Cache-Centric Analysis of Long-Context Methods"
                },
                "summary": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context LLMs have enabled numerous downstream applications but also\nintroduced significant challenges related to computational and memory\nefficiency. To address these challenges, optimizations for long-context\ninference have been developed, centered around the KV cache. However, existing\nbenchmarks often evaluate in single-request, neglecting the full lifecycle of\nthe KV cache in real-world use. This oversight is particularly critical, as KV\ncache reuse has become widely adopted in LLMs inference frameworks, such as\nvLLM and SGLang, as well as by LLM providers, including OpenAI, Microsoft,\nGoogle, and Anthropic. To address this gap, we introduce\nSCBench(SharedContextBench), a comprehensive benchmark for evaluating\nlong-context methods from a KV cachecentric perspective: 1) KV cache\ngeneration, 2) KV cache compression, 3) KV cache retrieval, 4) KV cache\nloading. Specifically, SCBench uses test examples with shared context, ranging\n12 tasks with two shared context modes, covering four categories of\nlong-context capabilities: string retrieval, semantic retrieval, global\ninformation, and multi-task. With it, we provide an extensive KV cache-centric\nanalysis of eight categories long-context solutions, including Gated Linear\nRNNs, Mamba-Attention hybrids, and efficient methods such as sparse attention,\nKV cache dropping, quantization, retrieval, loading, and prompt compression.\nThe evaluation is conducted on 8 long-context LLMs. Our findings show that\nsub-O(n) memory methods suffer in multi-turn scenarios, while sparse encoding\nwith O(n) memory and sub-O(n^2) pre-filling computation perform robustly.\nDynamic sparsity yields more expressive KV caches than static patterns, and\nlayer-level sparsity in hybrid architectures reduces memory usage with strong\nperformance. Additionally, we identify attention distribution shift issues in\nlong-generation scenarios. https://aka.ms/SCBench."
                },
                "authors": [
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Xufang Luo"
                    },
                    {
                        "name": "Surin Ahn"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Amir H. Abdi"
                    },
                    {
                        "name": "Dongsheng Li"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10302v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10302v1",
                "updated": "2024-12-13T17:37:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T17:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    17,
                    37,
                    48,
                    4,
                    348,
                    0
                ],
                "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced\n  Multimodal Understanding"
                },
                "summary": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2."
                },
                "authors": [
                    {
                        "name": "Zhiyu Wu"
                    },
                    {
                        "name": "Xiaokang Chen"
                    },
                    {
                        "name": "Zizheng Pan"
                    },
                    {
                        "name": "Xingchao Liu"
                    },
                    {
                        "name": "Wen Liu"
                    },
                    {
                        "name": "Damai Dai"
                    },
                    {
                        "name": "Huazuo Gao"
                    },
                    {
                        "name": "Yiyang Ma"
                    },
                    {
                        "name": "Chengyue Wu"
                    },
                    {
                        "name": "Bingxuan Wang"
                    },
                    {
                        "name": "Zhenda Xie"
                    },
                    {
                        "name": "Yu Wu"
                    },
                    {
                        "name": "Kai Hu"
                    },
                    {
                        "name": "Jiawei Wang"
                    },
                    {
                        "name": "Yaofeng Sun"
                    },
                    {
                        "name": "Yukun Li"
                    },
                    {
                        "name": "Yishi Piao"
                    },
                    {
                        "name": "Kang Guan"
                    },
                    {
                        "name": "Aixin Liu"
                    },
                    {
                        "name": "Xin Xie"
                    },
                    {
                        "name": "Yuxiang You"
                    },
                    {
                        "name": "Kai Dong"
                    },
                    {
                        "name": "Xingkai Yu"
                    },
                    {
                        "name": "Haowei Zhang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Yisong Wang"
                    },
                    {
                        "name": "Chong Ruan"
                    }
                ],
                "author_detail": {
                    "name": "Chong Ruan"
                },
                "author": "Chong Ruan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10302v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10302v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v2",
                "updated": "2024-12-13T16:13:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    16,
                    13,
                    39,
                    4,
                    348,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas Köstler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v1",
                "updated": "2024-12-13T14:11:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.12021v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.12021v2",
                "updated": "2024-12-13T14:08:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    8,
                    55,
                    4,
                    348,
                    0
                ],
                "published": "2024-09-18T14:31:33Z",
                "published_parsed": [
                    2024,
                    9,
                    18,
                    14,
                    31,
                    33,
                    2,
                    262,
                    0
                ],
                "title": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Offline ORAM with Perfect Security via Simple Oblivious Priority\n  Queues"
                },
                "summary": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oblivious RAM (ORAM) is a well-researched primitive to hide the memory access\npattern of a RAM computation; it has a variety of applications in trusted\ncomputing, outsourced storage, and multiparty computation. In this paper, we\nstudy the so-called offline ORAM in which the sequence of memory access\nlocations to be hidden is known in advance. Apart from their theoretical\nsignificance, offline ORAMs can be used to construct efficient oblivious\nalgorithms.\n  We obtain the first optimal offline ORAM with perfect security from oblivious\npriority queues via time-forward processing. For this, we present a simple\nconstruction of an oblivious priority queue with perfect security. Our\nconstruction achieves an asymptotically optimal (amortized) runtime of\n$\\Theta(\\log N)$ per operation for a capacity of $N$ elements and is of\nindependent interest.\n  Building on our construction, we additionally present efficient\nexternal-memory instantiations of our oblivious, perfectly-secure construction:\nFor the cache-aware setting, we match the optimal I/O complexity of\n$\\Theta(\\frac{1}{B} \\log \\frac{N}{M})$ per operation (amortized), and for the\ncache-oblivious setting we achieve a near-optimal I/O complexity of\n$O(\\frac{1}{B} \\log \\frac{N}{M} \\log\\log_M N)$ per operation (amortized)."
                },
                "authors": [
                    {
                        "name": "Thore Thießen"
                    },
                    {
                        "name": "Jan Vahrenhold"
                    }
                ],
                "author_detail": {
                    "name": "Jan Vahrenhold"
                },
                "author": "Jan Vahrenhold",
                "arxiv_doi": "10.4230/LIPIcs.ISAAC.2024.55",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.4230/LIPIcs.ISAAC.2024.55",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.12021v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.12021v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "23 pages, full version of the paper in ISAAC 2024; minor changes",
                "arxiv_journal_ref": "Thore Thie{\\ss}en and Jan Vahrenhold. Optimal offline ORAM with\n  perfect security via simple oblivious priority queues. In 35th International\n  Symposium on Algorithms and Computation (ISAAC 2024), 18 pages. 2024",
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v1",
                "updated": "2024-12-13T02:26:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize compression\nrate while maintaining great accuracy. LLMs' Feed-Forward Network (FFN)\ncomponents, which typically comprise a large proportion of parameters (around\n3/2), ensure that our FFN optimizations would have a better chance of achieving\neffective compression. Moreover, our findings are beneficial to general LLMs\nand are not restricted to ReLU-based models. This work systematically\ninvestigates the tradeoff between enforcing activation sparsity and perplexity\n(accuracy) on state-of-the-art LLMs. Our empirical analysis demonstrates that\nwe can obtain around 50% of main memory and computing reductions for critical\nFFN components with negligible accuracy degradation. This extra 50% sparsity\ndoes not naturally exist in the current LLMs, which require tuning LLMs'\nactivation outputs by injecting zero-enforcing thresholds. To obtain the\nbenefits of activation sparsity, we provide a guideline for the system\narchitect for LLM prediction and prefetching. The success prediction allows the\nsystem to prefetch the necessary weights while omitting the inactive ones and\ntheir successors, therefore lowering cache and memory pollution and reducing\nLLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_comment": "Conference submission for IPCCC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12178v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09474v1",
                "updated": "2024-12-12T17:20:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T17:20:26Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    17,
                    20,
                    26,
                    3,
                    347,
                    0
                ],
                "title": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing CDN Architectures: Multi-Metric Algorithmic Breakthroughs for\n  Edge and Distributed Performance"
                },
                "summary": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Content Delivery Network (CDN) is a powerful system of distributed caching\nservers that aims to accelerate content delivery, like high-definition video,\nIoT applications, and ultra-low-latency services, efficiently and with fast\nvelocity. This has become of paramount importance in the post-pandemic era.\nChallenges arise when exponential content volume growth and scalability across\ndifferent geographic locations are required. This paper investigates\ndata-driven evaluations of CDN algorithms in dynamic server selection for\nlatency reduction, bandwidth throttling for efficient resource management,\nreal-time Round Trip Time analysis for adaptive routing, and programmatic\nnetwork delay simulation to emulate various conditions. Key performance\nmetrics, such as round-trip time (RTT) and CPU usage, are carefully analyzed to\nevaluate scalability and algorithmic efficiency through two experimental\nsetups: a constrained edge-like local system and a scalable FABRIC testbed. The\nstatistical validation of RTT trends, alongside CPU utilization, is presented\nin the results. The optimization process reveals significant trade-offs between\nscalability and resource consumption, providing actionable insights for\neffectively deploying and enhancing CDN algorithms in edge and distributed\ncomputing environments."
                },
                "authors": [
                    {
                        "name": "Md Nurul Absur"
                    },
                    {
                        "name": "Sourya Saha"
                    },
                    {
                        "name": "Sifat Nawrin Nova"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Md Rahat Ul Nasib"
                    }
                ],
                "author_detail": {
                    "name": "Md Rahat Ul Nasib"
                },
                "author": "Md Rahat Ul Nasib",
                "arxiv_comment": "6 Pages, 10 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09416v1",
                "updated": "2024-12-12T16:24:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T16:24:35Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    16,
                    24,
                    35,
                    3,
                    347,
                    0
                ],
                "title": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying AI Tutor Evaluation: An Evaluation Taxonomy for Pedagogical\n  Ability Assessment of LLM-Powered AI Tutors"
                },
                "summary": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we investigate whether current state-of-the-art large language\nmodels (LLMs) are effective as AI tutors and whether they demonstrate\npedagogical abilities necessary for good AI tutoring in educational dialogues.\nPrevious efforts towards evaluation have been limited to subjective protocols\nand benchmarks. To bridge this gap, we propose a unified evaluation taxonomy\nwith eight pedagogical dimensions based on key learning sciences principles,\nwhich is designed to assess the pedagogical value of LLM-powered AI tutor\nresponses grounded in student mistakes or confusion in the mathematical domain.\nWe release MRBench -- a new evaluation benchmark containing 192 conversations\nand 1,596 responses from seven state-of-the-art LLM-based and human tutors,\nproviding gold annotations for eight pedagogical dimensions. We assess\nreliability of the popular Prometheus2 LLM as an evaluator and analyze each\ntutor's pedagogical abilities, highlighting which LLMs are good tutors and\nwhich ones are more suitable as question-answering systems. We believe that the\npresented taxonomy, benchmark, and human-annotated labels will streamline the\nevaluation process and help track the progress in AI tutors' development."
                },
                "authors": [
                    {
                        "name": "Kaushal Kumar Maurya"
                    },
                    {
                        "name": "KV Aditya Srivatsa"
                    },
                    {
                        "name": "Kseniia Petukhova"
                    },
                    {
                        "name": "Ekaterina Kochmar"
                    }
                ],
                "author_detail": {
                    "name": "Ekaterina Kochmar"
                },
                "author": "Ekaterina Kochmar",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03174v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03174v3",
                "updated": "2024-12-12T15:39:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    15,
                    39,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-05T15:22:11Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    15,
                    22,
                    11,
                    1,
                    310,
                    0
                ],
                "title": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZipCache: A DRAM/SSD Cache with Built-in Transparent Compression"
                },
                "summary": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a core component in modern data centers, key-value cache provides\nhigh-throughput and low-latency services for high-speed data processing. The\neffectiveness of a key-value cache relies on its ability of accommodating the\nneeded data. However, expanding the cache capacity is often more difficult than\ncommonly expected because of many practical constraints, such as server costs,\ncooling issues, rack space, and even human resource expenses. A potential\nsolution is compression, which virtually extends the cache capacity by\ncondensing data in cache. In practice, this seemingly simple idea has not\ngained much traction in key-value cache system design, due to several critical\nissues: the compression-unfriendly index structure, severe read/write\namplification, wasteful decompression operations, and heavy computing cost.\nThis paper presents a hybrid DRAM-SSD cache design to realize a systematic\nintegration of data compression in key-value cache. By treating compression as\nan essential component, we have redesigned the indexing structure, data\nmanagement, and leveraged the emerging computational SSD hardware for\ncollaborative optimizations. We have developed a prototype, called ZipCache.\nOur experimental results show that ZipCache can achieve up to 72.4% higher\nthroughput and 42.4% lower latency, while reducing the write amplification by\nup to 26.2 times."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Alex Zhong"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03174v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03174v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08760v2",
                "updated": "2024-12-12T14:43:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    14,
                    43,
                    48,
                    3,
                    347,
                    0
                ],
                "published": "2024-10-11T12:19:18Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    12,
                    19,
                    18,
                    4,
                    285,
                    0
                ],
                "title": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking FedNL: Self-Contained Compute-Optimized Implementation"
                },
                "summary": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is an emerging paradigm that enables intelligent\nagents to collaboratively train Machine Learning (ML) models in a distributed\nmanner, eliminating the need for sharing their local data. The recent work\n(arXiv:2106.02969) introduces a family of Federated Newton Learn (FedNL)\nalgorithms, marking a significant step towards applying second-order methods to\nFL and large-scale optimization. However, the reference FedNL prototype\nexhibits three serious practical drawbacks: (i) It requires 4.8 hours to launch\na single experiment in a sever-grade workstation; (ii) The prototype only\nsimulates multi-node setting; (iii) Prototype integration into\nresource-constrained applications is challenging. To bridge the gap between\ntheory and practice, we present a self-contained implementation of FedNL,\nFedNL-LS, FedNL-PP for single-node and multi-node settings. Our work resolves\nthe aforementioned issues and reduces the wall clock time by x1000. With this\nFedNL outperforms alternatives for training logistic regression in a\nsingle-node -- CVXPY (arXiv:1603.00943), and in a multi-node -- Apache Spark\n(arXiv:1505.06807), Ray/Scikit-Learn (arXiv:1712.05889). Finally, we propose\ntwo practical-orientated compressors for FedNL - adaptive TopLEK and\ncache-aware RandSeqK, which fulfill the theory of FedNL."
                },
                "authors": [
                    {
                        "name": "Konstantin Burlachenko"
                    },
                    {
                        "name": "Peter Richtárik"
                    }
                ],
                "author_detail": {
                    "name": "Peter Richtárik"
                },
                "author": "Peter Richtárik",
                "arxiv_comment": "55 pages, 12 figures, 12 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4; C.3; I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06282v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06282v3",
                "updated": "2024-12-12T12:24:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    24,
                    18,
                    3,
                    347,
                    0
                ],
                "published": "2024-06-10T14:01:21Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    14,
                    1,
                    21,
                    0,
                    162,
                    0
                ],
                "title": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PowerInfer-2: Fast Large Language Model Inference on a Smartphone"
                },
                "summary": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) on smartphones enable real-time AI assistance\nand privacy-preserving, offline operation. However, resource constraints of\nsmartphones limit current deployments to small language models (SLMs),\nsignificantly compromising their capabilities. This paper introduces\nPowerInfer-2, a smartphone-based framework that enables fast inference for LLMs\nexceeding the memory capacity. The key insight is decomposing matrix operations\ninto neuron clusters as the basic processing unit, which enables flexible\nscheduling and efficient I/O-computation pipelining. PowerInfer-2 leverages\nthis neuron-cluster-based design in both computation and storage. For\ncomputation, neuron clusters with dense activations are processed on NPU, while\nsparse clusters use CPU. The storage engine provides a fine-grained pipeline\nmechanism that coordinates cluster-level computation and I/O operations,\nenhanced by a segmented neuron cache to reduce I/O activities. PowerInfer-2\nachieves up to a 27.8x speed increase compared to state-of-the-art frameworks.\nPowerInfer-2 is the first system to serve a 47B LLM on a smartphone, achieving\n11.68 tokens/s. Notably, these performance improvements preserve model quality\nwith negligible accuracy degradation."
                },
                "authors": [
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06282v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06282v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v3",
                "updated": "2024-12-12T12:03:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    12,
                    3,
                    19,
                    3,
                    347,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HEXA-MoE: Efficient and Heterogeneous-aware MoE Acceleration with ZERO\n  Computation Redundancy"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v2",
                "updated": "2024-12-12T10:07:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    10,
                    7,
                    17,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09057v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09057v1",
                "updated": "2024-12-12T08:33:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T08:33:39Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    8,
                    33,
                    39,
                    3,
                    347,
                    0
                ],
                "title": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PhishIntel: Toward Practical Deployment of Reference-based Phishing\n  Detection"
                },
                "summary": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phishing is a critical cyber threat, exploiting deceptive tactics to\ncompromise victims and cause significant financial losses. While\nreference-based phishing detectors (RBPDs) achieve high precision by analyzing\nbrand-domain consistency, their real-world deployment is hindered by challenges\nsuch as high latency and inefficiency in URL analysis. To address these\nlimitations, we present PhishIntel, an end-to-end phishing detection system for\nreal-world deployment. PhishIntel intelligently determines whether a URL can be\nprocessed immediately or not, segmenting the detection process into two\ndistinct tasks: a fast task that checks against local blacklists and result\ncache, and a slow task that conducts online blacklist verification, URL\ncrawling, and webpage analysis using an RBPD. This fast-slow task system\narchitecture ensures low response latency while retaining the robust detection\ncapabilities of RBPDs for zero-day phishing threats. Furthermore, we develop\ntwo downstream applications based on PhishIntel: a phishing intelligence\nplatform and a phishing email detection plugin for Microsoft Outlook,\ndemonstrating its practical efficacy and utility."
                },
                "authors": [
                    {
                        "name": "Yuexin Li"
                    },
                    {
                        "name": "Hiok Kuek Tan"
                    },
                    {
                        "name": "Qiaoran Meng"
                    },
                    {
                        "name": "Mei Lin Lock"
                    },
                    {
                        "name": "Tri Cao"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Nay Oo"
                    },
                    {
                        "name": "Hoon Wei Lim"
                    },
                    {
                        "name": "Bryan Hooi"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Hooi"
                },
                "author": "Bryan Hooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09057v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09057v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.09036v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.09036v1",
                "updated": "2024-12-12T07:52:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T07:52:56Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    7,
                    52,
                    56,
                    3,
                    347,
                    0
                ],
                "title": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based\n  on Layer Uncertainty"
                },
                "summary": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language models (LLMs) have become a research hotspot. To accelerate\nthe inference of LLMs, storing computed caches in memory has become the\nstandard technique. However, as the inference length increases, growing KV\ncaches might lead to out-of-memory issues. Many existing methods address this\nissue through KV cache compression, primarily by preserving key tokens\nthroughout all layers to reduce information loss. Most of them allocate a\nuniform budget size for each layer to retain. However, we observe that the\nminimum budget sizes needed to retain essential information vary across layers\nand models based on the perspectives of attention and hidden state output.\nBuilding on this observation, this paper proposes a simple yet effective KV\ncache compression method that leverages layer uncertainty to allocate budget\nsize for each layer. Experimental results show that the proposed method can\nreduce memory usage of the KV caches to only $\\sim$20\\% when compared to Full\nKV inference while achieving nearly lossless performance."
                },
                "authors": [
                    {
                        "name": "Meizhi Zhong"
                    },
                    {
                        "name": "Xikai Liu"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yikun Lei"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Kehai Chen"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.09036v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.09036v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.13853v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.13853v3",
                "updated": "2024-12-12T03:21:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    21,
                    13,
                    3,
                    347,
                    0
                ],
                "published": "2024-07-18T18:47:52Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    18,
                    47,
                    52,
                    3,
                    200,
                    0
                ],
                "title": "Forecasting GPU Performance for Deep Learning Training and Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting GPU Performance for Deep Learning Training and Inference"
                },
                "summary": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning kernels exhibit predictable memory accesses and compute\npatterns, making GPUs' parallel architecture well-suited for their execution.\nSoftware and runtime systems for GPUs are optimized to better utilize the\nstream multiprocessors, on-chip cache, and off-chip high-bandwidth memory. As\ndeep learning models and GPUs evolve, access to newer GPUs is often limited,\nraising questions about the performance of new model architectures on existing\nGPUs, existing models on new GPUs, and new model architectures on new GPUs. To\naddress these questions, we introduce NeuSight, a framework to predict the\nperformance of various deep learning models, for both training and inference,\non unseen GPUs without requiring actual execution. The framework leverages both\nGPU hardware behavior and software library optimizations to estimate end-to-end\nperformance. Previous work uses regression models that capture linear trends or\nmultilayer perceptrons to predict the overall latency of deep learning kernels\non GPUs. These approaches suffer from higher error percentages when forecasting\nperformance on unseen models and new GPUs. Instead, NeuSight decomposes the\nprediction problem into smaller problems, bounding the prediction through\nfundamental performance laws. NeuSight decomposes a single deep learning kernel\nprediction into smaller working sets called tiles, which are executed\nindependently on the GPU. Tile-granularity predictions are determined using a\nmachine learning approach and aggregated to estimate end-to-end latency.\nNeuSight outperforms prior work across various deep learning workloads and the\nlatest GPUs. It reduces the percentage error from 121.4% and 30.8% to 2.3% in\npredicting the latency of GPT3 model for training and inference on H100,\ncompared to state-of-the-art prior work, where both GPT3 and H100 were not used\nto train the framework."
                },
                "authors": [
                    {
                        "name": "Seonho Lee"
                    },
                    {
                        "name": "Amar Phanishayee"
                    },
                    {
                        "name": "Divya Mahajan"
                    }
                ],
                "author_detail": {
                    "name": "Divya Mahajan"
                },
                "author": "Divya Mahajan",
                "arxiv_doi": "10.1145/3669940.3707265",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3669940.3707265",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.13853v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.13853v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the 30th ACM International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS), 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08890v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08890v1",
                "updated": "2024-12-12T03:00:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "published": "2024-12-12T03:00:29Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    3,
                    0,
                    29,
                    3,
                    347,
                    0
                ],
                "title": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lexico: Extreme KV Cache Compression via Sparse Coding over Universal\n  Dictionaries"
                },
                "summary": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Lexico, a novel KV cache compression method that leverages\nsparse coding with a universal dictionary. Our key finding is that key-value\ncache in modern LLMs can be accurately approximated using sparse linear\ncombination from a small, input-agnostic dictionary of ~4k atoms, enabling\nefficient compression across different input prompts, tasks and models. Using\northogonal matching pursuit for sparse approximation, Lexico achieves flexible\ncompression ratios through direct sparsity control. On GSM8K, across multiple\nmodel families (Mistral, Llama 3, Qwen2.5), Lexico maintains 90-95% of the\noriginal performance while using only 15-25% of the full KV-cache memory,\noutperforming both quantization and token eviction methods. Notably, Lexico\nremains effective in low memory regimes where 2-bit quantization fails,\nachieving up to 1.7x better compression on LongBench and GSM8K while\nmaintaining high accuracy."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08890v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08890v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.01957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01957v1",
                "updated": "2025-01-03T18:59:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    59,
                    52,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T18:59:52Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    59,
                    52,
                    4,
                    3,
                    0
                ],
                "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction."
                },
                "authors": [
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Haojia Lin"
                    },
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Yangze Li"
                    },
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Heting Gao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "https://github.com/VITA-MLLM/VITA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01956v1",
                "updated": "2025-01-03T18:59:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    59,
                    23,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T18:59:23Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    59,
                    23,
                    4,
                    3,
                    0
                ],
                "title": "Metadata Conditioning Accelerates Language Model Pre-training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Metadata Conditioning Accelerates Language Model Pre-training"
                },
                "summary": "The vast diversity of styles, domains, and quality levels present in language\nmodel pre-training corpora is essential in developing general model\ncapabilities, but efficiently learning and deploying the correct behaviors\nexemplified in each of these heterogeneous data sources is challenging. To\naddress this, we propose a new method, termed Metadata Conditioning then\nCooldown (MeCo), to incorporate additional learning cues during pre-training.\nMeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside the\ntext during training and later uses a cooldown phase with only the standard\ntext, thereby enabling the model to function normally even without metadata.\nMeCo significantly accelerates pre-training across different model scales (600M\nto 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For\ninstance, a 1.6B language model trained with MeCo matches the downstream task\nperformance of standard pre-training while using 33% less data. Additionally,\nMeCo enables us to steer language models by conditioning the inference prompt\non either real or fabricated metadata that encodes the desired properties of\nthe output: for example, prepending wikipedia.org to reduce harmful generations\nor factquizmaster.com (fabricated) to improve common knowledge task\nperformance. We also demonstrate that MeCo is compatible with different types\nof metadata, such as model-generated topics. MeCo is remarkably simple, adds no\ncomputational overhead, and demonstrates promise in producing more capable and\nsteerable language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The vast diversity of styles, domains, and quality levels present in language\nmodel pre-training corpora is essential in developing general model\ncapabilities, but efficiently learning and deploying the correct behaviors\nexemplified in each of these heterogeneous data sources is challenging. To\naddress this, we propose a new method, termed Metadata Conditioning then\nCooldown (MeCo), to incorporate additional learning cues during pre-training.\nMeCo first provides metadata (e.g., URLs like en.wikipedia.org) alongside the\ntext during training and later uses a cooldown phase with only the standard\ntext, thereby enabling the model to function normally even without metadata.\nMeCo significantly accelerates pre-training across different model scales (600M\nto 8B parameters) and training sources (C4, RefinedWeb, and DCLM). For\ninstance, a 1.6B language model trained with MeCo matches the downstream task\nperformance of standard pre-training while using 33% less data. Additionally,\nMeCo enables us to steer language models by conditioning the inference prompt\non either real or fabricated metadata that encodes the desired properties of\nthe output: for example, prepending wikipedia.org to reduce harmful generations\nor factquizmaster.com (fabricated) to improve common knowledge task\nperformance. We also demonstrate that MeCo is compatible with different types\nof metadata, such as model-generated topics. MeCo is remarkably simple, adds no\ncomputational overhead, and demonstrates promise in producing more capable and\nsteerable language models."
                },
                "authors": [
                    {
                        "name": "Tianyu Gao"
                    },
                    {
                        "name": "Alexander Wettig"
                    },
                    {
                        "name": "Luxi He"
                    },
                    {
                        "name": "Yihe Dong"
                    },
                    {
                        "name": "Sadhika Malladi"
                    },
                    {
                        "name": "Danqi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Danqi Chen"
                },
                "author": "Danqi Chen",
                "arxiv_comment": "Code available at https://github.com/princeton-pli/MeCo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01945v1",
                "updated": "2025-01-03T18:51:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    51,
                    18,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T18:51:18Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    51,
                    18,
                    4,
                    3,
                    0
                ],
                "title": "Cold-Start Recommendation towards the Era of Large Language Models\n  (LLMs): A Comprehensive Survey and Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold-Start Recommendation towards the Era of Large Language Models\n  (LLMs): A Comprehensive Survey and Roadmap"
                },
                "summary": "Cold-start problem is one of the long-standing challenges in recommender\nsystems, focusing on accurately modeling new or interaction-limited users or\nitems to provide better recommendations. Due to the diversification of internet\nplatforms and the exponential growth of users and items, the importance of\ncold-start recommendation (CSR) is becoming increasingly evident. At the same\ntime, large language models (LLMs) have achieved tremendous success and possess\nstrong capabilities in modeling user and item information, providing new\npotential for cold-start recommendations. However, the research community on\nCSR still lacks a comprehensive review and reflection in this field. Based on\nthis, in this paper, we stand in the context of the era of large language\nmodels and provide a comprehensive review and discussion on the roadmap,\nrelated literature, and future directions of CSR. Specifically, we have\nconducted an exploration of the development path of how existing CSR utilizes\ninformation, from content features, graph relations, and domain information, to\nthe world knowledge possessed by large language models, aiming to provide new\ninsights for both the research and industrial communities on CSR. Related\nresources of cold-start recommendations are collected and continuously updated\nfor the community in\nhttps://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold-start problem is one of the long-standing challenges in recommender\nsystems, focusing on accurately modeling new or interaction-limited users or\nitems to provide better recommendations. Due to the diversification of internet\nplatforms and the exponential growth of users and items, the importance of\ncold-start recommendation (CSR) is becoming increasingly evident. At the same\ntime, large language models (LLMs) have achieved tremendous success and possess\nstrong capabilities in modeling user and item information, providing new\npotential for cold-start recommendations. However, the research community on\nCSR still lacks a comprehensive review and reflection in this field. Based on\nthis, in this paper, we stand in the context of the era of large language\nmodels and provide a comprehensive review and discussion on the roadmap,\nrelated literature, and future directions of CSR. Specifically, we have\nconducted an exploration of the development path of how existing CSR utilizes\ninformation, from content features, graph relations, and domain information, to\nthe world knowledge possessed by large language models, aiming to provide new\ninsights for both the research and industrial communities on CSR. Related\nresources of cold-start recommendations are collected and continuously updated\nfor the community in\nhttps://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation."
                },
                "authors": [
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Liangwei Yang"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Peilin Zhou"
                    },
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Jianling Wang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Feiran Huang"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Jiajun Bu"
                    },
                    {
                        "name": "Allen Lin"
                    },
                    {
                        "name": "James Caverlee"
                    },
                    {
                        "name": "Fakhri Karray"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01933v1",
                "updated": "2025-01-03T18:12:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    12,
                    13,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T18:12:13Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    12,
                    13,
                    4,
                    3,
                    0
                ],
                "title": "Abstractive Text Summarization for Contemporary Sanskrit Prose: Issues\n  and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abstractive Text Summarization for Contemporary Sanskrit Prose: Issues\n  and Challenges"
                },
                "summary": "This thesis presents Abstractive Text Summarization models for contemporary\nSanskrit prose. The first chapter, titled Introduction, presents the motivation\nbehind this work, the research questions, and the conceptual framework.\nSanskrit is a low-resource inflectional language. The key research question\nthat this thesis investigates is what the challenges in developing an\nabstractive TS for Sanskrit. To answer the key research questions,\nsub-questions based on four different themes have been posed in this work. The\nsecond chapter, Literature Review, surveys the previous works done. The third\nchapter, data preparation, answers the remaining three questions from the third\ntheme. It reports the data collection and preprocessing challenges for both\nlanguage model and summarization model trainings. The fourth chapter reports\nthe training and inference of models and the results obtained therein. This\nresearch has initiated a pipeline for Sanskrit abstractive text summarization\nand has reported the challenges faced at every stage of the development. The\nresearch questions based on every theme have been answered to answer the key\nresearch question.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This thesis presents Abstractive Text Summarization models for contemporary\nSanskrit prose. The first chapter, titled Introduction, presents the motivation\nbehind this work, the research questions, and the conceptual framework.\nSanskrit is a low-resource inflectional language. The key research question\nthat this thesis investigates is what the challenges in developing an\nabstractive TS for Sanskrit. To answer the key research questions,\nsub-questions based on four different themes have been posed in this work. The\nsecond chapter, Literature Review, surveys the previous works done. The third\nchapter, data preparation, answers the remaining three questions from the third\ntheme. It reports the data collection and preprocessing challenges for both\nlanguage model and summarization model trainings. The fourth chapter reports\nthe training and inference of models and the results obtained therein. This\nresearch has initiated a pipeline for Sanskrit abstractive text summarization\nand has reported the challenges faced at every stage of the development. The\nresearch questions based on every theme have been answered to answer the key\nresearch question."
                },
                "authors": [
                    {
                        "name": "Shagun Sinha"
                    }
                ],
                "author_detail": {
                    "name": "Shagun Sinha"
                },
                "author": "Shagun Sinha",
                "arxiv_comment": "PhD Thesis",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01926v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01926v1",
                "updated": "2025-01-03T17:56:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    56,
                    28,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T17:56:28Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    56,
                    28,
                    4,
                    3,
                    0
                ],
                "title": "Mitigating Hallucination for Large Vision Language Model by\n  Inter-Modality Correlation Calibration Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Hallucination for Large Vision Language Model by\n  Inter-Modality Correlation Calibration Decoding"
                },
                "summary": "Large vision-language models (LVLMs) have shown remarkable capabilities in\nvisual-language understanding for downstream multi-modal tasks. Despite their\nsuccess, LVLMs still suffer from generating hallucinations in complex\ngeneration tasks, leading to inconsistencies between visual inputs and\ngenerated content. To address this issue, some approaches have introduced\ninference-time interventions, such as contrastive decoding and attention\nrectification, to reduce overreliance on language priors. However, these\napproaches overlook hallucinations stemming from spurious inter-modality\ncorrelations. In this paper, we propose an Inter-Modality Correlation\nCalibration Decoding (IMCCD) method to mitigate hallucinations in LVLMs in a\ntraining-free manner. In this method, we design a Cross-Modal Value-Enhanced\nDecoding(CMVED) module to alleviate hallucination by a novel contrastive\ndecoding mechanism. During the estimation of distorted distribution, CMVED\nmasks the value vectors associated with significant cross-modal attention\nweights, which address both uni-modality overreliance and misleading\ninter-modality correlations. Additionally, a Content-Driven Attention\nRefinement(CDAR) module refines cross-modal attention weights, guiding LVLMs to\nfocus on important visual content. Experimental results on diverse\nhallucination benchmarks validate the superiority of our method over existing\nstate-of-the-art techniques in reducing hallucinations in LVLM text generation.\nOur code will be available at https://github.com/lijm48/IMCCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) have shown remarkable capabilities in\nvisual-language understanding for downstream multi-modal tasks. Despite their\nsuccess, LVLMs still suffer from generating hallucinations in complex\ngeneration tasks, leading to inconsistencies between visual inputs and\ngenerated content. To address this issue, some approaches have introduced\ninference-time interventions, such as contrastive decoding and attention\nrectification, to reduce overreliance on language priors. However, these\napproaches overlook hallucinations stemming from spurious inter-modality\ncorrelations. In this paper, we propose an Inter-Modality Correlation\nCalibration Decoding (IMCCD) method to mitigate hallucinations in LVLMs in a\ntraining-free manner. In this method, we design a Cross-Modal Value-Enhanced\nDecoding(CMVED) module to alleviate hallucination by a novel contrastive\ndecoding mechanism. During the estimation of distorted distribution, CMVED\nmasks the value vectors associated with significant cross-modal attention\nweights, which address both uni-modality overreliance and misleading\ninter-modality correlations. Additionally, a Content-Driven Attention\nRefinement(CDAR) module refines cross-modal attention weights, guiding LVLMs to\nfocus on important visual content. Experimental results on diverse\nhallucination benchmarks validate the superiority of our method over existing\nstate-of-the-art techniques in reducing hallucinations in LVLM text generation.\nOur code will be available at https://github.com/lijm48/IMCCD."
                },
                "authors": [
                    {
                        "name": "Jiaming Li"
                    },
                    {
                        "name": "Jiacheng Zhang"
                    },
                    {
                        "name": "Zequn Jie"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Guanbin Li"
                    }
                ],
                "author_detail": {
                    "name": "Guanbin Li"
                },
                "author": "Guanbin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01926v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01926v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01925v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01925v1",
                "updated": "2025-01-03T17:54:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    54,
                    39,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T17:54:39Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    54,
                    39,
                    4,
                    3,
                    0
                ],
                "title": "Inference in matrix-valued time series with common stochastic trends and\n  multifactor error structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference in matrix-valued time series with common stochastic trends and\n  multifactor error structure"
                },
                "summary": "We develop an estimation methodology for a factor model for high-dimensional\nmatrix-valued time series, where common stochastic trends and common stationary\nfactors can be present. We study, in particular, the estimation of (row and\ncolumn) loading spaces, of the common stochastic trends and of the common\nstationary factors, and the row and column ranks thereof. In a set of\n(negative) preliminary results, we show that a projection-based technique fails\nto improve the rates of convergence compared to a \"flattened\" estimation\ntechnique which does not take into account the matrix nature of the data.\nHence, we develop a three-step algorithm where: (i) we first project the data\nonto the orthogonal complement to the (row and column) loadings of the common\nstochastic trends; (ii) we subsequently use such \"trend free\" data to estimate\nthe stationary common component; (iii) we remove the estimated common\nstationary component from the data, and re-estimate, using a projection-based\nestimator, the row and column common stochastic trends and their loadings. We\nshow that this estimator succeeds in refining the rates of convergence of the\ninitial, \"flattened\" estimator. As a by-product, we develop consistent\neigenvalue-ratio based estimators for the number of stationary and\nnonstationary common factors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We develop an estimation methodology for a factor model for high-dimensional\nmatrix-valued time series, where common stochastic trends and common stationary\nfactors can be present. We study, in particular, the estimation of (row and\ncolumn) loading spaces, of the common stochastic trends and of the common\nstationary factors, and the row and column ranks thereof. In a set of\n(negative) preliminary results, we show that a projection-based technique fails\nto improve the rates of convergence compared to a \"flattened\" estimation\ntechnique which does not take into account the matrix nature of the data.\nHence, we develop a three-step algorithm where: (i) we first project the data\nonto the orthogonal complement to the (row and column) loadings of the common\nstochastic trends; (ii) we subsequently use such \"trend free\" data to estimate\nthe stationary common component; (iii) we remove the estimated common\nstationary component from the data, and re-estimate, using a projection-based\nestimator, the row and column common stochastic trends and their loadings. We\nshow that this estimator succeeds in refining the rates of convergence of the\ninitial, \"flattened\" estimator. As a by-product, we develop consistent\neigenvalue-ratio based estimators for the number of stationary and\nnonstationary common factors."
                },
                "authors": [
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Simone Giannerini"
                    },
                    {
                        "name": "Greta Goracci"
                    },
                    {
                        "name": "Lorenzo Trapani"
                    }
                ],
                "author_detail": {
                    "name": "Lorenzo Trapani"
                },
                "author": "Lorenzo Trapani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01925v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01925v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01907v1",
                "updated": "2025-01-03T17:22:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    22,
                    46,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T17:22:46Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    22,
                    46,
                    4,
                    3,
                    0
                ],
                "title": "QCD Phase Diagram and Astrophysical Implications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QCD Phase Diagram and Astrophysical Implications"
                },
                "summary": "I make a brief review about the QCD phases and the equation of state inferred\nfrom the neutron star data. Along the temperature axis at low baryon density,\nthe QCD phase transition is a smooth crossover, and it is a natural extension\nof our imagination to postulate a similar crossover along the density axis at\nlow temperature. Even without phase transitions, the inferred thermodynamic\nproperties of neutron star matter turn out to be highly nontrivial already at\ntwice of the nuclear saturation density. I also give some discussions about the\nsubstantiation of quark matter by means of the gravitational wave signals\nincluding the multi-messenger prospect.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I make a brief review about the QCD phases and the equation of state inferred\nfrom the neutron star data. Along the temperature axis at low baryon density,\nthe QCD phase transition is a smooth crossover, and it is a natural extension\nof our imagination to postulate a similar crossover along the density axis at\nlow temperature. Even without phase transitions, the inferred thermodynamic\nproperties of neutron star matter turn out to be highly nontrivial already at\ntwice of the nuclear saturation density. I also give some discussions about the\nsubstantiation of quark matter by means of the gravitational wave signals\nincluding the multi-messenger prospect."
                },
                "authors": [
                    {
                        "name": "Kenji Fukushima"
                    }
                ],
                "author_detail": {
                    "name": "Kenji Fukushima"
                },
                "author": "Kenji Fukushima",
                "arxiv_comment": "8 pages, 7 figures; a proceedings contribution to Aspects of\n  Criticality II, Wroclaw, Poland",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19420v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19420v2",
                "updated": "2025-01-03T17:21:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    21,
                    23,
                    4,
                    3,
                    0
                ],
                "published": "2024-10-25T09:23:59Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    9,
                    23,
                    59,
                    4,
                    299,
                    0
                ],
                "title": "Doppler correlation-driven vetoes for the Frequency Hough analysis in\n  continuous gravitational-wave searches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doppler correlation-driven vetoes for the Frequency Hough analysis in\n  continuous gravitational-wave searches"
                },
                "summary": "We present an improved method for vetoing candidates of continuous\ngravitational-wave sources during all-sky searches utilizing the Frequency\nHough pipeline. This approach leverages linear correlations between source\nparameters induced by the Earth Doppler effect, which can be effectively\nidentified through the Hough Transform. Candidates that do not align with these\npatterns are considered spurious and can thus be vetoed, enhancing the depth\nand statistical significance of follow-up analyses. Additionally, we provide a\ncomprehensive explanation of the method calibration, which intrinsically linked\nto the total duration of the observing run. On average, the procedure\nsuccessfully vetoes $56\\%$ of candidates. To assess the method performance, we\nconducted a Monte-Carlo simulation injecting fake continuous-wave signals into\ndata from the third observing run of the LIGO detectors. This analysis allowed\nus to infer strain amplitude upper limits at a $90\\%$ confidence level. We\nfound that the optimal sensitivity is $h_0^{90\\%} = 3.62^{+0.23}_{-0.22}\\times\n10^{-26}$ in the [128, 200] Hz band, which is within the most sensible\nfrequency band of the LIGO detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an improved method for vetoing candidates of continuous\ngravitational-wave sources during all-sky searches utilizing the Frequency\nHough pipeline. This approach leverages linear correlations between source\nparameters induced by the Earth Doppler effect, which can be effectively\nidentified through the Hough Transform. Candidates that do not align with these\npatterns are considered spurious and can thus be vetoed, enhancing the depth\nand statistical significance of follow-up analyses. Additionally, we provide a\ncomprehensive explanation of the method calibration, which intrinsically linked\nto the total duration of the observing run. On average, the procedure\nsuccessfully vetoes $56\\%$ of candidates. To assess the method performance, we\nconducted a Monte-Carlo simulation injecting fake continuous-wave signals into\ndata from the third observing run of the LIGO detectors. This analysis allowed\nus to infer strain amplitude upper limits at a $90\\%$ confidence level. We\nfound that the optimal sensitivity is $h_0^{90\\%} = 3.62^{+0.23}_{-0.22}\\times\n10^{-26}$ in the [128, 200] Hz band, which is within the most sensible\nfrequency band of the LIGO detectors."
                },
                "authors": [
                    {
                        "name": "Matteo Di Giovanni"
                    },
                    {
                        "name": "Paola Leaci"
                    },
                    {
                        "name": "Pia Astone"
                    },
                    {
                        "name": "Stefano Dal Pra"
                    },
                    {
                        "name": "Sabrina D'Antonio"
                    },
                    {
                        "name": "Luca D'Onofrio"
                    },
                    {
                        "name": "Sergio Frasca"
                    },
                    {
                        "name": "Federico Muciaccia"
                    },
                    {
                        "name": "Cristiano Palomba"
                    },
                    {
                        "name": "Lorenzo Pierini"
                    },
                    {
                        "name": "Francesco Safai Tehrani"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Safai Tehrani"
                },
                "author": "Francesco Safai Tehrani",
                "arxiv_comment": "13 pages, 9 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19420v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19420v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01904v1",
                "updated": "2025-01-03T17:14:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    14,
                    16,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T17:14:16Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    14,
                    16,
                    4,
                    3,
                    0
                ],
                "title": "Virgo: A Preliminary Exploration on Reproducing o1-like MLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virgo: A Preliminary Exploration on Reproducing o1-like MLLM"
                },
                "summary": "Recently, slow-thinking reasoning systems, built upon large language models\n(LLMs), have garnered widespread attention by scaling the thinking time during\ninference. There is also growing interest in adapting this capability to\nmultimodal large language models (MLLMs). Given that MLLMs handle more complex\ndata semantics across different modalities, it is intuitively more challenging\nto implement multimodal slow-thinking systems.\n  To address this issue, in this paper, we explore a straightforward approach\nby fine-tuning a capable MLLM with a small amount of textual long-form thought\ndata, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning\nwith long thought). We find that these long-form reasoning processes, expressed\nin natural language, can be effectively transferred to MLLMs. Moreover, it\nseems that such textual reasoning data can be even more effective than visual\nreasoning data in eliciting the slow-thinking capacities of MLLMs. While this\nwork is preliminary, it demonstrates that slow-thinking capacities are\nfundamentally associated with the language model component, which can be\ntransferred across modalities or domains. This finding can be leveraged to\nguide the development of more powerful slow-thinking reasoning systems. We\nrelease our resources at https://github.com/RUCAIBox/Virgo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, slow-thinking reasoning systems, built upon large language models\n(LLMs), have garnered widespread attention by scaling the thinking time during\ninference. There is also growing interest in adapting this capability to\nmultimodal large language models (MLLMs). Given that MLLMs handle more complex\ndata semantics across different modalities, it is intuitively more challenging\nto implement multimodal slow-thinking systems.\n  To address this issue, in this paper, we explore a straightforward approach\nby fine-tuning a capable MLLM with a small amount of textual long-form thought\ndata, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning\nwith long thought). We find that these long-form reasoning processes, expressed\nin natural language, can be effectively transferred to MLLMs. Moreover, it\nseems that such textual reasoning data can be even more effective than visual\nreasoning data in eliciting the slow-thinking capacities of MLLMs. While this\nwork is preliminary, it demonstrates that slow-thinking capacities are\nfundamentally associated with the language model component, which can be\ntransferred across modalities or domains. This finding can be leveraged to\nguide the development of more powerful slow-thinking reasoning systems. We\nrelease our resources at https://github.com/RUCAIBox/Virgo."
                },
                "authors": [
                    {
                        "name": "Yifan Du"
                    },
                    {
                        "name": "Zikang Liu"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Yuqi Huo"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Technical Report on Slow Thinking with LLMs: Visual Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19784v2",
                "updated": "2025-01-03T17:03:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    3,
                    26,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-27T18:25:27Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    25,
                    27,
                    4,
                    362,
                    0
                ],
                "title": "Can AI Help with Your Personal Finances?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI Help with Your Personal Finances?"
                },
                "summary": "In recent years, Large Language Models (LLMs) have emerged as a\ntransformative development in artificial intelligence (AI), drawing significant\nattention from industry and academia. Trained on vast datasets, these\nsophisticated AI systems exhibit impressive natural language processing and\ncontent generation capabilities. This paper explores the potential of LLMs to\naddress key challenges in personal finance, focusing on the United States. We\nevaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,\nAnthropic's Claude, and Meta's Llama, to assess their effectiveness in\nproviding accurate financial advice on topics such as mortgages, taxes, loans,\nand investments. Our findings show that while these models achieve an average\naccuracy rate of approximately 70%, they also display notable limitations in\ncertain areas. Specifically, LLMs struggle to provide accurate responses for\ncomplex financial queries, with performance varying significantly across\ndifferent topics. Despite these limitations, the analysis reveals notable\nimprovements in newer versions of these models, highlighting their growing\nutility for individuals and financial advisors. As these AI systems continue to\nevolve, their potential for advancing AI-driven applications in personal\nfinance becomes increasingly promising.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have emerged as a\ntransformative development in artificial intelligence (AI), drawing significant\nattention from industry and academia. Trained on vast datasets, these\nsophisticated AI systems exhibit impressive natural language processing and\ncontent generation capabilities. This paper explores the potential of LLMs to\naddress key challenges in personal finance, focusing on the United States. We\nevaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,\nAnthropic's Claude, and Meta's Llama, to assess their effectiveness in\nproviding accurate financial advice on topics such as mortgages, taxes, loans,\nand investments. Our findings show that while these models achieve an average\naccuracy rate of approximately 70%, they also display notable limitations in\ncertain areas. Specifically, LLMs struggle to provide accurate responses for\ncomplex financial queries, with performance varying significantly across\ndifferent topics. Despite these limitations, the analysis reveals notable\nimprovements in newer versions of these models, highlighting their growing\nutility for individuals and financial advisors. As these AI systems continue to\nevolve, their potential for advancing AI-driven applications in personal\nfinance becomes increasingly promising."
                },
                "authors": [
                    {
                        "name": "Oudom Hean"
                    },
                    {
                        "name": "Utsha Saha"
                    },
                    {
                        "name": "Binita Saha"
                    }
                ],
                "author_detail": {
                    "name": "Binita Saha"
                },
                "author": "Binita Saha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14205v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14205v4",
                "updated": "2025-01-03T16:44:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    16,
                    44,
                    55,
                    4,
                    3,
                    0
                ],
                "published": "2024-05-23T06:03:19Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    6,
                    3,
                    19,
                    3,
                    144,
                    0
                ],
                "title": "Agent Planning with World Knowledge Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Planning with World Knowledge Model"
                },
                "summary": "Recent endeavors towards directly using large language models (LLMs) as agent\nmodels to execute interactive planning tasks have shown commendable results.\nDespite their achievements, however, they still struggle with brainless\ntrial-and-error in global planning and generating hallucinatory actions in\nlocal planning due to their poor understanding of the ``real'' physical world.\nImitating humans' mental world knowledge model which provides global prior\nknowledge before the task and maintains local dynamic knowledge during the\ntask, in this paper, we introduce parametric World Knowledge Model (WKM) to\nfacilitate agent planning. Concretely, we steer the agent model to\nself-synthesize knowledge from both expert and sampled trajectories. Then we\ndevelop WKM, providing prior task knowledge to guide the global planning and\ndynamic state knowledge to assist the local planning. Experimental results on\nthree complex real-world simulated datasets with three state-of-the-art\nopen-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our\nmethod can achieve superior performance compared to various strong baselines.\nBesides, we analyze to illustrate that our WKM can effectively alleviate the\nblind trial-and-error and hallucinatory action issues, providing strong support\nfor the agent's understanding of the world. Other interesting findings include:\n1) our instance-level task knowledge can generalize better to unseen tasks, 2)\nweak WKM can guide strong agent model planning, and 3) unified WKM training has\npromising potential for further development. The code is available at\nhttps://github.com/zjunlp/WKM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent endeavors towards directly using large language models (LLMs) as agent\nmodels to execute interactive planning tasks have shown commendable results.\nDespite their achievements, however, they still struggle with brainless\ntrial-and-error in global planning and generating hallucinatory actions in\nlocal planning due to their poor understanding of the ``real'' physical world.\nImitating humans' mental world knowledge model which provides global prior\nknowledge before the task and maintains local dynamic knowledge during the\ntask, in this paper, we introduce parametric World Knowledge Model (WKM) to\nfacilitate agent planning. Concretely, we steer the agent model to\nself-synthesize knowledge from both expert and sampled trajectories. Then we\ndevelop WKM, providing prior task knowledge to guide the global planning and\ndynamic state knowledge to assist the local planning. Experimental results on\nthree complex real-world simulated datasets with three state-of-the-art\nopen-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our\nmethod can achieve superior performance compared to various strong baselines.\nBesides, we analyze to illustrate that our WKM can effectively alleviate the\nblind trial-and-error and hallucinatory action issues, providing strong support\nfor the agent's understanding of the world. Other interesting findings include:\n1) our instance-level task knowledge can generalize better to unseen tasks, 2)\nweak WKM can guide strong agent model planning, and 3) unified WKM training has\npromising potential for further development. The code is available at\nhttps://github.com/zjunlp/WKM."
                },
                "authors": [
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14205v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14205v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01257v2",
                "updated": "2025-01-03T16:36:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    16,
                    36,
                    12,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-02T13:49:00Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    49,
                    0,
                    3,
                    2,
                    0
                ],
                "title": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with\n  Human-comparable Elo Ratings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with\n  Human-comparable Elo Ratings"
                },
                "summary": "With the increasing code reasoning capabilities of existing large language\nmodels (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3,\nthere is a growing need to develop more challenging and comprehensive\nbenchmarks that effectively test their sophisticated competition-level coding\nabilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to\nthe unavailability of private test cases, lack of support for special judges,\nand misaligned execution environments. To bridge this gap, we introduce\nCodeElo, a standardized competition-level code generation benchmark that\neffectively addresses all these challenges for the first time. CodeElo\nbenchmark is mainly based on the official CodeForces platform and tries to\nalign with the platform as much as possible. We compile the recent six months\nof contest problems on CodeForces with detailed information such as contest\ndivisions, problem difficulty ratings, and problem algorithm tags. We introduce\na unique judging method in which problems are submitted directly to the\nplatform and develop a reliable Elo rating calculation system that aligns with\nthe platform and is comparable with human participants but has lower variance.\nBy testing on our CodeElo, we provide the Elo ratings of 30 existing popular\nopen-source and 3 proprietary LLMs for the first time. The results show that\no1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of\n1578 and 1261, respectively, while other models struggle even with the easiest\nproblems, placing in the lowest 25 percent among all human participants.\nDetailed analysis experiments are also conducted to provide insights into\nperformance across algorithms and comparisons between using C++ and Python,\nwhich can suggest directions for future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing code reasoning capabilities of existing large language\nmodels (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3,\nthere is a growing need to develop more challenging and comprehensive\nbenchmarks that effectively test their sophisticated competition-level coding\nabilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to\nthe unavailability of private test cases, lack of support for special judges,\nand misaligned execution environments. To bridge this gap, we introduce\nCodeElo, a standardized competition-level code generation benchmark that\neffectively addresses all these challenges for the first time. CodeElo\nbenchmark is mainly based on the official CodeForces platform and tries to\nalign with the platform as much as possible. We compile the recent six months\nof contest problems on CodeForces with detailed information such as contest\ndivisions, problem difficulty ratings, and problem algorithm tags. We introduce\na unique judging method in which problems are submitted directly to the\nplatform and develop a reliable Elo rating calculation system that aligns with\nthe platform and is comparable with human participants but has lower variance.\nBy testing on our CodeElo, we provide the Elo ratings of 30 existing popular\nopen-source and 3 proprietary LLMs for the first time. The results show that\no1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of\n1578 and 1261, respectively, while other models struggle even with the easiest\nproblems, placing in the lowest 25 percent among all human participants.\nDetailed analysis experiments are also conducted to provide insights into\nperformance across algorithms and comparisons between using C++ and Python,\nwhich can suggest directions for future studies."
                },
                "authors": [
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Xuancheng Ren"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Yunlong Feng"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Yang Fan"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14561v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14561v3",
                "updated": "2025-01-03T16:06:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    16,
                    6,
                    56,
                    4,
                    3,
                    0
                ],
                "published": "2024-07-18T17:59:01Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    17,
                    59,
                    1,
                    3,
                    200,
                    0
                ],
                "title": "NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model\n  Internals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model\n  Internals"
                },
                "summary": "We introduce NNsight and NDIF, technologies that work in tandem to enable\nscientific study of very large neural networks. NNsight is an open-source\nsystem that extends PyTorch to introduce deferred remote execution. NDIF is a\nscalable inference service that executes NNsight requests, allowing users to\nshare GPU resources and pretrained models. These technologies are enabled by\nthe intervention graph, an architecture developed to decouple experiment design\nfrom model runtime. Together, this framework provides transparent and efficient\naccess to the internals of deep neural networks such as very large language\nmodels (LLMs) without imposing the cost or complexity of hosting customized\nmodels individually. We conduct a quantitative survey of the machine learning\nliterature that reveals a growing gap in the study of the internals of\nlarge-scale AI. We demonstrate the design and use of our framework to address\nthis gap by enabling a range of research methods on huge models. Finally, we\nconduct benchmarks to compare performance with previous approaches. Code\ndocumentation, and materials are available at https://nnsight.net/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NNsight and NDIF, technologies that work in tandem to enable\nscientific study of very large neural networks. NNsight is an open-source\nsystem that extends PyTorch to introduce deferred remote execution. NDIF is a\nscalable inference service that executes NNsight requests, allowing users to\nshare GPU resources and pretrained models. These technologies are enabled by\nthe intervention graph, an architecture developed to decouple experiment design\nfrom model runtime. Together, this framework provides transparent and efficient\naccess to the internals of deep neural networks such as very large language\nmodels (LLMs) without imposing the cost or complexity of hosting customized\nmodels individually. We conduct a quantitative survey of the machine learning\nliterature that reveals a growing gap in the study of the internals of\nlarge-scale AI. We demonstrate the design and use of our framework to address\nthis gap by enabling a range of research methods on huge models. Finally, we\nconduct benchmarks to compare performance with previous approaches. Code\ndocumentation, and materials are available at https://nnsight.net/."
                },
                "authors": [
                    {
                        "name": "Jaden Fiotto-Kaufman"
                    },
                    {
                        "name": "Alexander R. Loftus"
                    },
                    {
                        "name": "Eric Todd"
                    },
                    {
                        "name": "Jannik Brinkmann"
                    },
                    {
                        "name": "Koyena Pal"
                    },
                    {
                        "name": "Dmitrii Troitskii"
                    },
                    {
                        "name": "Michael Ripa"
                    },
                    {
                        "name": "Adam Belfki"
                    },
                    {
                        "name": "Can Rager"
                    },
                    {
                        "name": "Caden Juang"
                    },
                    {
                        "name": "Aaron Mueller"
                    },
                    {
                        "name": "Samuel Marks"
                    },
                    {
                        "name": "Arnab Sen Sharma"
                    },
                    {
                        "name": "Francesca Lucchetti"
                    },
                    {
                        "name": "Nikhil Prakash"
                    },
                    {
                        "name": "Carla Brodley"
                    },
                    {
                        "name": "Arjun Guha"
                    },
                    {
                        "name": "Jonathan Bell"
                    },
                    {
                        "name": "Byron C. Wallace"
                    },
                    {
                        "name": "David Bau"
                    }
                ],
                "author_detail": {
                    "name": "David Bau"
                },
                "author": "David Bau",
                "arxiv_comment": "Code at https://nnsight.net",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14561v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14561v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.13895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.13895v4",
                "updated": "2025-01-03T15:29:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    15,
                    29,
                    27,
                    4,
                    3,
                    0
                ],
                "published": "2024-08-25T17:17:52Z",
                "published_parsed": [
                    2024,
                    8,
                    25,
                    17,
                    17,
                    52,
                    6,
                    238,
                    0
                ],
                "title": "ESG Rating Disagreement and Corporate Total Factor\n  Productivity:Inference and Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ESG Rating Disagreement and Corporate Total Factor\n  Productivity:Inference and Prediction"
                },
                "summary": "This paper examines how ESG rating disagreement (Dis) affects corporate total\nfactor productivity (TFP) in China based on data of A-share listed companies\nfrom 2015 to 2022. We find that Dis reduces TFP, especially in state-owned,\nnon-capital-intensive, low-pollution and high-tech firms, green innovation\nstrengthens the dampening effect of Dis on TFP, and that Dis lowers corporate\nTFP by increasing financing constraints and weakening human capital.\nFurthermore, XGBoost regression demonstrates that Dis plays a significant role\nin predicting TFP, with SHAP showing that the dampening effect of ESG rating\ndisagreement on TFP is still pronounced in firms with large Dis values.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines how ESG rating disagreement (Dis) affects corporate total\nfactor productivity (TFP) in China based on data of A-share listed companies\nfrom 2015 to 2022. We find that Dis reduces TFP, especially in state-owned,\nnon-capital-intensive, low-pollution and high-tech firms, green innovation\nstrengthens the dampening effect of Dis on TFP, and that Dis lowers corporate\nTFP by increasing financing constraints and weakening human capital.\nFurthermore, XGBoost regression demonstrates that Dis plays a significant role\nin predicting TFP, with SHAP showing that the dampening effect of ESG rating\ndisagreement on TFP is still pronounced in firms with large Dis values."
                },
                "authors": [
                    {
                        "name": "Zhanli Li"
                    },
                    {
                        "name": "Zichao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zichao Yang"
                },
                "author": "Zichao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.13895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.13895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.09126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.09126v2",
                "updated": "2025-01-03T15:24:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    15,
                    24,
                    28,
                    4,
                    3,
                    0
                ],
                "published": "2024-04-14T02:35:23Z",
                "published_parsed": [
                    2024,
                    4,
                    14,
                    2,
                    35,
                    23,
                    6,
                    105,
                    0
                ],
                "title": "Treatment Effect Heterogeneity and Importance Measures for Multivariate\n  Continuous Treatments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Treatment Effect Heterogeneity and Importance Measures for Multivariate\n  Continuous Treatments"
                },
                "summary": "Estimating the joint effect of a multivariate, continuous exposure is\ncrucial, particularly in environmental health where interest lies in\nsimultaneously evaluating the impact of multiple environmental pollutants on\nhealth. We develop novel methodology that addresses two key issues for\nestimation of treatment effects of multivariate, continuous exposures. We use\nnonparametric Bayesian methodology that is flexible to ensure our approach can\ncapture a wide range of data generating processes. Additionally, we allow the\neffect of the exposures to be heterogeneous with respect to covariates.\nTreatment effect heterogeneity has not been well explored in the causal\ninference literature for multivariate, continuous exposures, and therefore we\nintroduce novel estimands that summarize the nature and extent of the\nheterogeneity, and propose estimation procedures for new estimands related to\ntreatment effect heterogeneity. We provide theoretical support for the proposed\nmodels in the form of posterior contraction rates and show that it works well\nin simulated examples both with and without heterogeneity. Our approach is\nmotivated by a study of the health effects of simultaneous exposure to the\ncomponents of PM$_{2.5}$, where we find that the negative health effects of\nexposure to environmental pollutants are exacerbated by low socioeconomic\nstatus, race and age.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Estimating the joint effect of a multivariate, continuous exposure is\ncrucial, particularly in environmental health where interest lies in\nsimultaneously evaluating the impact of multiple environmental pollutants on\nhealth. We develop novel methodology that addresses two key issues for\nestimation of treatment effects of multivariate, continuous exposures. We use\nnonparametric Bayesian methodology that is flexible to ensure our approach can\ncapture a wide range of data generating processes. Additionally, we allow the\neffect of the exposures to be heterogeneous with respect to covariates.\nTreatment effect heterogeneity has not been well explored in the causal\ninference literature for multivariate, continuous exposures, and therefore we\nintroduce novel estimands that summarize the nature and extent of the\nheterogeneity, and propose estimation procedures for new estimands related to\ntreatment effect heterogeneity. We provide theoretical support for the proposed\nmodels in the form of posterior contraction rates and show that it works well\nin simulated examples both with and without heterogeneity. Our approach is\nmotivated by a study of the health effects of simultaneous exposure to the\ncomponents of PM$_{2.5}$, where we find that the negative health effects of\nexposure to environmental pollutants are exacerbated by low socioeconomic\nstatus, race and age."
                },
                "authors": [
                    {
                        "name": "Heejun Shin"
                    },
                    {
                        "name": "Antonio Linero"
                    },
                    {
                        "name": "Michelle Audirac"
                    },
                    {
                        "name": "Kezia Irene"
                    },
                    {
                        "name": "Danielle Braun"
                    },
                    {
                        "name": "Joseph Antonelli"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Antonelli"
                },
                "author": "Joseph Antonelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.09126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.09126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01861v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01861v1",
                "updated": "2025-01-03T15:18:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    15,
                    18,
                    30,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T15:18:30Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    15,
                    18,
                    30,
                    4,
                    3,
                    0
                ],
                "title": "CycleFlow: Leveraging Cycle Consistency in Flow Matching for Speaker\n  Style Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CycleFlow: Leveraging Cycle Consistency in Flow Matching for Speaker\n  Style Adaptation"
                },
                "summary": "Voice Conversion (VC) aims to convert the style of a source speaker, such as\ntimbre and pitch, to the style of any target speaker while preserving the\nlinguistic content. However, the ground truth of the converted speech does not\nexist in a non-parallel VC scenario, which induces the train-inference mismatch\nproblem. Moreover, existing methods still have an inaccurate pitch and low\nspeaker adaptation quality, there is a significant disparity in pitch between\nthe source and target speaker style domains. As a result, the models tend to\ngenerate speech with hoarseness, posing challenges in achieving high-quality\nvoice conversion. In this study, we propose CycleFlow, a novel VC approach that\nleverages cycle consistency in conditional flow matching (CFM) for speaker\ntimbre adaptation training on non-parallel data. Furthermore, we design a\nDual-CFM based on VoiceCFM and PitchCFM to generate speech and improve speaker\npitch adaptation quality. Experiments show that our method can significantly\nimprove speaker similarity, generating natural and higher-quality speech.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voice Conversion (VC) aims to convert the style of a source speaker, such as\ntimbre and pitch, to the style of any target speaker while preserving the\nlinguistic content. However, the ground truth of the converted speech does not\nexist in a non-parallel VC scenario, which induces the train-inference mismatch\nproblem. Moreover, existing methods still have an inaccurate pitch and low\nspeaker adaptation quality, there is a significant disparity in pitch between\nthe source and target speaker style domains. As a result, the models tend to\ngenerate speech with hoarseness, posing challenges in achieving high-quality\nvoice conversion. In this study, we propose CycleFlow, a novel VC approach that\nleverages cycle consistency in conditional flow matching (CFM) for speaker\ntimbre adaptation training on non-parallel data. Furthermore, we design a\nDual-CFM based on VoiceCFM and PitchCFM to generate speech and improve speaker\npitch adaptation quality. Experiments show that our method can significantly\nimprove speaker similarity, generating natural and higher-quality speech."
                },
                "authors": [
                    {
                        "name": "Ziqi Liang"
                    },
                    {
                        "name": "Xulong Zhang"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Weifeng Zhao"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by 2025 IEEE International Conference on Acoustics, Speech,\n  and Signal Processing (ICASSP2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01861v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01861v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12100v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12100v3",
                "updated": "2025-01-03T15:17:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    15,
                    17,
                    13,
                    4,
                    3,
                    0
                ],
                "published": "2024-06-17T21:25:36Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    21,
                    25,
                    36,
                    0,
                    169,
                    0
                ],
                "title": "CUQDS: Conformal Uncertainty Quantification under Distribution Shift for\n  Trajectory Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CUQDS: Conformal Uncertainty Quantification under Distribution Shift for\n  Trajectory Prediction"
                },
                "summary": "Trajectory prediction models that can infer both finite future trajectories\nand their associated uncertainties of the target vehicles in an online setting\n(e.g., real-world application scenarios) is crucial for ensuring the safe and\nrobust navigation and path planning of autonomous vehicle motion. However, the\nmajority of existing trajectory prediction models have neither considered\nreducing the uncertainty as one objective during the training stage nor\nprovided reliable uncertainty quantification during inference stage under\npotential distribution shift. Therefore, in this paper, we propose the\nConformal Uncertainty Quantification under Distribution Shift framework, CUQDS,\nto quantify the uncertainty of the predicted trajectories of existing\ntrajectory prediction models under potential data distribution shift, while\nconsidering improving the prediction accuracy of the models and reducing the\nestimated uncertainty during the training stage. Specifically, CUQDS includes\n1) a learning-based Gaussian process regression module that models the output\ndistribution of the base model (any existing trajectory prediction or time\nseries forecasting neural networks) and reduces the estimated uncertainty by\nadditional loss term, and 2) a statistical-based Conformal P control module to\ncalibrate the estimated uncertainty from the Gaussian process regression module\nin an online setting under potential distribution shift between training and\ntesting data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trajectory prediction models that can infer both finite future trajectories\nand their associated uncertainties of the target vehicles in an online setting\n(e.g., real-world application scenarios) is crucial for ensuring the safe and\nrobust navigation and path planning of autonomous vehicle motion. However, the\nmajority of existing trajectory prediction models have neither considered\nreducing the uncertainty as one objective during the training stage nor\nprovided reliable uncertainty quantification during inference stage under\npotential distribution shift. Therefore, in this paper, we propose the\nConformal Uncertainty Quantification under Distribution Shift framework, CUQDS,\nto quantify the uncertainty of the predicted trajectories of existing\ntrajectory prediction models under potential data distribution shift, while\nconsidering improving the prediction accuracy of the models and reducing the\nestimated uncertainty during the training stage. Specifically, CUQDS includes\n1) a learning-based Gaussian process regression module that models the output\ndistribution of the base model (any existing trajectory prediction or time\nseries forecasting neural networks) and reduces the estimated uncertainty by\nadditional loss term, and 2) a statistical-based Conformal P control module to\ncalibrate the estimated uncertainty from the Gaussian process regression module\nin an online setting under potential distribution shift between training and\ntesting data."
                },
                "authors": [
                    {
                        "name": "Huiqun Huang"
                    },
                    {
                        "name": "Sihong He"
                    },
                    {
                        "name": "Fei Miao"
                    }
                ],
                "author_detail": {
                    "name": "Fei Miao"
                },
                "author": "Fei Miao",
                "arxiv_comment": "9 pages, 2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12100v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12100v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11464v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11464v2",
                "updated": "2025-01-03T15:09:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    15,
                    9,
                    59,
                    4,
                    3,
                    0
                ],
                "published": "2024-04-17T15:15:38Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    15,
                    15,
                    38,
                    2,
                    108,
                    0
                ],
                "title": "Rates of convergence and normal approximations for estimators of local\n  dependence random graph models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rates of convergence and normal approximations for estimators of local\n  dependence random graph models"
                },
                "summary": "Local dependence random graph models are a class of block models for network\ndata which allow for dependence among edges under a local dependence assumption\ndefined around the block structure of the network. Since being introduced by\nSchweinberger and Handcock (2015), research in the statistical network analysis\nand network science literatures have demonstrated the potential and utility of\nthis class of models. In this work, we provide the first theory for estimation\nand inference which ensures consistent and valid inference of parameter vectors\nof local dependence random graph models. This is accomplished by deriving\nconvergence rates of estimation and inference procedures for local dependence\nrandom graph models based on a single observation of the graph, allowing both\nthe number of model parameters and the sizes of blocks to tend to infinity.\nFirst, we derive non-asymptotic bounds on the $\\ell_2$-error of maximum\nlikelihood estimators with convergence rates, outlining conditions under which\nthese rates are minimax optimal. Second, and more importantly, we derive\nnon-asymptotic bounds on the error of the multivariate normal approximation.\nThese theoretical results are the first to achieve both optimal rates of\nconvergence and non-asymptotic bounds on the error of the multivariate normal\napproximation for parameter vectors of local dependence random graph models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local dependence random graph models are a class of block models for network\ndata which allow for dependence among edges under a local dependence assumption\ndefined around the block structure of the network. Since being introduced by\nSchweinberger and Handcock (2015), research in the statistical network analysis\nand network science literatures have demonstrated the potential and utility of\nthis class of models. In this work, we provide the first theory for estimation\nand inference which ensures consistent and valid inference of parameter vectors\nof local dependence random graph models. This is accomplished by deriving\nconvergence rates of estimation and inference procedures for local dependence\nrandom graph models based on a single observation of the graph, allowing both\nthe number of model parameters and the sizes of blocks to tend to infinity.\nFirst, we derive non-asymptotic bounds on the $\\ell_2$-error of maximum\nlikelihood estimators with convergence rates, outlining conditions under which\nthese rates are minimax optimal. Second, and more importantly, we derive\nnon-asymptotic bounds on the error of the multivariate normal approximation.\nThese theoretical results are the first to achieve both optimal rates of\nconvergence and non-asymptotic bounds on the error of the multivariate normal\napproximation for parameter vectors of local dependence random graph models."
                },
                "authors": [
                    {
                        "name": "Jonathan R. Stewart"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan R. Stewart"
                },
                "author": "Jonathan R. Stewart",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11464v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11464v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01849v1",
                "updated": "2025-01-03T14:59:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    59,
                    38,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T14:59:38Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    59,
                    38,
                    4,
                    3,
                    0
                ],
                "title": "Multi-Agent Conversational Online Learning for Adaptive LLM Response\n  Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Conversational Online Learning for Adaptive LLM Response\n  Identification"
                },
                "summary": "The remarkable generative capability of large language models (LLMs) has\nsparked a growing interest in automatically generating responses for different\napplications. Given the dynamic nature of user preferences and the uncertainty\nof LLM response performance, it is crucial to design efficient online learning\nalgorithms to identify optimal LLM responses (i.e., high-quality responses that\nalso meet user preferences). Most existing online algorithms adopt a\ncentralized approach and fail to leverage explicit user preferences for more\nefficient and personalized LLM response identification. In contrast, this paper\nintroduces \\textit{MACO} (\\underline{M}ulti-\\underline{A}gent\n\\underline{C}onversational \\underline{O}nline Learning for Adaptive LLM\nResponse Identification): 1) The online LLM response identification process is\naccelerated by multiple local agents (such as smartphones), while enhancing\ndata privacy; 2) A novel conversational mechanism is proposed to adaptively\nconduct conversations for soliciting user preferences (e.g., a preference for a\nhumorous tone over a serious one in generated responses), so to minimize\nuncertainty in preference estimation. Our theoretical analysis demonstrates\nthat \\cadi\\ is near-optimal regarding cumulative regret. Additionally, \\cadi\\\noffers reduced communication costs and computational complexity by eliminating\nthe traditional, computing-intensive ``G-optimal design\" found in previous\nworks. Extensive experiments with the open LLM \\textit{Llama}, coupled with two\ndifferent embedding models from Google and OpenAI for text vector\nrepresentation, demonstrate that \\cadi\\ significantly outperforms the current\nstate-of-the-art in online LLM response identification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable generative capability of large language models (LLMs) has\nsparked a growing interest in automatically generating responses for different\napplications. Given the dynamic nature of user preferences and the uncertainty\nof LLM response performance, it is crucial to design efficient online learning\nalgorithms to identify optimal LLM responses (i.e., high-quality responses that\nalso meet user preferences). Most existing online algorithms adopt a\ncentralized approach and fail to leverage explicit user preferences for more\nefficient and personalized LLM response identification. In contrast, this paper\nintroduces \\textit{MACO} (\\underline{M}ulti-\\underline{A}gent\n\\underline{C}onversational \\underline{O}nline Learning for Adaptive LLM\nResponse Identification): 1) The online LLM response identification process is\naccelerated by multiple local agents (such as smartphones), while enhancing\ndata privacy; 2) A novel conversational mechanism is proposed to adaptively\nconduct conversations for soliciting user preferences (e.g., a preference for a\nhumorous tone over a serious one in generated responses), so to minimize\nuncertainty in preference estimation. Our theoretical analysis demonstrates\nthat \\cadi\\ is near-optimal regarding cumulative regret. Additionally, \\cadi\\\noffers reduced communication costs and computational complexity by eliminating\nthe traditional, computing-intensive ``G-optimal design\" found in previous\nworks. Extensive experiments with the open LLM \\textit{Llama}, coupled with two\ndifferent embedding models from Google and OpenAI for text vector\nrepresentation, demonstrate that \\cadi\\ significantly outperforms the current\nstate-of-the-art in online LLM response identification."
                },
                "authors": [
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Yuejin Xie"
                    },
                    {
                        "name": "Maoli Liu"
                    },
                    {
                        "name": "Xuchuang Wang"
                    },
                    {
                        "name": "Zhuohua Li"
                    },
                    {
                        "name": "Huanyu Wang"
                    },
                    {
                        "name": "John C. S. Lui"
                    }
                ],
                "author_detail": {
                    "name": "John C. S. Lui"
                },
                "author": "John C. S. Lui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01841v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01841v1",
                "updated": "2025-01-03T14:46:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    46,
                    34,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T14:46:34Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    46,
                    34,
                    4,
                    3,
                    0
                ],
                "title": "Dedicated Inference Engine and Binary-Weight Neural Networks for\n  Lightweight Instance Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dedicated Inference Engine and Binary-Weight Neural Networks for\n  Lightweight Instance Segmentation"
                },
                "summary": "Reducing computational costs is an important issue for development of\nembedded systems. Binary-weight Neural Networks (BNNs), in which weights are\nbinarized and activations are quantized, are employed to reduce computational\ncosts of various kinds of applications. In this paper, a design methodology of\nhardware architecture for inference engines is proposed to handle modern BNNs\nwith two operation modes. Multiply-Accumulate (MAC) operations can be\nsimplified by replacing multiply operations with bitwise operations. The\nproposed method can effectively reduce the gate count of inference engines by\nremoving a part of computational costs from the hardware system. The\narchitecture of MAC operations can calculate the inference results of BNNs\nefficiently with only 52% of hardware costs compared with the related works. To\nshow that the inference engine can handle practical applications, two\nlightweight networks which combine the backbones of SegNeXt and the decoder of\nSparseInst for instance segmentation are also proposed. The output results of\nthe lightweight networks are computed using only bitwise operations and add\noperations. The proposed inference engine has lower hardware costs than related\nworks. The experimental results show that the proposed inference engine can\nhandle the proposed instance-segmentation networks and achieves higher accuracy\nthan YOLACT on the \"Person\" category although the model size is 77.7$\\times$\nsmaller compared with YOLACT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing computational costs is an important issue for development of\nembedded systems. Binary-weight Neural Networks (BNNs), in which weights are\nbinarized and activations are quantized, are employed to reduce computational\ncosts of various kinds of applications. In this paper, a design methodology of\nhardware architecture for inference engines is proposed to handle modern BNNs\nwith two operation modes. Multiply-Accumulate (MAC) operations can be\nsimplified by replacing multiply operations with bitwise operations. The\nproposed method can effectively reduce the gate count of inference engines by\nremoving a part of computational costs from the hardware system. The\narchitecture of MAC operations can calculate the inference results of BNNs\nefficiently with only 52% of hardware costs compared with the related works. To\nshow that the inference engine can handle practical applications, two\nlightweight networks which combine the backbones of SegNeXt and the decoder of\nSparseInst for instance segmentation are also proposed. The output results of\nthe lightweight networks are computed using only bitwise operations and add\noperations. The proposed inference engine has lower hardware costs than related\nworks. The experimental results show that the proposed inference engine can\nhandle the proposed instance-segmentation networks and achieves higher accuracy\nthan YOLACT on the \"Person\" category although the model size is 77.7$\\times$\nsmaller compared with YOLACT."
                },
                "authors": [
                    {
                        "name": "Tse-Wei Chen"
                    },
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Dongyue Zhao"
                    },
                    {
                        "name": "Kazuhiro Mima"
                    },
                    {
                        "name": "Tadayuki Ito"
                    },
                    {
                        "name": "Kinya Osa"
                    },
                    {
                        "name": "Masami Kato"
                    }
                ],
                "author_detail": {
                    "name": "Masami Kato"
                },
                "author": "Masami Kato",
                "arxiv_comment": "Camera-ready version for CVPR 2024 workshop (Embedded Vision\n  Workshop)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01841v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01841v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01834v1",
                "updated": "2025-01-03T14:38:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    38,
                    1,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T14:38:01Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    38,
                    1,
                    4,
                    3,
                    0
                ],
                "title": "MoColl: Agent-Based Specific and General Model Collaboration for Image\n  Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoColl: Agent-Based Specific and General Model Collaboration for Image\n  Captioning"
                },
                "summary": "Image captioning is a critical task at the intersection of computer vision\nand natural language processing, with wide-ranging applications across various\ndomains. For complex tasks such as diagnostic report generation, deep learning\nmodels require not only domain-specific image-caption datasets but also the\nincorporation of relevant general knowledge to provide contextual accuracy.\nExisting approaches exhibit inherent limitations: specialized models excel in\ncapturing domain-specific details but lack generalization, while\nvision-language models (VLMs) built on large language models (LLMs) leverage\ngeneral knowledge but struggle with domain-specific adaptation. To address\nthese limitations, this paper proposes a novel agent-enhanced model\ncollaboration framework, which we called \\textbf{MoColl}, designed to\neffectively integrate domain-specific and general knowledge. Specifically, our\napproach is to decompose complex image captioning tasks into a series of\ninterconnected question-answer subtasks. A trainable visual question answering\n(VQA) model is employed as a specialized tool to focus on domain-specific\nvisual analysis, answering task-specific questions based on image content.\nConcurrently, an LLM-based agent with general knowledge formulates these\nquestions and synthesizes the resulting question-answer pairs into coherent\ncaptions. Beyond its role in leveraging the VQA model, the agent further guides\nits training to enhance its domain-specific capabilities. Experimental results\non radiology report generation validate the effectiveness of the proposed\nframework, demonstrating significant improvements in the quality of generated\nreports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image captioning is a critical task at the intersection of computer vision\nand natural language processing, with wide-ranging applications across various\ndomains. For complex tasks such as diagnostic report generation, deep learning\nmodels require not only domain-specific image-caption datasets but also the\nincorporation of relevant general knowledge to provide contextual accuracy.\nExisting approaches exhibit inherent limitations: specialized models excel in\ncapturing domain-specific details but lack generalization, while\nvision-language models (VLMs) built on large language models (LLMs) leverage\ngeneral knowledge but struggle with domain-specific adaptation. To address\nthese limitations, this paper proposes a novel agent-enhanced model\ncollaboration framework, which we called \\textbf{MoColl}, designed to\neffectively integrate domain-specific and general knowledge. Specifically, our\napproach is to decompose complex image captioning tasks into a series of\ninterconnected question-answer subtasks. A trainable visual question answering\n(VQA) model is employed as a specialized tool to focus on domain-specific\nvisual analysis, answering task-specific questions based on image content.\nConcurrently, an LLM-based agent with general knowledge formulates these\nquestions and synthesizes the resulting question-answer pairs into coherent\ncaptions. Beyond its role in leveraging the VQA model, the agent further guides\nits training to enhance its domain-specific capabilities. Experimental results\non radiology report generation validate the effectiveness of the proposed\nframework, demonstrating significant improvements in the quality of generated\nreports."
                },
                "authors": [
                    {
                        "name": "Pu Yang"
                    },
                    {
                        "name": "Bin Dong"
                    }
                ],
                "author_detail": {
                    "name": "Bin Dong"
                },
                "author": "Bin Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01832v1",
                "updated": "2025-01-03T14:34:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    34,
                    30,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T14:34:30Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    34,
                    30,
                    4,
                    3,
                    0
                ],
                "title": "Time Series Language Model for Descriptive Caption Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Language Model for Descriptive Caption Generation"
                },
                "summary": "The automatic generation of representative natural language descriptions for\nobservable patterns in time series data enhances interpretability, simplifies\nanalysis and increases cross-domain utility of temporal data. While pre-trained\nfoundation models have made considerable progress in natural language\nprocessing (NLP) and computer vision (CV), their application to time series\nanalysis has been hindered by data scarcity. Although several large language\nmodel (LLM)-based methods have been proposed for time series forecasting, time\nseries captioning is under-explored in the context of LLMs. In this paper, we\nintroduce TSLM, a novel time series language model designed specifically for\ntime series captioning. TSLM operates as an encoder-decoder model, leveraging\nboth text prompts and time series data representations to capture subtle\ntemporal patterns across multiple phases and generate precise textual\ndescriptions of time series inputs. TSLM addresses the data scarcity problem in\ntime series captioning by first leveraging an in-context prompting synthetic\ndata generation, and second denoising the generated data via a novel\ncross-modal dense retrieval scoring applied to time series-caption pairs.\nExperimental findings on various time series captioning datasets demonstrate\nthat TSLM outperforms existing state-of-the-art approaches from multiple data\nmodalities by a significant margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic generation of representative natural language descriptions for\nobservable patterns in time series data enhances interpretability, simplifies\nanalysis and increases cross-domain utility of temporal data. While pre-trained\nfoundation models have made considerable progress in natural language\nprocessing (NLP) and computer vision (CV), their application to time series\nanalysis has been hindered by data scarcity. Although several large language\nmodel (LLM)-based methods have been proposed for time series forecasting, time\nseries captioning is under-explored in the context of LLMs. In this paper, we\nintroduce TSLM, a novel time series language model designed specifically for\ntime series captioning. TSLM operates as an encoder-decoder model, leveraging\nboth text prompts and time series data representations to capture subtle\ntemporal patterns across multiple phases and generate precise textual\ndescriptions of time series inputs. TSLM addresses the data scarcity problem in\ntime series captioning by first leveraging an in-context prompting synthetic\ndata generation, and second denoising the generated data via a novel\ncross-modal dense retrieval scoring applied to time series-caption pairs.\nExperimental findings on various time series captioning datasets demonstrate\nthat TSLM outperforms existing state-of-the-art approaches from multiple data\nmodalities by a significant margin."
                },
                "authors": [
                    {
                        "name": "Mohamed Trabelsi"
                    },
                    {
                        "name": "Aidan Boyd"
                    },
                    {
                        "name": "Jin Cao"
                    },
                    {
                        "name": "Huseyin Uzunalioglu"
                    }
                ],
                "author_detail": {
                    "name": "Huseyin Uzunalioglu"
                },
                "author": "Huseyin Uzunalioglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01830v1",
                "updated": "2025-01-03T14:30:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    30,
                    14,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T14:30:14Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    30,
                    14,
                    4,
                    3,
                    0
                ],
                "title": "Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large\n  Language Models"
                },
                "summary": "Automated red-teaming has become a crucial approach for uncovering\nvulnerabilities in large language models (LLMs). However, most existing methods\nfocus on isolated safety flaws, limiting their ability to adapt to dynamic\ndefenses and uncover complex vulnerabilities efficiently. To address this\nchallenge, we propose Auto-RT, a reinforcement learning framework that\nautomatically explores and optimizes complex attack strategies to effectively\nuncover security vulnerabilities through malicious queries. Specifically, we\nintroduce two key mechanisms to reduce exploration complexity and improve\nstrategy optimization: 1) Early-terminated Exploration, which accelerate\nexploration by focusing on high-potential attack strategies; and 2) Progressive\nReward Tracking algorithm with intermediate downgrade models, which dynamically\nrefine the search trajectory toward successful vulnerability exploitation.\nExtensive experiments across diverse LLMs demonstrate that, by significantly\nimproving exploration efficiency and automatically optimizing attack\nstrategies, Auto-RT detects a boarder range of vulnerabilities, achieving a\nfaster detection speed and 16.63\\% higher success rates compared to existing\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated red-teaming has become a crucial approach for uncovering\nvulnerabilities in large language models (LLMs). However, most existing methods\nfocus on isolated safety flaws, limiting their ability to adapt to dynamic\ndefenses and uncover complex vulnerabilities efficiently. To address this\nchallenge, we propose Auto-RT, a reinforcement learning framework that\nautomatically explores and optimizes complex attack strategies to effectively\nuncover security vulnerabilities through malicious queries. Specifically, we\nintroduce two key mechanisms to reduce exploration complexity and improve\nstrategy optimization: 1) Early-terminated Exploration, which accelerate\nexploration by focusing on high-potential attack strategies; and 2) Progressive\nReward Tracking algorithm with intermediate downgrade models, which dynamically\nrefine the search trajectory toward successful vulnerability exploitation.\nExtensive experiments across diverse LLMs demonstrate that, by significantly\nimproving exploration efficiency and automatically optimizing attack\nstrategies, Auto-RT detects a boarder range of vulnerabilities, achieving a\nfaster detection speed and 16.63\\% higher success rates compared to existing\nmethods."
                },
                "authors": [
                    {
                        "name": "Yanjiang Liu"
                    },
                    {
                        "name": "Shuhen Zhou"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Huijia Zhu"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Ben He"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14915v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14915v2",
                "updated": "2025-01-03T14:21:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    21,
                    20,
                    4,
                    3,
                    0
                ],
                "published": "2024-08-27T09:44:01Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    9,
                    44,
                    1,
                    1,
                    240,
                    0
                ],
                "title": "Can Transformers Do Enumerative Geometry?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Transformers Do Enumerative Geometry?"
                },
                "summary": "How can Transformers model and learn enumerative geometry? What is a robust\nprocedure for using Transformers in abductive knowledge discovery within a\nmathematician-machine collaboration? In this work, we introduce a\nTransformer-based approach to computational enumerative geometry, specifically\ntargeting the computation of $\\psi$-class intersection numbers on the moduli\nspace of curves. By reformulating the problem as a continuous optimization\ntask, we compute intersection numbers across a wide value range from $10^{-45}$\nto $10^{45}$. To capture the recursive nature inherent in these intersection\nnumbers, we propose the Dynamic Range Activator (DRA), a new activation\nfunction that enhances the Transformer's ability to model recursive patterns\nand handle severe heteroscedasticity. Given precision requirements for\ncomputing the intersections, we quantify the uncertainty of the predictions\nusing Conformal Prediction with a dynamic sliding window adaptive to the\npartitions of equivalent number of marked points. To the best of our knowledge,\nthere has been no prior work on modeling recursive functions with such a\nhigh-variance and factorial growth. Beyond simply computing intersection\nnumbers, we explore the enumerative \"world-model\" of Transformers. Our\ninterpretability analysis reveals that the network is implicitly modeling the\nVirasoro constraints in a purely data-driven manner. Moreover, through\nabductive hypothesis testing, probing, and causal inference, we uncover\nevidence of an emergent internal representation of the the large-genus\nasymptotic of $\\psi$-class intersection numbers. These findings suggest that\nthe network internalizes the parameters of the asymptotic closed-form and the\npolynomiality phenomenon of $\\psi$-class intersection numbers in a non-linear\nmanner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can Transformers model and learn enumerative geometry? What is a robust\nprocedure for using Transformers in abductive knowledge discovery within a\nmathematician-machine collaboration? In this work, we introduce a\nTransformer-based approach to computational enumerative geometry, specifically\ntargeting the computation of $\\psi$-class intersection numbers on the moduli\nspace of curves. By reformulating the problem as a continuous optimization\ntask, we compute intersection numbers across a wide value range from $10^{-45}$\nto $10^{45}$. To capture the recursive nature inherent in these intersection\nnumbers, we propose the Dynamic Range Activator (DRA), a new activation\nfunction that enhances the Transformer's ability to model recursive patterns\nand handle severe heteroscedasticity. Given precision requirements for\ncomputing the intersections, we quantify the uncertainty of the predictions\nusing Conformal Prediction with a dynamic sliding window adaptive to the\npartitions of equivalent number of marked points. To the best of our knowledge,\nthere has been no prior work on modeling recursive functions with such a\nhigh-variance and factorial growth. Beyond simply computing intersection\nnumbers, we explore the enumerative \"world-model\" of Transformers. Our\ninterpretability analysis reveals that the network is implicitly modeling the\nVirasoro constraints in a purely data-driven manner. Moreover, through\nabductive hypothesis testing, probing, and causal inference, we uncover\nevidence of an emergent internal representation of the the large-genus\nasymptotic of $\\psi$-class intersection numbers. These findings suggest that\nthe network internalizes the parameters of the asymptotic closed-form and the\npolynomiality phenomenon of $\\psi$-class intersection numbers in a non-linear\nmanner."
                },
                "authors": [
                    {
                        "name": "Baran Hashemi"
                    },
                    {
                        "name": "Roderic G. Corominas"
                    },
                    {
                        "name": "Alessandro Giacchetto"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Giacchetto"
                },
                "author": "Alessandro Giacchetto",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14915v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14915v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.AG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07031v2",
                "updated": "2025-01-03T14:19:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    19,
                    58,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-09T22:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    22,
                    37,
                    48,
                    0,
                    344,
                    0
                ],
                "title": "Large Language Models: An Applied Econometric Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models: An Applied Econometric Framework"
                },
                "summary": "How can we use the novel capacities of large language models (LLMs) in\nempirical research? And how can we do so while accounting for their\nlimitations, which are themselves only poorly understood? We develop an\neconometric framework to answer this question that distinguishes between two\ntypes of empirical tasks. Using LLMs for prediction problems (including\nhypothesis generation) is valid under one condition: no ``leakage'' between the\nLLM's training dataset and the researcher's sample. No leakage can be ensured\nby using open-source LLMs with documented training data and published weights.\nUsing LLM outputs for estimation problems to automate the measurement of some\neconomic concept (expressed either by some text or from human subjects)\nrequires the researcher to collect at least some validation data: without such\ndata, the errors of the LLM's automation cannot be assessed and accounted for.\nAs long as these steps are taken, LLM outputs can be used in empirical research\nwith the familiar econometric guarantees we desire. Using two illustrative\napplications to finance and political economy, we find that these requirements\nare stringent; when they are violated, the limitations of LLMs now result in\nunreliable empirical estimates. Our results suggest the excitement around the\nempirical uses of LLMs is warranted -- they allow researchers to effectively\nuse even small amounts of language data for both prediction and estimation --\nbut only with these safeguards in place.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we use the novel capacities of large language models (LLMs) in\nempirical research? And how can we do so while accounting for their\nlimitations, which are themselves only poorly understood? We develop an\neconometric framework to answer this question that distinguishes between two\ntypes of empirical tasks. Using LLMs for prediction problems (including\nhypothesis generation) is valid under one condition: no ``leakage'' between the\nLLM's training dataset and the researcher's sample. No leakage can be ensured\nby using open-source LLMs with documented training data and published weights.\nUsing LLM outputs for estimation problems to automate the measurement of some\neconomic concept (expressed either by some text or from human subjects)\nrequires the researcher to collect at least some validation data: without such\ndata, the errors of the LLM's automation cannot be assessed and accounted for.\nAs long as these steps are taken, LLM outputs can be used in empirical research\nwith the familiar econometric guarantees we desire. Using two illustrative\napplications to finance and political economy, we find that these requirements\nare stringent; when they are violated, the limitations of LLMs now result in\nunreliable empirical estimates. Our results suggest the excitement around the\nempirical uses of LLMs is warranted -- they allow researchers to effectively\nuse even small amounts of language data for both prediction and estimation --\nbut only with these safeguards in place."
                },
                "authors": [
                    {
                        "name": "Jens Ludwig"
                    },
                    {
                        "name": "Sendhil Mullainathan"
                    },
                    {
                        "name": "Ashesh Rambachan"
                    }
                ],
                "author_detail": {
                    "name": "Ashesh Rambachan"
                },
                "author": "Ashesh Rambachan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01825v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01825v1",
                "updated": "2025-01-03T14:17:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    17,
                    41,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T14:17:41Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    17,
                    41,
                    4,
                    3,
                    0
                ],
                "title": "Unified Native Spaces in Kernel Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Native Spaces in Kernel Methods"
                },
                "summary": "There exists a plethora of parametric models for positive definite kernels,\nand their use is ubiquitous in disciplines as diverse as statistics, machine\nlearning, numerical analysis, and approximation theory. Usually, the kernel\nparameters index certain features of an associated process. Amongst those\nfeatures, smoothness (in the sense of Sobolev spaces, mean square\ndifferentiability, and fractal dimensions), compact or global supports, and\nnegative dependencies (hole effects) are of interest to several theoretical and\napplied disciplines. This paper unifies a wealth of well-known kernels into a\nsingle parametric class that encompasses them as special cases, attained either\nby exact parameterization or through parametric asymptotics. We furthermore\ncharacterize the Sobolev space that is norm equivalent to the RKHS associated\nwith the new kernel. As a by-product, we infer the Sobolev spaces that are\nassociated with existing classes of kernels. We illustrate the main properties\nof the new class, show how this class can switch from compact to global\nsupports, and provide special cases for which the kernel attains negative\nvalues over nontrivial intervals. Hence, the proposed class of kernel is the\nreproducing kernel of a very rich Hilbert space that contains many special\ncases, including the celebrated Mat\\'ern and Wendland kernels, as well as their\naliases with hole effects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There exists a plethora of parametric models for positive definite kernels,\nand their use is ubiquitous in disciplines as diverse as statistics, machine\nlearning, numerical analysis, and approximation theory. Usually, the kernel\nparameters index certain features of an associated process. Amongst those\nfeatures, smoothness (in the sense of Sobolev spaces, mean square\ndifferentiability, and fractal dimensions), compact or global supports, and\nnegative dependencies (hole effects) are of interest to several theoretical and\napplied disciplines. This paper unifies a wealth of well-known kernels into a\nsingle parametric class that encompasses them as special cases, attained either\nby exact parameterization or through parametric asymptotics. We furthermore\ncharacterize the Sobolev space that is norm equivalent to the RKHS associated\nwith the new kernel. As a by-product, we infer the Sobolev spaces that are\nassociated with existing classes of kernels. We illustrate the main properties\nof the new class, show how this class can switch from compact to global\nsupports, and provide special cases for which the kernel attains negative\nvalues over nontrivial intervals. Hence, the proposed class of kernel is the\nreproducing kernel of a very rich Hilbert space that contains many special\ncases, including the celebrated Mat\\'ern and Wendland kernels, as well as their\naliases with hole effects."
                },
                "authors": [
                    {
                        "name": "Xavier Emery"
                    },
                    {
                        "name": "Emilio Porcu"
                    },
                    {
                        "name": "Moreno Bevilacqua"
                    }
                ],
                "author_detail": {
                    "name": "Moreno Bevilacqua"
                },
                "author": "Moreno Bevilacqua",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01825v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01825v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01821v1",
                "updated": "2025-01-03T14:09:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    9,
                    46,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T14:09:46Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    9,
                    46,
                    4,
                    3,
                    0
                ],
                "title": "SDPO: Segment-Level Direct Preference Optimization for Social Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDPO: Segment-Level Direct Preference Optimization for Social Agents"
                },
                "summary": "Social agents powered by large language models (LLMs) can simulate human\nsocial behaviors but fall short in handling complex goal-oriented social\ndialogues. Direct Preference Optimization (DPO) has proven effective in\naligning LLM behavior with human preferences across a variety of agent tasks.\nExisting DPO-based approaches for multi-turn interactions are divided into\nturn-level and session-level methods. The turn-level method is overly\nfine-grained, focusing exclusively on individual turns, while session-level\nmethods are too coarse-grained, often introducing training noise. To address\nthese limitations, we propose Segment-Level Direct Preference Optimization\n(SDPO), which focuses on specific key segments within interactions to optimize\nmulti-turn agent behavior while minimizing training noise. Evaluations on the\nSOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform\nboth existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring\nSDPO's potential to advance the social intelligence of LLM-based agents. We\nrelease our code and data at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social agents powered by large language models (LLMs) can simulate human\nsocial behaviors but fall short in handling complex goal-oriented social\ndialogues. Direct Preference Optimization (DPO) has proven effective in\naligning LLM behavior with human preferences across a variety of agent tasks.\nExisting DPO-based approaches for multi-turn interactions are divided into\nturn-level and session-level methods. The turn-level method is overly\nfine-grained, focusing exclusively on individual turns, while session-level\nmethods are too coarse-grained, often introducing training noise. To address\nthese limitations, we propose Segment-Level Direct Preference Optimization\n(SDPO), which focuses on specific key segments within interactions to optimize\nmulti-turn agent behavior while minimizing training noise. Evaluations on the\nSOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform\nboth existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring\nSDPO's potential to advance the social intelligence of LLM-based agents. We\nrelease our code and data at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO."
                },
                "authors": [
                    {
                        "name": "Aobo Kong"
                    },
                    {
                        "name": "Wentao Ma"
                    },
                    {
                        "name": "Shiwan Zhao"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Yuchuan Wu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Qicheng Li"
                    },
                    {
                        "name": "Yong Qin"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01818v1",
                "updated": "2025-01-03T14:03:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    3,
                    14,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T14:03:14Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    3,
                    14,
                    4,
                    3,
                    0
                ],
                "title": "Rerouting LLM Routers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rerouting LLM Routers"
                },
                "summary": "LLM routers aim to balance quality and cost of generation by classifying\nqueries and routing them to a cheaper or more expensive LLM depending on their\ncomplexity. Routers represent one type of what we call LLM control planes:\nsystems that orchestrate use of one or more LLMs. In this paper, we investigate\nrouters' adversarial robustness.\n  We first define LLM control plane integrity, i.e., robustness of LLM\norchestration to adversarial inputs, as a distinct problem in AI safety. Next,\nwe demonstrate that an adversary can generate query-independent token sequences\nwe call ``confounder gadgets'' that, when added to any query, cause LLM routers\nto send the query to a strong LLM.\n  Our quantitative evaluation shows that this attack is successful both in\nwhite-box and black-box settings against a variety of open-source and\ncommercial routers, and that confounding queries do not affect the quality of\nLLM responses. Finally, we demonstrate that gadgets can be effective while\nmaintaining low perplexity, thus perplexity-based filtering is not an effective\ndefense. We finish by investigating alternative defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM routers aim to balance quality and cost of generation by classifying\nqueries and routing them to a cheaper or more expensive LLM depending on their\ncomplexity. Routers represent one type of what we call LLM control planes:\nsystems that orchestrate use of one or more LLMs. In this paper, we investigate\nrouters' adversarial robustness.\n  We first define LLM control plane integrity, i.e., robustness of LLM\norchestration to adversarial inputs, as a distinct problem in AI safety. Next,\nwe demonstrate that an adversary can generate query-independent token sequences\nwe call ``confounder gadgets'' that, when added to any query, cause LLM routers\nto send the query to a strong LLM.\n  Our quantitative evaluation shows that this attack is successful both in\nwhite-box and black-box settings against a variety of open-source and\ncommercial routers, and that confounding queries do not affect the quality of\nLLM responses. Finally, we demonstrate that gadgets can be effective while\nmaintaining low perplexity, thus perplexity-based filtering is not an effective\ndefense. We finish by investigating alternative defenses."
                },
                "authors": [
                    {
                        "name": "Avital Shafran"
                    },
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Thomas Ristenpart"
                    },
                    {
                        "name": "Vitaly Shmatikov"
                    }
                ],
                "author_detail": {
                    "name": "Vitaly Shmatikov"
                },
                "author": "Vitaly Shmatikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00958v2",
                "updated": "2025-01-03T13:25:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    25,
                    27,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-01T21:29:37Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    29,
                    37,
                    2,
                    1,
                    0
                ],
                "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining"
                },
                "summary": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving~\\footnote{Our code are\navailable at \\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving~\\footnote{Our code are\navailable at \\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}}."
                },
                "authors": [
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Jiashuo Sun"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04307v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04307v2",
                "updated": "2025-01-03T13:17:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    17,
                    32,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-05T16:26:37Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    26,
                    37,
                    3,
                    340,
                    0
                ],
                "title": "Feature Coding in the Era of Large Models: Dataset, Test Conditions, and\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Coding in the Era of Large Models: Dataset, Test Conditions, and\n  Benchmark"
                },
                "summary": "Large models have achieved remarkable performance across various tasks, yet\nthey incur significant computational costs and privacy concerns during both\ntraining and inference. Distributed deployment has emerged as a potential\nsolution, but it necessitates the exchange of intermediate information between\nmodel segments, with feature representations serving as crucial information\ncarriers. To optimize information exchange, feature coding methods are applied\nto reduce transmission and storage overhead. Despite its importance, feature\ncoding for large models remains an under-explored area. In this paper, we draw\nattention to large model feature coding and make three contributions to this\nfield. First, we introduce a comprehensive dataset encompassing diverse\nfeatures generated by three representative types of large models. Second, we\nestablish unified test conditions, enabling standardized evaluation pipelines\nand fair comparisons across future feature coding studies. Third, we introduce\ntwo baseline methods derived from widely used image coding techniques and\nbenchmark their performance on the proposed dataset. These contributions aim to\nadvance the field of feature coding, facilitating more efficient large model\ndeployment. All source code and the dataset are now available at\n\\href{https://github.com/chansongoal/FCM-LM/tree/master}{https://github.com/chansongoal/FCM-LM/tree/master}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large models have achieved remarkable performance across various tasks, yet\nthey incur significant computational costs and privacy concerns during both\ntraining and inference. Distributed deployment has emerged as a potential\nsolution, but it necessitates the exchange of intermediate information between\nmodel segments, with feature representations serving as crucial information\ncarriers. To optimize information exchange, feature coding methods are applied\nto reduce transmission and storage overhead. Despite its importance, feature\ncoding for large models remains an under-explored area. In this paper, we draw\nattention to large model feature coding and make three contributions to this\nfield. First, we introduce a comprehensive dataset encompassing diverse\nfeatures generated by three representative types of large models. Second, we\nestablish unified test conditions, enabling standardized evaluation pipelines\nand fair comparisons across future feature coding studies. Third, we introduce\ntwo baseline methods derived from widely used image coding techniques and\nbenchmark their performance on the proposed dataset. These contributions aim to\nadvance the field of feature coding, facilitating more efficient large model\ndeployment. All source code and the dataset are now available at\n\\href{https://github.com/chansongoal/FCM-LM/tree/master}{https://github.com/chansongoal/FCM-LM/tree/master}."
                },
                "authors": [
                    {
                        "name": "Changsheng Gao"
                    },
                    {
                        "name": "Yifan Ma"
                    },
                    {
                        "name": "Qiaoxi Chen"
                    },
                    {
                        "name": "Yenan Xu"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Weisi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weisi Lin"
                },
                "author": "Weisi Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04307v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04307v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01793v1",
                "updated": "2025-01-03T12:52:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    52,
                    51,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:52:51Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    52,
                    51,
                    4,
                    3,
                    0
                ],
                "title": "Creating Artificial Students that Never Existed: Leveraging Large\n  Language Models and CTGANs for Synthetic Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating Artificial Students that Never Existed: Leveraging Large\n  Language Models and CTGANs for Synthetic Data Generation"
                },
                "summary": "In this study, we explore the growing potential of AI and deep learning\ntechnologies, particularly Generative Adversarial Networks (GANs) and Large\nLanguage Models (LLMs), for generating synthetic tabular data. Access to\nquality students data is critical for advancing learning analytics, but privacy\nconcerns and stricter data protection regulations worldwide limit their\navailability and usage. Synthetic data offers a promising alternative. We\ninvestigate whether synthetic data can be leveraged to create artificial\nstudents for serving learning analytics models. Using the popular GAN model\nCTGAN and three LLMs- GPT2, DistilGPT2, and DialoGPT, we generate synthetic\ntabular student data. Our results demonstrate the strong potential of these\nmethods to produce high-quality synthetic datasets that resemble real students\ndata. To validate our findings, we apply a comprehensive set of utility\nevaluation metrics to assess the statistical and predictive performance of the\nsynthetic data and compare the different generator models used, specially the\nperformance of LLMs. Our study aims to provide the learning analytics community\nwith valuable insights into the use of synthetic data, laying the groundwork\nfor expanding the field methodological toolbox with new innovative approaches\nfor learning analytics data generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we explore the growing potential of AI and deep learning\ntechnologies, particularly Generative Adversarial Networks (GANs) and Large\nLanguage Models (LLMs), for generating synthetic tabular data. Access to\nquality students data is critical for advancing learning analytics, but privacy\nconcerns and stricter data protection regulations worldwide limit their\navailability and usage. Synthetic data offers a promising alternative. We\ninvestigate whether synthetic data can be leveraged to create artificial\nstudents for serving learning analytics models. Using the popular GAN model\nCTGAN and three LLMs- GPT2, DistilGPT2, and DialoGPT, we generate synthetic\ntabular student data. Our results demonstrate the strong potential of these\nmethods to produce high-quality synthetic datasets that resemble real students\ndata. To validate our findings, we apply a comprehensive set of utility\nevaluation metrics to assess the statistical and predictive performance of the\nsynthetic data and compare the different generator models used, specially the\nperformance of LLMs. Our study aims to provide the learning analytics community\nwith valuable insights into the use of synthetic data, laying the groundwork\nfor expanding the field methodological toolbox with new innovative approaches\nfor learning analytics data generation."
                },
                "authors": [
                    {
                        "name": "Mohammad Khalil"
                    },
                    {
                        "name": "Farhad Vadiee"
                    },
                    {
                        "name": "Ronas Shakya"
                    },
                    {
                        "name": "Qinyi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qinyi Liu"
                },
                "author": "Qinyi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17837v2",
                "updated": "2025-01-03T12:32:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    32,
                    35,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-17T07:42:39Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    42,
                    39,
                    1,
                    352,
                    0
                ],
                "title": "Evaluating the Capabilities of Large Language Models for Multi-label\n  Emotion Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Capabilities of Large Language Models for Multi-label\n  Emotion Understanding"
                },
                "summary": "Large Language Models (LLMs) show promising learning and reasoning abilities.\nCompared to other NLP tasks, multilingual and multi-label emotion evaluation\ntasks are under-explored in LLMs. In this paper, we present EthioEmo, a\nmulti-label emotion classification dataset for four Ethiopian languages,\nnamely, Amharic (amh), Afan Oromo (orm), Somali (som), and Tigrinya (tir). We\nperform extensive experiments with an additional English multi-label emotion\ndataset from SemEval 2018 Task 1. Our evaluation includes encoder-only,\nencoder-decoder, and decoder-only language models. We compare zero and few-shot\napproaches of LLMs to fine-tuning smaller language models. The results show\nthat accurate multi-label emotion classification is still insufficient even for\nhigh-resource languages such as English, and there is a large gap between the\nperformance of high-resource and low-resource languages. The results also show\nvarying performance levels depending on the language and model type. EthioEmo\nis available publicly to further improve the understanding of emotions in\nlanguage models and how people convey emotions through various languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show promising learning and reasoning abilities.\nCompared to other NLP tasks, multilingual and multi-label emotion evaluation\ntasks are under-explored in LLMs. In this paper, we present EthioEmo, a\nmulti-label emotion classification dataset for four Ethiopian languages,\nnamely, Amharic (amh), Afan Oromo (orm), Somali (som), and Tigrinya (tir). We\nperform extensive experiments with an additional English multi-label emotion\ndataset from SemEval 2018 Task 1. Our evaluation includes encoder-only,\nencoder-decoder, and decoder-only language models. We compare zero and few-shot\napproaches of LLMs to fine-tuning smaller language models. The results show\nthat accurate multi-label emotion classification is still insufficient even for\nhigh-resource languages such as English, and there is a large gap between the\nperformance of high-resource and low-resource languages. The results also show\nvarying performance levels depending on the language and model type. EthioEmo\nis available publicly to further improve the understanding of emotions in\nlanguage models and how people convey emotions through various languages."
                },
                "authors": [
                    {
                        "name": "Tadesse Destaw Belay"
                    },
                    {
                        "name": "Israel Abebe Azime"
                    },
                    {
                        "name": "Abinew Ali Ayele"
                    },
                    {
                        "name": "Grigori Sidorov"
                    },
                    {
                        "name": "Dietrich Klakow"
                    },
                    {
                        "name": "Philipp Slusallek"
                    },
                    {
                        "name": "Olga Kolesnikova"
                    },
                    {
                        "name": "Seid Muhie Yimam"
                    }
                ],
                "author_detail": {
                    "name": "Seid Muhie Yimam"
                },
                "author": "Seid Muhie Yimam",
                "arxiv_comment": "COLING 2025, main conference, long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01783v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01783v1",
                "updated": "2025-01-03T12:32:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    32,
                    19,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:32:19Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    32,
                    19,
                    4,
                    3,
                    0
                ],
                "title": "Nonparametric estimation of a factorizable density using diffusion\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric estimation of a factorizable density using diffusion\n  models"
                },
                "summary": "In recent years, diffusion models, and more generally score-based deep\ngenerative models, have achieved remarkable success in various applications,\nincluding image and audio generation. In this paper, we view diffusion models\nas an implicit approach to nonparametric density estimation and study them\nwithin a statistical framework to analyze their surprising performance. A key\nchallenge in high-dimensional statistical inference is leveraging\nlow-dimensional structures inherent in the data to mitigate the curse of\ndimensionality. We assume that the underlying density exhibits a\nlow-dimensional structure by factorizing into low-dimensional components, a\nproperty common in examples such as Bayesian networks and Markov random fields.\nUnder suitable assumptions, we demonstrate that an implicit density estimator\nconstructed from diffusion models adapts to the factorization structure and\nachieves the minimax optimal rate with respect to the total variation distance.\nIn constructing the estimator, we design a sparse weight-sharing neural network\narchitecture, where sparsity and weight-sharing are key features of practical\narchitectures such as convolutional neural networks and recurrent neural\nnetworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, diffusion models, and more generally score-based deep\ngenerative models, have achieved remarkable success in various applications,\nincluding image and audio generation. In this paper, we view diffusion models\nas an implicit approach to nonparametric density estimation and study them\nwithin a statistical framework to analyze their surprising performance. A key\nchallenge in high-dimensional statistical inference is leveraging\nlow-dimensional structures inherent in the data to mitigate the curse of\ndimensionality. We assume that the underlying density exhibits a\nlow-dimensional structure by factorizing into low-dimensional components, a\nproperty common in examples such as Bayesian networks and Markov random fields.\nUnder suitable assumptions, we demonstrate that an implicit density estimator\nconstructed from diffusion models adapts to the factorization structure and\nachieves the minimax optimal rate with respect to the total variation distance.\nIn constructing the estimator, we design a sparse weight-sharing neural network\narchitecture, where sparsity and weight-sharing are key features of practical\narchitectures such as convolutional neural networks and recurrent neural\nnetworks."
                },
                "authors": [
                    {
                        "name": "Hyeok Kyu Kwon"
                    },
                    {
                        "name": "Dongha Kim"
                    },
                    {
                        "name": "Ilsang Ohn"
                    },
                    {
                        "name": "Minwoo Chae"
                    }
                ],
                "author_detail": {
                    "name": "Minwoo Chae"
                },
                "author": "Minwoo Chae",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01783v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01783v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62G07 (Primary) 62C20, 68T07 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01428v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01428v2",
                "updated": "2025-01-03T12:30:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    30,
                    16,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-02T18:59:59Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    59,
                    3,
                    2,
                    0
                ],
                "title": "GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT4Scene: Understand 3D Scenes from Videos with Vision-Language Models"
                },
                "summary": "In recent years, 2D Vision-Language Models (VLMs) have made significant\nstrides in image-text understanding tasks. However, their performance in 3D\nspatial comprehension, which is critical for embodied intelligence, remains\nlimited. Recent advances have leveraged 3D point clouds and multi-view images\nas inputs, yielding promising results. However, we propose exploring a purely\nvision-based solution inspired by human perception, which merely relies on\nvisual cues for 3D spatial understanding. This paper empirically investigates\nthe limitations of VLMs in 3D spatial knowledge, revealing that their primary\nshortcoming lies in the lack of global-local correspondence between the scene\nand individual frames. To address this, we introduce GPT4Scene, a novel visual\nprompting paradigm in VLM training and inference that helps build the\nglobal-local relationship, significantly improving the 3D spatial understanding\nof indoor scenes. Specifically, GPT4Scene constructs a 3D Bird's Eye View (BEV)\nimage from the video and marks consistent object IDs across both frames and the\nBEV image. The model then inputs the concatenated BEV image and video frames\nwith markers. In zero-shot evaluations, GPT4Scene improves performance over\nclosed-source VLMs like GPT-4o. Additionally, we prepare a processed video\ndataset consisting of 165K text annotation to fine-tune open-source VLMs,\nachieving state-of-the-art performance on all 3D understanding tasks.\nSurprisingly, after training with the GPT4Scene paradigm, VLMs consistently\nimprove during inference, even without visual prompting and BEV image as\nexplicit correspondence. It demonstrates that the proposed paradigm helps VLMs\ndevelop an intrinsic ability to understand 3D scenes, which paves the way for a\nnoninvasive approach to extending pre-trained VLMs for 3D scene understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, 2D Vision-Language Models (VLMs) have made significant\nstrides in image-text understanding tasks. However, their performance in 3D\nspatial comprehension, which is critical for embodied intelligence, remains\nlimited. Recent advances have leveraged 3D point clouds and multi-view images\nas inputs, yielding promising results. However, we propose exploring a purely\nvision-based solution inspired by human perception, which merely relies on\nvisual cues for 3D spatial understanding. This paper empirically investigates\nthe limitations of VLMs in 3D spatial knowledge, revealing that their primary\nshortcoming lies in the lack of global-local correspondence between the scene\nand individual frames. To address this, we introduce GPT4Scene, a novel visual\nprompting paradigm in VLM training and inference that helps build the\nglobal-local relationship, significantly improving the 3D spatial understanding\nof indoor scenes. Specifically, GPT4Scene constructs a 3D Bird's Eye View (BEV)\nimage from the video and marks consistent object IDs across both frames and the\nBEV image. The model then inputs the concatenated BEV image and video frames\nwith markers. In zero-shot evaluations, GPT4Scene improves performance over\nclosed-source VLMs like GPT-4o. Additionally, we prepare a processed video\ndataset consisting of 165K text annotation to fine-tune open-source VLMs,\nachieving state-of-the-art performance on all 3D understanding tasks.\nSurprisingly, after training with the GPT4Scene paradigm, VLMs consistently\nimprove during inference, even without visual prompting and BEV image as\nexplicit correspondence. It demonstrates that the proposed paradigm helps VLMs\ndevelop an intrinsic ability to understand 3D scenes, which paves the way for a\nnoninvasive approach to extending pre-trained VLMs for 3D scene understanding."
                },
                "authors": [
                    {
                        "name": "Zhangyang Qi"
                    },
                    {
                        "name": "Zhixiong Zhang"
                    },
                    {
                        "name": "Ye Fang"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Hengshuang Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hengshuang Zhao"
                },
                "author": "Hengshuang Zhao",
                "arxiv_comment": "Project page: https://gpt4scene.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01428v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01428v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01235v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01235v2",
                "updated": "2025-01-03T12:26:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    26,
                    32,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-02T12:51:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    12,
                    51,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "SVFR: A Unified Framework for Generalized Video Face Restoration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVFR: A Unified Framework for Generalized Video Face Restoration"
                },
                "summary": "Face Restoration (FR) is a crucial area within image and video processing,\nfocusing on reconstructing high-quality portraits from degraded inputs. Despite\nadvancements in image FR, video FR remains relatively under-explored, primarily\ndue to challenges related to temporal consistency, motion artifacts, and the\nlimited availability of high-quality video data. Moreover, traditional face\nrestoration typically prioritizes enhancing resolution and may not give as much\nconsideration to related tasks such as facial colorization and inpainting. In\nthis paper, we propose a novel approach for the Generalized Video Face\nRestoration (GVFR) task, which integrates video BFR, inpainting, and\ncolorization tasks that we empirically show to benefit each other. We present a\nunified framework, termed as stable video face restoration (SVFR), which\nleverages the generative and motion priors of Stable Video Diffusion (SVD) and\nincorporates task-specific information through a unified face restoration\nframework. A learnable task embedding is introduced to enhance task\nidentification. Meanwhile, a novel Unified Latent Regularization (ULR) is\nemployed to encourage the shared feature representation learning among\ndifferent subtasks. To further enhance the restoration quality and temporal\nstability, we introduce the facial prior learning and the self-referred\nrefinement as auxiliary strategies used for both training and inference. The\nproposed framework effectively combines the complementary strengths of these\ntasks, enhancing temporal coherence and achieving superior restoration quality.\nThis work advances the state-of-the-art in video FR and establishes a new\nparadigm for generalized video face restoration. Code and video demo are\navailable at https://github.com/wangzhiyaoo/SVFR.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Face Restoration (FR) is a crucial area within image and video processing,\nfocusing on reconstructing high-quality portraits from degraded inputs. Despite\nadvancements in image FR, video FR remains relatively under-explored, primarily\ndue to challenges related to temporal consistency, motion artifacts, and the\nlimited availability of high-quality video data. Moreover, traditional face\nrestoration typically prioritizes enhancing resolution and may not give as much\nconsideration to related tasks such as facial colorization and inpainting. In\nthis paper, we propose a novel approach for the Generalized Video Face\nRestoration (GVFR) task, which integrates video BFR, inpainting, and\ncolorization tasks that we empirically show to benefit each other. We present a\nunified framework, termed as stable video face restoration (SVFR), which\nleverages the generative and motion priors of Stable Video Diffusion (SVD) and\nincorporates task-specific information through a unified face restoration\nframework. A learnable task embedding is introduced to enhance task\nidentification. Meanwhile, a novel Unified Latent Regularization (ULR) is\nemployed to encourage the shared feature representation learning among\ndifferent subtasks. To further enhance the restoration quality and temporal\nstability, we introduce the facial prior learning and the self-referred\nrefinement as auxiliary strategies used for both training and inference. The\nproposed framework effectively combines the complementary strengths of these\ntasks, enhancing temporal coherence and achieving superior restoration quality.\nThis work advances the state-of-the-art in video FR and establishes a new\nparadigm for generalized video face restoration. Code and video demo are\navailable at https://github.com/wangzhiyaoo/SVFR.git."
                },
                "authors": [
                    {
                        "name": "Zhiyao Wang"
                    },
                    {
                        "name": "Xu Chen"
                    },
                    {
                        "name": "Chengming Xu"
                    },
                    {
                        "name": "Junwei Zhu"
                    },
                    {
                        "name": "Xiaobin Hu"
                    },
                    {
                        "name": "Jiangning Zhang"
                    },
                    {
                        "name": "Chengjie Wang"
                    },
                    {
                        "name": "Yuqi Liu"
                    },
                    {
                        "name": "Yiyi Zhou"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01235v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01235v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01779v1",
                "updated": "2025-01-03T12:22:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    22,
                    7,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:22:07Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    22,
                    7,
                    4,
                    3,
                    0
                ],
                "title": "From Occasional to Steady: Habit Formation Insights From a Comprehensive\n  Fitness Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Occasional to Steady: Habit Formation Insights From a Comprehensive\n  Fitness Study"
                },
                "summary": "Exercising regularly is widely recognized as a cornerstone of health, yet the\nchallenge of sustaining consistent exercise habits persists. Understanding the\nfactors that influence the formation of these habits is crucial for developing\neffective interventions. This study utilizes data from Mars Athletic Club,\nT\\\"urkiye's largest sports chain, to investigate the dynamics of gym attendance\nand habit formation. The general problem addressed by this study is identifying\nthe critical periods and factors that contribute to the successful\nestablishment of consistent exercise routines among gym-goers. Here we show\nthat there are specific periods during which gym attendance is most crucial for\nhabit formation. By developing a survival metric based on gym attendance\npatterns, we pinpoint these critical periods and segment members into distinct\nclusters based on their visit patterns. Our analysis reveals significant\ndifferences in how various subgroups respond to interventions, such as group\nclasses, personal trainer sessions, and visiting different clubs. Using causal\ninference analysis, we demonstrate that personalized guidance and social\ndynamics are key drivers of sustained long-term engagement. By systematically\nexamining these variables and considering the specific characteristics of\ndifferent clusters, our research demonstrates the importance of a tailored,\nmulti-dimensional approach to promoting exercise habits, which integrates\nsocial dynamics, personalized guidance, and strategic interventions to sustain\nlong-term engagement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exercising regularly is widely recognized as a cornerstone of health, yet the\nchallenge of sustaining consistent exercise habits persists. Understanding the\nfactors that influence the formation of these habits is crucial for developing\neffective interventions. This study utilizes data from Mars Athletic Club,\nT\\\"urkiye's largest sports chain, to investigate the dynamics of gym attendance\nand habit formation. The general problem addressed by this study is identifying\nthe critical periods and factors that contribute to the successful\nestablishment of consistent exercise routines among gym-goers. Here we show\nthat there are specific periods during which gym attendance is most crucial for\nhabit formation. By developing a survival metric based on gym attendance\npatterns, we pinpoint these critical periods and segment members into distinct\nclusters based on their visit patterns. Our analysis reveals significant\ndifferences in how various subgroups respond to interventions, such as group\nclasses, personal trainer sessions, and visiting different clubs. Using causal\ninference analysis, we demonstrate that personalized guidance and social\ndynamics are key drivers of sustained long-term engagement. By systematically\nexamining these variables and considering the specific characteristics of\ndifferent clusters, our research demonstrates the importance of a tailored,\nmulti-dimensional approach to promoting exercise habits, which integrates\nsocial dynamics, personalized guidance, and strategic interventions to sustain\nlong-term engagement."
                },
                "authors": [
                    {
                        "name": "Ege Demirci"
                    },
                    {
                        "name": "Efe Tuzun"
                    },
                    {
                        "name": "Ahmet Furkan Un"
                    },
                    {
                        "name": "Taner Giray Sonmez"
                    },
                    {
                        "name": "Onur Varol"
                    }
                ],
                "author_detail": {
                    "name": "Onur Varol"
                },
                "author": "Onur Varol",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09916v2",
                "updated": "2025-01-03T12:01:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    1,
                    55,
                    4,
                    3,
                    0
                ],
                "published": "2024-08-19T11:44:40Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    44,
                    40,
                    0,
                    232,
                    0
                ],
                "title": "Attribution Analysis Meets Model Editing: Advancing Knowledge Correction\n  in Vision Language Models with VisEdit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribution Analysis Meets Model Editing: Advancing Knowledge Correction\n  in Vision Language Models with VisEdit"
                },
                "summary": "Model editing aims to correct outdated or erroneous knowledge in large models\nwithout costly retraining. Recent research discovered that the mid-layer\nrepresentation of the subject's final token in a prompt has a strong influence\non factual predictions, and developed Large Language Model (LLM) editing\ntechniques based on this observation. However, for Vision-LLMs (VLLMs), how\nvisual representations impact the predictions from a decoder-only language\nmodel remains largely unexplored. To the best of our knowledge, model editing\nfor VLLMs has not been extensively studied in the literature. In this work, we\nemploy the contribution allocation and noise perturbation methods to measure\nthe contributions of visual representations for token predictions. Our\nattribution analysis shows that visual representations in mid-to-later layers\nthat are highly relevant to the prompt contribute significantly to predictions.\nBased on these insights, we propose VisEdit, a novel model editor for VLLMs\nthat effectively corrects knowledge by editing intermediate visual\nrepresentations in regions important to the edit prompt. We evaluated VisEdit\nusing multiple VLLM backbones and public VLLM editing benchmark datasets. The\nresults show the superiority of VisEdit over the strong baselines adapted from\nexisting state-of-the-art editors for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model editing aims to correct outdated or erroneous knowledge in large models\nwithout costly retraining. Recent research discovered that the mid-layer\nrepresentation of the subject's final token in a prompt has a strong influence\non factual predictions, and developed Large Language Model (LLM) editing\ntechniques based on this observation. However, for Vision-LLMs (VLLMs), how\nvisual representations impact the predictions from a decoder-only language\nmodel remains largely unexplored. To the best of our knowledge, model editing\nfor VLLMs has not been extensively studied in the literature. In this work, we\nemploy the contribution allocation and noise perturbation methods to measure\nthe contributions of visual representations for token predictions. Our\nattribution analysis shows that visual representations in mid-to-later layers\nthat are highly relevant to the prompt contribute significantly to predictions.\nBased on these insights, we propose VisEdit, a novel model editor for VLLMs\nthat effectively corrects knowledge by editing intermediate visual\nrepresentations in regions important to the edit prompt. We evaluated VisEdit\nusing multiple VLLM backbones and public VLLM editing benchmark datasets. The\nresults show the superiority of VisEdit over the strong baselines adapted from\nexisting state-of-the-art editors for LLMs."
                },
                "authors": [
                    {
                        "name": "Qizhou Chen"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Xiaofeng He"
                    },
                    {
                        "name": "Dakan Wang"
                    },
                    {
                        "name": "Tingting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Liu"
                },
                "author": "Tingting Liu",
                "arxiv_comment": "Accepted by AAAI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15811v2",
                "updated": "2025-01-03T11:47:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    11,
                    47,
                    52,
                    4,
                    3,
                    0
                ],
                "published": "2024-11-24T12:34:02Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    12,
                    34,
                    2,
                    6,
                    329,
                    0
                ],
                "title": "FastTrackTr:Towards Fast Multi-Object Tracking with Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastTrackTr:Towards Fast Multi-Object Tracking with Transformers"
                },
                "summary": "Transformer-based multi-object tracking (MOT) methods have captured the\nattention of many researchers in recent years. However, these models often\nsuffer from slow inference speeds due to their structure or other issues. To\naddress this problem, we revisited the Joint Detection and Tracking (JDT)\nmethod by looking back at past approaches. By integrating the original JDT\napproach with some advanced theories, this paper employs an efficient method of\ninformation transfer between frames on the DETR, constructing a fast and novel\nJDT-type MOT framework: FastTrackTr. Thanks to the superiority of this\ninformation transfer method, our approach not only reduces the number of\nqueries required during tracking but also avoids the excessive introduction of\nnetwork structures, ensuring model simplicity. Experimental results indicate\nthat our method has the potential to achieve real-time tracking and exhibits\ncompetitive tracking accuracy across multiple datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based multi-object tracking (MOT) methods have captured the\nattention of many researchers in recent years. However, these models often\nsuffer from slow inference speeds due to their structure or other issues. To\naddress this problem, we revisited the Joint Detection and Tracking (JDT)\nmethod by looking back at past approaches. By integrating the original JDT\napproach with some advanced theories, this paper employs an efficient method of\ninformation transfer between frames on the DETR, constructing a fast and novel\nJDT-type MOT framework: FastTrackTr. Thanks to the superiority of this\ninformation transfer method, our approach not only reduces the number of\nqueries required during tracking but also avoids the excessive introduction of\nnetwork structures, ensuring model simplicity. Experimental results indicate\nthat our method has the potential to achieve real-time tracking and exhibits\ncompetitive tracking accuracy across multiple datasets."
                },
                "authors": [
                    {
                        "name": "Pan Liao"
                    },
                    {
                        "name": "Feng Yang"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Jinwen Yu"
                    },
                    {
                        "name": "Wenhui Zhao"
                    },
                    {
                        "name": "Bo Liu"
                    }
                ],
                "author_detail": {
                    "name": "Bo Liu"
                },
                "author": "Bo Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15204v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15204v2",
                "updated": "2025-01-03T11:44:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    11,
                    44,
                    51,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-19T18:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    59,
                    17,
                    3,
                    354,
                    0
                ],
                "title": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic\n  Long-context Multitasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic\n  Long-context Multitasks"
                },
                "summary": "This paper introduces LongBench v2, a benchmark designed to assess the\nability of LLMs to handle long-context problems requiring deep understanding\nand reasoning across real-world multitasks. LongBench v2 consists of 503\nchallenging multiple-choice questions, with contexts ranging from 8k to 2M\nwords, across six major task categories: single-document QA, multi-document QA,\nlong in-context learning, long-dialogue history understanding, code repository\nunderstanding, and long structured data understanding. To ensure the breadth\nand the practicality, we collect data from nearly 100 highly educated\nindividuals with diverse professional backgrounds. We employ both automated and\nmanual review processes to maintain high quality and difficulty, resulting in\nhuman experts achieving only 53.7% accuracy under a 15-minute time constraint.\nOur evaluation reveals that the best-performing model, when directly answers\nthe questions, achieves only 50.1% accuracy. In contrast, the o1-preview model,\nwhich includes longer reasoning, achieves 57.7%, surpassing the human baseline\nby 4%. These results highlight the importance of enhanced reasoning ability and\nscaling inference-time compute to tackle the long-context challenges in\nLongBench v2. The project is available at https://longbench2.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces LongBench v2, a benchmark designed to assess the\nability of LLMs to handle long-context problems requiring deep understanding\nand reasoning across real-world multitasks. LongBench v2 consists of 503\nchallenging multiple-choice questions, with contexts ranging from 8k to 2M\nwords, across six major task categories: single-document QA, multi-document QA,\nlong in-context learning, long-dialogue history understanding, code repository\nunderstanding, and long structured data understanding. To ensure the breadth\nand the practicality, we collect data from nearly 100 highly educated\nindividuals with diverse professional backgrounds. We employ both automated and\nmanual review processes to maintain high quality and difficulty, resulting in\nhuman experts achieving only 53.7% accuracy under a 15-minute time constraint.\nOur evaluation reveals that the best-performing model, when directly answers\nthe questions, achieves only 50.1% accuracy. In contrast, the o1-preview model,\nwhich includes longer reasoning, achieves 57.7%, surpassing the human baseline\nby 4%. These results highlight the importance of enhanced reasoning ability and\nscaling inference-time compute to tackle the long-context challenges in\nLongBench v2. The project is available at https://longbench2.github.io."
                },
                "authors": [
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Jiazheng Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "26 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15204v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15204v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01765v1",
                "updated": "2025-01-03T11:34:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    11,
                    34,
                    28,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T11:34:28Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    11,
                    34,
                    28,
                    4,
                    3,
                    0
                ],
                "title": "SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation"
                },
                "summary": "As advancements in large language models (LLMs) continue and the demand for\npersonalized models increases, parameter-efficient fine-tuning (PEFT) methods\n(e.g., LoRA) will become essential due to their efficiency in reducing\ncomputation costs. However, recent studies have raised alarming concerns that\nLoRA fine-tuning could potentially compromise the safety alignment in LLMs,\nposing significant risks for the model owner. In this paper, we first\ninvestigate the underlying mechanism by analyzing the changes in safety\nalignment related features before and after fine-tuning. Then, we propose a\nfixed safety module calculated by safety data and a task-specific\ninitialization for trainable parameters in low-rank adaptations, termed\nSafety-alignment preserved Low-Rank Adaptation (SaLoRA). Unlike previous LoRA\nmethods and their variants, SaLoRA enables targeted modifications to LLMs\nwithout disrupting their original alignments. Our experiments show that SaLoRA\noutperforms various adapters-based approaches across various evaluation metrics\nin different fine-tuning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As advancements in large language models (LLMs) continue and the demand for\npersonalized models increases, parameter-efficient fine-tuning (PEFT) methods\n(e.g., LoRA) will become essential due to their efficiency in reducing\ncomputation costs. However, recent studies have raised alarming concerns that\nLoRA fine-tuning could potentially compromise the safety alignment in LLMs,\nposing significant risks for the model owner. In this paper, we first\ninvestigate the underlying mechanism by analyzing the changes in safety\nalignment related features before and after fine-tuning. Then, we propose a\nfixed safety module calculated by safety data and a task-specific\ninitialization for trainable parameters in low-rank adaptations, termed\nSafety-alignment preserved Low-Rank Adaptation (SaLoRA). Unlike previous LoRA\nmethods and their variants, SaLoRA enables targeted modifications to LLMs\nwithout disrupting their original alignments. Our experiments show that SaLoRA\noutperforms various adapters-based approaches across various evaluation metrics\nin different fine-tuning tasks."
                },
                "authors": [
                    {
                        "name": "Mingjie Li"
                    },
                    {
                        "name": "Wai Man Si"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Yisen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yisen Wang"
                },
                "author": "Yisen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23242v2",
                "updated": "2025-01-03T11:29:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    11,
                    29,
                    35,
                    4,
                    3,
                    0
                ],
                "published": "2024-10-30T17:28:28Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    28,
                    28,
                    2,
                    304,
                    0
                ],
                "title": "A little less conversation, a little more action, please: Investigating\n  the physical common-sense of LLMs in a 3D embodied environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A little less conversation, a little more action, please: Investigating\n  the physical common-sense of LLMs in a 3D embodied environment"
                },
                "summary": "As general-purpose tools, Large Language Models (LLMs) must often reason\nabout everyday physical environments. In a question-and-answer capacity,\nunderstanding the interactions of physical objects may be necessary to give\nappropriate responses. Moreover, LLMs are increasingly used as reasoning\nengines in agentic systems, designing and controlling their action sequences.\nThe vast majority of research has tackled this issue using static benchmarks,\ncomprised of text or image-based questions about the physical world. However,\nthese benchmarks do not capture the complexity and nuance of real-life physical\nprocesses. Here we advocate for a second, relatively unexplored, approach:\n'embodying' the LLMs by granting them control of an agent within a 3D\nenvironment. We present the first embodied and cognitively meaningful\nevaluation of physical common-sense reasoning in LLMs. Our framework allows\ndirect comparison of LLMs with other embodied agents, such as those based on\nDeep Reinforcement Learning, and human and non-human animals. We employ the\nAnimal-AI (AAI) environment, a simulated 3D virtual laboratory, to study\nphysical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a\nsuite of experiments that replicate laboratory studies with non-human animals,\nto study physical reasoning capabilities including distance estimation,\ntracking out-of-sight objects, and tool use. We demonstrate that\nstate-of-the-art multi-modal models with no finetuning can complete this style\nof task, allowing meaningful comparison to the entrants of the 2019 Animal-AI\nOlympics competition and to human children. Our results show that LLMs are\ncurrently outperformed by human children on these tasks. We argue that this\napproach allows the study of physical reasoning using ecologically valid\nexperiments drawn directly from cognitive science, improving the predictability\nand reliability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As general-purpose tools, Large Language Models (LLMs) must often reason\nabout everyday physical environments. In a question-and-answer capacity,\nunderstanding the interactions of physical objects may be necessary to give\nappropriate responses. Moreover, LLMs are increasingly used as reasoning\nengines in agentic systems, designing and controlling their action sequences.\nThe vast majority of research has tackled this issue using static benchmarks,\ncomprised of text or image-based questions about the physical world. However,\nthese benchmarks do not capture the complexity and nuance of real-life physical\nprocesses. Here we advocate for a second, relatively unexplored, approach:\n'embodying' the LLMs by granting them control of an agent within a 3D\nenvironment. We present the first embodied and cognitively meaningful\nevaluation of physical common-sense reasoning in LLMs. Our framework allows\ndirect comparison of LLMs with other embodied agents, such as those based on\nDeep Reinforcement Learning, and human and non-human animals. We employ the\nAnimal-AI (AAI) environment, a simulated 3D virtual laboratory, to study\nphysical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a\nsuite of experiments that replicate laboratory studies with non-human animals,\nto study physical reasoning capabilities including distance estimation,\ntracking out-of-sight objects, and tool use. We demonstrate that\nstate-of-the-art multi-modal models with no finetuning can complete this style\nof task, allowing meaningful comparison to the entrants of the 2019 Animal-AI\nOlympics competition and to human children. Our results show that LLMs are\ncurrently outperformed by human children on these tasks. We argue that this\napproach allows the study of physical reasoning using ecologically valid\nexperiments drawn directly from cognitive science, improving the predictability\nand reliability of LLMs."
                },
                "authors": [
                    {
                        "name": "Matteo G. Mecattaf"
                    },
                    {
                        "name": "Ben Slater"
                    },
                    {
                        "name": "Marko Tešić"
                    },
                    {
                        "name": "Jonathan Prunty"
                    },
                    {
                        "name": "Konstantinos Voudouris"
                    },
                    {
                        "name": "Lucy G. Cheke"
                    }
                ],
                "author_detail": {
                    "name": "Lucy G. Cheke"
                },
                "author": "Lucy G. Cheke",
                "arxiv_comment": "25 pages, 4 figures; v2: Added AFMR Acknowledgment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07752v2",
                "updated": "2025-01-03T11:21:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    11,
                    21,
                    25,
                    4,
                    3,
                    0
                ],
                "published": "2024-10-10T09:28:36Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    28,
                    36,
                    3,
                    284,
                    0
                ],
                "title": "TVBench: Redesigning Video-Language Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TVBench: Redesigning Video-Language Evaluation"
                },
                "summary": "Large language models have demonstrated impressive performance when\nintegrated with vision models even enabling video understanding. However,\nevaluating these video models presents its own unique challenges, for which\nseveral benchmarks have been proposed. In this paper, we show that the\ncurrently most used video-language benchmarks can be solved without requiring\nmuch temporal reasoning. We identified three main issues in existing datasets:\n(i) static information from single frames is often sufficient to solve the\ntasks (ii) the text of the questions and candidate answers is overly\ninformative, allowing models to answer correctly without relying on any visual\ninput (iii) world knowledge alone can answer many of the questions, making the\nbenchmarks a test of knowledge replication rather than visual reasoning. In\naddition, we found that open-ended question-answering benchmarks for video\nunderstanding suffer from similar issues while the automatic evaluation process\nwith LLMs is unreliable, making it an unsuitable alternative. As a solution, we\npropose TVBench, a novel open-source video multiple-choice question-answering\nbenchmark, and demonstrate through extensive evaluations that it requires a\nhigh level of temporal understanding. Surprisingly, we find that most recent\nstate-of-the-art video-language models perform similarly to random performance\non TVBench, with only a few models such as Qwen2-VL, and Tarsier clearly\nsurpassing this baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated impressive performance when\nintegrated with vision models even enabling video understanding. However,\nevaluating these video models presents its own unique challenges, for which\nseveral benchmarks have been proposed. In this paper, we show that the\ncurrently most used video-language benchmarks can be solved without requiring\nmuch temporal reasoning. We identified three main issues in existing datasets:\n(i) static information from single frames is often sufficient to solve the\ntasks (ii) the text of the questions and candidate answers is overly\ninformative, allowing models to answer correctly without relying on any visual\ninput (iii) world knowledge alone can answer many of the questions, making the\nbenchmarks a test of knowledge replication rather than visual reasoning. In\naddition, we found that open-ended question-answering benchmarks for video\nunderstanding suffer from similar issues while the automatic evaluation process\nwith LLMs is unreliable, making it an unsuitable alternative. As a solution, we\npropose TVBench, a novel open-source video multiple-choice question-answering\nbenchmark, and demonstrate through extensive evaluations that it requires a\nhigh level of temporal understanding. Surprisingly, we find that most recent\nstate-of-the-art video-language models perform similarly to random performance\non TVBench, with only a few models such as Qwen2-VL, and Tarsier clearly\nsurpassing this baseline."
                },
                "authors": [
                    {
                        "name": "Daniel Cores"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "Manuel Mucientes"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16597v2",
                "updated": "2025-01-03T10:57:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    10,
                    57,
                    17,
                    4,
                    3,
                    0
                ],
                "published": "2024-09-25T03:49:46Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    3,
                    49,
                    46,
                    2,
                    269,
                    0
                ],
                "title": "EventHallusion: Diagnosing Event Hallucinations in Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EventHallusion: Diagnosing Event Hallucinations in Video LLMs"
                },
                "summary": "Recently, Multimodal Large Language Models (MLLMs) have made significant\nprogress in the video comprehension field. Despite remarkable content reasoning\nand instruction following capabilities they demonstrated, the hallucination\nproblem of these VideoLLMs is less explored compared with its counterpart in\nthe image domain. To mitigate this gap, we propose EventHallusion, a novel\nbenchmark that focuses on assessing the VideoLLMs' hallucination toward event,\nthe crux of video analysis. From a hallucination attribution perspective, our\nEventHallusion benchmark is curated to assess a VideoLLM's susceptibility\ntoward language priors and vision-language biases. On the other hand, we also\npropose a simple yet effective method, called Temporal Contrastive Decoding\n(TCD), to tackle the hallucination problems of VideoLLMs. The proposed TCD\nmethod rectifies the model's bias toward its priors during the decoding stage\nby comparing the original video with a modified version, in which temporal cues\nare disrupted. Through comprehensive evaluation of eight open-source and two\nclosed-source VideoLLMs on the proposed EventHallusion benchmark, we observe\nthat the open-source models suffer significantly from hallucination problems,\nwhereas the closed-source ones perform markedly better. By further equipping\nopen-source VideoLLMs with the proposed TCD approach, evident performance\nimprovements are achieved across most metrics in the EventHallusion benchmark.\nOur codes and benchmark data are available at\nhttps://github.com/Stevetich/EventHallusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multimodal Large Language Models (MLLMs) have made significant\nprogress in the video comprehension field. Despite remarkable content reasoning\nand instruction following capabilities they demonstrated, the hallucination\nproblem of these VideoLLMs is less explored compared with its counterpart in\nthe image domain. To mitigate this gap, we propose EventHallusion, a novel\nbenchmark that focuses on assessing the VideoLLMs' hallucination toward event,\nthe crux of video analysis. From a hallucination attribution perspective, our\nEventHallusion benchmark is curated to assess a VideoLLM's susceptibility\ntoward language priors and vision-language biases. On the other hand, we also\npropose a simple yet effective method, called Temporal Contrastive Decoding\n(TCD), to tackle the hallucination problems of VideoLLMs. The proposed TCD\nmethod rectifies the model's bias toward its priors during the decoding stage\nby comparing the original video with a modified version, in which temporal cues\nare disrupted. Through comprehensive evaluation of eight open-source and two\nclosed-source VideoLLMs on the proposed EventHallusion benchmark, we observe\nthat the open-source models suffer significantly from hallucination problems,\nwhereas the closed-source ones perform markedly better. By further equipping\nopen-source VideoLLMs with the proposed TCD approach, evident performance\nimprovements are achieved across most metrics in the EventHallusion benchmark.\nOur codes and benchmark data are available at\nhttps://github.com/Stevetich/EventHallusion."
                },
                "authors": [
                    {
                        "name": "Jiacheng Zhang"
                    },
                    {
                        "name": "Yang Jiao"
                    },
                    {
                        "name": "Shaoxiang Chen"
                    },
                    {
                        "name": "Na Zhao"
                    },
                    {
                        "name": "Jingjing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jingjing Chen"
                },
                "author": "Jingjing Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01743v1",
                "updated": "2025-01-03T10:11:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    10,
                    11,
                    38,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T10:11:38Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    10,
                    11,
                    38,
                    4,
                    3,
                    0
                ],
                "title": "Automating Legal Concept Interpretation with LLMs: Retrieval,\n  Generation, and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Legal Concept Interpretation with LLMs: Retrieval,\n  Generation, and Evaluation"
                },
                "summary": "Legal articles often include vague concepts to adapt to the ever-changing\nsociety. Providing detailed interpretations of these concepts is a critical\ntask for legal practitioners, which requires meticulous and professional\nannotations by legal experts, admittedly time-consuming and expensive to\ncollect at scale. In this paper, we introduce a novel retrieval-augmented\ngeneration framework, ATRI, for AuTomatically Retrieving relevant information\nfrom past judicial precedents and Interpreting vague legal concepts. We further\npropose a new benchmark, Legal Concept Entailment, to automate the evaluation\nof generated concept interpretations without expert involvement. Automatic\nevaluations indicate that our generated interpretations can effectively assist\nlarge language models (LLMs) in understanding vague legal concepts.\nMulti-faceted evaluations by legal experts indicate that the quality of our\nconcept interpretations is comparable to those written by human experts. Our\nwork has strong implications for leveraging LLMs to support legal practitioners\nin interpreting vague legal concepts and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal articles often include vague concepts to adapt to the ever-changing\nsociety. Providing detailed interpretations of these concepts is a critical\ntask for legal practitioners, which requires meticulous and professional\nannotations by legal experts, admittedly time-consuming and expensive to\ncollect at scale. In this paper, we introduce a novel retrieval-augmented\ngeneration framework, ATRI, for AuTomatically Retrieving relevant information\nfrom past judicial precedents and Interpreting vague legal concepts. We further\npropose a new benchmark, Legal Concept Entailment, to automate the evaluation\nof generated concept interpretations without expert involvement. Automatic\nevaluations indicate that our generated interpretations can effectively assist\nlarge language models (LLMs) in understanding vague legal concepts.\nMulti-faceted evaluations by legal experts indicate that the quality of our\nconcept interpretations is comparable to those written by human experts. Our\nwork has strong implications for leveraging LLMs to support legal practitioners\nin interpreting vague legal concepts and beyond."
                },
                "authors": [
                    {
                        "name": "Kangcheng Luo"
                    },
                    {
                        "name": "Quzhe Huang"
                    },
                    {
                        "name": "Cong Jiang"
                    },
                    {
                        "name": "Yansong Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Feng"
                },
                "author": "Yansong Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01741v1",
                "updated": "2025-01-03T10:08:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    10,
                    8,
                    49,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T10:08:49Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    10,
                    8,
                    49,
                    4,
                    3,
                    0
                ],
                "title": "How Toxic Can You Get? Search-based Toxicity Testing for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Toxic Can You Get? Search-based Toxicity Testing for Large Language\n  Models"
                },
                "summary": "Language is a deep-rooted means of perpetration of stereotypes and\ndiscrimination. Large Language Models (LLMs), now a pervasive technology in our\neveryday lives, can cause extensive harm when prone to generating toxic\nresponses. The standard way to address this issue is to align the LLM, which,\nhowever, dampens the issue without constituting a definitive solution.\nTherefore, testing LLM even after alignment efforts remains crucial for\ndetecting any residual deviations with respect to ethical standards. We present\nEvoTox, an automated testing framework for LLMs' inclination to toxicity,\nproviding a way to quantitatively assess how much LLMs can be pushed towards\ntoxic responses even in the presence of alignment. The framework adopts an\niterative evolution strategy that exploits the interplay between two LLMs, the\nSystem Under Test (SUT) and the Prompt Generator steering SUT responses toward\nhigher toxicity. The toxicity level is assessed by an automated oracle based on\nan existing toxicity classifier. We conduct a quantitative and qualitative\nempirical evaluation using four state-of-the-art LLMs as evaluation subjects\nhaving increasing complexity (7-13 billion parameters). Our quantitative\nevaluation assesses the cost-effectiveness of four alternative versions of\nEvoTox against existing baseline methods, based on random search, curated\ndatasets of toxic prompts, and adversarial attacks. Our qualitative assessment\nengages human evaluators to rate the fluency of the generated prompts and the\nperceived toxicity of the responses collected during the testing sessions.\nResults indicate that the effectiveness, in terms of detected toxicity level,\nis significantly higher than the selected baseline methods (effect size up to\n1.0 against random search and up to 0.99 against adversarial attacks).\nFurthermore, EvoTox yields a limited cost overhead (from 22% to 35% on\naverage).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language is a deep-rooted means of perpetration of stereotypes and\ndiscrimination. Large Language Models (LLMs), now a pervasive technology in our\neveryday lives, can cause extensive harm when prone to generating toxic\nresponses. The standard way to address this issue is to align the LLM, which,\nhowever, dampens the issue without constituting a definitive solution.\nTherefore, testing LLM even after alignment efforts remains crucial for\ndetecting any residual deviations with respect to ethical standards. We present\nEvoTox, an automated testing framework for LLMs' inclination to toxicity,\nproviding a way to quantitatively assess how much LLMs can be pushed towards\ntoxic responses even in the presence of alignment. The framework adopts an\niterative evolution strategy that exploits the interplay between two LLMs, the\nSystem Under Test (SUT) and the Prompt Generator steering SUT responses toward\nhigher toxicity. The toxicity level is assessed by an automated oracle based on\nan existing toxicity classifier. We conduct a quantitative and qualitative\nempirical evaluation using four state-of-the-art LLMs as evaluation subjects\nhaving increasing complexity (7-13 billion parameters). Our quantitative\nevaluation assesses the cost-effectiveness of four alternative versions of\nEvoTox against existing baseline methods, based on random search, curated\ndatasets of toxic prompts, and adversarial attacks. Our qualitative assessment\nengages human evaluators to rate the fluency of the generated prompts and the\nperceived toxicity of the responses collected during the testing sessions.\nResults indicate that the effectiveness, in terms of detected toxicity level,\nis significantly higher than the selected baseline methods (effect size up to\n1.0 against random search and up to 0.99 against adversarial attacks).\nFurthermore, EvoTox yields a limited cost overhead (from 22% to 35% on\naverage)."
                },
                "authors": [
                    {
                        "name": "Simone Corbo"
                    },
                    {
                        "name": "Luca Bancale"
                    },
                    {
                        "name": "Valeria De Gennaro"
                    },
                    {
                        "name": "Livia Lestingi"
                    },
                    {
                        "name": "Vincenzo Scotti"
                    },
                    {
                        "name": "Matteo Camilli"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Camilli"
                },
                "author": "Matteo Camilli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.06970v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.06970v4",
                "updated": "2025-01-03T09:34:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    9,
                    34,
                    10,
                    4,
                    3,
                    0
                ],
                "published": "2024-08-13T15:27:43Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    15,
                    27,
                    43,
                    1,
                    226,
                    0
                ],
                "title": "Prompt-Based Segmentation at Multiple Resolutions and Lighting\n  Conditions using Segment Anything Model 2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompt-Based Segmentation at Multiple Resolutions and Lighting\n  Conditions using Segment Anything Model 2"
                },
                "summary": "This paper provides insights on the effectiveness of the zero shot,\nprompt-based Segment Anything Model (SAM) and its updated versions, SAM 2 and\nSAM 2.1, along with the non-promptable conventional neural network (CNN), for\nsegmenting solar panels in RGB aerial remote sensing imagery. The study\nevaluates these models across diverse lighting conditions, spatial resolutions,\nand prompt strategies. SAM 2 showed slight improvements over SAM, while SAM 2.1\ndemonstrated notable improvements, particularly in sub-optimal lighting and low\nresolution conditions. SAM models, when prompted by user-defined boxes,\noutperformed CNN in all scenarios; in particular, user-box prompts were found\ncrucial for achieving reasonable performance in low resolution data.\nAdditionally, under high resolution, YOLOv9 automatic prompting outperformed\nuser-points prompting by providing reliable prompts to SAM. Under low\nresolution, SAM 2.1 prompted by user points showed similar performance to SAM\n2.1 prompted by YOLOv9, highlighting its zero shot improvements with a single\nclick. In high resolution with optimal lighting imagery, Eff-UNet outperformed\nSAMs prompted by YOLOv9, while under sub-optimal lighting conditions, Eff-UNet,\nand SAM 2.1 prompted by YOLOv9, had similar performance. However, SAM is more\nresource-intensive, and despite improved inference time of SAM 2.1, Eff-UNet is\nmore suitable for automatic segmentation in high resolution data. This research\ndetails strengths and limitations of each model and outlines the robustness of\nuser-prompted image segmentation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides insights on the effectiveness of the zero shot,\nprompt-based Segment Anything Model (SAM) and its updated versions, SAM 2 and\nSAM 2.1, along with the non-promptable conventional neural network (CNN), for\nsegmenting solar panels in RGB aerial remote sensing imagery. The study\nevaluates these models across diverse lighting conditions, spatial resolutions,\nand prompt strategies. SAM 2 showed slight improvements over SAM, while SAM 2.1\ndemonstrated notable improvements, particularly in sub-optimal lighting and low\nresolution conditions. SAM models, when prompted by user-defined boxes,\noutperformed CNN in all scenarios; in particular, user-box prompts were found\ncrucial for achieving reasonable performance in low resolution data.\nAdditionally, under high resolution, YOLOv9 automatic prompting outperformed\nuser-points prompting by providing reliable prompts to SAM. Under low\nresolution, SAM 2.1 prompted by user points showed similar performance to SAM\n2.1 prompted by YOLOv9, highlighting its zero shot improvements with a single\nclick. In high resolution with optimal lighting imagery, Eff-UNet outperformed\nSAMs prompted by YOLOv9, while under sub-optimal lighting conditions, Eff-UNet,\nand SAM 2.1 prompted by YOLOv9, had similar performance. However, SAM is more\nresource-intensive, and despite improved inference time of SAM 2.1, Eff-UNet is\nmore suitable for automatic segmentation in high resolution data. This research\ndetails strengths and limitations of each model and outlines the robustness of\nuser-prompted image segmentation models."
                },
                "authors": [
                    {
                        "name": "Osher Rafaeli"
                    },
                    {
                        "name": "Tal Svoray"
                    },
                    {
                        "name": "Roni Blushtein-Livnon"
                    },
                    {
                        "name": "Ariel Nahlieli"
                    }
                ],
                "author_detail": {
                    "name": "Ariel Nahlieli"
                },
                "author": "Ariel Nahlieli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.06970v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.06970v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01144v2",
                "updated": "2025-01-03T09:27:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    9,
                    27,
                    46,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-02T08:57:00Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    57,
                    0,
                    3,
                    2,
                    0
                ],
                "title": "BlockDialect: Block-wise Fine-grained Mixed Format for Energy-Efficient\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDialect: Block-wise Fine-grained Mixed Format for Energy-Efficient\n  LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success, but their\nincreasing size poses significant challenges in memory usage and computational\ncosts. Quantizing both weights and activations can address these issues, with\nfine-grained block-wise quantization emerging as a promising hardware-supported\nsolution to mitigate outliers. However, existing methods struggle to capture\nnuanced block data distributions. To address this, we propose BlockDialect, a\nblock-wise fine-grained mixed format technique that assigns a per-block optimal\nnumber format from formatbook for better data representation. Additionally, we\nintroduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that\nadapt to diverse data distributions. To leverage this efficiently, we propose a\ntwo-stage approach for online DialectFP4 activation quantization. Importantly,\nDialectFP4 ensures hardware efficiency by selecting representable values as\nscaled integers compatible with low-precision integer arithmetic. BlockDialect\nachieves 11.83% (7.56%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model\ncompared to MXFP4 format with lower bit usage per data, while being only 5.46%\n(2.65%) below full precision even when quantizing full-path matrix\nmultiplication. Focusing on how to represent over how to scale, our work\npresents a promising path for energy-efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success, but their\nincreasing size poses significant challenges in memory usage and computational\ncosts. Quantizing both weights and activations can address these issues, with\nfine-grained block-wise quantization emerging as a promising hardware-supported\nsolution to mitigate outliers. However, existing methods struggle to capture\nnuanced block data distributions. To address this, we propose BlockDialect, a\nblock-wise fine-grained mixed format technique that assigns a per-block optimal\nnumber format from formatbook for better data representation. Additionally, we\nintroduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that\nadapt to diverse data distributions. To leverage this efficiently, we propose a\ntwo-stage approach for online DialectFP4 activation quantization. Importantly,\nDialectFP4 ensures hardware efficiency by selecting representable values as\nscaled integers compatible with low-precision integer arithmetic. BlockDialect\nachieves 11.83% (7.56%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model\ncompared to MXFP4 format with lower bit usage per data, while being only 5.46%\n(2.65%) below full precision even when quantizing full-path matrix\nmultiplication. Focusing on how to represent over how to scale, our work\npresents a promising path for energy-efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Wonsuk Jang"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01722v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01722v1",
                "updated": "2025-01-03T09:27:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    9,
                    27,
                    36,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T09:27:36Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    9,
                    27,
                    36,
                    4,
                    3,
                    0
                ],
                "title": "AR4D: Autoregressive 4D Generation from Monocular Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AR4D: Autoregressive 4D Generation from Monocular Videos"
                },
                "summary": "Recent advancements in generative models have ignited substantial interest in\ndynamic 3D content creation (\\ie, 4D generation). Existing approaches primarily\nrely on Score Distillation Sampling (SDS) to infer novel-view videos, typically\nleading to issues such as limited diversity, spatial-temporal inconsistency and\npoor prompt alignment, due to the inherent randomness of SDS. To tackle these\nproblems, we propose AR4D, a novel paradigm for SDS-free 4D generation.\nSpecifically, our paradigm consists of three stages. To begin with, for a\nmonocular video that is either generated or captured, we first utilize\npre-trained expert models to create a 3D representation of the first frame,\nwhich is further fine-tuned to serve as the canonical space. Subsequently,\nmotivated by the fact that videos happen naturally in an autoregressive manner,\nwe propose to generate each frame's 3D representation based on its previous\nframe's representation, as this autoregressive generation manner can facilitate\nmore accurate geometry and motion estimation. Meanwhile, to prevent overfitting\nduring this process, we introduce a progressive view sampling strategy,\nutilizing priors from pre-trained large-scale 3D reconstruction models. To\navoid appearance drift introduced by autoregressive generation, we further\nincorporate a refinement stage based on a global deformation field and the\ngeometry of each frame's 3D representation. Extensive experiments have\ndemonstrated that AR4D can achieve state-of-the-art 4D generation without SDS,\ndelivering greater diversity, improved spatial-temporal consistency, and better\nalignment with input prompts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in generative models have ignited substantial interest in\ndynamic 3D content creation (\\ie, 4D generation). Existing approaches primarily\nrely on Score Distillation Sampling (SDS) to infer novel-view videos, typically\nleading to issues such as limited diversity, spatial-temporal inconsistency and\npoor prompt alignment, due to the inherent randomness of SDS. To tackle these\nproblems, we propose AR4D, a novel paradigm for SDS-free 4D generation.\nSpecifically, our paradigm consists of three stages. To begin with, for a\nmonocular video that is either generated or captured, we first utilize\npre-trained expert models to create a 3D representation of the first frame,\nwhich is further fine-tuned to serve as the canonical space. Subsequently,\nmotivated by the fact that videos happen naturally in an autoregressive manner,\nwe propose to generate each frame's 3D representation based on its previous\nframe's representation, as this autoregressive generation manner can facilitate\nmore accurate geometry and motion estimation. Meanwhile, to prevent overfitting\nduring this process, we introduce a progressive view sampling strategy,\nutilizing priors from pre-trained large-scale 3D reconstruction models. To\navoid appearance drift introduced by autoregressive generation, we further\nincorporate a refinement stage based on a global deformation field and the\ngeometry of each frame's 3D representation. Extensive experiments have\ndemonstrated that AR4D can achieve state-of-the-art 4D generation without SDS,\ndelivering greater diversity, improved spatial-temporal consistency, and better\nalignment with input prompts."
                },
                "authors": [
                    {
                        "name": "Hanxin Zhu"
                    },
                    {
                        "name": "Tianyu He"
                    },
                    {
                        "name": "Xiqian Yu"
                    },
                    {
                        "name": "Junliang Guo"
                    },
                    {
                        "name": "Zhibo Chen"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "TL;DR: We present a novel method for 4D generation from monocular\n  videos without relying on SDS, delivering greater diversity, improved\n  spatial-temporal consistency, and better alignment with input prompts.\n  Project page: https://hanxinzhu-lab.github.io/AR4D/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01722v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01722v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01711v1",
                "updated": "2025-01-03T09:12:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    9,
                    12,
                    35,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T09:12:35Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    9,
                    12,
                    35,
                    4,
                    3,
                    0
                ],
                "title": "LLMs & Legal Aid: Understanding Legal Needs Exhibited Through User\n  Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs & Legal Aid: Understanding Legal Needs Exhibited Through User\n  Queries"
                },
                "summary": "The paper presents a preliminary analysis of an experiment conducted by Frank\nBold, a Czech expert group, to explore user interactions with GPT-4 for\naddressing legal queries. Between May 3, 2023, and July 25, 2023, 1,252 users\nsubmitted 3,847 queries. Unlike studies that primarily focus on the accuracy,\nfactuality, or hallucination tendencies of large language models (LLMs), our\nanalysis focuses on the user query dimension of the interaction. Using GPT-4o\nfor zero-shot classification, we categorized queries on (1) whether users\nprovided factual information about their issue (29.95%) or not (70.05%), (2)\nwhether they sought legal information (64.93%) or advice on the course of\naction (35.07\\%), and (3) whether they imposed requirements to shape or control\nthe model's answer (28.57%) or not (71.43%). We provide both quantitative and\nqualitative insight into user needs and contribute to a better understanding of\nuser engagement with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper presents a preliminary analysis of an experiment conducted by Frank\nBold, a Czech expert group, to explore user interactions with GPT-4 for\naddressing legal queries. Between May 3, 2023, and July 25, 2023, 1,252 users\nsubmitted 3,847 queries. Unlike studies that primarily focus on the accuracy,\nfactuality, or hallucination tendencies of large language models (LLMs), our\nanalysis focuses on the user query dimension of the interaction. Using GPT-4o\nfor zero-shot classification, we categorized queries on (1) whether users\nprovided factual information about their issue (29.95%) or not (70.05%), (2)\nwhether they sought legal information (64.93%) or advice on the course of\naction (35.07\\%), and (3) whether they imposed requirements to shape or control\nthe model's answer (28.57%) or not (71.43%). We provide both quantitative and\nqualitative insight into user needs and contribute to a better understanding of\nuser engagement with LLMs."
                },
                "authors": [
                    {
                        "name": "Michal Kuk"
                    },
                    {
                        "name": "Jakub Harasta"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Harasta"
                },
                "author": "Jakub Harasta",
                "arxiv_comment": "Accepted at AI for Access to Justice Workshop at Jurix 2024, Brno,\n  Czechia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01705v1",
                "updated": "2025-01-03T09:04:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    9,
                    4,
                    45,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T09:04:45Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    9,
                    4,
                    45,
                    4,
                    3,
                    0
                ],
                "title": "The Essence of Contextual Understanding in Theory of Mind: A Study on\n  Question Answering with Story Characters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Essence of Contextual Understanding in Theory of Mind: A Study on\n  Question Answering with Story Characters"
                },
                "summary": "Theory-of-Mind (ToM) is a fundamental psychological capability that allows\nhumans to understand and interpret the mental states of others. Humans infer\nothers' thoughts by integrating causal cues and indirect clues from broad\ncontextual information, often derived from past interactions. In other words,\nhuman ToM heavily relies on the understanding about the backgrounds and life\nstories of others. Unfortunately, this aspect is largely overlooked in existing\nbenchmarks for evaluating machines' ToM capabilities, due to their usage of\nshort narratives without global backgrounds. In this paper, we verify the\nimportance of understanding long personal backgrounds in ToM and assess the\nperformance of LLMs in such realistic evaluation scenarios. To achieve this, we\nintroduce a novel benchmark, CharToM-QA, comprising 1,035 ToM questions based\non characters from classic novels. Our human study reveals a significant\ndisparity in performance: the same group of educated participants performs\ndramatically better when they have read the novels compared to when they have\nnot. In parallel, our experiments on state-of-the-art LLMs, including the very\nrecent o1 model, show that LLMs still perform notably worse than humans,\ndespite that they have seen these stories during pre-training. This highlights\nthe limitations of current LLMs in capturing the nuanced contextual information\nrequired for ToM reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory-of-Mind (ToM) is a fundamental psychological capability that allows\nhumans to understand and interpret the mental states of others. Humans infer\nothers' thoughts by integrating causal cues and indirect clues from broad\ncontextual information, often derived from past interactions. In other words,\nhuman ToM heavily relies on the understanding about the backgrounds and life\nstories of others. Unfortunately, this aspect is largely overlooked in existing\nbenchmarks for evaluating machines' ToM capabilities, due to their usage of\nshort narratives without global backgrounds. In this paper, we verify the\nimportance of understanding long personal backgrounds in ToM and assess the\nperformance of LLMs in such realistic evaluation scenarios. To achieve this, we\nintroduce a novel benchmark, CharToM-QA, comprising 1,035 ToM questions based\non characters from classic novels. Our human study reveals a significant\ndisparity in performance: the same group of educated participants performs\ndramatically better when they have read the novels compared to when they have\nnot. In parallel, our experiments on state-of-the-art LLMs, including the very\nrecent o1 model, show that LLMs still perform notably worse than humans,\ndespite that they have seen these stories during pre-training. This highlights\nthe limitations of current LLMs in capturing the nuanced contextual information\nrequired for ToM reasoning."
                },
                "authors": [
                    {
                        "name": "Chulun Zhou"
                    },
                    {
                        "name": "Qiujing Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Xiaoqian Yue"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Jiangnan Li"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Shunchi Zhang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "17 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01702v1",
                "updated": "2025-01-03T08:55:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    8,
                    55,
                    19,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T08:55:19Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    8,
                    55,
                    19,
                    4,
                    3,
                    0
                ],
                "title": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning"
                },
                "summary": "Large Language Model (LLM) based agents have proved their ability to perform\ncomplex tasks like humans. However, there is still a large gap between\nopen-sourced LLMs and commercial models like the GPT series. In this paper, we\nfocus on improving the agent generalization capabilities of LLMs via\ninstruction tuning. We first observe that the existing agent training corpus\nexhibits satisfactory results on held-in evaluation sets but fails to\ngeneralize to held-out sets. These agent-tuning works face severe formatting\nerrors and are frequently stuck in the same mistake for a long while. We\nanalyze that the poor generalization ability comes from overfitting to several\nmanual agent environments and a lack of adaptation to new situations. They\nstruggle with the wrong action steps and can not learn from the experience but\njust memorize existing observation-action relations. Inspired by the insight,\nwe propose a novel AgentRefine framework for agent-tuning. The core idea is to\nenable the model to learn to correct its mistakes via observation in the\ntrajectory. Specifically, we propose an agent synthesis framework to encompass\na diverse array of environments and tasks and prompt a strong LLM to refine its\nerror action according to the environment feedback. AgentRefine significantly\noutperforms state-of-the-art agent-tuning work in terms of generalization\nability on diverse agent tasks. It also has better robustness facing\nperturbation and can generate diversified thought in inference. Our findings\nestablish the correlation between agent generalization and self-refinement and\nprovide a new paradigm for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based agents have proved their ability to perform\ncomplex tasks like humans. However, there is still a large gap between\nopen-sourced LLMs and commercial models like the GPT series. In this paper, we\nfocus on improving the agent generalization capabilities of LLMs via\ninstruction tuning. We first observe that the existing agent training corpus\nexhibits satisfactory results on held-in evaluation sets but fails to\ngeneralize to held-out sets. These agent-tuning works face severe formatting\nerrors and are frequently stuck in the same mistake for a long while. We\nanalyze that the poor generalization ability comes from overfitting to several\nmanual agent environments and a lack of adaptation to new situations. They\nstruggle with the wrong action steps and can not learn from the experience but\njust memorize existing observation-action relations. Inspired by the insight,\nwe propose a novel AgentRefine framework for agent-tuning. The core idea is to\nenable the model to learn to correct its mistakes via observation in the\ntrajectory. Specifically, we propose an agent synthesis framework to encompass\na diverse array of environments and tasks and prompt a strong LLM to refine its\nerror action according to the environment feedback. AgentRefine significantly\noutperforms state-of-the-art agent-tuning work in terms of generalization\nability on diverse agent tasks. It also has better robustness facing\nperturbation and can generate diversified thought in inference. Our findings\nestablish the correlation between agent generalization and self-refinement and\nprovide a new paradigm for future research."
                },
                "authors": [
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Keqing He"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Zhuoma Gongque"
                    },
                    {
                        "name": "Weihao Zeng"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Weiran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Weiran Xu"
                },
                "author": "Weiran Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11543v2",
                "updated": "2025-01-03T08:44:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    8,
                    44,
                    46,
                    4,
                    3,
                    0
                ],
                "published": "2024-11-18T13:01:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    1,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment"
                },
                "summary": "Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark."
                },
                "authors": [
                    {
                        "name": "Zhendong Liu"
                    },
                    {
                        "name": "Yuanbi Nie"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Qiushi Cui"
                    },
                    {
                        "name": "Chongjun Wang"
                    },
                    {
                        "name": "Xiaoyong Zhu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2405.13581",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01306v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01306v2",
                "updated": "2025-01-03T08:29:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    8,
                    29,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-02T15:36:50Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    36,
                    50,
                    3,
                    2,
                    0
                ],
                "title": "Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process\n  of Fast and Slow Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process\n  of Fast and Slow Thinking"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional capabilities, yet still\nface the hallucination issue. Typical text generation approaches adopt an\nauto-regressive generation without deliberate reasoning, which often results in\nuntrustworthy and factually inaccurate responses. In this paper, we propose\nHaluSearch, a novel framework that incorporates tree search-based algorithms\n(e.g. MCTS) to enable an explicit slow thinking generation process for\nmitigating hallucinations of LLMs during inference. Specifically, HaluSearch\nframes text generation as a step-by-step reasoning process, using a\nself-evaluation reward model to score each generation step and guide the tree\nsearch towards the most reliable generation pathway for fully exploiting the\ninternal knowledge of LLMs. To balance efficiency and quality, we introduce a\nhierarchical thinking system switch mechanism inspired by the dual process\ntheory in cognitive science, which dynamically alternates between fast and slow\nthinking modes at both the instance and step levels, adapting to the complexity\nof questions and reasoning states. We conduct extensive experiments on both\nEnglish and Chinese datasets and the results show that our approach\nsignificantly outperforms baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional capabilities, yet still\nface the hallucination issue. Typical text generation approaches adopt an\nauto-regressive generation without deliberate reasoning, which often results in\nuntrustworthy and factually inaccurate responses. In this paper, we propose\nHaluSearch, a novel framework that incorporates tree search-based algorithms\n(e.g. MCTS) to enable an explicit slow thinking generation process for\nmitigating hallucinations of LLMs during inference. Specifically, HaluSearch\nframes text generation as a step-by-step reasoning process, using a\nself-evaluation reward model to score each generation step and guide the tree\nsearch towards the most reliable generation pathway for fully exploiting the\ninternal knowledge of LLMs. To balance efficiency and quality, we introduce a\nhierarchical thinking system switch mechanism inspired by the dual process\ntheory in cognitive science, which dynamically alternates between fast and slow\nthinking modes at both the instance and step levels, adapting to the complexity\nof questions and reasoning states. We conduct extensive experiments on both\nEnglish and Chinese datasets and the results show that our approach\nsignificantly outperforms baseline approaches."
                },
                "authors": [
                    {
                        "name": "Xiaoxue Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01306v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01679v1",
                "updated": "2025-01-03T07:47:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    47,
                    59,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T07:47:59Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    47,
                    59,
                    4,
                    3,
                    0
                ],
                "title": "Adaptive Few-shot Prompting for Machine Translation with Pre-trained\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Few-shot Prompting for Machine Translation with Pre-trained\n  Language Models"
                },
                "summary": "Recently, Large language models (LLMs) with in-context learning have\ndemonstrated remarkable potential in handling neural machine translation.\nHowever, existing evidence shows that LLMs are prompt-sensitive and it is\nsub-optimal to apply the fixed prompt to any input for downstream machine\ntranslation tasks. To address this issue, we propose an adaptive few-shot\nprompting (AFSP) framework to automatically select suitable translation\ndemonstrations for various source input sentences to further elicit the\ntranslation capability of an LLM for better machine translation. First, we\nbuild a translation demonstration retrieval module based on LLM's embedding to\nretrieve top-k semantic-similar translation demonstrations from aligned\nparallel translation corpus. Rather than using other embedding models for\nsemantic demonstration retrieval, we build a hybrid demonstration retrieval\nmodule based on the embedding layer of the deployed LLM to build better input\nrepresentation for retrieving more semantic-related translation demonstrations.\nThen, to ensure better semantic consistency between source inputs and target\noutputs, we force the deployed LLM itself to generate multiple output\ncandidates in the target language with the help of translation demonstrations\nand rerank these candidates. Besides, to better evaluate the effectiveness of\nour AFSP framework on the latest language and extend the research boundary of\nneural machine translation, we construct a high-quality diplomatic\nChinese-English parallel dataset that consists of 5,528 parallel\nChinese-English sentences. Finally, extensive experiments on the proposed\ndiplomatic Chinese-English parallel dataset and the United Nations Parallel\nCorpus (Chinese-English part) show the effectiveness and superiority of our\nproposed AFSP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large language models (LLMs) with in-context learning have\ndemonstrated remarkable potential in handling neural machine translation.\nHowever, existing evidence shows that LLMs are prompt-sensitive and it is\nsub-optimal to apply the fixed prompt to any input for downstream machine\ntranslation tasks. To address this issue, we propose an adaptive few-shot\nprompting (AFSP) framework to automatically select suitable translation\ndemonstrations for various source input sentences to further elicit the\ntranslation capability of an LLM for better machine translation. First, we\nbuild a translation demonstration retrieval module based on LLM's embedding to\nretrieve top-k semantic-similar translation demonstrations from aligned\nparallel translation corpus. Rather than using other embedding models for\nsemantic demonstration retrieval, we build a hybrid demonstration retrieval\nmodule based on the embedding layer of the deployed LLM to build better input\nrepresentation for retrieving more semantic-related translation demonstrations.\nThen, to ensure better semantic consistency between source inputs and target\noutputs, we force the deployed LLM itself to generate multiple output\ncandidates in the target language with the help of translation demonstrations\nand rerank these candidates. Besides, to better evaluate the effectiveness of\nour AFSP framework on the latest language and extend the research boundary of\nneural machine translation, we construct a high-quality diplomatic\nChinese-English parallel dataset that consists of 5,528 parallel\nChinese-English sentences. Finally, extensive experiments on the proposed\ndiplomatic Chinese-English parallel dataset and the United Nations Parallel\nCorpus (Chinese-English part) show the effectiveness and superiority of our\nproposed AFSP."
                },
                "authors": [
                    {
                        "name": "Lei Tang"
                    },
                    {
                        "name": "Jinghui Qin"
                    },
                    {
                        "name": "Wenxuan Ye"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Zhijing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Yang"
                },
                "author": "Zhijing Yang",
                "arxiv_comment": "published to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00906v2",
                "updated": "2025-01-03T07:47:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    47,
                    36,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-01T17:38:40Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    17,
                    38,
                    40,
                    2,
                    1,
                    0
                ],
                "title": "Large Language Model Based Multi-Agent System Augmented Complex Event\n  Processing Pipeline for Internet of Multimedia Things",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Based Multi-Agent System Augmented Complex Event\n  Processing Pipeline for Internet of Multimedia Things"
                },
                "summary": "This paper presents the development and evaluation of a Large Language Model\n(LLM), also known as foundation models, based multi-agent system framework for\ncomplex event processing (CEP) with a focus on video query processing use\ncases. The primary goal is to create a proof-of-concept (POC) that integrates\nstate-of-the-art LLM orchestration frameworks with publish/subscribe (pub/sub)\ntools to address the integration of LLMs with current CEP systems. Utilizing\nthe Autogen framework in conjunction with Kafka message brokers, the system\ndemonstrates an autonomous CEP pipeline capable of handling complex workflows.\nExtensive experiments evaluate the system's performance across varying\nconfigurations, complexities, and video resolutions, revealing the trade-offs\nbetween functionality and latency. The results show that while higher agent\ncount and video complexities increase latency, the system maintains high\nconsistency in narrative coherence. This research builds upon and contributes\nto, existing novel approaches to distributed AI systems, offering detailed\ninsights into integrating such systems into existing infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the development and evaluation of a Large Language Model\n(LLM), also known as foundation models, based multi-agent system framework for\ncomplex event processing (CEP) with a focus on video query processing use\ncases. The primary goal is to create a proof-of-concept (POC) that integrates\nstate-of-the-art LLM orchestration frameworks with publish/subscribe (pub/sub)\ntools to address the integration of LLMs with current CEP systems. Utilizing\nthe Autogen framework in conjunction with Kafka message brokers, the system\ndemonstrates an autonomous CEP pipeline capable of handling complex workflows.\nExtensive experiments evaluate the system's performance across varying\nconfigurations, complexities, and video resolutions, revealing the trade-offs\nbetween functionality and latency. The results show that while higher agent\ncount and video complexities increase latency, the system maintains high\nconsistency in narrative coherence. This research builds upon and contributes\nto, existing novel approaches to distributed AI systems, offering detailed\ninsights into integrating such systems into existing infrastructures."
                },
                "authors": [
                    {
                        "name": "Talha Zeeshan"
                    },
                    {
                        "name": "Abhishek Kumar"
                    },
                    {
                        "name": "Susanna Pirttikangas"
                    },
                    {
                        "name": "Sasu Tarkoma"
                    }
                ],
                "author_detail": {
                    "name": "Sasu Tarkoma"
                },
                "author": "Sasu Tarkoma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04119v2",
                "updated": "2025-01-03T07:29:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    29,
                    3,
                    4,
                    3,
                    0
                ],
                "published": "2024-02-06T16:12:36Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    16,
                    12,
                    36,
                    1,
                    37,
                    0
                ],
                "title": "A quantitative analysis of knowledge-learning preferences in large\n  language models in molecular science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A quantitative analysis of knowledge-learning preferences in large\n  language models in molecular science"
                },
                "summary": "Deep learning has significantly advanced molecular modeling and design,\nenabling efficient understanding and discovery of novel molecules. In\nparticular, large language models (LLMs) introduce a fresh research paradigm to\ntackle scientific problems from a natural language processing (NLP)\nperspective. LLMs significantly enhance our understanding and generation of\nmolecules, often surpassing existing methods with their capabilities to decode\nand synthesize complex molecular patterns. However, two key issues remain: how\nto quantify the match between model and data modalities and how to identify the\nknowledge-learning preferences of models. To address these challenges, we\npropose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263\nexperiments to assess the model's compatibility with data modalities and\nknowledge acquisition. Through the modal transition probability matrix, we\nprovide insights into the most suitable modalities for tasks. Furthermore, we\nintroduce a statistically interpretable approach to discover context-specific\nknowledge mapping by localized feature filtering. Our analysis offers an\nexploration of the learning mechanism and paves the way for advancing LLMs in\nmolecular science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has significantly advanced molecular modeling and design,\nenabling efficient understanding and discovery of novel molecules. In\nparticular, large language models (LLMs) introduce a fresh research paradigm to\ntackle scientific problems from a natural language processing (NLP)\nperspective. LLMs significantly enhance our understanding and generation of\nmolecules, often surpassing existing methods with their capabilities to decode\nand synthesize complex molecular patterns. However, two key issues remain: how\nto quantify the match between model and data modalities and how to identify the\nknowledge-learning preferences of models. To address these challenges, we\npropose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263\nexperiments to assess the model's compatibility with data modalities and\nknowledge acquisition. Through the modal transition probability matrix, we\nprovide insights into the most suitable modalities for tasks. Furthermore, we\nintroduce a statistically interpretable approach to discover context-specific\nknowledge mapping by localized feature filtering. Our analysis offers an\nexploration of the learning mechanism and paves the way for advancing LLMs in\nmolecular science."
                },
                "authors": [
                    {
                        "name": "Pengfei Liu"
                    },
                    {
                        "name": "Jun Tao"
                    },
                    {
                        "name": "Zhixiang Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhixiang Ren"
                },
                "author": "Zhixiang Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01672v1",
                "updated": "2025-01-03T07:19:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    19,
                    23,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T07:19:23Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    19,
                    23,
                    4,
                    3,
                    0
                ],
                "title": "Practical Secure Inference Algorithm for Fine-tuned Large Language Model\n  Based on Fully Homomorphic Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Secure Inference Algorithm for Fine-tuned Large Language Model\n  Based on Fully Homomorphic Encryption"
                },
                "summary": "Large language models(LLMs) are currently at the forefront of the machine\nlearning field, which show a broad application prospect but at the same time\nexpose some risks of privacy leakage. We combined Fully Homomorphic\nEncryption(FHE) and provable security theory with Parameter-Efficient\nFine-Tuning(PEFT) to propose an efficient and secure inference scheme for LLMs.\nMore specially, we focus on pre-trained LLMs who rely on open-sourced base\nmodel and then fine-tuned with the private datasets by LoRA. This is a popular\nroad-map for Vertical Domain Models such as LawGPT and BenTsao. We use two key\ntechnologies below. Firstly, we divide the whole model into the public part and\nthe private part. The weights of public part are publicly accessible(e.g. the\nopen-sourced base model) while the private part needs to be protected(e.g. the\nLoRA matrices). In this way, the overhead brought by computing on private data\ncan be greatly reduced. Secondly, we propose a general method to transform a\nlinear layer into another one which provides security against model extraction\nattacks and preserves its original functionality, which denotes as Private\nLinear Layer(PLL). Then we use this method on the LoRA matrices to make sure\nthat the server protects their private weights without restricting the user's\ninput. We also show that the difficulty of performing model extraction attacks\nfor PLL can be generalized to the well-known hard problem Learning with\nErrors(LWE). Combing this method with FHE, we can protect user's input at the\nsame time. This transform method can be applied to any linear layer to gain an\nextra protection against model extraction attacks. In this paper, we use the\nopen-source model ChatGLM2-6B as the base model which is fine-tuned by LoRA.\nExperimental results show the inference efficiency of our scheme reaches\n1.61s/token which shows that the scheme has good practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models(LLMs) are currently at the forefront of the machine\nlearning field, which show a broad application prospect but at the same time\nexpose some risks of privacy leakage. We combined Fully Homomorphic\nEncryption(FHE) and provable security theory with Parameter-Efficient\nFine-Tuning(PEFT) to propose an efficient and secure inference scheme for LLMs.\nMore specially, we focus on pre-trained LLMs who rely on open-sourced base\nmodel and then fine-tuned with the private datasets by LoRA. This is a popular\nroad-map for Vertical Domain Models such as LawGPT and BenTsao. We use two key\ntechnologies below. Firstly, we divide the whole model into the public part and\nthe private part. The weights of public part are publicly accessible(e.g. the\nopen-sourced base model) while the private part needs to be protected(e.g. the\nLoRA matrices). In this way, the overhead brought by computing on private data\ncan be greatly reduced. Secondly, we propose a general method to transform a\nlinear layer into another one which provides security against model extraction\nattacks and preserves its original functionality, which denotes as Private\nLinear Layer(PLL). Then we use this method on the LoRA matrices to make sure\nthat the server protects their private weights without restricting the user's\ninput. We also show that the difficulty of performing model extraction attacks\nfor PLL can be generalized to the well-known hard problem Learning with\nErrors(LWE). Combing this method with FHE, we can protect user's input at the\nsame time. This transform method can be applied to any linear layer to gain an\nextra protection against model extraction attacks. In this paper, we use the\nopen-source model ChatGLM2-6B as the base model which is fine-tuned by LoRA.\nExperimental results show the inference efficiency of our scheme reaches\n1.61s/token which shows that the scheme has good practicality."
                },
                "authors": [
                    {
                        "name": "Zhang Ruoyan"
                    },
                    {
                        "name": "Zheng Zhongxiang"
                    },
                    {
                        "name": "Bao Wankang"
                    }
                ],
                "author_detail": {
                    "name": "Bao Wankang"
                },
                "author": "Bao Wankang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16500v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16500v3",
                "updated": "2025-01-03T07:18:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    18,
                    30,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-21T06:16:04Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    6,
                    16,
                    4,
                    5,
                    356,
                    0
                ],
                "title": "Speech Retrieval-Augmented Generation without Automatic Speech\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech Retrieval-Augmented Generation without Automatic Speech\n  Recognition"
                },
                "summary": "One common approach for question answering over speech data is to first\ntranscribe speech using automatic speech recognition (ASR) and then employ\ntext-based retrieval-augmented generation (RAG) on the transcriptions. While\nthis cascaded pipeline has proven effective in many practical settings, ASR\nerrors can propagate to the retrieval and generation steps. To overcome this\nlimitation, we introduce SpeechRAG, a novel framework designed for\nopen-question answering over spoken data. Our proposed approach fine-tunes a\npre-trained speech encoder into a speech adapter fed into a frozen large\nlanguage model (LLM)--based retrieval model. By aligning the embedding spaces\nof text and speech, our speech retriever directly retrieves audio passages from\ntext-based queries, leveraging the retrieval capacity of the frozen text\nretriever. Our retrieval experiments on spoken question answering datasets show\nthat direct speech retrieval does not degrade over the text-based baseline, and\noutperforms the cascaded systems using ASR. For generation, we use a speech\nlanguage model (SLM) as a generator, conditioned on audio passages rather than\ntranscripts. Without fine-tuning of the SLM, this approach outperforms cascaded\ntext-based models when there is high WER in the transcripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One common approach for question answering over speech data is to first\ntranscribe speech using automatic speech recognition (ASR) and then employ\ntext-based retrieval-augmented generation (RAG) on the transcriptions. While\nthis cascaded pipeline has proven effective in many practical settings, ASR\nerrors can propagate to the retrieval and generation steps. To overcome this\nlimitation, we introduce SpeechRAG, a novel framework designed for\nopen-question answering over spoken data. Our proposed approach fine-tunes a\npre-trained speech encoder into a speech adapter fed into a frozen large\nlanguage model (LLM)--based retrieval model. By aligning the embedding spaces\nof text and speech, our speech retriever directly retrieves audio passages from\ntext-based queries, leveraging the retrieval capacity of the frozen text\nretriever. Our retrieval experiments on spoken question answering datasets show\nthat direct speech retrieval does not degrade over the text-based baseline, and\noutperforms the cascaded systems using ASR. For generation, we use a speech\nlanguage model (SLM) as a generator, conditioned on audio passages rather than\ntranscripts. Without fine-tuning of the SLM, this approach outperforms cascaded\ntext-based models when there is high WER in the transcripts."
                },
                "authors": [
                    {
                        "name": "Do June Min"
                    },
                    {
                        "name": "Karel Mundnich"
                    },
                    {
                        "name": "Andy Lapastora"
                    },
                    {
                        "name": "Erfan Soltanmohammadi"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Kyu Han"
                    }
                ],
                "author_detail": {
                    "name": "Kyu Han"
                },
                "author": "Kyu Han",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16500v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16500v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11814v2",
                "updated": "2025-01-03T07:18:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    18,
                    19,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-16T14:29:49Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    29,
                    49,
                    0,
                    351,
                    0
                ],
                "title": "EventSum: A Large-Scale Event-Centric Summarization Dataset for Chinese\n  Multi-News Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EventSum: A Large-Scale Event-Centric Summarization Dataset for Chinese\n  Multi-News Documents"
                },
                "summary": "In real life, many dynamic events, such as major disasters and large-scale\nsports events, evolve continuously over time. Obtaining an overview of these\nevents can help people quickly understand the situation and respond more\neffectively. This is challenging because the key information of the event is\noften scattered across multiple documents, involving complex event knowledge\nunderstanding and reasoning, which is under-explored in previous work.\nTherefore, we proposed the Event-Centric Multi-Document Summarization (ECS)\ntask, which aims to generate concise and comprehensive summaries of a given\nevent based on multiple related news documents. Based on this, we constructed\nthe EventSum dataset, which was constructed using Baidu Baike entries and\nunderwent extensive human annotation, to facilitate relevant research. It is\nthe first large scale Chinese multi-document summarization dataset, containing\n5,100 events and a total of 57,984 news documents, with an average of 11.4\ninput news documents and 13,471 characters per event. To ensure data quality\nand mitigate potential data leakage, we adopted a multi-stage annotation\napproach for manually labeling the test set. Given the complexity of\nevent-related information, existing metrics struggle to comprehensively assess\nthe quality of generated summaries. We designed specific metrics including\nEvent Recall, Argument Recall, Causal Recall, and Temporal Recall along with\ncorresponding calculation methods for evaluation. We conducted comprehensive\nexperiments on EventSum to evaluate the performance of advanced long-context\nLarge Language Models (LLMs) on this task. Our experimental results indicate\nthat: 1) The event-centric multi-document summarization task remains\nchallenging for existing long-context LLMs; 2) The recall metrics we designed\nare crucial for evaluating the comprehensiveness of the summary information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real life, many dynamic events, such as major disasters and large-scale\nsports events, evolve continuously over time. Obtaining an overview of these\nevents can help people quickly understand the situation and respond more\neffectively. This is challenging because the key information of the event is\noften scattered across multiple documents, involving complex event knowledge\nunderstanding and reasoning, which is under-explored in previous work.\nTherefore, we proposed the Event-Centric Multi-Document Summarization (ECS)\ntask, which aims to generate concise and comprehensive summaries of a given\nevent based on multiple related news documents. Based on this, we constructed\nthe EventSum dataset, which was constructed using Baidu Baike entries and\nunderwent extensive human annotation, to facilitate relevant research. It is\nthe first large scale Chinese multi-document summarization dataset, containing\n5,100 events and a total of 57,984 news documents, with an average of 11.4\ninput news documents and 13,471 characters per event. To ensure data quality\nand mitigate potential data leakage, we adopted a multi-stage annotation\napproach for manually labeling the test set. Given the complexity of\nevent-related information, existing metrics struggle to comprehensively assess\nthe quality of generated summaries. We designed specific metrics including\nEvent Recall, Argument Recall, Causal Recall, and Temporal Recall along with\ncorresponding calculation methods for evaluation. We conducted comprehensive\nexperiments on EventSum to evaluate the performance of advanced long-context\nLarge Language Models (LLMs) on this task. Our experimental results indicate\nthat: 1) The event-centric multi-document summarization task remains\nchallenging for existing long-context LLMs; 2) The recall metrics we designed\nare crucial for evaluating the comprehensiveness of the summary information."
                },
                "authors": [
                    {
                        "name": "Mengna Zhu"
                    },
                    {
                        "name": "Kaisheng Zeng"
                    },
                    {
                        "name": "Mao Wang"
                    },
                    {
                        "name": "Kaiming Xiao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Hongbin Huang"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "Extended version for paper accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06289v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06289v2",
                "updated": "2025-01-03T07:10:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    10,
                    28,
                    4,
                    3,
                    0
                ],
                "published": "2024-02-09T09:58:35Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    9,
                    58,
                    35,
                    4,
                    40,
                    0
                ],
                "title": "FedMIA: An Effective Membership Inference Attack Exploiting \"All for\n  One\" Principle in Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedMIA: An Effective Membership Inference Attack Exploiting \"All for\n  One\" Principle in Federated Learning"
                },
                "summary": "Federated Learning (FL) is a promising approach for training machine learning\nmodels on decentralized data while preserving privacy. However, privacy risks,\nparticularly Membership Inference Attacks (MIAs), which aim to determine\nwhether a specific data point belongs to a target client's training set, remain\na significant concern. Existing methods for implementing MIAs in FL primarily\nanalyze updates from the target client, focusing on metrics such as loss,\ngradient norm, and gradient difference. However, these methods fail to leverage\nupdates from non-target clients, potentially underutilizing available\ninformation. In this paper, we first formulate a one-tailed likelihood-ratio\nhypothesis test based on the likelihood of updates from non-target clients.\nBuilding upon this formulation, we introduce a three-step Membership Inference\nAttack (MIA) method, called FedMIA, which follows the \"all for one\"--leveraging\nupdates from all clients across multiple communication rounds to enhance MIA\neffectiveness. Both theoretical analysis and extensive experimental results\ndemonstrate that FedMIA outperforms existing MIAs in both classification and\ngenerative tasks. Additionally, it can be integrated as an extension to\nexisting methods and is robust against various defense strategies, Non-IID\ndata, and different federated structures. Our code is available in\nhttps://github.com/Liar-Mask/FedMIA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) is a promising approach for training machine learning\nmodels on decentralized data while preserving privacy. However, privacy risks,\nparticularly Membership Inference Attacks (MIAs), which aim to determine\nwhether a specific data point belongs to a target client's training set, remain\na significant concern. Existing methods for implementing MIAs in FL primarily\nanalyze updates from the target client, focusing on metrics such as loss,\ngradient norm, and gradient difference. However, these methods fail to leverage\nupdates from non-target clients, potentially underutilizing available\ninformation. In this paper, we first formulate a one-tailed likelihood-ratio\nhypothesis test based on the likelihood of updates from non-target clients.\nBuilding upon this formulation, we introduce a three-step Membership Inference\nAttack (MIA) method, called FedMIA, which follows the \"all for one\"--leveraging\nupdates from all clients across multiple communication rounds to enhance MIA\neffectiveness. Both theoretical analysis and extensive experimental results\ndemonstrate that FedMIA outperforms existing MIAs in both classification and\ngenerative tasks. Additionally, it can be integrated as an extension to\nexisting methods and is robust against various defense strategies, Non-IID\ndata, and different federated structures. Our code is available in\nhttps://github.com/Liar-Mask/FedMIA."
                },
                "authors": [
                    {
                        "name": "Gongxi Zhu"
                    },
                    {
                        "name": "Donghao Li"
                    },
                    {
                        "name": "Hanlin Gu"
                    },
                    {
                        "name": "Yuan Yao"
                    },
                    {
                        "name": "Lixin Fan"
                    },
                    {
                        "name": "Yuxing Han"
                    }
                ],
                "author_detail": {
                    "name": "Yuxing Han"
                },
                "author": "Yuxing Han",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06289v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06289v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19878v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19878v3",
                "updated": "2025-01-03T06:55:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    55,
                    2,
                    4,
                    3,
                    0
                ],
                "published": "2024-09-30T02:23:31Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    23,
                    31,
                    0,
                    274,
                    0
                ],
                "title": "HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic\n  Thresholds for Fine-Tuning LLM-based ASR Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic\n  Thresholds for Fine-Tuning LLM-based ASR Models"
                },
                "summary": "Recent advancements in integrating Large Language Models (LLM) with automatic\nspeech recognition (ASR) have performed remarkably in general domains. While\nsupervised fine-tuning (SFT) of all model parameters is often employed to adapt\npre-trained LLM-based ASR models to specific domains, it imposes high\ncomputational costs and notably reduces their performance in general domains.\nIn this paper, we propose a novel parameter-efficient multi-domain fine-tuning\nmethod for adapting pre-trained LLM-based ASR models to multi-accent domains\nwithout catastrophic forgetting named \\textit{HDMoLE}, which leverages\nhierarchical routing and dynamic thresholds based on combining low-rank\nadaptation (LoRA) with the mixer of experts (MoE) and can be generalized to any\nlinear layer. Hierarchical routing establishes a clear correspondence between\nLoRA experts and accent domains, improving cross-domain collaboration among the\nLoRA experts. Unlike the static Top-K strategy for activating LoRA experts,\ndynamic thresholds can adaptively activate varying numbers of LoRA experts at\neach MoE layer. Experiments on the multi-accent and standard Mandarin datasets\ndemonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model\nprojector module achieves similar performance to full fine-tuning in the target\nmulti-accent domains while using only 9.6% of the trainable parameters required\nfor full fine-tuning and minimal degradation in the source general domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in integrating Large Language Models (LLM) with automatic\nspeech recognition (ASR) have performed remarkably in general domains. While\nsupervised fine-tuning (SFT) of all model parameters is often employed to adapt\npre-trained LLM-based ASR models to specific domains, it imposes high\ncomputational costs and notably reduces their performance in general domains.\nIn this paper, we propose a novel parameter-efficient multi-domain fine-tuning\nmethod for adapting pre-trained LLM-based ASR models to multi-accent domains\nwithout catastrophic forgetting named \\textit{HDMoLE}, which leverages\nhierarchical routing and dynamic thresholds based on combining low-rank\nadaptation (LoRA) with the mixer of experts (MoE) and can be generalized to any\nlinear layer. Hierarchical routing establishes a clear correspondence between\nLoRA experts and accent domains, improving cross-domain collaboration among the\nLoRA experts. Unlike the static Top-K strategy for activating LoRA experts,\ndynamic thresholds can adaptively activate varying numbers of LoRA experts at\neach MoE layer. Experiments on the multi-accent and standard Mandarin datasets\ndemonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model\nprojector module achieves similar performance to full fine-tuning in the target\nmulti-accent domains while using only 9.6% of the trainable parameters required\nfor full fine-tuning and minimal degradation in the source general domain."
                },
                "authors": [
                    {
                        "name": "Bingshen Mu"
                    },
                    {
                        "name": "Kun Wei"
                    },
                    {
                        "name": "Qijie Shao"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19878v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19878v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01668v1",
                "updated": "2025-01-03T06:50:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    50,
                    6,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T06:50:06Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    50,
                    6,
                    4,
                    3,
                    0
                ],
                "title": "CoT-based Synthesizer: Enhancing LLM Performance through Answer\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT-based Synthesizer: Enhancing LLM Performance through Answer\n  Synthesis"
                },
                "summary": "Current inference scaling methods, such as Self-consistency and Best-of-N,\nhave proven effective in improving the accuracy of LLMs on complex reasoning\ntasks. However, these methods rely heavily on the quality of candidate\nresponses and are unable to produce correct answers when all candidates are\nincorrect. In this paper, we propose a novel inference scaling strategy,\nCoT-based Synthesizer, which leverages CoT reasoning to synthesize superior\nanswers by analyzing complementary information from multiple candidate\nresponses, even when all candidate responses are flawed. To enable a\nlightweight and cost-effective implementation, we introduce an automated data\ngeneration pipeline that creates diverse training data. This allows smaller\nLLMs trained on this data to improve the inference accuracy of larger models,\nincluding API-based LLMs. Experimental results across four benchmark datasets\nwith seven policy models demonstrate that our method significantly enhances\nperformance, with gains of 11.8% for Llama3-8B and 10.3% for GPT-4o on the MATH\ndataset. The corresponding training data and code are publicly available on\nhttps://github.com/RUCKBReasoning/CoT-based-Synthesizer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current inference scaling methods, such as Self-consistency and Best-of-N,\nhave proven effective in improving the accuracy of LLMs on complex reasoning\ntasks. However, these methods rely heavily on the quality of candidate\nresponses and are unable to produce correct answers when all candidates are\nincorrect. In this paper, we propose a novel inference scaling strategy,\nCoT-based Synthesizer, which leverages CoT reasoning to synthesize superior\nanswers by analyzing complementary information from multiple candidate\nresponses, even when all candidate responses are flawed. To enable a\nlightweight and cost-effective implementation, we introduce an automated data\ngeneration pipeline that creates diverse training data. This allows smaller\nLLMs trained on this data to improve the inference accuracy of larger models,\nincluding API-based LLMs. Experimental results across four benchmark datasets\nwith seven policy models demonstrate that our method significantly enhances\nperformance, with gains of 11.8% for Llama3-8B and 10.3% for GPT-4o on the MATH\ndataset. The corresponding training data and code are publicly available on\nhttps://github.com/RUCKBReasoning/CoT-based-Synthesizer."
                },
                "authors": [
                    {
                        "name": "Bohan Zhang"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Jifan Yu"
                    },
                    {
                        "name": "Sijia Luo"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01664v1",
                "updated": "2025-01-03T06:37:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    37,
                    39,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T06:37:39Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    37,
                    39,
                    4,
                    3,
                    0
                ],
                "title": "BARTPredict: Empowering IoT Security with LLM-Driven Cyber Threat\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BARTPredict: Empowering IoT Security with LLM-Driven Cyber Threat\n  Prediction"
                },
                "summary": "The integration of Internet of Things (IoT) technology in various domains has\nled to operational advancements, but it has also introduced new vulnerabilities\nto cybersecurity threats, as evidenced by recent widespread cyberattacks on IoT\ndevices. Intrusion detection systems are often reactive, triggered by specific\npatterns or anomalies observed within the network. To address this challenge,\nthis work proposes a proactive approach to anticipate and preemptively mitigate\nmalicious activities, aiming to prevent potential damage before it occurs. This\npaper proposes an innovative intrusion prediction framework empowered by\nPre-trained Large Language Models (LLMs). The framework incorporates two LLMs:\na fine-tuned Bidirectional and AutoRegressive Transformers (BART) model for\npredicting network traffic and a fine-tuned Bidirectional Encoder\nRepresentations from Transformers (BERT) model for evaluating the predicted\ntraffic. By harnessing the bidirectional capabilities of BART the framework\nthen identifies malicious packets among these predictions. Evaluated using the\nCICIoT2023 IoT attack dataset, our framework showcases a notable enhancement in\npredictive performance, attaining an impressive 98% overall accuracy, providing\na powerful response to the cybersecurity challenges that confront IoT networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Internet of Things (IoT) technology in various domains has\nled to operational advancements, but it has also introduced new vulnerabilities\nto cybersecurity threats, as evidenced by recent widespread cyberattacks on IoT\ndevices. Intrusion detection systems are often reactive, triggered by specific\npatterns or anomalies observed within the network. To address this challenge,\nthis work proposes a proactive approach to anticipate and preemptively mitigate\nmalicious activities, aiming to prevent potential damage before it occurs. This\npaper proposes an innovative intrusion prediction framework empowered by\nPre-trained Large Language Models (LLMs). The framework incorporates two LLMs:\na fine-tuned Bidirectional and AutoRegressive Transformers (BART) model for\npredicting network traffic and a fine-tuned Bidirectional Encoder\nRepresentations from Transformers (BERT) model for evaluating the predicted\ntraffic. By harnessing the bidirectional capabilities of BART the framework\nthen identifies malicious packets among these predictions. Evaluated using the\nCICIoT2023 IoT attack dataset, our framework showcases a notable enhancement in\npredictive performance, attaining an impressive 98% overall accuracy, providing\na powerful response to the cybersecurity challenges that confront IoT networks."
                },
                "authors": [
                    {
                        "name": "Alaeddine Diaf"
                    },
                    {
                        "name": "Abdelaziz Amara Korba"
                    },
                    {
                        "name": "Nour Elislem Karabadji"
                    },
                    {
                        "name": "Yacine Ghamri-Doudane"
                    }
                ],
                "author_detail": {
                    "name": "Yacine Ghamri-Doudane"
                },
                "author": "Yacine Ghamri-Doudane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20662v2",
                "updated": "2025-01-03T06:22:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    22,
                    52,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-30T02:40:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    2,
                    40,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "Enhancing Table Recognition with Vision LLMs: A Benchmark and\n  Neighbor-Guided Toolchain Reasoner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Table Recognition with Vision LLMs: A Benchmark and\n  Neighbor-Guided Toolchain Reasoner"
                },
                "summary": "Pre-trained foundation models have recently significantly progressed in\nstructured table understanding and reasoning. However, despite advancements in\nareas such as table semantic understanding and table question answering,\nrecognizing the structure and content of unstructured tables using Vision Large\nLanguage Models (VLLMs) remains under-explored. In this work, we address this\nresearch gap by employing VLLMs in a training-free reasoning paradigm. First,\nwe design a benchmark with various hierarchical dimensions relevant to table\nrecognition. Subsequently, we conduct in-depth evaluations using pre-trained\nVLLMs, finding that low-quality image input is a significant bottleneck in the\nrecognition process. Drawing inspiration from these findings, we propose the\nNeighbor-Guided Toolchain Reasoner (NGTR) framework, which is characterized by\nintegrating multiple lightweight models for low-level visual processing\noperations aimed at mitigating issues with low-quality input images.\nSpecifically, we utilize a neighbor retrieval mechanism to guide the generation\nof multiple tool invocation plans, transferring tool selection experiences from\nsimilar neighbors to the given input, thereby facilitating suitable tool\nselection. Additionally, we introduce a reflection module to supervise the tool\ninvocation process. Extensive experiments on public table recognition datasets\ndemonstrate that our approach significantly enhances the recognition\ncapabilities of the vanilla VLLMs. We believe that the designed benchmark and\nthe proposed NGTR framework could provide an alternative solution in table\nrecognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained foundation models have recently significantly progressed in\nstructured table understanding and reasoning. However, despite advancements in\nareas such as table semantic understanding and table question answering,\nrecognizing the structure and content of unstructured tables using Vision Large\nLanguage Models (VLLMs) remains under-explored. In this work, we address this\nresearch gap by employing VLLMs in a training-free reasoning paradigm. First,\nwe design a benchmark with various hierarchical dimensions relevant to table\nrecognition. Subsequently, we conduct in-depth evaluations using pre-trained\nVLLMs, finding that low-quality image input is a significant bottleneck in the\nrecognition process. Drawing inspiration from these findings, we propose the\nNeighbor-Guided Toolchain Reasoner (NGTR) framework, which is characterized by\nintegrating multiple lightweight models for low-level visual processing\noperations aimed at mitigating issues with low-quality input images.\nSpecifically, we utilize a neighbor retrieval mechanism to guide the generation\nof multiple tool invocation plans, transferring tool selection experiences from\nsimilar neighbors to the given input, thereby facilitating suitable tool\nselection. Additionally, we introduce a reflection module to supervise the tool\ninvocation process. Extensive experiments on public table recognition datasets\ndemonstrate that our approach significantly enhances the recognition\ncapabilities of the vanilla VLLMs. We believe that the designed benchmark and\nthe proposed NGTR framework could provide an alternative solution in table\nrecognition."
                },
                "authors": [
                    {
                        "name": "Yitong Zhou"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Qingyang Mao"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Feiyang Xu"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12247v2",
                "updated": "2025-01-03T06:19:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    19,
                    14,
                    4,
                    3,
                    0
                ],
                "published": "2024-10-16T05:17:49Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    5,
                    17,
                    49,
                    2,
                    290,
                    0
                ],
                "title": "EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient MoE Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) model has emerged as a prominent architecture in\nthe field of Large Language Models (LLMs), providing a better balance between\nmodel performance and computational efficiency. However the General Matrix\nMultiply (GEMM) operations and large parameters introduce challenges related to\ncomputational efficiency and communication overhead, which become throughput\nbottlenecks during inference. Applying a single parallelism strategy like EP,\nDP, TP or a straightforward combination of them to MoE usually achieves\nsub-optimal inference throughput. This paper introduces EPS-MoE, a novel expert\npipeline scheduler for MoE that surpasses the existing parallelism schemes. Our\napproach optimizes the computation of MoE FeedForward Network (FFN) modules by\ndynamically selecting the best kernel implementation of GroupGemm and DenseGemm\nfor different loads and adaptively overlapping these computations with\ncommunication, leading to a substantial increase in throughput. Our\nexperimental results demonstrate at most 52.4\\% improvement in prefill\nthroughput compared to existing parallel inference methods. Specifically, our\nmethod accelerated the highly optimized DeepSeekV2 model from a claimed 100K\ntokens per second to at least 120K tokens per second.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) model has emerged as a prominent architecture in\nthe field of Large Language Models (LLMs), providing a better balance between\nmodel performance and computational efficiency. However the General Matrix\nMultiply (GEMM) operations and large parameters introduce challenges related to\ncomputational efficiency and communication overhead, which become throughput\nbottlenecks during inference. Applying a single parallelism strategy like EP,\nDP, TP or a straightforward combination of them to MoE usually achieves\nsub-optimal inference throughput. This paper introduces EPS-MoE, a novel expert\npipeline scheduler for MoE that surpasses the existing parallelism schemes. Our\napproach optimizes the computation of MoE FeedForward Network (FFN) modules by\ndynamically selecting the best kernel implementation of GroupGemm and DenseGemm\nfor different loads and adaptively overlapping these computations with\ncommunication, leading to a substantial increase in throughput. Our\nexperimental results demonstrate at most 52.4\\% improvement in prefill\nthroughput compared to existing parallel inference methods. Specifically, our\nmethod accelerated the highly optimized DeepSeekV2 model from a claimed 100K\ntokens per second to at least 120K tokens per second."
                },
                "authors": [
                    {
                        "name": "Yulei Qian"
                    },
                    {
                        "name": "Fengcun Li"
                    },
                    {
                        "name": "Xiangyang Ji"
                    },
                    {
                        "name": "Xiaoyu Zhao"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17799v2",
                "updated": "2025-01-03T06:15:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    15,
                    58,
                    4,
                    3,
                    0
                ],
                "published": "2024-10-23T11:58:58Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    58,
                    58,
                    2,
                    297,
                    0
                ],
                "title": "OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation"
                },
                "summary": "Full-duplex spoken dialogue systems significantly surpass traditional\nturn-based dialogue systems, as they allow simultaneous bidirectional\ncommunication, closely mirroring human-human interactions. However, achieving\nlow latency and natural interactions in full-duplex dialogue systems remains a\nsignificant challenge, especially considering human conversation dynamics such\nas interruptions, backchannels, and overlapping speech. In this paper, we\nintroduce a novel End-to-End GPT-based model OmniFlatten for full-duplex\nconversation, capable of effectively modeling the complex behaviors inherent to\nnatural conversations with low latency. To achieve full-duplex conversation\ncapabilities, we propose a multi-stage post-training scheme that progressively\nadapts a text large language model (LLM) backbone into a speech-text dialogue\nLLM, capable of generating text and speech in real time, without modifying the\narchitecture of the backbone LLM. The training process comprises three stages:\nmodality alignment, half-duplex dialogue learning, and full-duplex dialogue\nlearning. In all training stages, we standardize the data using a flattening\noperation, which enables unifying the training methods and the GPT backbone\nacross different modalities and tasks. Our approach offers a simple modeling\ntechnique and a promising research direction for developing efficient and\nnatural end-to-end full-duplex spoken dialogue systems. Audio samples of\ndialogues generated by OmniFlatten can be found at this web site\n(https://omniflatten.github.io/).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-duplex spoken dialogue systems significantly surpass traditional\nturn-based dialogue systems, as they allow simultaneous bidirectional\ncommunication, closely mirroring human-human interactions. However, achieving\nlow latency and natural interactions in full-duplex dialogue systems remains a\nsignificant challenge, especially considering human conversation dynamics such\nas interruptions, backchannels, and overlapping speech. In this paper, we\nintroduce a novel End-to-End GPT-based model OmniFlatten for full-duplex\nconversation, capable of effectively modeling the complex behaviors inherent to\nnatural conversations with low latency. To achieve full-duplex conversation\ncapabilities, we propose a multi-stage post-training scheme that progressively\nadapts a text large language model (LLM) backbone into a speech-text dialogue\nLLM, capable of generating text and speech in real time, without modifying the\narchitecture of the backbone LLM. The training process comprises three stages:\nmodality alignment, half-duplex dialogue learning, and full-duplex dialogue\nlearning. In all training stages, we standardize the data using a flattening\noperation, which enables unifying the training methods and the GPT backbone\nacross different modalities and tasks. Our approach offers a simple modeling\ntechnique and a promising research direction for developing efficient and\nnatural end-to-end full-duplex spoken dialogue systems. Audio samples of\ndialogues generated by OmniFlatten can be found at this web site\n(https://omniflatten.github.io/)."
                },
                "authors": [
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Luyao Cheng"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Siqi Zheng"
                    },
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Hai Yu"
                    },
                    {
                        "name": "Chaohong Tan"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Shiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shiliang Zhang"
                },
                "author": "Shiliang Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01652v1",
                "updated": "2025-01-03T06:07:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    7,
                    48,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T06:07:48Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    7,
                    48,
                    4,
                    3,
                    0
                ],
                "title": "MIRAGE: Exploring How Large Language Models Perform in Complex Social\n  Interactive Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: Exploring How Large Language Models Perform in Complex Social\n  Interactive Environments"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nenvironmental perception, reasoning-based decision-making, and simulating\ncomplex human behaviors, particularly in interactive role-playing contexts.\nThis paper introduces the Multiverse Interactive Role-play Ability General\nEvaluation (MIRAGE), a comprehensive framework designed to assess LLMs'\nproficiency in portraying advanced human behaviors through murder mystery\ngames. MIRAGE features eight intricately crafted scripts encompassing diverse\nthemes and styles, providing a rich simulation. To evaluate LLMs' performance,\nMIRAGE employs four distinct methods: the Trust Inclination Index (TII) to\nmeasure dynamics of trust and suspicion, the Clue Investigation Capability\n(CIC) to measure LLMs' capability of conducting information, the Interactivity\nCapability Index (ICI) to assess role-playing capabilities and the Script\nCompliance Index (SCI) to assess LLMs' capability of understanding and\nfollowing instructions. Our experiments indicate that even popular models like\nGPT-4 face significant challenges in navigating the complexities presented by\nthe MIRAGE. The datasets and simulation codes are available in\n\\href{https://github.com/lime728/MIRAGE}{github}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in\nenvironmental perception, reasoning-based decision-making, and simulating\ncomplex human behaviors, particularly in interactive role-playing contexts.\nThis paper introduces the Multiverse Interactive Role-play Ability General\nEvaluation (MIRAGE), a comprehensive framework designed to assess LLMs'\nproficiency in portraying advanced human behaviors through murder mystery\ngames. MIRAGE features eight intricately crafted scripts encompassing diverse\nthemes and styles, providing a rich simulation. To evaluate LLMs' performance,\nMIRAGE employs four distinct methods: the Trust Inclination Index (TII) to\nmeasure dynamics of trust and suspicion, the Clue Investigation Capability\n(CIC) to measure LLMs' capability of conducting information, the Interactivity\nCapability Index (ICI) to assess role-playing capabilities and the Script\nCompliance Index (SCI) to assess LLMs' capability of understanding and\nfollowing instructions. Our experiments indicate that even popular models like\nGPT-4 face significant challenges in navigating the complexities presented by\nthe MIRAGE. The datasets and simulation codes are available in\n\\href{https://github.com/lime728/MIRAGE}{github}."
                },
                "authors": [
                    {
                        "name": "Cai Yin"
                    },
                    {
                        "name": "Gu Zhouhong"
                    },
                    {
                        "name": "Du Zhaohan"
                    },
                    {
                        "name": "Ye Zheyu"
                    },
                    {
                        "name": "Cao Shaosheng"
                    },
                    {
                        "name": "Xu Yiqian"
                    },
                    {
                        "name": "Feng Hongwei"
                    },
                    {
                        "name": "Chen Ping"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ping"
                },
                "author": "Chen Ping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.01048v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.01048v2",
                "updated": "2025-01-03T05:04:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    5,
                    4,
                    20,
                    4,
                    3,
                    0
                ],
                "published": "2022-12-02T09:37:29Z",
                "published_parsed": [
                    2022,
                    12,
                    2,
                    9,
                    37,
                    29,
                    4,
                    336,
                    0
                ],
                "title": "Empirical Asset Pricing via Ensemble Gaussian Process Regression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirical Asset Pricing via Ensemble Gaussian Process Regression"
                },
                "summary": "We introduce an ensemble learning method based on Gaussian Process Regression\n(GPR) for predicting conditional expected stock returns given stock-level and\nmacro-economic information. Our ensemble learning approach significantly\nreduces the computational complexity inherent in GPR inference and lends itself\nto general online learning tasks. We conduct an empirical analysis on a large\ncross-section of US stocks from 1962 to 2016. We find that our method dominates\nexisting machine learning models statistically and economically in terms of\nout-of-sample $R$-squared and Sharpe ratio of prediction-sorted portfolios.\nExploiting the Bayesian nature of GPR, we introduce the mean-variance optimal\nportfolio with respect to the prediction uncertainty distribution of the\nexpected stock returns. It appeals to an uncertainty averse investor and\nsignificantly dominates the equal- and value-weighted prediction-sorted\nportfolios, which outperform the S&P 500.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an ensemble learning method based on Gaussian Process Regression\n(GPR) for predicting conditional expected stock returns given stock-level and\nmacro-economic information. Our ensemble learning approach significantly\nreduces the computational complexity inherent in GPR inference and lends itself\nto general online learning tasks. We conduct an empirical analysis on a large\ncross-section of US stocks from 1962 to 2016. We find that our method dominates\nexisting machine learning models statistically and economically in terms of\nout-of-sample $R$-squared and Sharpe ratio of prediction-sorted portfolios.\nExploiting the Bayesian nature of GPR, we introduce the mean-variance optimal\nportfolio with respect to the prediction uncertainty distribution of the\nexpected stock returns. It appeals to an uncertainty averse investor and\nsignificantly dominates the equal- and value-weighted prediction-sorted\nportfolios, which outperform the S&P 500."
                },
                "authors": [
                    {
                        "name": "Damir Filipović"
                    },
                    {
                        "name": "Puneet Pasricha"
                    }
                ],
                "author_detail": {
                    "name": "Puneet Pasricha"
                },
                "author": "Puneet Pasricha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.01048v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.01048v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.RM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.RM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10819v2",
                "updated": "2025-01-03T04:12:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    4,
                    12,
                    32,
                    4,
                    3,
                    0
                ],
                "published": "2024-08-20T13:13:41Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    13,
                    41,
                    1,
                    233,
                    0
                ],
                "title": "GS-KGC: A Generative Subgraph-based Framework for Knowledge Graph\n  Completion with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GS-KGC: A Generative Subgraph-based Framework for Knowledge Graph\n  Completion with Large Language Models"
                },
                "summary": "Knowledge graph completion (KGC) focuses on identifying missing triples in a\nknowledge graph (KG) , which is crucial for many downstream applications. Given\nthe rapid development of large language models (LLMs), some LLM-based methods\nare proposed for KGC task. However, most of them focus on prompt engineering\nwhile overlooking the fact that finer-grained subgraph information can aid LLMs\nin generating more accurate answers. In this paper, we propose a novel\ncompletion framework called \\textbf{G}enerative \\textbf{S}ubgraph-based KGC\n(GS-KGC), which utilizes subgraph information as contextual reasoning and\nemploys a QA approach to achieve the KGC task. This framework primarily\nincludes a subgraph partitioning algorithm designed to generate negatives and\nneighbors. Specifically, negatives can encourage LLMs to generate a broader\nrange of answers, while neighbors provide additional contextual insights for\nLLM reasoning. Furthermore, we found that GS-KGC can discover potential triples\nwithin the KGs and new facts beyond the KGs. Experiments conducted on four\ncommon KGC datasets highlight the advantages of the proposed GS-KGC, e.g., it\nshows a 5.6\\% increase in Hits@3 compared to the LLM-based model CP-KGC on the\nFB15k-237N, and a 9.3\\% increase over the LLM-based model TECHS on the ICEWS14.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graph completion (KGC) focuses on identifying missing triples in a\nknowledge graph (KG) , which is crucial for many downstream applications. Given\nthe rapid development of large language models (LLMs), some LLM-based methods\nare proposed for KGC task. However, most of them focus on prompt engineering\nwhile overlooking the fact that finer-grained subgraph information can aid LLMs\nin generating more accurate answers. In this paper, we propose a novel\ncompletion framework called \\textbf{G}enerative \\textbf{S}ubgraph-based KGC\n(GS-KGC), which utilizes subgraph information as contextual reasoning and\nemploys a QA approach to achieve the KGC task. This framework primarily\nincludes a subgraph partitioning algorithm designed to generate negatives and\nneighbors. Specifically, negatives can encourage LLMs to generate a broader\nrange of answers, while neighbors provide additional contextual insights for\nLLM reasoning. Furthermore, we found that GS-KGC can discover potential triples\nwithin the KGs and new facts beyond the KGs. Experiments conducted on four\ncommon KGC datasets highlight the advantages of the proposed GS-KGC, e.g., it\nshows a 5.6\\% increase in Hits@3 compared to the LLM-based model CP-KGC on the\nFB15k-237N, and a 9.3\\% increase over the LLM-based model TECHS on the ICEWS14."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Jiahao Zhu"
                    },
                    {
                        "name": "Jianping Man"
                    },
                    {
                        "name": "Hongze Liu"
                    },
                    {
                        "name": "Li Fang"
                    },
                    {
                        "name": "Yi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhou"
                },
                "author": "Yi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01625v1",
                "updated": "2025-01-03T03:46:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    3,
                    46,
                    51,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T03:46:51Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    3,
                    46,
                    51,
                    4,
                    3,
                    0
                ],
                "title": "ICPC: In-context Prompt Compression with Faster Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICPC: In-context Prompt Compression with Faster Inference"
                },
                "summary": "Despite the recent success of Large Language Models (LLMs), it remains\nchallenging to feed LLMs with long prompts due to the fixed size of LLM inputs.\nAs a remedy, prompt compression becomes a promising solution by removing\nredundant tokens in the prompt. However, using LLM in the existing works\nrequires additional computation resources and leads to memory overheads. To\naddress it, we propose ICPC (In-context Prompt Compression), a novel and\nscalable prompt compression method that adaptively reduces the prompt length.\nThe key idea of ICPC is to calculate the probability of each word appearing in\nthe prompt using encoders and calculate information carried by each word\nthrough the information function, which effectively reduces the information\nloss during prompt compression and increases the speed of compression.\nEmpirically, we demonstrate that ICPC can effectively compress long texts of\ndifferent categories and thus achieve better performance and speed on different\ntypes of NLP tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of Large Language Models (LLMs), it remains\nchallenging to feed LLMs with long prompts due to the fixed size of LLM inputs.\nAs a remedy, prompt compression becomes a promising solution by removing\nredundant tokens in the prompt. However, using LLM in the existing works\nrequires additional computation resources and leads to memory overheads. To\naddress it, we propose ICPC (In-context Prompt Compression), a novel and\nscalable prompt compression method that adaptively reduces the prompt length.\nThe key idea of ICPC is to calculate the probability of each word appearing in\nthe prompt using encoders and calculate information carried by each word\nthrough the information function, which effectively reduces the information\nloss during prompt compression and increases the speed of compression.\nEmpirically, we demonstrate that ICPC can effectively compress long texts of\ndifferent categories and thus achieve better performance and speed on different\ntypes of NLP tasks."
                },
                "authors": [
                    {
                        "name": "Ziyang Yu"
                    },
                    {
                        "name": "Yuyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Liu"
                },
                "author": "Yuyu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01028v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01028v2",
                "updated": "2025-01-03T03:16:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    3,
                    16,
                    10,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-02T03:17:51Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    17,
                    51,
                    3,
                    2,
                    0
                ],
                "title": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model"
                },
                "summary": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters."
                },
                "authors": [
                    {
                        "name": "Xinshuo Hu"
                    },
                    {
                        "name": "Zifei Shan"
                    },
                    {
                        "name": "Xinping Zhao"
                    },
                    {
                        "name": "Zetian Sun"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Dongfang Li"
                    },
                    {
                        "name": "Shaolin Ye"
                    },
                    {
                        "name": "Xinyuan Wei"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Technical Report. 23 pages, 6 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01028v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01028v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01610v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01610v1",
                "updated": "2025-01-03T03:09:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    3,
                    9,
                    33,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T03:09:33Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    3,
                    9,
                    33,
                    4,
                    3,
                    0
                ],
                "title": "Bootstrap Nonparametric Inference under Data Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrap Nonparametric Inference under Data Integration"
                },
                "summary": "We propose multiplier bootstrap procedures for nonparametric inference and\nuncertainty quantification of the target mean function, based on a novel\nframework of integrating target and source data. We begin with the relatively\neasier covariate shift scenario with equal target and source mean functions and\npropose estimation and inferential procedures through a straightforward\ncombination of all target and source datasets. We next consider the more\ngeneral and flexible distribution shift scenario with arbitrary target and\nsource mean functions, and propose a two-step inferential procedure. First, we\nestimate the target-to-source differences based on separate portions of the\ntarget and source data. Second, the remaining source data are adjusted by these\ndifferences and combined with the remaining target data to perform the\nmultiplier bootstrap procedure. Our method enables local and global inference\non the target mean function without using asymptotic distributions. To justify\nour approach, we derive an optimal convergence rate for the nonparametric\nestimator and establish bootstrap consistency to estimate the asymptotic\ndistribution of the nonparametric estimator. The proof of global bootstrap\nconsistency involves a central limit theorem for quadratic forms with dependent\nvariables under a conditional probability measure. Our method applies to\narbitrary source and target datasets, provided that the data sizes meet a\nspecific quantitative relationship. Simulation studies and real data analysis\nare provided to examine the performance of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose multiplier bootstrap procedures for nonparametric inference and\nuncertainty quantification of the target mean function, based on a novel\nframework of integrating target and source data. We begin with the relatively\neasier covariate shift scenario with equal target and source mean functions and\npropose estimation and inferential procedures through a straightforward\ncombination of all target and source datasets. We next consider the more\ngeneral and flexible distribution shift scenario with arbitrary target and\nsource mean functions, and propose a two-step inferential procedure. First, we\nestimate the target-to-source differences based on separate portions of the\ntarget and source data. Second, the remaining source data are adjusted by these\ndifferences and combined with the remaining target data to perform the\nmultiplier bootstrap procedure. Our method enables local and global inference\non the target mean function without using asymptotic distributions. To justify\nour approach, we derive an optimal convergence rate for the nonparametric\nestimator and establish bootstrap consistency to estimate the asymptotic\ndistribution of the nonparametric estimator. The proof of global bootstrap\nconsistency involves a central limit theorem for quadratic forms with dependent\nvariables under a conditional probability measure. Our method applies to\narbitrary source and target datasets, provided that the data sizes meet a\nspecific quantitative relationship. Simulation studies and real data analysis\nare provided to examine the performance of our approach."
                },
                "authors": [
                    {
                        "name": "Zuofeng Shang"
                    },
                    {
                        "name": "Peijun Sang"
                    },
                    {
                        "name": "Chong Jin"
                    }
                ],
                "author_detail": {
                    "name": "Chong Jin"
                },
                "author": "Chong Jin",
                "arxiv_comment": "29 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01610v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01610v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14674v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14674v4",
                "updated": "2025-01-03T03:03:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    3,
                    3,
                    24,
                    4,
                    3,
                    0
                ],
                "published": "2024-11-22T02:15:38Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    2,
                    15,
                    38,
                    4,
                    327,
                    0
                ],
                "title": "Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal\n  Transport Metrics for Gaussian Mixtures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal\n  Transport Metrics for Gaussian Mixtures"
                },
                "summary": "Existing methods to summarize posterior inference for mixture models focus on\nidentifying a point estimate of the implied random partition for clustering,\nwith density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et\nal., 2022). We propose a novel approach for summarizing posterior inference in\nnonparametric Bayesian mixture models, prioritizing density estimation of the\nmixing measure (or mixture) as an inference target. One of the key features is\nthe model-agnostic nature of the approach, which remains valid under\narbitrarily complex dependence structures in the underlying sampling model.\nUsing a decision-theoretic framework, our method identifies a point estimate by\nminimizing posterior expected loss. A loss function is defined as a discrepancy\nbetween mixing measures. Estimating the mixing measure implies inference on the\nmixture density and the random partition. Exploiting the discrete nature of the\nmixing measure, we use a version of sliced Wasserstein distance. We introduce\ntwo specific variants for Gaussian mixtures. The first, mixed sliced\nWasserstein, applies generalized geodesic projections on the product of the\nEuclidean space and the manifold of symmetric positive definite matrices. The\nsecond, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture\nmeasures for efficient projection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing methods to summarize posterior inference for mixture models focus on\nidentifying a point estimate of the implied random partition for clustering,\nwith density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et\nal., 2022). We propose a novel approach for summarizing posterior inference in\nnonparametric Bayesian mixture models, prioritizing density estimation of the\nmixing measure (or mixture) as an inference target. One of the key features is\nthe model-agnostic nature of the approach, which remains valid under\narbitrarily complex dependence structures in the underlying sampling model.\nUsing a decision-theoretic framework, our method identifies a point estimate by\nminimizing posterior expected loss. A loss function is defined as a discrepancy\nbetween mixing measures. Estimating the mixing measure implies inference on the\nmixture density and the random partition. Exploiting the discrete nature of the\nmixing measure, we use a version of sliced Wasserstein distance. We introduce\ntwo specific variants for Gaussian mixtures. The first, mixed sliced\nWasserstein, applies generalized geodesic projections on the product of the\nEuclidean space and the manifold of symmetric positive definite matrices. The\nsecond, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture\nmeasures for efficient projection."
                },
                "authors": [
                    {
                        "name": "Khai Nguyen"
                    },
                    {
                        "name": "Peter Mueller"
                    }
                ],
                "author_detail": {
                    "name": "Peter Mueller"
                },
                "author": "Peter Mueller",
                "arxiv_comment": "38 pages, 2 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14674v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14674v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20840v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20840v3",
                "updated": "2025-01-03T02:59:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    2,
                    59,
                    2,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-30T10:09:51Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    10,
                    9,
                    51,
                    0,
                    365,
                    0
                ],
                "title": "Identifying average causal effect in regression discontinuity design\n  with auxiliary data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying average causal effect in regression discontinuity design\n  with auxiliary data"
                },
                "summary": "Regression discontinuity designs are widely used when treatment assignment is\ndetermined by whether a running variable exceeds a predefined threshold.\nHowever, most research focuses on estimating local causal effects at the\nthreshold, leaving the challenge of identifying treatment effects away from the\ncutoff largely unaddressed. The primary difficulty in this context is that the\ntreatment assignment is deterministically defined by the running variable,\nviolating the commonly assumed positivity assumption. In this paper, we\nintroduce a novel framework for identifying the average causal effect in\nregression discontinuity designs. Our approach assumes the existence of an\nauxiliary variable for which the running variable can be seen as a surrogate,\nand an additional dataset that consists of the running variable and the\nauxiliary variable alongside the traditional regression discontinuity design\nsetup. Under this framework, we propose three estimation methods for the ATE,\nwhich resembles the outcome regression, inverse propensity weighted and doubly\nrobust estimators in classical causal inference literature. Asymptotically\nvalid inference procedures are also provided. To demonstrate the practical\napplication of our method, simulations are conducted to show the good\nperformance of our methods; besides, we use the proposed methods to assess the\ncausal effects of vitamin A supplementation on the severity of autism spectrum\ndisorders in children, where a positive effect is found but with no statistical\nsignificance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regression discontinuity designs are widely used when treatment assignment is\ndetermined by whether a running variable exceeds a predefined threshold.\nHowever, most research focuses on estimating local causal effects at the\nthreshold, leaving the challenge of identifying treatment effects away from the\ncutoff largely unaddressed. The primary difficulty in this context is that the\ntreatment assignment is deterministically defined by the running variable,\nviolating the commonly assumed positivity assumption. In this paper, we\nintroduce a novel framework for identifying the average causal effect in\nregression discontinuity designs. Our approach assumes the existence of an\nauxiliary variable for which the running variable can be seen as a surrogate,\nand an additional dataset that consists of the running variable and the\nauxiliary variable alongside the traditional regression discontinuity design\nsetup. Under this framework, we propose three estimation methods for the ATE,\nwhich resembles the outcome regression, inverse propensity weighted and doubly\nrobust estimators in classical causal inference literature. Asymptotically\nvalid inference procedures are also provided. To demonstrate the practical\napplication of our method, simulations are conducted to show the good\nperformance of our methods; besides, we use the proposed methods to assess the\ncausal effects of vitamin A supplementation on the severity of autism spectrum\ndisorders in children, where a positive effect is found but with no statistical\nsignificance."
                },
                "authors": [
                    {
                        "name": "Xinqin Feng"
                    },
                    {
                        "name": "Wenjie Hu"
                    },
                    {
                        "name": "Pu Yang"
                    },
                    {
                        "name": "Tingyu Li"
                    },
                    {
                        "name": "Xiao-Hua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Hua Zhou"
                },
                "author": "Xiao-Hua Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20840v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20840v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00430v2",
                "updated": "2025-01-03T02:50:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    2,
                    50,
                    59,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-31T13:11:20Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    13,
                    11,
                    20,
                    1,
                    366,
                    0
                ],
                "title": "Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and\n  Reflection agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and\n  Reflection agents"
                },
                "summary": "Agents have demonstrated their potential in scientific reasoning tasks\nthrough large language models. However, they often face challenges such as\ninsufficient accuracy and degeneration of thought when handling complex\nreasoning tasks, which impede their performance. To overcome these issues, we\npropose the Reactive and Reflection agents with Multi-Path Reasoning (RR-MP)\nFramework, aimed at enhancing the reasoning capabilities of LLMs. Our approach\nimproves scientific reasoning accuracy by employing a multi-path reasoning\nmechanism where each path consists of a reactive agent and a reflection agent\nthat collaborate to prevent degeneration of thought inherent in single-agent\nreliance. Additionally, the RR-MP framework does not require additional\ntraining; it utilizes multiple dialogue instances for each reasoning path and a\nseparate summarizer to consolidate insights from all paths. This design\nintegrates diverse perspectives and strengthens reasoning across each path. We\nconducted zero-shot and few-shot evaluations on tasks involving moral\nscenarios, college-level physics, and mathematics. Experimental results\ndemonstrate that our method outperforms baseline approaches, highlighting the\neffectiveness and advantages of the RR-MP framework in managing complex\nscientific reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents have demonstrated their potential in scientific reasoning tasks\nthrough large language models. However, they often face challenges such as\ninsufficient accuracy and degeneration of thought when handling complex\nreasoning tasks, which impede their performance. To overcome these issues, we\npropose the Reactive and Reflection agents with Multi-Path Reasoning (RR-MP)\nFramework, aimed at enhancing the reasoning capabilities of LLMs. Our approach\nimproves scientific reasoning accuracy by employing a multi-path reasoning\nmechanism where each path consists of a reactive agent and a reflection agent\nthat collaborate to prevent degeneration of thought inherent in single-agent\nreliance. Additionally, the RR-MP framework does not require additional\ntraining; it utilizes multiple dialogue instances for each reasoning path and a\nseparate summarizer to consolidate insights from all paths. This design\nintegrates diverse perspectives and strengthens reasoning across each path. We\nconducted zero-shot and few-shot evaluations on tasks involving moral\nscenarios, college-level physics, and mathematics. Experimental results\ndemonstrate that our method outperforms baseline approaches, highlighting the\neffectiveness and advantages of the RR-MP framework in managing complex\nscientific reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Chengbo He"
                    },
                    {
                        "name": "Bochao Zou"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Jiansheng Chen"
                    },
                    {
                        "name": "Junliang Xing"
                    },
                    {
                        "name": "Huimin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Huimin Ma"
                },
                "author": "Huimin Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02318v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02318v3",
                "updated": "2025-01-03T02:19:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    2,
                    19,
                    3,
                    4,
                    3,
                    0
                ],
                "published": "2024-11-04T17:44:11Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    44,
                    11,
                    0,
                    309,
                    0
                ],
                "title": "Evaluating the Ability of Large Language Models to Generate Verifiable\n  Specifications in VeriFast",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Ability of Large Language Models to Generate Verifiable\n  Specifications in VeriFast"
                },
                "summary": "Static verification is a powerful method for enhancing software quality, but\nit demands significant human labor and resources. This is particularly true of\nstatic verifiers that reason about heap manipulating programs using an\nownership logic. LLMs have shown promise in a number of software engineering\nactivities, including code generation, test generation, proof generation for\ntheorem provers, and specification generation for static verifiers. However,\nprior work has not explored how well LLMs can perform specification generation\nfor specifications based in an ownership logic, such as separation logic. To\naddress this gap, this paper explores OpenAI's GPT-4o model's effectiveness in\ngenerating specifications on C programs that are verifiable with VeriFast, a\nseparation logic based static verifier. Our experiment employs three different\ntypes of user inputs as well as basic and Chain-of-Thought (CoT) prompting to\nassess GPT's capabilities. Our results indicate that the specifications\ngenerated by GPT-4o preserve functional behavior, but struggle to be\nverifiable. When the specifications are verifiable they contain redundancies.\nFuture directions are discussed to improve the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static verification is a powerful method for enhancing software quality, but\nit demands significant human labor and resources. This is particularly true of\nstatic verifiers that reason about heap manipulating programs using an\nownership logic. LLMs have shown promise in a number of software engineering\nactivities, including code generation, test generation, proof generation for\ntheorem provers, and specification generation for static verifiers. However,\nprior work has not explored how well LLMs can perform specification generation\nfor specifications based in an ownership logic, such as separation logic. To\naddress this gap, this paper explores OpenAI's GPT-4o model's effectiveness in\ngenerating specifications on C programs that are verifiable with VeriFast, a\nseparation logic based static verifier. Our experiment employs three different\ntypes of user inputs as well as basic and Chain-of-Thought (CoT) prompting to\nassess GPT's capabilities. Our results indicate that the specifications\ngenerated by GPT-4o preserve functional behavior, but struggle to be\nverifiable. When the specifications are verifiable they contain redundancies.\nFuture directions are discussed to improve the performance."
                },
                "authors": [
                    {
                        "name": "Wen Fan"
                    },
                    {
                        "name": "Marilyn Rego"
                    },
                    {
                        "name": "Xin Hu"
                    },
                    {
                        "name": "Sanya Dod"
                    },
                    {
                        "name": "Zhaorui Ni"
                    },
                    {
                        "name": "Danning Xie"
                    },
                    {
                        "name": "Jenna DiVincenzo"
                    },
                    {
                        "name": "Lin Tan"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tan"
                },
                "author": "Lin Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02318v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02318v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15115v2",
                "updated": "2025-01-03T02:18:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    2,
                    18,
                    21,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-19T17:56:09Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    56,
                    9,
                    3,
                    354,
                    0
                ],
                "title": "Qwen2.5 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen2.5 Technical Report"
                },
                "summary": "In this report, we introduce Qwen2.5, a comprehensive series of large\nlanguage models (LLMs) designed to meet diverse needs. Compared to previous\niterations, Qwen 2.5 has been significantly improved during both the\npre-training and post-training stages. In terms of pre-training, we have scaled\nthe high-quality pre-training datasets from the previous 7 trillion tokens to\n18 trillion tokens. This provides a strong foundation for common sense, expert\nknowledge, and reasoning capabilities. In terms of post-training, we implement\nintricate supervised finetuning with over 1 million samples, as well as\nmultistage reinforcement learning. Post-training techniques enhance human\npreference, and notably improve long text generation, structural data analysis,\nand instruction following. To handle diverse and varied use cases effectively,\nwe present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base\nand instruction-tuned models, with quantized versions available. In addition,\nfor hosted solutions, the proprietary models currently include two\nmixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both\navailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier\nperformance on a wide range of benchmarks evaluating language understanding,\nreasoning, mathematics, coding, human preference alignment, etc. Specifically,\nthe open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and\nproprietary models and demonstrates competitive performance to the\nstate-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5\ntimes larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness\nwhile performing competitively against GPT-4o-mini and GPT-4o respectively.\nAdditionally, as the foundation, Qwen2.5 models have been instrumental in\ntraining specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and\nmultimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we introduce Qwen2.5, a comprehensive series of large\nlanguage models (LLMs) designed to meet diverse needs. Compared to previous\niterations, Qwen 2.5 has been significantly improved during both the\npre-training and post-training stages. In terms of pre-training, we have scaled\nthe high-quality pre-training datasets from the previous 7 trillion tokens to\n18 trillion tokens. This provides a strong foundation for common sense, expert\nknowledge, and reasoning capabilities. In terms of post-training, we implement\nintricate supervised finetuning with over 1 million samples, as well as\nmultistage reinforcement learning. Post-training techniques enhance human\npreference, and notably improve long text generation, structural data analysis,\nand instruction following. To handle diverse and varied use cases effectively,\nwe present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base\nand instruction-tuned models, with quantized versions available. In addition,\nfor hosted solutions, the proprietary models currently include two\nmixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both\navailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier\nperformance on a wide range of benchmarks evaluating language understanding,\nreasoning, mathematics, coding, human preference alignment, etc. Specifically,\nthe open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and\nproprietary models and demonstrates competitive performance to the\nstate-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5\ntimes larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness\nwhile performing competitively against GPT-4o-mini and GPT-4o respectively.\nAdditionally, as the foundation, Qwen2.5 models have been instrumental in\ntraining specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and\nmultimodal models."
                },
                "authors": [
                    {
                        "name": "Qwen"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Chengyuan Li"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Huan Lin"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jianhong Tu"
                    },
                    {
                        "name": "Jianwei Zhang"
                    },
                    {
                        "name": "Jianxin Yang"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Kai Dang"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Keqin Bao"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Le Yu"
                    },
                    {
                        "name": "Mei Li"
                    },
                    {
                        "name": "Mingfeng Xue"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Qin Zhu"
                    },
                    {
                        "name": "Rui Men"
                    },
                    {
                        "name": "Runji Lin"
                    },
                    {
                        "name": "Tianhao Li"
                    },
                    {
                        "name": "Tianyi Tang"
                    },
                    {
                        "name": "Tingyu Xia"
                    },
                    {
                        "name": "Xingzhang Ren"
                    },
                    {
                        "name": "Xuancheng Ren"
                    },
                    {
                        "name": "Yang Fan"
                    },
                    {
                        "name": "Yang Su"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Yu Wan"
                    },
                    {
                        "name": "Yuqiong Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Zihan Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Qiu"
                },
                "arxiv_affiliation": "additional authors not shown",
                "author": "Zihan Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10622v2",
                "updated": "2025-01-03T02:12:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    2,
                    12,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-14T00:05:42Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    0,
                    5,
                    42,
                    5,
                    349,
                    0
                ],
                "title": "A recent evaluation on the performance of LLMs on radiation oncology\n  physics using questions of randomly shuffled options",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent evaluation on the performance of LLMs on radiation oncology\n  physics using questions of randomly shuffled options"
                },
                "summary": "Purpose: We present an updated study evaluating the performance of large\nlanguage models (LLMs) in answering radiation oncology physics questions,\nfocusing on the recently released models.\n  Methods: A set of 100 multiple choice radiation oncology physics questions,\npreviously created by a well-experienced physicist, was used for this study.\nThe answer options of the questions were randomly shuffled to create \"new\" exam\nsets. Five LLMs (OpenAI o1-preview, GPT-4o, LLaMA 3.1 (405B), Gemini 1.5 Pro,\nand Claude 3.5 Sonnet) with the versions released before September 30, 2024,\nwere queried using these new exam sets. To evaluate their deductive reasoning\ncapabilities, the correct answers in the questions were replaced with \"None of\nthe above.\" Then, the explaining-first and step-by-step instruction prompts\nwere used to test if this strategy improved their reasoning capabilities. The\nperformance of the LLMs was compared with the answers from medical physicists.\n  Results: All models demonstrated expert-level performance on these questions,\nwith o1-preview even surpassing medical physicists with a majority vote. When\nreplacing the correct answers with \"None of the above,\" all models exhibited a\nconsiderable decline in performance, suggesting room for improvement. The\nexplaining-first and step-by-step instruction prompts helped enhance the\nreasoning capabilities of the LLaMA 3.1 (405B), Gemini 1.5 Pro, and Claude 3.5\nSonnet models.\n  Conclusion: These recently released LLMs demonstrated expert-level\nperformance in answering radiation oncology physics questions, exhibiting great\npotential to assist in radiation oncology physics training and education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: We present an updated study evaluating the performance of large\nlanguage models (LLMs) in answering radiation oncology physics questions,\nfocusing on the recently released models.\n  Methods: A set of 100 multiple choice radiation oncology physics questions,\npreviously created by a well-experienced physicist, was used for this study.\nThe answer options of the questions were randomly shuffled to create \"new\" exam\nsets. Five LLMs (OpenAI o1-preview, GPT-4o, LLaMA 3.1 (405B), Gemini 1.5 Pro,\nand Claude 3.5 Sonnet) with the versions released before September 30, 2024,\nwere queried using these new exam sets. To evaluate their deductive reasoning\ncapabilities, the correct answers in the questions were replaced with \"None of\nthe above.\" Then, the explaining-first and step-by-step instruction prompts\nwere used to test if this strategy improved their reasoning capabilities. The\nperformance of the LLMs was compared with the answers from medical physicists.\n  Results: All models demonstrated expert-level performance on these questions,\nwith o1-preview even surpassing medical physicists with a majority vote. When\nreplacing the correct answers with \"None of the above,\" all models exhibited a\nconsiderable decline in performance, suggesting room for improvement. The\nexplaining-first and step-by-step instruction prompts helped enhance the\nreasoning capabilities of the LLaMA 3.1 (405B), Gemini 1.5 Pro, and Claude 3.5\nSonnet models.\n  Conclusion: These recently released LLMs demonstrated expert-level\nperformance in answering radiation oncology physics questions, exhibiting great\npotential to assist in radiation oncology physics training and education."
                },
                "authors": [
                    {
                        "name": "Peilong Wang"
                    },
                    {
                        "name": "Jason Holmes"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Dequan Chen"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Jiajian Shen"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02642v2",
                "updated": "2025-01-03T02:00:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    2,
                    0,
                    1,
                    4,
                    3,
                    0
                ],
                "published": "2024-06-04T10:59:43Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    10,
                    59,
                    43,
                    1,
                    156,
                    0
                ],
                "title": "E-ICL: Enhancing Fine-Grained Emotion Recognition through the Lens of\n  Prototype Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-ICL: Enhancing Fine-Grained Emotion Recognition through the Lens of\n  Prototype Theory"
                },
                "summary": "In-context learning (ICL) achieves remarkable performance in various domains\nsuch as knowledge acquisition, commonsense reasoning, and semantic\nunderstanding. However, its performance significantly deteriorates for emotion\ndetection tasks, especially fine-grained emotion recognition. The underlying\nreasons for this remain unclear. In this paper, we identify the reasons behind\nICL's poor performance from the perspective of prototype theory and propose a\nmethod to address this issue. Specifically, we conduct extensive pilot\nexperiments and find that ICL conforms to the prototype theory on fine-grained\nemotion recognition. Based on this theory, we uncover the following\ndeficiencies in ICL: (1) It relies on prototypes (example-label pairs) that are\nsemantically similar but emotionally inaccurate to predict emotions. (2) It is\nprone to interference from irrelevant categories, affecting the accuracy and\nrobustness of the predictions. To address these issues, we propose an Emotion\nContext Learning method (E-ICL) on fine-grained emotion recognition. E-ICL\nrelies on more emotionally accurate prototypes to predict categories by\nreferring to emotionally similar examples with dynamic labels. Simultaneously,\nE-ICL employs an exclusionary emotion prediction strategy to avoid interference\nfrom irrelevant categories, thereby increasing its accuracy and robustness.\nNote that the entire process is accomplished with the assistance of a\nplug-and-play emotion auxiliary model, without additional training. Experiments\non the fine-grained emotion datasets EDOS, Empathetic-Dialogues,\nEmpatheticIntent, and GoEmotions show that E-ICL achieves superior emotion\nprediction performance. Furthermore, even when the emotion auxiliary model used\nis lower than 10% of the LLMs, E-ICL can still boost the performance of LLMs by\nover 4% on multiple datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) achieves remarkable performance in various domains\nsuch as knowledge acquisition, commonsense reasoning, and semantic\nunderstanding. However, its performance significantly deteriorates for emotion\ndetection tasks, especially fine-grained emotion recognition. The underlying\nreasons for this remain unclear. In this paper, we identify the reasons behind\nICL's poor performance from the perspective of prototype theory and propose a\nmethod to address this issue. Specifically, we conduct extensive pilot\nexperiments and find that ICL conforms to the prototype theory on fine-grained\nemotion recognition. Based on this theory, we uncover the following\ndeficiencies in ICL: (1) It relies on prototypes (example-label pairs) that are\nsemantically similar but emotionally inaccurate to predict emotions. (2) It is\nprone to interference from irrelevant categories, affecting the accuracy and\nrobustness of the predictions. To address these issues, we propose an Emotion\nContext Learning method (E-ICL) on fine-grained emotion recognition. E-ICL\nrelies on more emotionally accurate prototypes to predict categories by\nreferring to emotionally similar examples with dynamic labels. Simultaneously,\nE-ICL employs an exclusionary emotion prediction strategy to avoid interference\nfrom irrelevant categories, thereby increasing its accuracy and robustness.\nNote that the entire process is accomplished with the assistance of a\nplug-and-play emotion auxiliary model, without additional training. Experiments\non the fine-grained emotion datasets EDOS, Empathetic-Dialogues,\nEmpatheticIntent, and GoEmotions show that E-ICL achieves superior emotion\nprediction performance. Furthermore, even when the emotion auxiliary model used\nis lower than 10% of the LLMs, E-ICL can still boost the performance of LLMs by\nover 4% on multiple datasets."
                },
                "authors": [
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "Chenglong Ye"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Haizhou Sun"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Xiaofei Zhu"
                    },
                    {
                        "name": "Yunbing Wu"
                    },
                    {
                        "name": "Xiangwen Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangwen Liao"
                },
                "author": "Xiangwen Liao",
                "arxiv_comment": "16 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15221v2",
                "updated": "2025-01-03T01:55:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    1,
                    55,
                    35,
                    4,
                    3,
                    0
                ],
                "published": "2024-11-20T23:08:01Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    23,
                    8,
                    1,
                    2,
                    325,
                    0
                ],
                "title": "Reflections from the 2024 Large Language Model (LLM) Hackathon for\n  Applications in Materials Science and Chemistry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflections from the 2024 Large Language Model (LLM) Hackathon for\n  Applications in Materials Science and Chemistry"
                },
                "summary": "Here, we present the outcomes from the second Large Language Model (LLM)\nHackathon for Applications in Materials Science and Chemistry, which engaged\nparticipants across global hybrid locations, resulting in 34 team submissions.\nThe submissions spanned seven key application areas and demonstrated the\ndiverse utility of LLMs for applications in (1) molecular and material property\nprediction; (2) molecular and material design; (3) automation and novel\ninterfaces; (4) scientific communication and education; (5) research data\nmanagement and automation; (6) hypothesis generation and evaluation; and (7)\nknowledge extraction and reasoning from scientific literature. Each team\nsubmission is presented in a summary table with links to the code and as brief\npapers in the appendix. Beyond team results, we discuss the hackathon event and\nits hybrid format, which included physical hubs in Toronto, Montreal, San\nFrancisco, Berlin, Lausanne, and Tokyo, alongside a global online hub to enable\nlocal and virtual collaboration. Overall, the event highlighted significant\nimprovements in LLM capabilities since the previous year's hackathon,\nsuggesting continued expansion of LLMs for applications in materials science\nand chemistry research. These outcomes demonstrate the dual utility of LLMs as\nboth multipurpose models for diverse machine learning tasks and platforms for\nrapid prototyping custom applications in scientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Here, we present the outcomes from the second Large Language Model (LLM)\nHackathon for Applications in Materials Science and Chemistry, which engaged\nparticipants across global hybrid locations, resulting in 34 team submissions.\nThe submissions spanned seven key application areas and demonstrated the\ndiverse utility of LLMs for applications in (1) molecular and material property\nprediction; (2) molecular and material design; (3) automation and novel\ninterfaces; (4) scientific communication and education; (5) research data\nmanagement and automation; (6) hypothesis generation and evaluation; and (7)\nknowledge extraction and reasoning from scientific literature. Each team\nsubmission is presented in a summary table with links to the code and as brief\npapers in the appendix. Beyond team results, we discuss the hackathon event and\nits hybrid format, which included physical hubs in Toronto, Montreal, San\nFrancisco, Berlin, Lausanne, and Tokyo, alongside a global online hub to enable\nlocal and virtual collaboration. Overall, the event highlighted significant\nimprovements in LLM capabilities since the previous year's hackathon,\nsuggesting continued expansion of LLMs for applications in materials science\nand chemistry research. These outcomes demonstrate the dual utility of LLMs as\nboth multipurpose models for diverse machine learning tasks and platforms for\nrapid prototyping custom applications in scientific research."
                },
                "authors": [
                    {
                        "name": "Yoel Zimmermann"
                    },
                    {
                        "name": "Adib Bazgir"
                    },
                    {
                        "name": "Zartashia Afzal"
                    },
                    {
                        "name": "Fariha Agbere"
                    },
                    {
                        "name": "Qianxiang Ai"
                    },
                    {
                        "name": "Nawaf Alampara"
                    },
                    {
                        "name": "Alexander Al-Feghali"
                    },
                    {
                        "name": "Mehrad Ansari"
                    },
                    {
                        "name": "Dmytro Antypov"
                    },
                    {
                        "name": "Amro Aswad"
                    },
                    {
                        "name": "Jiaru Bai"
                    },
                    {
                        "name": "Viktoriia Baibakova"
                    },
                    {
                        "name": "Devi Dutta Biswajeet"
                    },
                    {
                        "name": "Erik Bitzek"
                    },
                    {
                        "name": "Joshua D. Bocarsly"
                    },
                    {
                        "name": "Anna Borisova"
                    },
                    {
                        "name": "Andres M Bran"
                    },
                    {
                        "name": "L. Catherine Brinson"
                    },
                    {
                        "name": "Marcel Moran Calderon"
                    },
                    {
                        "name": "Alessandro Canalicchio"
                    },
                    {
                        "name": "Victor Chen"
                    },
                    {
                        "name": "Yuan Chiang"
                    },
                    {
                        "name": "Defne Circi"
                    },
                    {
                        "name": "Benjamin Charmes"
                    },
                    {
                        "name": "Vikrant Chaudhary"
                    },
                    {
                        "name": "Zizhang Chen"
                    },
                    {
                        "name": "Min-Hsueh Chiu"
                    },
                    {
                        "name": "Judith Clymo"
                    },
                    {
                        "name": "Kedar Dabhadkar"
                    },
                    {
                        "name": "Nathan Daelman"
                    },
                    {
                        "name": "Archit Datar"
                    },
                    {
                        "name": "Wibe A. de Jong"
                    },
                    {
                        "name": "Matthew L. Evans"
                    },
                    {
                        "name": "Maryam Ghazizade Fard"
                    },
                    {
                        "name": "Giuseppe Fisicaro"
                    },
                    {
                        "name": "Abhijeet Sadashiv Gangan"
                    },
                    {
                        "name": "Janine George"
                    },
                    {
                        "name": "Jose D. Cojal Gonzalez"
                    },
                    {
                        "name": "Michael Götte"
                    },
                    {
                        "name": "Ankur K. Gupta"
                    },
                    {
                        "name": "Hassan Harb"
                    },
                    {
                        "name": "Pengyu Hong"
                    },
                    {
                        "name": "Abdelrahman Ibrahim"
                    },
                    {
                        "name": "Ahmed Ilyas"
                    },
                    {
                        "name": "Alishba Imran"
                    },
                    {
                        "name": "Kevin Ishimwe"
                    },
                    {
                        "name": "Ramsey Issa"
                    },
                    {
                        "name": "Kevin Maik Jablonka"
                    },
                    {
                        "name": "Colin Jones"
                    },
                    {
                        "name": "Tyler R. Josephson"
                    },
                    {
                        "name": "Greg Juhasz"
                    },
                    {
                        "name": "Sarthak Kapoor"
                    },
                    {
                        "name": "Rongda Kang"
                    },
                    {
                        "name": "Ghazal Khalighinejad"
                    },
                    {
                        "name": "Sartaaj Khan"
                    },
                    {
                        "name": "Sascha Klawohn"
                    },
                    {
                        "name": "Suneel Kuman"
                    },
                    {
                        "name": "Alvin Noe Ladines"
                    },
                    {
                        "name": "Sarom Leang"
                    },
                    {
                        "name": "Magdalena Lederbauer"
                    },
                    {
                        "name": "Sheng-Lun"
                    },
                    {
                        "name": "Liao"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Xuefeng Liu"
                    },
                    {
                        "name": "Stanley Lo"
                    },
                    {
                        "name": "Sandeep Madireddy"
                    },
                    {
                        "name": "Piyush Ranjan Maharana"
                    },
                    {
                        "name": "Shagun Maheshwari"
                    },
                    {
                        "name": "Soroush Mahjoubi"
                    },
                    {
                        "name": "José A. Márquez"
                    },
                    {
                        "name": "Rob Mills"
                    },
                    {
                        "name": "Trupti Mohanty"
                    },
                    {
                        "name": "Bernadette Mohr"
                    },
                    {
                        "name": "Seyed Mohamad Moosavi"
                    },
                    {
                        "name": "Alexander Moßhammer"
                    },
                    {
                        "name": "Amirhossein D. Naghdi"
                    },
                    {
                        "name": "Aakash Naik"
                    },
                    {
                        "name": "Oleksandr Narykov"
                    },
                    {
                        "name": "Hampus Näsström"
                    },
                    {
                        "name": "Xuan Vu Nguyen"
                    },
                    {
                        "name": "Xinyi Ni"
                    },
                    {
                        "name": "Dana O'Connor"
                    },
                    {
                        "name": "Teslim Olayiwola"
                    },
                    {
                        "name": "Federico Ottomano"
                    },
                    {
                        "name": "Aleyna Beste Ozhan"
                    },
                    {
                        "name": "Sebastian Pagel"
                    },
                    {
                        "name": "Chiku Parida"
                    },
                    {
                        "name": "Jaehee Park"
                    },
                    {
                        "name": "Vraj Patel"
                    },
                    {
                        "name": "Elena Patyukova"
                    },
                    {
                        "name": "Martin Hoffmann Petersen"
                    },
                    {
                        "name": "Luis Pinto"
                    },
                    {
                        "name": "José M. Pizarro"
                    },
                    {
                        "name": "Dieter Plessers"
                    },
                    {
                        "name": "Tapashree Pradhan"
                    },
                    {
                        "name": "Utkarsh Pratiush"
                    },
                    {
                        "name": "Charishma Puli"
                    },
                    {
                        "name": "Andrew Qin"
                    },
                    {
                        "name": "Mahyar Rajabi"
                    },
                    {
                        "name": "Francesco Ricci"
                    },
                    {
                        "name": "Elliot Risch"
                    },
                    {
                        "name": "Martiño Ríos-García"
                    },
                    {
                        "name": "Aritra Roy"
                    },
                    {
                        "name": "Tehseen Rug"
                    },
                    {
                        "name": "Hasan M Sayeed"
                    },
                    {
                        "name": "Markus Scheidgen"
                    },
                    {
                        "name": "Mara Schilling-Wilhelmi"
                    },
                    {
                        "name": "Marcel Schloz"
                    },
                    {
                        "name": "Fabian Schöppach"
                    },
                    {
                        "name": "Julia Schumann"
                    },
                    {
                        "name": "Philippe Schwaller"
                    },
                    {
                        "name": "Marcus Schwarting"
                    },
                    {
                        "name": "Samiha Sharlin"
                    },
                    {
                        "name": "Kevin Shen"
                    },
                    {
                        "name": "Jiale Shi"
                    },
                    {
                        "name": "Pradip Si"
                    },
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Taylor Sparks"
                    },
                    {
                        "name": "Suraj Sudhakar"
                    },
                    {
                        "name": "Leopold Talirz"
                    },
                    {
                        "name": "Dandan Tang"
                    },
                    {
                        "name": "Olga Taran"
                    },
                    {
                        "name": "Carla Terboven"
                    },
                    {
                        "name": "Mark Tropin"
                    },
                    {
                        "name": "Anastasiia Tsymbal"
                    },
                    {
                        "name": "Katharina Ueltzen"
                    },
                    {
                        "name": "Pablo Andres Unzueta"
                    },
                    {
                        "name": "Archit Vasan"
                    },
                    {
                        "name": "Tirtha Vinchurkar"
                    },
                    {
                        "name": "Trung Vo"
                    },
                    {
                        "name": "Gabriel Vogel"
                    },
                    {
                        "name": "Christoph Völker"
                    },
                    {
                        "name": "Jan Weinreich"
                    },
                    {
                        "name": "Faradawn Yang"
                    },
                    {
                        "name": "Mohd Zaki"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Sylvester Zhang"
                    },
                    {
                        "name": "Weijie Zhang"
                    },
                    {
                        "name": "Ruijie Zhu"
                    },
                    {
                        "name": "Shang Zhu"
                    },
                    {
                        "name": "Jan Janssen"
                    },
                    {
                        "name": "Calvin Li"
                    },
                    {
                        "name": "Ian Foster"
                    },
                    {
                        "name": "Ben Blaiszik"
                    }
                ],
                "author_detail": {
                    "name": "Ben Blaiszik"
                },
                "arxiv_affiliation": "Mark",
                "author": "Ben Blaiszik",
                "arxiv_comment": "Updating author information, the submission remains largely\n  unchanged. 98 pages total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01594v1",
                "updated": "2025-01-03T01:38:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    1,
                    38,
                    46,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T01:38:46Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    1,
                    38,
                    46,
                    4,
                    3,
                    0
                ],
                "title": "PSYCHE: A Multi-faceted Patient Simulation Framework for Evaluation of\n  Psychiatric Assessment Conversational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PSYCHE: A Multi-faceted Patient Simulation Framework for Evaluation of\n  Psychiatric Assessment Conversational Agents"
                },
                "summary": "Recent advances in large language models (LLMs) have accelerated the\ndevelopment of conversational agents capable of generating human-like\nresponses. Since psychiatric assessments typically involve complex\nconversational interactions between psychiatrists and patients, there is\ngrowing interest in developing LLM-based psychiatric assessment conversational\nagents (PACAs) that aim to simulate the role of psychiatrists in clinical\nevaluations. However, standardized methods for benchmarking the clinical\nappropriateness of PACAs' interaction with patients still remain underexplored.\nHere, we propose PSYCHE, a novel framework designed to enable the 1) clinically\nrelevant, 2) ethically safe, 3) cost-efficient, and 4) quantitative evaluation\nof PACAs. This is achieved by simulating psychiatric patients based on a\nmulti-faceted psychiatric construct that defines the simulated patients'\nprofiles, histories, and behaviors, which PACAs are expected to assess. We\nvalidate the effectiveness of PSYCHE through a study with 10 board-certified\npsychiatrists, supported by an in-depth analysis of the simulated patient\nutterances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have accelerated the\ndevelopment of conversational agents capable of generating human-like\nresponses. Since psychiatric assessments typically involve complex\nconversational interactions between psychiatrists and patients, there is\ngrowing interest in developing LLM-based psychiatric assessment conversational\nagents (PACAs) that aim to simulate the role of psychiatrists in clinical\nevaluations. However, standardized methods for benchmarking the clinical\nappropriateness of PACAs' interaction with patients still remain underexplored.\nHere, we propose PSYCHE, a novel framework designed to enable the 1) clinically\nrelevant, 2) ethically safe, 3) cost-efficient, and 4) quantitative evaluation\nof PACAs. This is achieved by simulating psychiatric patients based on a\nmulti-faceted psychiatric construct that defines the simulated patients'\nprofiles, histories, and behaviors, which PACAs are expected to assess. We\nvalidate the effectiveness of PSYCHE through a study with 10 board-certified\npsychiatrists, supported by an in-depth analysis of the simulated patient\nutterances."
                },
                "authors": [
                    {
                        "name": "Jingoo Lee"
                    },
                    {
                        "name": "Kyungho Lim"
                    },
                    {
                        "name": "Young-Chul Jung"
                    },
                    {
                        "name": "Byung-Hoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Byung-Hoon Kim"
                },
                "author": "Byung-Hoon Kim",
                "arxiv_comment": "The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.04928v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.04928v3",
                "updated": "2025-01-03T01:32:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    1,
                    32,
                    6,
                    4,
                    3,
                    0
                ],
                "published": "2024-09-07T23:06:23Z",
                "published_parsed": [
                    2024,
                    9,
                    7,
                    23,
                    6,
                    23,
                    5,
                    251,
                    0
                ],
                "title": "Triple equivalence for the emergence of biological intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Triple equivalence for the emergence of biological intelligence"
                },
                "summary": "Characterising the intelligence of biological organisms is challenging. This\nwork considers intelligent algorithms developed evolutionarily within neural\nsystems. Mathematical analyses unveil a natural equivalence between canonical\nneural networks, variational Bayesian inference under a class of partially\nobservable Markov decision processes, and differentiable Turing machines, by\nshowing that they minimise the shared Helmholtz energy. Consequently, canonical\nneural networks can biologically plausibly equip Turing machines and conduct\nvariational Bayesian inferences of external Turing machines in the environment.\nApplying Helmholtz energy minimisation at the species level facilitates\nderiving active Bayesian model selection inherent in natural selection,\nresulting in the emergence of adaptive algorithms. In particular, canonical\nneural networks with two mental actions can separately memorise transition\nmappings of multiple external Turing machines to form a universal machine.\nThese propositions were corroborated by numerical simulations of algorithm\nimplementation and neural network evolution. These notions offer a universal\ncharacterisation of biological intelligence emerging from evolution in terms of\nBayesian model selection and belief updating.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterising the intelligence of biological organisms is challenging. This\nwork considers intelligent algorithms developed evolutionarily within neural\nsystems. Mathematical analyses unveil a natural equivalence between canonical\nneural networks, variational Bayesian inference under a class of partially\nobservable Markov decision processes, and differentiable Turing machines, by\nshowing that they minimise the shared Helmholtz energy. Consequently, canonical\nneural networks can biologically plausibly equip Turing machines and conduct\nvariational Bayesian inferences of external Turing machines in the environment.\nApplying Helmholtz energy minimisation at the species level facilitates\nderiving active Bayesian model selection inherent in natural selection,\nresulting in the emergence of adaptive algorithms. In particular, canonical\nneural networks with two mental actions can separately memorise transition\nmappings of multiple external Turing machines to form a universal machine.\nThese propositions were corroborated by numerical simulations of algorithm\nimplementation and neural network evolution. These notions offer a universal\ncharacterisation of biological intelligence emerging from evolution in terms of\nBayesian model selection and belief updating."
                },
                "authors": [
                    {
                        "name": "Takuya Isomura"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Isomura"
                },
                "author": "Takuya Isomura",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.04928v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.04928v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.PE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01588v1",
                "updated": "2025-01-03T00:56:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    0,
                    56,
                    46,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T00:56:46Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    0,
                    56,
                    46,
                    4,
                    3,
                    0
                ],
                "title": "(WhyPHI) Fine-Tuning PHI-3 for Multiple-Choice Question Answering:\n  Methodology, Results, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "(WhyPHI) Fine-Tuning PHI-3 for Multiple-Choice Question Answering:\n  Methodology, Results, and Challenges"
                },
                "summary": "Large Language Models (LLMs) have become essential tools across various\ndomains due to their impressive capabilities in understanding and generating\nhuman-like text. The ability to accurately answer multiple-choice questions\n(MCQs) holds significant value in education, particularly in automated tutoring\nsystems and assessment platforms. However, adapting LLMs to handle MCQ tasks\neffectively remains challenging due to the hallucinations and unclear prompts.\nThis work explores the potential of Microsoft's PHI-3\\cite{Abdin2024}, a\ncompact yet efficient LLM, for MCQ answering. Our contributions include\nfine-tuning the model on the TruthfulQA dataset, designing optimized prompts to\nenhance model performance, and evaluating using perplexity and traditional\nmetrics like accuracy and F1 score. Results show a remarkable improvement in\nPHI-3.5's MCQ handling post-fine-tuning, with perplexity decreasing from 4.68\nto 2.27, and accuracy rising from 62\\% to 90.8\\%. This research underlines the\nimportance of efficient models in adaptive learning systems and educational\nassessments, paving the way for broader integration into the classroom,\nparticularly in fields like test preparation, student feedback, and\npersonalized learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become essential tools across various\ndomains due to their impressive capabilities in understanding and generating\nhuman-like text. The ability to accurately answer multiple-choice questions\n(MCQs) holds significant value in education, particularly in automated tutoring\nsystems and assessment platforms. However, adapting LLMs to handle MCQ tasks\neffectively remains challenging due to the hallucinations and unclear prompts.\nThis work explores the potential of Microsoft's PHI-3\\cite{Abdin2024}, a\ncompact yet efficient LLM, for MCQ answering. Our contributions include\nfine-tuning the model on the TruthfulQA dataset, designing optimized prompts to\nenhance model performance, and evaluating using perplexity and traditional\nmetrics like accuracy and F1 score. Results show a remarkable improvement in\nPHI-3.5's MCQ handling post-fine-tuning, with perplexity decreasing from 4.68\nto 2.27, and accuracy rising from 62\\% to 90.8\\%. This research underlines the\nimportance of efficient models in adaptive learning systems and educational\nassessments, paving the way for broader integration into the classroom,\nparticularly in fields like test preparation, student feedback, and\npersonalized learning."
                },
                "authors": [
                    {
                        "name": "Mohamed Hisham Abdellatif"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Hisham Abdellatif"
                },
                "author": "Mohamed Hisham Abdellatif",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18947v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18947v2",
                "updated": "2025-01-03T00:16:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    0,
                    16,
                    52,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-25T16:51:29Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    16,
                    51,
                    29,
                    2,
                    360,
                    0
                ],
                "title": "MedHallBench: A New Benchmark for Assessing Hallucination in Medical\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedHallBench: A New Benchmark for Assessing Hallucination in Medical\n  Large Language Models"
                },
                "summary": "Medical Large Language Models (MLLMs) have demonstrated potential in\nhealthcare applications, yet their propensity for hallucinations -- generating\nmedically implausible or inaccurate information -- presents substantial risks\nto patient care. This paper introduces MedHallBench, a comprehensive benchmark\nframework for evaluating and mitigating hallucinations in MLLMs. Our\nmethodology integrates expert-validated medical case scenarios with established\nmedical databases to create a robust evaluation dataset. The framework employs\na sophisticated measurement system that combines automated ACHMI (Automatic\nCaption Hallucination Measurement in Medical Imaging) scoring with rigorous\nclinical expert evaluations and utilizes reinforcement learning methods to\nachieve automatic annotation. Through an optimized reinforcement learning from\nhuman feedback (RLHF) training pipeline specifically designed for medical\napplications, MedHallBench enables thorough evaluation of MLLMs across diverse\nclinical contexts while maintaining stringent accuracy standards. We conducted\ncomparative experiments involving various models, utilizing the benchmark to\nestablish a baseline for widely adopted large language models (LLMs). Our\nfindings indicate that ACHMI provides a more nuanced understanding of the\neffects of hallucinations compared to traditional metrics, thereby highlighting\nits advantages in hallucination assessment. This research establishes a\nfoundational framework for enhancing MLLMs' reliability in healthcare settings\nand presents actionable strategies for addressing the critical challenge of AI\nhallucinations in medical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Large Language Models (MLLMs) have demonstrated potential in\nhealthcare applications, yet their propensity for hallucinations -- generating\nmedically implausible or inaccurate information -- presents substantial risks\nto patient care. This paper introduces MedHallBench, a comprehensive benchmark\nframework for evaluating and mitigating hallucinations in MLLMs. Our\nmethodology integrates expert-validated medical case scenarios with established\nmedical databases to create a robust evaluation dataset. The framework employs\na sophisticated measurement system that combines automated ACHMI (Automatic\nCaption Hallucination Measurement in Medical Imaging) scoring with rigorous\nclinical expert evaluations and utilizes reinforcement learning methods to\nachieve automatic annotation. Through an optimized reinforcement learning from\nhuman feedback (RLHF) training pipeline specifically designed for medical\napplications, MedHallBench enables thorough evaluation of MLLMs across diverse\nclinical contexts while maintaining stringent accuracy standards. We conducted\ncomparative experiments involving various models, utilizing the benchmark to\nestablish a baseline for widely adopted large language models (LLMs). Our\nfindings indicate that ACHMI provides a more nuanced understanding of the\neffects of hallucinations compared to traditional metrics, thereby highlighting\nits advantages in hallucination assessment. This research establishes a\nfoundational framework for enhancing MLLMs' reliability in healthcare settings\nand presents actionable strategies for addressing the critical challenge of AI\nhallucinations in medical applications."
                },
                "authors": [
                    {
                        "name": "Kaiwen Zuo"
                    },
                    {
                        "name": "Yirui Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yirui Jiang"
                },
                "author": "Yirui Jiang",
                "arxiv_comment": "Published to AAAI-25 Bridge Program",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18947v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18947v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16833v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16833v2",
                "updated": "2025-01-03T00:07:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    0,
                    7,
                    9,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-22T02:40:59Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    2,
                    40,
                    59,
                    6,
                    357,
                    0
                ],
                "title": "KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge\n  Graph Enhancement for Medical Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge\n  Graph Enhancement for Medical Diagnosis"
                },
                "summary": "Integrating Large Language Models (LLMs) in healthcare diagnosis demands\nsystematic frameworks that can handle complex medical scenarios while\nmaintaining specialized expertise. We present KG4Diagnosis, a novel\nhierarchical multi-agent framework that combines LLMs with automated knowledge\ngraph construction, encompassing 362 common diseases across medical\nspecialties. Our framework mirrors real-world medical systems through a\ntwo-tier architecture: a general practitioner (GP) agent for initial assessment\nand triage, coordinating with specialized agents for in-depth diagnosis in\nspecific domains. The core innovation lies in our end-to-end knowledge graph\ngeneration methodology, incorporating: (1) semantic-driven entity and relation\nextraction optimized for medical terminology, (2) multi-dimensional decision\nrelationship reconstruction from unstructured medical texts, and (3)\nhuman-guided reasoning for knowledge expansion. KG4Diagnosis serves as an\nextensible foundation for specialized medical diagnosis systems, with\ncapabilities to incorporate new diseases and medical knowledge. The framework's\nmodular design enables seamless integration of domain-specific enhancements,\nmaking it valuable for developing targeted medical diagnosis systems. We\nprovide architectural guidelines and protocols to facilitate adoption across\nmedical contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models (LLMs) in healthcare diagnosis demands\nsystematic frameworks that can handle complex medical scenarios while\nmaintaining specialized expertise. We present KG4Diagnosis, a novel\nhierarchical multi-agent framework that combines LLMs with automated knowledge\ngraph construction, encompassing 362 common diseases across medical\nspecialties. Our framework mirrors real-world medical systems through a\ntwo-tier architecture: a general practitioner (GP) agent for initial assessment\nand triage, coordinating with specialized agents for in-depth diagnosis in\nspecific domains. The core innovation lies in our end-to-end knowledge graph\ngeneration methodology, incorporating: (1) semantic-driven entity and relation\nextraction optimized for medical terminology, (2) multi-dimensional decision\nrelationship reconstruction from unstructured medical texts, and (3)\nhuman-guided reasoning for knowledge expansion. KG4Diagnosis serves as an\nextensible foundation for specialized medical diagnosis systems, with\ncapabilities to incorporate new diseases and medical knowledge. The framework's\nmodular design enables seamless integration of domain-specific enhancements,\nmaking it valuable for developing targeted medical diagnosis systems. We\nprovide architectural guidelines and protocols to facilitate adoption across\nmedical contexts."
                },
                "authors": [
                    {
                        "name": "Kaiwen Zuo"
                    },
                    {
                        "name": "Yirui Jiang"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Pietro Lio"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Lio"
                },
                "author": "Pietro Lio",
                "arxiv_comment": "10 pages,5 figures,published to AAAI-25 Bridge Program",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16833v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16833v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20302v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20302v3",
                "updated": "2025-01-02T23:08:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    23,
                    8,
                    47,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-27T00:50:30Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    0,
                    50,
                    30,
                    6,
                    301,
                    0
                ],
                "title": "Sequential Large Language Model-Based Hyper-parameter Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Large Language Model-Based Hyper-parameter Optimization"
                },
                "summary": "This study introduces SLLMBO, an innovative framework leveraging large\nlanguage models (LLMs) for hyperparameter optimization (HPO), incorporating\ndynamic search space adaptability, enhanced parameter space exploitation, and a\nnovel LLM-tree-structured parzen estimator (LLM-TPE) sampler. By addressing\nlimitations in recent fully LLM-based methods and traditional bayesian\noptimization (BO), SLLMBO achieves more robust optimization. This comprehensive\nbenchmarking evaluates multiple LLMs, including GPT-3.5-Turbo, GPT-4o,\nClaude-Sonnet-3.5, and Gemini-1.5-Flash, extending prior work and establishing\nSLLMBO as the first framework to benchmark a diverse set of LLMs for HPO. By\nintegrating LLMs' established strengths in parameter initialization with the\nexploitation abilities demonstrated in this study, alongside TPE's exploration\ncapabilities, the LLM-TPE sampler achieves a balanced exploration-exploitation\ntrade-off, reduces API costs, and mitigates premature early stoppings for more\neffective parameter searches. Across 14 tabular tasks in classification and\nregression, the LLM-TPE sampler outperformed fully LLM-based methods and\nachieved superior results over BO methods in 9 tasks. Testing early stopping in\nbudget-constrained scenarios demonstrated competitive performance, indicating\nthat LLM-based methods generally benefit from extended iterations for optimal\nresults. This work lays the foundation for future research exploring\nopen-source LLMs, reproducibility of LLM results in HPO, and benchmarking\nSLLMBO on complex datasets, such as image classification, segmentation, and\nmachine translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces SLLMBO, an innovative framework leveraging large\nlanguage models (LLMs) for hyperparameter optimization (HPO), incorporating\ndynamic search space adaptability, enhanced parameter space exploitation, and a\nnovel LLM-tree-structured parzen estimator (LLM-TPE) sampler. By addressing\nlimitations in recent fully LLM-based methods and traditional bayesian\noptimization (BO), SLLMBO achieves more robust optimization. This comprehensive\nbenchmarking evaluates multiple LLMs, including GPT-3.5-Turbo, GPT-4o,\nClaude-Sonnet-3.5, and Gemini-1.5-Flash, extending prior work and establishing\nSLLMBO as the first framework to benchmark a diverse set of LLMs for HPO. By\nintegrating LLMs' established strengths in parameter initialization with the\nexploitation abilities demonstrated in this study, alongside TPE's exploration\ncapabilities, the LLM-TPE sampler achieves a balanced exploration-exploitation\ntrade-off, reduces API costs, and mitigates premature early stoppings for more\neffective parameter searches. Across 14 tabular tasks in classification and\nregression, the LLM-TPE sampler outperformed fully LLM-based methods and\nachieved superior results over BO methods in 9 tasks. Testing early stopping in\nbudget-constrained scenarios demonstrated competitive performance, indicating\nthat LLM-based methods generally benefit from extended iterations for optimal\nresults. This work lays the foundation for future research exploring\nopen-source LLMs, reproducibility of LLM results in HPO, and benchmarking\nSLLMBO on complex datasets, such as image classification, segmentation, and\nmachine translation."
                },
                "authors": [
                    {
                        "name": "Kanan Mahammadli"
                    },
                    {
                        "name": "Seyda Ertekin"
                    }
                ],
                "author_detail": {
                    "name": "Seyda Ertekin"
                },
                "author": "Seyda Ertekin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20302v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20302v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01568v1",
                "updated": "2025-01-02T23:03:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    23,
                    3,
                    3,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T23:03:03Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    23,
                    3,
                    3,
                    3,
                    2,
                    0
                ],
                "title": "Interruption Handling for Conversational Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interruption Handling for Conversational Robots"
                },
                "summary": "Interruptions, a fundamental component of human communication, can enhance\nthe dynamism and effectiveness of conversations, but only when effectively\nmanaged by all parties involved. Despite advancements in robotic systems,\nstate-of-the-art systems still have limited capabilities in handling\nuser-initiated interruptions in real-time. Prior research has primarily focused\non post hoc analysis of interruptions. To address this gap, we present a system\nthat detects user-initiated interruptions and manages them in real-time based\non the interrupter's intent (i.e., cooperative agreement, cooperative\nassistance, cooperative clarification, or disruptive interruption). The system\nwas designed based on interaction patterns identified from human-human\ninteraction data. We integrated our system into an LLM-powered social robot and\nvalidated its effectiveness through a timed decision-making task and a\ncontentious discussion task with 21 participants. Our system successfully\nhandled 93.69% (n=104/111) of user-initiated interruptions. We discuss our\nlearnings and their implications for designing interruption-handling behaviors\nin conversational robots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interruptions, a fundamental component of human communication, can enhance\nthe dynamism and effectiveness of conversations, but only when effectively\nmanaged by all parties involved. Despite advancements in robotic systems,\nstate-of-the-art systems still have limited capabilities in handling\nuser-initiated interruptions in real-time. Prior research has primarily focused\non post hoc analysis of interruptions. To address this gap, we present a system\nthat detects user-initiated interruptions and manages them in real-time based\non the interrupter's intent (i.e., cooperative agreement, cooperative\nassistance, cooperative clarification, or disruptive interruption). The system\nwas designed based on interaction patterns identified from human-human\ninteraction data. We integrated our system into an LLM-powered social robot and\nvalidated its effectiveness through a timed decision-making task and a\ncontentious discussion task with 21 participants. Our system successfully\nhandled 93.69% (n=104/111) of user-initiated interruptions. We discuss our\nlearnings and their implications for designing interruption-handling behaviors\nin conversational robots."
                },
                "authors": [
                    {
                        "name": "Shiye Cao"
                    },
                    {
                        "name": "Jiwon Moon"
                    },
                    {
                        "name": "Amama Mahmood"
                    },
                    {
                        "name": "Victor Nikhil Antony"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Anqi Liu"
                    },
                    {
                        "name": "Chien-Ming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Ming Huang"
                },
                "author": "Chien-Ming Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01558v1",
                "updated": "2025-01-02T22:26:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    22,
                    26,
                    54,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T22:26:54Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    22,
                    26,
                    54,
                    3,
                    2,
                    0
                ],
                "title": "Predicting the Performance of Black-box LLMs through Self-Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the Performance of Black-box LLMs through Self-Queries"
                },
                "summary": "As large language models (LLMs) are increasingly relied on in AI systems,\npredicting when they make mistakes is crucial. While a great deal of work in\nthe field uses internal representations to interpret model behavior, these\nrepresentations are inaccessible when given solely black-box access through an\nAPI. In this paper, we extract features of LLMs in a black-box manner by using\nfollow-up prompts and taking the probabilities of different responses as\nrepresentations to train reliable predictors of model behavior. We demonstrate\nthat training a linear model on these low-dimensional representations produces\nreliable and generalizable predictors of model performance at the instance\nlevel (e.g., if a particular generation correctly answers a question).\nRemarkably, these can often outperform white-box linear predictors that operate\nover a model's hidden state or the full distribution over its vocabulary. In\naddition, we demonstrate that these extracted features can be used to evaluate\nmore nuanced aspects of a language model's state. For instance, they can be\nused to distinguish between a clean version of GPT-4o-mini and a version that\nhas been influenced via an adversarial system prompt that answers\nquestion-answering tasks incorrectly or introduces bugs into generated code.\nFurthermore, they can reliably distinguish between different model\narchitectures and sizes, enabling the detection of misrepresented models\nprovided through an API (e.g., identifying if GPT-3.5 is supplied instead of\nGPT-4o-mini).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly relied on in AI systems,\npredicting when they make mistakes is crucial. While a great deal of work in\nthe field uses internal representations to interpret model behavior, these\nrepresentations are inaccessible when given solely black-box access through an\nAPI. In this paper, we extract features of LLMs in a black-box manner by using\nfollow-up prompts and taking the probabilities of different responses as\nrepresentations to train reliable predictors of model behavior. We demonstrate\nthat training a linear model on these low-dimensional representations produces\nreliable and generalizable predictors of model performance at the instance\nlevel (e.g., if a particular generation correctly answers a question).\nRemarkably, these can often outperform white-box linear predictors that operate\nover a model's hidden state or the full distribution over its vocabulary. In\naddition, we demonstrate that these extracted features can be used to evaluate\nmore nuanced aspects of a language model's state. For instance, they can be\nused to distinguish between a clean version of GPT-4o-mini and a version that\nhas been influenced via an adversarial system prompt that answers\nquestion-answering tasks incorrectly or introduces bugs into generated code.\nFurthermore, they can reliably distinguish between different model\narchitectures and sizes, enabling the detection of misrepresented models\nprovided through an API (e.g., identifying if GPT-3.5 is supplied instead of\nGPT-4o-mini)."
                },
                "authors": [
                    {
                        "name": "Dylan Sam"
                    },
                    {
                        "name": "Marc Finzi"
                    },
                    {
                        "name": "J. Zico Kolter"
                    }
                ],
                "author_detail": {
                    "name": "J. Zico Kolter"
                },
                "author": "J. Zico Kolter",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01545v1",
                "updated": "2025-01-02T21:31:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    21,
                    31,
                    56,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T21:31:56Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    21,
                    31,
                    56,
                    3,
                    2,
                    0
                ],
                "title": "Enhancing User Engagement in Large-Scale Social Annotation Platforms:\n  Community-Based Design Interventions and Implications for Large Language\n  Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing User Engagement in Large-Scale Social Annotation Platforms:\n  Community-Based Design Interventions and Implications for Large Language\n  Models (LLMs)"
                },
                "summary": "Social annotation platforms enable student engagement by integrating\ndiscussions directly into course materials. However, in large online courses,\nthe sheer volume of comments can overwhelm students and impede learning. This\npaper investigates community-based design interventions on a social annotation\nplatform (NB) to address this challenge and foster more meaningful online\neducational discussions. By examining student preferences and reactions to\ndifferent curation strategies, this research aims to optimize the utility of\nsocial annotations in educational contexts. A key emphasis is placed on how the\nvisibility of comments shapes group interactions, guides conversational flows,\nand enriches learning experiences.\n  The study combined iterative design and development with two large-scale\nexperiments to create and refine comment curation strategies, involving\nthousands of students. The study introduced specific features of the platform,\nsuch as targeted comment visibility controls, which demonstrably improved peer\ninteractions and reduced discussion overload. These findings inform the design\nof next-generation social annotation systems and highlight opportunities to\nintegrate Large Language Models (LLMs) for key activities like summarizing\nannotations, improving clarity in student writing, and assisting instructors\nwith efficient comment curation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social annotation platforms enable student engagement by integrating\ndiscussions directly into course materials. However, in large online courses,\nthe sheer volume of comments can overwhelm students and impede learning. This\npaper investigates community-based design interventions on a social annotation\nplatform (NB) to address this challenge and foster more meaningful online\neducational discussions. By examining student preferences and reactions to\ndifferent curation strategies, this research aims to optimize the utility of\nsocial annotations in educational contexts. A key emphasis is placed on how the\nvisibility of comments shapes group interactions, guides conversational flows,\nand enriches learning experiences.\n  The study combined iterative design and development with two large-scale\nexperiments to create and refine comment curation strategies, involving\nthousands of students. The study introduced specific features of the platform,\nsuch as targeted comment visibility controls, which demonstrably improved peer\ninteractions and reduced discussion overload. These findings inform the design\nof next-generation social annotation systems and highlight opportunities to\nintegrate Large Language Models (LLMs) for key activities like summarizing\nannotations, improving clarity in student writing, and assisting instructors\nwith efficient comment curation."
                },
                "authors": [
                    {
                        "name": "Jumana Almahmoud"
                    },
                    {
                        "name": "Marc Facciotti"
                    },
                    {
                        "name": "Michele Igo"
                    },
                    {
                        "name": "Kamali Sripathi"
                    },
                    {
                        "name": "David Karger"
                    }
                ],
                "author_detail": {
                    "name": "David Karger"
                },
                "author": "David Karger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01544v1",
                "updated": "2025-01-02T21:31:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    21,
                    31,
                    38,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T21:31:38Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    21,
                    31,
                    38,
                    3,
                    2,
                    0
                ],
                "title": "Many of Your DPOs are Secretly One: Attempting Unification Through\n  Mutual Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many of Your DPOs are Secretly One: Attempting Unification Through\n  Mutual Information"
                },
                "summary": "Post-alignment of large language models (LLMs) is critical in improving their\nutility, safety, and alignment with human intentions. Direct preference\noptimisation (DPO) has become one of the most widely used algorithms for\nachieving this alignment, given its ability to optimise models based on human\nfeedback directly. However, the vast number of DPO variants in the literature\nhas made it increasingly difficult for researchers to navigate and fully grasp\nthe connections between these approaches. This paper introduces a unifying\nframework inspired by mutual information, which proposes a new loss function\nwith flexible priors. By carefully specifying these priors, we demonstrate that\nmany existing algorithms, such as SimPO, TDPO, SparsePO, and others, can be\nderived from our framework. This unification offers a clearer and more\nstructured approach, allowing researchers to understand the relationships\nbetween different DPO variants better. We aim to simplify the landscape of DPO\nalgorithms, making it easier for the research community to gain insights and\nfoster further advancements in LLM alignment. Ultimately, we hope our framework\ncan be a foundation for developing more robust and interpretable alignment\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-alignment of large language models (LLMs) is critical in improving their\nutility, safety, and alignment with human intentions. Direct preference\noptimisation (DPO) has become one of the most widely used algorithms for\nachieving this alignment, given its ability to optimise models based on human\nfeedback directly. However, the vast number of DPO variants in the literature\nhas made it increasingly difficult for researchers to navigate and fully grasp\nthe connections between these approaches. This paper introduces a unifying\nframework inspired by mutual information, which proposes a new loss function\nwith flexible priors. By carefully specifying these priors, we demonstrate that\nmany existing algorithms, such as SimPO, TDPO, SparsePO, and others, can be\nderived from our framework. This unification offers a clearer and more\nstructured approach, allowing researchers to understand the relationships\nbetween different DPO variants better. We aim to simplify the landscape of DPO\nalgorithms, making it easier for the research community to gain insights and\nfoster further advancements in LLM alignment. Ultimately, we hope our framework\ncan be a foundation for developing more robust and interpretable alignment\ntechniques."
                },
                "authors": [
                    {
                        "name": "Rasul Tutnov"
                    },
                    {
                        "name": "Antoine Grosnit"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    }
                ],
                "author_detail": {
                    "name": "Haitham Bou-Ammar"
                },
                "author": "Haitham Bou-Ammar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15088v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15088v2",
                "updated": "2025-01-02T21:27:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    21,
                    27,
                    15,
                    3,
                    2,
                    0
                ],
                "published": "2023-12-22T22:04:13Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    22,
                    4,
                    13,
                    4,
                    356,
                    0
                ],
                "title": "Adaptive Domain Inference Attack with Concept Hierarchy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Domain Inference Attack with Concept Hierarchy"
                },
                "summary": "With increasingly deployed deep neural networks in sensitive application\ndomains, such as healthcare and security, it's essential to understand what\nkind of sensitive information can be inferred from these models. Most known\nmodel-targeted attacks assume attackers have learned the application domain or\ntraining data distribution to ensure successful attacks. Can removing the\ndomain information from model APIs protect models from these attacks? This\npaper studies this critical problem. Unfortunately, even with minimal\nknowledge, i.e., accessing the model as an unnamed function without leaking the\nmeaning of input and output, the proposed adaptive domain inference attack\n(ADI) can still successfully estimate relevant subsets of training data. We\nshow that the extracted relevant data can significantly improve, for instance,\nthe performance of model-inversion attacks. Specifically, the ADI method\nutilizes a concept hierarchy extracted from a collection of available public\nand private datasets and a novel algorithm to adaptively tune the likelihood of\nleaf concepts showing up in the unseen training data. We also designed a\nstraightforward hypothesis-testing-based attack -- LDI. The ADI attack not only\nextracts partial training data at the concept level but also converges fastest\nand requires the fewest target-model accesses among all candidate methods. Our\ncode is available at \\url{https://anonymous.4open.science/r/KDD-362D}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With increasingly deployed deep neural networks in sensitive application\ndomains, such as healthcare and security, it's essential to understand what\nkind of sensitive information can be inferred from these models. Most known\nmodel-targeted attacks assume attackers have learned the application domain or\ntraining data distribution to ensure successful attacks. Can removing the\ndomain information from model APIs protect models from these attacks? This\npaper studies this critical problem. Unfortunately, even with minimal\nknowledge, i.e., accessing the model as an unnamed function without leaking the\nmeaning of input and output, the proposed adaptive domain inference attack\n(ADI) can still successfully estimate relevant subsets of training data. We\nshow that the extracted relevant data can significantly improve, for instance,\nthe performance of model-inversion attacks. Specifically, the ADI method\nutilizes a concept hierarchy extracted from a collection of available public\nand private datasets and a novel algorithm to adaptively tune the likelihood of\nleaf concepts showing up in the unseen training data. We also designed a\nstraightforward hypothesis-testing-based attack -- LDI. The ADI attack not only\nextracts partial training data at the concept level but also converges fastest\nand requires the fewest target-model accesses among all candidate methods. Our\ncode is available at \\url{https://anonymous.4open.science/r/KDD-362D}."
                },
                "authors": [
                    {
                        "name": "Yuechun Gu"
                    },
                    {
                        "name": "Jiajie He"
                    },
                    {
                        "name": "Keke Chen"
                    }
                ],
                "author_detail": {
                    "name": "Keke Chen"
                },
                "author": "Keke Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15088v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15088v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.07574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.07574v2",
                "updated": "2025-01-02T21:21:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    21,
                    21,
                    26,
                    3,
                    2,
                    0
                ],
                "published": "2023-07-14T18:37:57Z",
                "published_parsed": [
                    2023,
                    7,
                    14,
                    18,
                    37,
                    57,
                    4,
                    195,
                    0
                ],
                "title": "Sparsified Simultaneous Confidence Intervals for High-Dimensional Linear\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparsified Simultaneous Confidence Intervals for High-Dimensional Linear\n  Models"
                },
                "summary": "Statistical inference of the high-dimensional regression coefficients is\nchallenging because the uncertainty introduced by the model selection procedure\nis hard to account for. A critical question remains unsettled; that is, is it\npossible and how to embed the inference of the model into the simultaneous\ninference of the coefficients? To this end, we propose a notion of simultaneous\nconfidence intervals called the sparsified simultaneous confidence intervals.\nOur intervals are sparse in the sense that some of the intervals' upper and\nlower bounds are shrunken to zero (i.e., $[0,0]$), indicating the unimportance\nof the corresponding covariates. These covariates should be excluded from the\nfinal model. The rest of the intervals, either containing zero (e.g., $[-1,1]$\nor $[0,1]$) or not containing zero (e.g., $[2,3]$), indicate the plausible and\nsignificant covariates, respectively. The proposed method can be coupled with\nvarious selection procedures, making it ideal for comparing their uncertainty.\nFor the proposed method, we establish desirable asymptotic properties, develop\nintuitive graphical tools for visualization, and justify its superior\nperformance through simulation and real data analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference of the high-dimensional regression coefficients is\nchallenging because the uncertainty introduced by the model selection procedure\nis hard to account for. A critical question remains unsettled; that is, is it\npossible and how to embed the inference of the model into the simultaneous\ninference of the coefficients? To this end, we propose a notion of simultaneous\nconfidence intervals called the sparsified simultaneous confidence intervals.\nOur intervals are sparse in the sense that some of the intervals' upper and\nlower bounds are shrunken to zero (i.e., $[0,0]$), indicating the unimportance\nof the corresponding covariates. These covariates should be excluded from the\nfinal model. The rest of the intervals, either containing zero (e.g., $[-1,1]$\nor $[0,1]$) or not containing zero (e.g., $[2,3]$), indicate the plausible and\nsignificant covariates, respectively. The proposed method can be coupled with\nvarious selection procedures, making it ideal for comparing their uncertainty.\nFor the proposed method, we establish desirable asymptotic properties, develop\nintuitive graphical tools for visualization, and justify its superior\nperformance through simulation and real data analysis."
                },
                "authors": [
                    {
                        "name": "Xiaorui Zhu"
                    },
                    {
                        "name": "Yichen Qin"
                    },
                    {
                        "name": "Peng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Peng Wang"
                },
                "author": "Peng Wang",
                "arxiv_doi": "10.1007/s00184-024-00975-z",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s00184-024-00975-z",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.07574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.07574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "26 pages, 6 figures",
                "arxiv_journal_ref": "Metrika, 2024",
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62fxx",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01540v1",
                "updated": "2025-01-02T21:15:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    21,
                    15,
                    57,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T21:15:57Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    21,
                    15,
                    57,
                    3,
                    2,
                    0
                ],
                "title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and\n  Model Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BoxingGym: Benchmarking Progress in Automated Experimental Design and\n  Model Discovery"
                },
                "summary": "Understanding the world and explaining it with scientific theories is a\ncentral aspiration of artificial intelligence research. Proposing theories,\ndesigning experiments to test them, and then revising them based on data are\nfundamental to scientific discovery. Despite the significant promise of\nLLM-based scientific agents, no benchmarks systematically test LLM's ability to\npropose scientific models, collect experimental data, and revise them in light\nof new data. We introduce BoxingGym, a benchmark with 10 environments for\nsystematically evaluating both experimental design (e.g. collecting data to\ntest a scientific theory) and model discovery (e.g. proposing and revising\nscientific theories). To enable tractable and quantitative evaluation, we\nimplement each environment as a generative probabilistic model with which a\nscientific agent can run interactive experiments. These probabilistic models\nare drawn from various real-world scientific domains ranging from psychology to\necology. To quantitatively evaluate a scientific agent's ability to collect\ninformative experimental data, we compute the expected information gain (EIG),\nan information-theoretic quantity which measures how much an experiment reduces\nuncertainty about the parameters of a generative model. A good scientific\ntheory is a concise and predictive explanation. Therefore, to quantitatively\nevaluate model discovery, we ask a scientific agent to explain their model and\nthen assess whether this explanation enables another scientific agent to make\nreliable predictions about this environment. In addition to this\nexplanation-based evaluation, we compute standard model evaluation metrics such\nas prediction errors. We find that current LLMs, such as GPT-4o, struggle with\nboth experimental design and model discovery. We find that augmenting the\nLLM-based agent with an explicit statistical model does not reliably improve\nthese results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the world and explaining it with scientific theories is a\ncentral aspiration of artificial intelligence research. Proposing theories,\ndesigning experiments to test them, and then revising them based on data are\nfundamental to scientific discovery. Despite the significant promise of\nLLM-based scientific agents, no benchmarks systematically test LLM's ability to\npropose scientific models, collect experimental data, and revise them in light\nof new data. We introduce BoxingGym, a benchmark with 10 environments for\nsystematically evaluating both experimental design (e.g. collecting data to\ntest a scientific theory) and model discovery (e.g. proposing and revising\nscientific theories). To enable tractable and quantitative evaluation, we\nimplement each environment as a generative probabilistic model with which a\nscientific agent can run interactive experiments. These probabilistic models\nare drawn from various real-world scientific domains ranging from psychology to\necology. To quantitatively evaluate a scientific agent's ability to collect\ninformative experimental data, we compute the expected information gain (EIG),\nan information-theoretic quantity which measures how much an experiment reduces\nuncertainty about the parameters of a generative model. A good scientific\ntheory is a concise and predictive explanation. Therefore, to quantitatively\nevaluate model discovery, we ask a scientific agent to explain their model and\nthen assess whether this explanation enables another scientific agent to make\nreliable predictions about this environment. In addition to this\nexplanation-based evaluation, we compute standard model evaluation metrics such\nas prediction errors. We find that current LLMs, such as GPT-4o, struggle with\nboth experimental design and model discovery. We find that augmenting the\nLLM-based agent with an explicit statistical model does not reliably improve\nthese results."
                },
                "authors": [
                    {
                        "name": "Kanishk Gandhi"
                    },
                    {
                        "name": "Michael Y. Li"
                    },
                    {
                        "name": "Lyle Goodyear"
                    },
                    {
                        "name": "Louise Li"
                    },
                    {
                        "name": "Aditi Bhaskar"
                    },
                    {
                        "name": "Mohammed Zaman"
                    },
                    {
                        "name": "Noah D. Goodman"
                    }
                ],
                "author_detail": {
                    "name": "Noah D. Goodman"
                },
                "author": "Noah D. Goodman",
                "arxiv_comment": "KG and MYL contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01511v1",
                "updated": "2025-01-02T19:38:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    19,
                    38,
                    7,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T19:38:07Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    19,
                    38,
                    7,
                    3,
                    2,
                    0
                ],
                "title": "TreeLUT: An Efficient Alternative to Deep Neural Networks for Inference\n  Acceleration Using Gradient Boosted Decision Trees",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeLUT: An Efficient Alternative to Deep Neural Networks for Inference\n  Acceleration Using Gradient Boosted Decision Trees"
                },
                "summary": "Accelerating machine learning inference has been an active research area in\nrecent years. In this context, field-programmable gate arrays (FPGAs) have\ndemonstrated compelling performance by providing massive parallelism in deep\nneural networks (DNNs). Neural networks (NNs) are computationally intensive\nduring inference, as they require massive amounts of multiplication and\naddition, which makes their implementations costly. Numerous studies have\nrecently addressed this challenge to some extent using a combination of\nsparsity induction, quantization, and transformation of neurons or sub-networks\ninto lookup tables (LUTs) on FPGAs. Gradient boosted decision trees (GBDTs) are\na high-accuracy alternative to DNNs in a wide range of regression and\nclassification tasks, particularly for tabular datasets. The basic building\nblock of GBDTs is a decision tree, which resembles the structure of binary\ndecision diagrams. FPGA design flows are heavily optimized to implement such a\nstructure efficiently. In addition to decision trees, GBDTs perform simple\noperations during inference, including comparison and addition. We present\nTreeLUT as an open-source tool for implementing GBDTs using an efficient\nquantization scheme, hardware architecture, and pipelining strategy. It\nprimarily utilizes LUTs with no BRAMs or DSPs on FPGAs, resulting in high\nefficiency. We show the effectiveness of TreeLUT using multiple classification\ndatasets, commonly used to evaluate ultra-low area and latency architectures.\nUsing these benchmarks, we compare our implementation results with existing DNN\nand GBDT methods, such as DWN, PolyLUT-Add, NeuraLUT, LogicNets, FINN, hls4ml,\nand others. Our results show that TreeLUT significantly improves hardware\nutilization, latency, and throughput at competitive accuracy compared to\nprevious works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating machine learning inference has been an active research area in\nrecent years. In this context, field-programmable gate arrays (FPGAs) have\ndemonstrated compelling performance by providing massive parallelism in deep\nneural networks (DNNs). Neural networks (NNs) are computationally intensive\nduring inference, as they require massive amounts of multiplication and\naddition, which makes their implementations costly. Numerous studies have\nrecently addressed this challenge to some extent using a combination of\nsparsity induction, quantization, and transformation of neurons or sub-networks\ninto lookup tables (LUTs) on FPGAs. Gradient boosted decision trees (GBDTs) are\na high-accuracy alternative to DNNs in a wide range of regression and\nclassification tasks, particularly for tabular datasets. The basic building\nblock of GBDTs is a decision tree, which resembles the structure of binary\ndecision diagrams. FPGA design flows are heavily optimized to implement such a\nstructure efficiently. In addition to decision trees, GBDTs perform simple\noperations during inference, including comparison and addition. We present\nTreeLUT as an open-source tool for implementing GBDTs using an efficient\nquantization scheme, hardware architecture, and pipelining strategy. It\nprimarily utilizes LUTs with no BRAMs or DSPs on FPGAs, resulting in high\nefficiency. We show the effectiveness of TreeLUT using multiple classification\ndatasets, commonly used to evaluate ultra-low area and latency architectures.\nUsing these benchmarks, we compare our implementation results with existing DNN\nand GBDT methods, such as DWN, PolyLUT-Add, NeuraLUT, LogicNets, FINN, hls4ml,\nand others. Our results show that TreeLUT significantly improves hardware\nutilization, latency, and throughput at competitive accuracy compared to\nprevious works."
                },
                "authors": [
                    {
                        "name": "Alireza Khataei"
                    },
                    {
                        "name": "Kia Bazargan"
                    }
                ],
                "author_detail": {
                    "name": "Kia Bazargan"
                },
                "author": "Kia Bazargan",
                "arxiv_doi": "10.1145/3706628.3708877",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706628.3708877",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.01511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by FPGA'25 conference",
                "arxiv_journal_ref": "Proceedings of the 2025 ACM/SIGDA International Symposium on Field\n  Programmable Gate Arrays (FPGA '25), February 27-March 1, 2025, Monterey, CA,\n  USA",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.08468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.08468v2",
                "updated": "2025-01-02T19:34:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    19,
                    34,
                    55,
                    3,
                    2,
                    0
                ],
                "published": "2024-07-11T13:02:10Z",
                "published_parsed": [
                    2024,
                    7,
                    11,
                    13,
                    2,
                    10,
                    3,
                    193,
                    0
                ],
                "title": "Matching-Based Policy Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Matching-Based Policy Learning"
                },
                "summary": "The beneficial effects of treatments vary across individuals in most studies.\nTreatment heterogeneity motivates practitioners to search for the optimal\npolicy based on personal characteristics. A long-standing common practice in\npolicy learning has been estimating and maximizing the value function using\nweighting techniques. Matching is widely used in many applied disciplines to\ninfer causal effects, which is intuitively appealing because the observed\ncovariates are directly balanced across different treatment groups.\nNevertheless, matching is rarely explored in policy learning. In this work, we\npropose a matching-based policy learning framework. We adapt standard and\nbias-corrected matching methods to estimate an alternative form of the value\nfunction: the advantage function, which can be interpreted as the expected\nimprovement achieved by implementing a given policy compared to the\nequiprobable random policy. We then learn the optimal policy over a restricted\npolicy class by maximizing the matching estimator of the advantage function. We\nderive a non-asymptotic high probability bound for the regret of the learned\noptimal policy. Moreover, we show that the learned policy is almost\nrate-optimal. The competitive finite sample performance of the proposed method\ncompared to weighting-based and outcome modeling-based learning methods is\ndemonstrated in extensive simulation studies and a real data application.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The beneficial effects of treatments vary across individuals in most studies.\nTreatment heterogeneity motivates practitioners to search for the optimal\npolicy based on personal characteristics. A long-standing common practice in\npolicy learning has been estimating and maximizing the value function using\nweighting techniques. Matching is widely used in many applied disciplines to\ninfer causal effects, which is intuitively appealing because the observed\ncovariates are directly balanced across different treatment groups.\nNevertheless, matching is rarely explored in policy learning. In this work, we\npropose a matching-based policy learning framework. We adapt standard and\nbias-corrected matching methods to estimate an alternative form of the value\nfunction: the advantage function, which can be interpreted as the expected\nimprovement achieved by implementing a given policy compared to the\nequiprobable random policy. We then learn the optimal policy over a restricted\npolicy class by maximizing the matching estimator of the advantage function. We\nderive a non-asymptotic high probability bound for the regret of the learned\noptimal policy. Moreover, we show that the learned policy is almost\nrate-optimal. The competitive finite sample performance of the proposed method\ncompared to weighting-based and outcome modeling-based learning methods is\ndemonstrated in extensive simulation studies and a real data application."
                },
                "authors": [
                    {
                        "name": "Xuqiao Li"
                    },
                    {
                        "name": "Ying Yan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Yan"
                },
                "author": "Ying Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.08468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.08468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01508v1",
                "updated": "2025-01-02T19:30:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    19,
                    30,
                    53,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T19:30:53Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    19,
                    30,
                    53,
                    3,
                    2,
                    0
                ],
                "title": "Garbage in Garbage out: Impacts of data quality on criminal network\n  intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Garbage in Garbage out: Impacts of data quality on criminal network\n  intervention"
                },
                "summary": "Criminal networks such as human trafficking rings are threats to the rule of\nlaw, democracy and public safety in our global society. Network science\nprovides invaluable tools to identify key players and design interventions for\nLaw Enforcement Agencies (LEAs), e.g., to dismantle their organisation.\nHowever, poor data quality and the adaptiveness of criminal networks through\nself-organization make effective disruption extremely challenging. Although\nthere exists a large body of work building and applying network scientific\ntools to attack criminal networks, these work often implicitly assume that the\nnetwork measurements are accurate and complete. Moreover, there is thus far no\ncomprehensive understanding of the impacts of data quality on the downstream\neffectiveness of interventions. This work investigates the relationship between\ndata quality and intervention effectiveness based on classical graph theoretic\nand machine learning-based approaches. Decentralization emerges as a major\nfactor in network robustness, particularly under conditions of incomplete data,\nwhich renders attack strategies largely ineffective. Moreover, the robustness\nof centralized networks can be boosted using simple heuristics, making targeted\nattack more infeasible. Consequently, we advocate for a more cautious\napplication of network science in disrupting criminal networks, the continuous\ndevelopment of an interoperable intelligence ecosystem, and the creation of\nnovel network inference techniques to address data quality challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Criminal networks such as human trafficking rings are threats to the rule of\nlaw, democracy and public safety in our global society. Network science\nprovides invaluable tools to identify key players and design interventions for\nLaw Enforcement Agencies (LEAs), e.g., to dismantle their organisation.\nHowever, poor data quality and the adaptiveness of criminal networks through\nself-organization make effective disruption extremely challenging. Although\nthere exists a large body of work building and applying network scientific\ntools to attack criminal networks, these work often implicitly assume that the\nnetwork measurements are accurate and complete. Moreover, there is thus far no\ncomprehensive understanding of the impacts of data quality on the downstream\neffectiveness of interventions. This work investigates the relationship between\ndata quality and intervention effectiveness based on classical graph theoretic\nand machine learning-based approaches. Decentralization emerges as a major\nfactor in network robustness, particularly under conditions of incomplete data,\nwhich renders attack strategies largely ineffective. Moreover, the robustness\nof centralized networks can be boosted using simple heuristics, making targeted\nattack more infeasible. Consequently, we advocate for a more cautious\napplication of network science in disrupting criminal networks, the continuous\ndevelopment of an interoperable intelligence ecosystem, and the creation of\nnovel network inference techniques to address data quality challenges."
                },
                "authors": [
                    {
                        "name": "Wang Ngai Yeung"
                    },
                    {
                        "name": "Riccardo Di Clemente"
                    },
                    {
                        "name": "Renaud Lambiotte"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Lambiotte"
                },
                "author": "Renaud Lambiotte",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.soc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.soc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "J.4; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06708v2",
                "updated": "2025-01-02T19:27:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    19,
                    27,
                    1,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-09T09:27:07Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    27,
                    7,
                    2,
                    283,
                    0
                ],
                "title": "Do Developers Adopt Green Architectural Tactics for ML-Enabled Systems?\n  A Mining Software Repository Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Developers Adopt Green Architectural Tactics for ML-Enabled Systems?\n  A Mining Software Repository Study"
                },
                "summary": "As machine learning (ML) and artificial intelligence (AI) technologies become\nmore widespread, concerns about their environmental impact are increasing due\nto the resource-intensive nature of training and inference processes. Green AI\nadvocates for reducing computational demands while still maintaining accuracy.\nAlthough various strategies for creating sustainable ML systems have been\nidentified, their real-world implementation is still underexplored. This paper\naddresses this gap by studying 168 open-source ML projects on GitHub. It\nemploys a novel large language model (LLM)-based mining mechanism to identify\nand analyze green strategies. The findings reveal the adoption of established\ntactics that offer significant environmental benefits. This provides practical\ninsights for developers and paves the way for future automation of sustainable\npractices in ML systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine learning (ML) and artificial intelligence (AI) technologies become\nmore widespread, concerns about their environmental impact are increasing due\nto the resource-intensive nature of training and inference processes. Green AI\nadvocates for reducing computational demands while still maintaining accuracy.\nAlthough various strategies for creating sustainable ML systems have been\nidentified, their real-world implementation is still underexplored. This paper\naddresses this gap by studying 168 open-source ML projects on GitHub. It\nemploys a novel large language model (LLM)-based mining mechanism to identify\nand analyze green strategies. The findings reveal the adoption of established\ntactics that offer significant environmental benefits. This provides practical\ninsights for developers and paves the way for future automation of sustainable\npractices in ML systems."
                },
                "authors": [
                    {
                        "name": "Vincenzo De Martino"
                    },
                    {
                        "name": "Silverio Martínez-Fernández"
                    },
                    {
                        "name": "Fabio Palomba"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Palomba"
                },
                "author": "Fabio Palomba",
                "arxiv_comment": "Accepted at the 2025 IEEE/ACM 47th International Conference on\n  Software Engineering: Software Engineering in Society (ICSE-SEIS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01505v1",
                "updated": "2025-01-02T19:19:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    19,
                    19,
                    54,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T19:19:54Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    19,
                    19,
                    54,
                    3,
                    2,
                    0
                ],
                "title": "Reinforcement Learning for Respondent-Driven Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning for Respondent-Driven Sampling"
                },
                "summary": "Respondent-driven sampling (RDS) is widely used to study hidden or\nhard-to-reach populations by incentivizing study participants to recruit their\nsocial connections. The success and efficiency of RDS can depend critically on\nthe nature of the incentives, including their number, value, call to action,\netc. Standard RDS uses an incentive structure that is set a priori and held\nfixed throughout the study. Thus, it does not make use of accumulating\ninformation on which incentives are effective and for whom. We propose a\nreinforcement learning (RL) based adaptive RDS study design in which the\nincentives are tailored over time to maximize cumulative utility during the\nstudy. We show that these designs are more efficient, cost-effective, and can\ngenerate new insights into the social structure of hidden populations. In\naddition, we develop methods for valid post-study inference which are\nnon-trivial due to the adaptive sampling induced by RL as well as the complex\ndependencies among subjects due to latent (unobserved) social network\nstructure. We provide asymptotic regret bounds and illustrate its finite sample\nbehavior through a suite of simulation experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Respondent-driven sampling (RDS) is widely used to study hidden or\nhard-to-reach populations by incentivizing study participants to recruit their\nsocial connections. The success and efficiency of RDS can depend critically on\nthe nature of the incentives, including their number, value, call to action,\netc. Standard RDS uses an incentive structure that is set a priori and held\nfixed throughout the study. Thus, it does not make use of accumulating\ninformation on which incentives are effective and for whom. We propose a\nreinforcement learning (RL) based adaptive RDS study design in which the\nincentives are tailored over time to maximize cumulative utility during the\nstudy. We show that these designs are more efficient, cost-effective, and can\ngenerate new insights into the social structure of hidden populations. In\naddition, we develop methods for valid post-study inference which are\nnon-trivial due to the adaptive sampling induced by RL as well as the complex\ndependencies among subjects due to latent (unobserved) social network\nstructure. We provide asymptotic regret bounds and illustrate its finite sample\nbehavior through a suite of simulation experiments."
                },
                "authors": [
                    {
                        "name": "Justin Weltz"
                    },
                    {
                        "name": "Angela Yoon"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Alexander Volfovsky"
                    },
                    {
                        "name": "Eric Laber"
                    }
                ],
                "author_detail": {
                    "name": "Eric Laber"
                },
                "author": "Eric Laber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01488v1",
                "updated": "2025-01-02T19:00:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    19,
                    0,
                    0,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T19:00:00Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    19,
                    0,
                    0,
                    3,
                    2,
                    0
                ],
                "title": "Line detections in photospheric radius expansion bursts from 4U 1820-303",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Line detections in photospheric radius expansion bursts from 4U 1820-303"
                },
                "summary": "Context: NICER (Neutron star Interior Composition ExploreR) is the instrument\nof choice for the spectral analysis of type I X-ray bursts, as it provides high\nthroughput at X-ray CCD resolution, down to 0.3 keV. Aims: This study\ninvestigates whether the energies of absorption lines detected in photospheric\nradius expansion (PRE) bursts correlate with the inferred blackbody radius.\nPrevious reports suggested such a correlation, attributed to a combination of\nweaker gravitational redshift and higher blueshifts in bursts with larger\nradii. Methods: The analysis reexamines four previously studied PRE bursts and\nexamines eight additional bursts from 4U 1820-303, evidencing PRE. Spectral\nevolution is tracked on the shortest possible timescales (tenth of a second)\nadopting two parallel continuum descriptions to characterise the photospheric\nexpansion and line evolution. Applying the accretion-enhanced model, maximum\nblackbody radii of up to $\\sim$ 900 km are inferred, with peak bolometric\nluminosities exceeding the Eddington limit of an Helium accretor. Absorption\nlines are assessed for significance using Monte Carlo simulations, and spectral\nlines are characterised using the state-of-art plasma codes available within\n{\\sc{spex}} with a phenomenological continuum. A thorough parameter search\nexplores Doppler shifts to avoid local minima. Results: Several significant (>\n99.9%) absorption lines, including the previously reported 2.97 keV line, are\ndetected. While no consistent correlation between line energies and blackbody\nradii is confirmed, bursts with larger radii exhibit up to four lines and the\nline strength is higher. The modelling suggests that the observed lines mostly\noriginate from slightly redshifted (almost rest-frame) photo-/collisionally\nionised gas in emission. For the burst with the largest PRE, a combination of\nphoto-ionised plasma in both emission and absorption is preferred.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: NICER (Neutron star Interior Composition ExploreR) is the instrument\nof choice for the spectral analysis of type I X-ray bursts, as it provides high\nthroughput at X-ray CCD resolution, down to 0.3 keV. Aims: This study\ninvestigates whether the energies of absorption lines detected in photospheric\nradius expansion (PRE) bursts correlate with the inferred blackbody radius.\nPrevious reports suggested such a correlation, attributed to a combination of\nweaker gravitational redshift and higher blueshifts in bursts with larger\nradii. Methods: The analysis reexamines four previously studied PRE bursts and\nexamines eight additional bursts from 4U 1820-303, evidencing PRE. Spectral\nevolution is tracked on the shortest possible timescales (tenth of a second)\nadopting two parallel continuum descriptions to characterise the photospheric\nexpansion and line evolution. Applying the accretion-enhanced model, maximum\nblackbody radii of up to $\\sim$ 900 km are inferred, with peak bolometric\nluminosities exceeding the Eddington limit of an Helium accretor. Absorption\nlines are assessed for significance using Monte Carlo simulations, and spectral\nlines are characterised using the state-of-art plasma codes available within\n{\\sc{spex}} with a phenomenological continuum. A thorough parameter search\nexplores Doppler shifts to avoid local minima. Results: Several significant (>\n99.9%) absorption lines, including the previously reported 2.97 keV line, are\ndetected. While no consistent correlation between line energies and blackbody\nradii is confirmed, bursts with larger radii exhibit up to four lines and the\nline strength is higher. The modelling suggests that the observed lines mostly\noriginate from slightly redshifted (almost rest-frame) photo-/collisionally\nionised gas in emission. For the burst with the largest PRE, a combination of\nphoto-ionised plasma in both emission and absorption is preferred."
                },
                "authors": [
                    {
                        "name": "F. Barra"
                    },
                    {
                        "name": "D. Barret"
                    },
                    {
                        "name": "C. Pinto"
                    },
                    {
                        "name": "T. Di Salvo"
                    },
                    {
                        "name": "N. Weinberg"
                    },
                    {
                        "name": "S. Guichandut"
                    }
                ],
                "author_detail": {
                    "name": "S. Guichandut"
                },
                "author": "S. Guichandut",
                "arxiv_comment": "18 pages, 11 figures, accepted for publication in Astronomy &\n  Astrophysics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.01957v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01957v1",
                "updated": "2025-01-03T18:59:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    59,
                    52,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T18:59:52Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    59,
                    52,
                    4,
                    3,
                    0
                ],
                "title": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VITA-1.5: Towards GPT-4o Level Real-Time Vision and Speech Interaction"
                },
                "summary": "Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Multimodal Large Language Models (MLLMs) have typically focused on\nintegrating visual and textual modalities, with less emphasis placed on the\nrole of speech in enhancing interaction. However, speech plays a crucial role\nin multimodal dialogue systems, and implementing high-performance in both\nvision and speech tasks remains a significant challenge due to the fundamental\nmodality differences. In this paper, we propose a carefully designed\nmulti-stage training methodology that progressively trains LLM to understand\nboth visual and speech information, ultimately enabling fluent vision and\nspeech interaction. Our approach not only preserves strong vision-language\ncapacity, but also enables efficient speech-to-speech dialogue capabilities\nwithout separate ASR and TTS modules, significantly accelerating multimodal\nend-to-end response speed. By comparing our method against state-of-the-art\ncounterparts across benchmarks for image, video, and speech tasks, we\ndemonstrate that our model is equipped with both strong visual and speech\ncapabilities, making near real-time vision and speech interaction."
                },
                "authors": [
                    {
                        "name": "Chaoyou Fu"
                    },
                    {
                        "name": "Haojia Lin"
                    },
                    {
                        "name": "Xiong Wang"
                    },
                    {
                        "name": "Yi-Fan Zhang"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Xiaoyu Liu"
                    },
                    {
                        "name": "Yangze Li"
                    },
                    {
                        "name": "Zuwei Long"
                    },
                    {
                        "name": "Heting Gao"
                    },
                    {
                        "name": "Ke Li"
                    },
                    {
                        "name": "Xiawu Zheng"
                    },
                    {
                        "name": "Rongrong Ji"
                    },
                    {
                        "name": "Xing Sun"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Ran He"
                    }
                ],
                "author_detail": {
                    "name": "Ran He"
                },
                "author": "Ran He",
                "arxiv_comment": "https://github.com/VITA-MLLM/VITA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01957v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01957v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2302.11617v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2302.11617v2",
                "updated": "2025-01-03T18:58:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    58,
                    23,
                    4,
                    3,
                    0
                ],
                "published": "2023-02-22T19:46:00Z",
                "published_parsed": [
                    2023,
                    2,
                    22,
                    19,
                    46,
                    0,
                    2,
                    53,
                    0
                ],
                "title": "A Reference Architecture for Governance of Cloud Native Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reference Architecture for Governance of Cloud Native Applications"
                },
                "summary": "The evolution of cloud computing has given rise to Cloud Native Applications\n(CNAs), presenting new challenges in governance, particularly when faced with\nstrict compliance requirements. This work explores the unique characteristics\nof CNAs and their impact on governance. We introduce a comprehensive reference\narchitecture designed to streamline governance across CNAs along with a sample\nimplementation, offering insights for both single and multi-cloud environments.\nOur architecture seamlessly integrates governance within the CNA framework,\nadhering to a \"battery-included\" philosophy. Tailored for both expansive and\ncompact CNA deployments across various industries, this design enables cloud\npractitioners to prioritize product development by alleviating the complexities\nassociated with governance. In addition, it provides a building block for\nacademic exploration of generic CNA frameworks, highlighting their relevance in\nthe evolving cloud computing landscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of cloud computing has given rise to Cloud Native Applications\n(CNAs), presenting new challenges in governance, particularly when faced with\nstrict compliance requirements. This work explores the unique characteristics\nof CNAs and their impact on governance. We introduce a comprehensive reference\narchitecture designed to streamline governance across CNAs along with a sample\nimplementation, offering insights for both single and multi-cloud environments.\nOur architecture seamlessly integrates governance within the CNA framework,\nadhering to a \"battery-included\" philosophy. Tailored for both expansive and\ncompact CNA deployments across various industries, this design enables cloud\npractitioners to prioritize product development by alleviating the complexities\nassociated with governance. In addition, it provides a building block for\nacademic exploration of generic CNA frameworks, highlighting their relevance in\nthe evolving cloud computing landscape."
                },
                "authors": [
                    {
                        "name": "William Pourmajidi"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "John Steinbacher"
                    },
                    {
                        "name": "Tony Erwin"
                    },
                    {
                        "name": "Andriy Miranskyy"
                    }
                ],
                "author_detail": {
                    "name": "Andriy Miranskyy"
                },
                "author": "Andriy Miranskyy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2302.11617v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2302.11617v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01945v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01945v1",
                "updated": "2025-01-03T18:51:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    51,
                    18,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T18:51:18Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    18,
                    51,
                    18,
                    4,
                    3,
                    0
                ],
                "title": "Cold-Start Recommendation towards the Era of Large Language Models\n  (LLMs): A Comprehensive Survey and Roadmap",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold-Start Recommendation towards the Era of Large Language Models\n  (LLMs): A Comprehensive Survey and Roadmap"
                },
                "summary": "Cold-start problem is one of the long-standing challenges in recommender\nsystems, focusing on accurately modeling new or interaction-limited users or\nitems to provide better recommendations. Due to the diversification of internet\nplatforms and the exponential growth of users and items, the importance of\ncold-start recommendation (CSR) is becoming increasingly evident. At the same\ntime, large language models (LLMs) have achieved tremendous success and possess\nstrong capabilities in modeling user and item information, providing new\npotential for cold-start recommendations. However, the research community on\nCSR still lacks a comprehensive review and reflection in this field. Based on\nthis, in this paper, we stand in the context of the era of large language\nmodels and provide a comprehensive review and discussion on the roadmap,\nrelated literature, and future directions of CSR. Specifically, we have\nconducted an exploration of the development path of how existing CSR utilizes\ninformation, from content features, graph relations, and domain information, to\nthe world knowledge possessed by large language models, aiming to provide new\ninsights for both the research and industrial communities on CSR. Related\nresources of cold-start recommendations are collected and continuously updated\nfor the community in\nhttps://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cold-start problem is one of the long-standing challenges in recommender\nsystems, focusing on accurately modeling new or interaction-limited users or\nitems to provide better recommendations. Due to the diversification of internet\nplatforms and the exponential growth of users and items, the importance of\ncold-start recommendation (CSR) is becoming increasingly evident. At the same\ntime, large language models (LLMs) have achieved tremendous success and possess\nstrong capabilities in modeling user and item information, providing new\npotential for cold-start recommendations. However, the research community on\nCSR still lacks a comprehensive review and reflection in this field. Based on\nthis, in this paper, we stand in the context of the era of large language\nmodels and provide a comprehensive review and discussion on the roadmap,\nrelated literature, and future directions of CSR. Specifically, we have\nconducted an exploration of the development path of how existing CSR utilizes\ninformation, from content features, graph relations, and domain information, to\nthe world knowledge possessed by large language models, aiming to provide new\ninsights for both the research and industrial communities on CSR. Related\nresources of cold-start recommendations are collected and continuously updated\nfor the community in\nhttps://github.com/YuanchenBei/Awesome-Cold-Start-Recommendation."
                },
                "authors": [
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Yuanchen Bei"
                    },
                    {
                        "name": "Liangwei Yang"
                    },
                    {
                        "name": "Henry Peng Zou"
                    },
                    {
                        "name": "Peilin Zhou"
                    },
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Jianling Wang"
                    },
                    {
                        "name": "Yu Wang"
                    },
                    {
                        "name": "Feiran Huang"
                    },
                    {
                        "name": "Sheng Zhou"
                    },
                    {
                        "name": "Jiajun Bu"
                    },
                    {
                        "name": "Allen Lin"
                    },
                    {
                        "name": "James Caverlee"
                    },
                    {
                        "name": "Fakhri Karray"
                    },
                    {
                        "name": "Irwin King"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01945v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01945v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01904v1",
                "updated": "2025-01-03T17:14:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    14,
                    16,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T17:14:16Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    14,
                    16,
                    4,
                    3,
                    0
                ],
                "title": "Virgo: A Preliminary Exploration on Reproducing o1-like MLLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virgo: A Preliminary Exploration on Reproducing o1-like MLLM"
                },
                "summary": "Recently, slow-thinking reasoning systems, built upon large language models\n(LLMs), have garnered widespread attention by scaling the thinking time during\ninference. There is also growing interest in adapting this capability to\nmultimodal large language models (MLLMs). Given that MLLMs handle more complex\ndata semantics across different modalities, it is intuitively more challenging\nto implement multimodal slow-thinking systems.\n  To address this issue, in this paper, we explore a straightforward approach\nby fine-tuning a capable MLLM with a small amount of textual long-form thought\ndata, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning\nwith long thought). We find that these long-form reasoning processes, expressed\nin natural language, can be effectively transferred to MLLMs. Moreover, it\nseems that such textual reasoning data can be even more effective than visual\nreasoning data in eliciting the slow-thinking capacities of MLLMs. While this\nwork is preliminary, it demonstrates that slow-thinking capacities are\nfundamentally associated with the language model component, which can be\ntransferred across modalities or domains. This finding can be leveraged to\nguide the development of more powerful slow-thinking reasoning systems. We\nrelease our resources at https://github.com/RUCAIBox/Virgo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, slow-thinking reasoning systems, built upon large language models\n(LLMs), have garnered widespread attention by scaling the thinking time during\ninference. There is also growing interest in adapting this capability to\nmultimodal large language models (MLLMs). Given that MLLMs handle more complex\ndata semantics across different modalities, it is intuitively more challenging\nto implement multimodal slow-thinking systems.\n  To address this issue, in this paper, we explore a straightforward approach\nby fine-tuning a capable MLLM with a small amount of textual long-form thought\ndata, resulting in a multimodal slow-thinking system, Virgo (Visual reasoning\nwith long thought). We find that these long-form reasoning processes, expressed\nin natural language, can be effectively transferred to MLLMs. Moreover, it\nseems that such textual reasoning data can be even more effective than visual\nreasoning data in eliciting the slow-thinking capacities of MLLMs. While this\nwork is preliminary, it demonstrates that slow-thinking capacities are\nfundamentally associated with the language model component, which can be\ntransferred across modalities or domains. This finding can be leveraged to\nguide the development of more powerful slow-thinking reasoning systems. We\nrelease our resources at https://github.com/RUCAIBox/Virgo."
                },
                "authors": [
                    {
                        "name": "Yifan Du"
                    },
                    {
                        "name": "Zikang Liu"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Yuqi Huo"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Zhongyuan Wang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Technical Report on Slow Thinking with LLMs: Visual Reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01903v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01903v1",
                "updated": "2025-01-03T17:13:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    13,
                    34,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T17:13:34Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    13,
                    34,
                    4,
                    3,
                    0
                ],
                "title": "Teaching Mining Software Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teaching Mining Software Repositories"
                },
                "summary": "Mining Software Repositories (MSR) has become a popular research area\nrecently. MSR analyzes different sources of data, such as version control\nsystems, code repositories, defect tracking systems, archived communication,\ndeployment logs, and so on, to uncover interesting and actionable insights from\nthe data for improved software development, maintenance, and evolution. This\nchapter provides an overview of MSR and how to conduct an MSR study, including\nsetting up a study, formulating research goals and questions, identifying\nrepositories, extracting and cleaning the data, performing data analysis and\nsynthesis, and discussing MSR study limitations. Furthermore, the chapter\ndiscusses MSR as part of a mixed method study, how to mine data ethically, and\ngives an overview of recent trends in MSR as well as reflects on the future. As\na teaching aid, the chapter provides tips for educators, exercises for students\nat all levels, and a list of repositories that can be used as a starting point\nfor an MSR study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mining Software Repositories (MSR) has become a popular research area\nrecently. MSR analyzes different sources of data, such as version control\nsystems, code repositories, defect tracking systems, archived communication,\ndeployment logs, and so on, to uncover interesting and actionable insights from\nthe data for improved software development, maintenance, and evolution. This\nchapter provides an overview of MSR and how to conduct an MSR study, including\nsetting up a study, formulating research goals and questions, identifying\nrepositories, extracting and cleaning the data, performing data analysis and\nsynthesis, and discussing MSR study limitations. Furthermore, the chapter\ndiscusses MSR as part of a mixed method study, how to mine data ethically, and\ngives an overview of recent trends in MSR as well as reflects on the future. As\na teaching aid, the chapter provides tips for educators, exercises for students\nat all levels, and a list of repositories that can be used as a starting point\nfor an MSR study."
                },
                "authors": [
                    {
                        "name": "Zadia Codabux"
                    },
                    {
                        "name": "Fatemeh Fard"
                    },
                    {
                        "name": "Roberto Verdecchia"
                    },
                    {
                        "name": "Fabio Palomba"
                    },
                    {
                        "name": "Dario Di Nucci"
                    },
                    {
                        "name": "Gilberto Recupito"
                    }
                ],
                "author_detail": {
                    "name": "Gilberto Recupito"
                },
                "author": "Gilberto Recupito",
                "arxiv_comment": "41 pages, Preprint for the chapter \"Handbook for Teaching Empirical\n  Software Engineering\", Springer Book",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01903v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01903v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19784v2",
                "updated": "2025-01-03T17:03:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    17,
                    3,
                    26,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-27T18:25:27Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    18,
                    25,
                    27,
                    4,
                    362,
                    0
                ],
                "title": "Can AI Help with Your Personal Finances?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can AI Help with Your Personal Finances?"
                },
                "summary": "In recent years, Large Language Models (LLMs) have emerged as a\ntransformative development in artificial intelligence (AI), drawing significant\nattention from industry and academia. Trained on vast datasets, these\nsophisticated AI systems exhibit impressive natural language processing and\ncontent generation capabilities. This paper explores the potential of LLMs to\naddress key challenges in personal finance, focusing on the United States. We\nevaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,\nAnthropic's Claude, and Meta's Llama, to assess their effectiveness in\nproviding accurate financial advice on topics such as mortgages, taxes, loans,\nand investments. Our findings show that while these models achieve an average\naccuracy rate of approximately 70%, they also display notable limitations in\ncertain areas. Specifically, LLMs struggle to provide accurate responses for\ncomplex financial queries, with performance varying significantly across\ndifferent topics. Despite these limitations, the analysis reveals notable\nimprovements in newer versions of these models, highlighting their growing\nutility for individuals and financial advisors. As these AI systems continue to\nevolve, their potential for advancing AI-driven applications in personal\nfinance becomes increasingly promising.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Large Language Models (LLMs) have emerged as a\ntransformative development in artificial intelligence (AI), drawing significant\nattention from industry and academia. Trained on vast datasets, these\nsophisticated AI systems exhibit impressive natural language processing and\ncontent generation capabilities. This paper explores the potential of LLMs to\naddress key challenges in personal finance, focusing on the United States. We\nevaluate several leading LLMs, including OpenAI's ChatGPT, Google's Gemini,\nAnthropic's Claude, and Meta's Llama, to assess their effectiveness in\nproviding accurate financial advice on topics such as mortgages, taxes, loans,\nand investments. Our findings show that while these models achieve an average\naccuracy rate of approximately 70%, they also display notable limitations in\ncertain areas. Specifically, LLMs struggle to provide accurate responses for\ncomplex financial queries, with performance varying significantly across\ndifferent topics. Despite these limitations, the analysis reveals notable\nimprovements in newer versions of these models, highlighting their growing\nutility for individuals and financial advisors. As these AI systems continue to\nevolve, their potential for advancing AI-driven applications in personal\nfinance becomes increasingly promising."
                },
                "authors": [
                    {
                        "name": "Oudom Hean"
                    },
                    {
                        "name": "Utsha Saha"
                    },
                    {
                        "name": "Binita Saha"
                    }
                ],
                "author_detail": {
                    "name": "Binita Saha"
                },
                "author": "Binita Saha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01891v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01891v1",
                "updated": "2025-01-03T16:55:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    16,
                    55,
                    32,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T16:55:32Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    16,
                    55,
                    32,
                    4,
                    3,
                    0
                ],
                "title": "Two-cavity-mediated photon-pair emission by one atom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-cavity-mediated photon-pair emission by one atom"
                },
                "summary": "Photon-pair sources are widely used in quantum optics and quantum information\nexperiments. Despite their broad deployment, there has not yet been an\non-demand implementation with efficient into-fiber photon generation and high\nsingle-photon purity. Here we report on such a source based on a single atom\nwith three energy levels in ladder configuration and coupled to two optical\nfiber cavities. We efficiently generate photon pairs with in-fiber emission\nefficiency of $\\eta_{\\mathrm{pair}}=16(1)\\%$ and study their temporal\ncorrelation properties. We simulate theoretically a regime with strong\natom-cavity coupling and find that photons are directly emitted from the ground\nstate, i.e. without atomic population in any intermediate state. We propose a\nscenario to observe such a double-vacuum-stimulated effect experimentally.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photon-pair sources are widely used in quantum optics and quantum information\nexperiments. Despite their broad deployment, there has not yet been an\non-demand implementation with efficient into-fiber photon generation and high\nsingle-photon purity. Here we report on such a source based on a single atom\nwith three energy levels in ladder configuration and coupled to two optical\nfiber cavities. We efficiently generate photon pairs with in-fiber emission\nefficiency of $\\eta_{\\mathrm{pair}}=16(1)\\%$ and study their temporal\ncorrelation properties. We simulate theoretically a regime with strong\natom-cavity coupling and find that photons are directly emitted from the ground\nstate, i.e. without atomic population in any intermediate state. We propose a\nscenario to observe such a double-vacuum-stimulated effect experimentally."
                },
                "authors": [
                    {
                        "name": "Gianvito Chiarella"
                    },
                    {
                        "name": "Tobias Frank"
                    },
                    {
                        "name": "Pau Farrera"
                    },
                    {
                        "name": "Gerhard Rempe"
                    }
                ],
                "author_detail": {
                    "name": "Gerhard Rempe"
                },
                "author": "Gerhard Rempe",
                "arxiv_doi": "10.1364/OPTICAQ.529241",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1364/OPTICAQ.529241",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.01891v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01891v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 8 figures, 5 appendices",
                "arxiv_journal_ref": "Optica Quantum 2, 346-350 (2024)",
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01889v1",
                "updated": "2025-01-03T16:49:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    16,
                    49,
                    17,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T16:49:17Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    16,
                    49,
                    17,
                    4,
                    3,
                    0
                ],
                "title": "Exploring Equality: An Investigation into Custom Loss Functions for\n  Fairness Definitions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Equality: An Investigation into Custom Loss Functions for\n  Fairness Definitions"
                },
                "summary": "This paper explores the complex tradeoffs between various fairness metrics\nsuch as equalized odds, disparate impact, and equal opportunity and predictive\naccuracy within COMPAS by building neural networks trained with custom loss\nfunctions optimized to specific fairness criteria. This paper creates the first\nfairness-driven implementation of the novel Group Accuracy Parity (GAP)\nframework, as theoretically proposed by Gupta et al. (2024), and applies it to\nCOMPAS. To operationalize and accurately compare the fairness of COMPAS models\noptimized to differing fairness ideals, this paper develops and proposes a\ncombinatory analytical procedure that incorporates Pareto front and\nmultivariate analysis, leveraging data visualizations such as violin graphs.\nThis paper concludes that GAP achieves an enhanced equilibrium between fairness\nand accuracy compared to COMPAS's current nationwide implementation and\nalternative implementations of COMPAS optimized to more traditional fairness\ndefinitions. While this paper's algorithmic improvements of COMPAS\nsignificantly augment its fairness, external biases undermine the fairness of\nits implementation. Practices such as predictive policing and issues such as\nthe lack of transparency regarding COMPAS's internal workings have contributed\nto the algorithm's historical injustice. In conjunction with developments\nregarding COMPAS's predictive methodology, legal and institutional changes must\nhappen for COMPAS's just deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the complex tradeoffs between various fairness metrics\nsuch as equalized odds, disparate impact, and equal opportunity and predictive\naccuracy within COMPAS by building neural networks trained with custom loss\nfunctions optimized to specific fairness criteria. This paper creates the first\nfairness-driven implementation of the novel Group Accuracy Parity (GAP)\nframework, as theoretically proposed by Gupta et al. (2024), and applies it to\nCOMPAS. To operationalize and accurately compare the fairness of COMPAS models\noptimized to differing fairness ideals, this paper develops and proposes a\ncombinatory analytical procedure that incorporates Pareto front and\nmultivariate analysis, leveraging data visualizations such as violin graphs.\nThis paper concludes that GAP achieves an enhanced equilibrium between fairness\nand accuracy compared to COMPAS's current nationwide implementation and\nalternative implementations of COMPAS optimized to more traditional fairness\ndefinitions. While this paper's algorithmic improvements of COMPAS\nsignificantly augment its fairness, external biases undermine the fairness of\nits implementation. Practices such as predictive policing and issues such as\nthe lack of transparency regarding COMPAS's internal workings have contributed\nto the algorithm's historical injustice. In conjunction with developments\nregarding COMPAS's predictive methodology, legal and institutional changes must\nhappen for COMPAS's just deployment."
                },
                "authors": [
                    {
                        "name": "Gordon Lee"
                    },
                    {
                        "name": "Simeon Sayer"
                    }
                ],
                "author_detail": {
                    "name": "Simeon Sayer"
                },
                "author": "Simeon Sayer",
                "arxiv_comment": "17 Pages, 12 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14205v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14205v4",
                "updated": "2025-01-03T16:44:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    16,
                    44,
                    55,
                    4,
                    3,
                    0
                ],
                "published": "2024-05-23T06:03:19Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    6,
                    3,
                    19,
                    3,
                    144,
                    0
                ],
                "title": "Agent Planning with World Knowledge Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agent Planning with World Knowledge Model"
                },
                "summary": "Recent endeavors towards directly using large language models (LLMs) as agent\nmodels to execute interactive planning tasks have shown commendable results.\nDespite their achievements, however, they still struggle with brainless\ntrial-and-error in global planning and generating hallucinatory actions in\nlocal planning due to their poor understanding of the ``real'' physical world.\nImitating humans' mental world knowledge model which provides global prior\nknowledge before the task and maintains local dynamic knowledge during the\ntask, in this paper, we introduce parametric World Knowledge Model (WKM) to\nfacilitate agent planning. Concretely, we steer the agent model to\nself-synthesize knowledge from both expert and sampled trajectories. Then we\ndevelop WKM, providing prior task knowledge to guide the global planning and\ndynamic state knowledge to assist the local planning. Experimental results on\nthree complex real-world simulated datasets with three state-of-the-art\nopen-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our\nmethod can achieve superior performance compared to various strong baselines.\nBesides, we analyze to illustrate that our WKM can effectively alleviate the\nblind trial-and-error and hallucinatory action issues, providing strong support\nfor the agent's understanding of the world. Other interesting findings include:\n1) our instance-level task knowledge can generalize better to unseen tasks, 2)\nweak WKM can guide strong agent model planning, and 3) unified WKM training has\npromising potential for further development. The code is available at\nhttps://github.com/zjunlp/WKM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent endeavors towards directly using large language models (LLMs) as agent\nmodels to execute interactive planning tasks have shown commendable results.\nDespite their achievements, however, they still struggle with brainless\ntrial-and-error in global planning and generating hallucinatory actions in\nlocal planning due to their poor understanding of the ``real'' physical world.\nImitating humans' mental world knowledge model which provides global prior\nknowledge before the task and maintains local dynamic knowledge during the\ntask, in this paper, we introduce parametric World Knowledge Model (WKM) to\nfacilitate agent planning. Concretely, we steer the agent model to\nself-synthesize knowledge from both expert and sampled trajectories. Then we\ndevelop WKM, providing prior task knowledge to guide the global planning and\ndynamic state knowledge to assist the local planning. Experimental results on\nthree complex real-world simulated datasets with three state-of-the-art\nopen-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our\nmethod can achieve superior performance compared to various strong baselines.\nBesides, we analyze to illustrate that our WKM can effectively alleviate the\nblind trial-and-error and hallucinatory action issues, providing strong support\nfor the agent's understanding of the world. Other interesting findings include:\n1) our instance-level task knowledge can generalize better to unseen tasks, 2)\nweak WKM can guide strong agent model planning, and 3) unified WKM training has\npromising potential for further development. The code is available at\nhttps://github.com/zjunlp/WKM."
                },
                "authors": [
                    {
                        "name": "Shuofei Qiao"
                    },
                    {
                        "name": "Runnan Fang"
                    },
                    {
                        "name": "Ningyu Zhang"
                    },
                    {
                        "name": "Yuqi Zhu"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Shumin Deng"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Huajun Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huajun Chen"
                },
                "author": "Huajun Chen",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14205v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14205v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01257v2",
                "updated": "2025-01-03T16:36:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    16,
                    36,
                    12,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-02T13:49:00Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    49,
                    0,
                    3,
                    2,
                    0
                ],
                "title": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with\n  Human-comparable Elo Ratings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeElo: Benchmarking Competition-level Code Generation of LLMs with\n  Human-comparable Elo Ratings"
                },
                "summary": "With the increasing code reasoning capabilities of existing large language\nmodels (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3,\nthere is a growing need to develop more challenging and comprehensive\nbenchmarks that effectively test their sophisticated competition-level coding\nabilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to\nthe unavailability of private test cases, lack of support for special judges,\nand misaligned execution environments. To bridge this gap, we introduce\nCodeElo, a standardized competition-level code generation benchmark that\neffectively addresses all these challenges for the first time. CodeElo\nbenchmark is mainly based on the official CodeForces platform and tries to\nalign with the platform as much as possible. We compile the recent six months\nof contest problems on CodeForces with detailed information such as contest\ndivisions, problem difficulty ratings, and problem algorithm tags. We introduce\na unique judging method in which problems are submitted directly to the\nplatform and develop a reliable Elo rating calculation system that aligns with\nthe platform and is comparable with human participants but has lower variance.\nBy testing on our CodeElo, we provide the Elo ratings of 30 existing popular\nopen-source and 3 proprietary LLMs for the first time. The results show that\no1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of\n1578 and 1261, respectively, while other models struggle even with the easiest\nproblems, placing in the lowest 25 percent among all human participants.\nDetailed analysis experiments are also conducted to provide insights into\nperformance across algorithms and comparisons between using C++ and Python,\nwhich can suggest directions for future studies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the increasing code reasoning capabilities of existing large language\nmodels (LLMs) and breakthroughs in reasoning models like OpenAI o1 and o3,\nthere is a growing need to develop more challenging and comprehensive\nbenchmarks that effectively test their sophisticated competition-level coding\nabilities. Existing benchmarks, like LiveCodeBench and USACO, fall short due to\nthe unavailability of private test cases, lack of support for special judges,\nand misaligned execution environments. To bridge this gap, we introduce\nCodeElo, a standardized competition-level code generation benchmark that\neffectively addresses all these challenges for the first time. CodeElo\nbenchmark is mainly based on the official CodeForces platform and tries to\nalign with the platform as much as possible. We compile the recent six months\nof contest problems on CodeForces with detailed information such as contest\ndivisions, problem difficulty ratings, and problem algorithm tags. We introduce\na unique judging method in which problems are submitted directly to the\nplatform and develop a reliable Elo rating calculation system that aligns with\nthe platform and is comparable with human participants but has lower variance.\nBy testing on our CodeElo, we provide the Elo ratings of 30 existing popular\nopen-source and 3 proprietary LLMs for the first time. The results show that\no1-mini and QwQ-32B-Preview stand out significantly, achieving Elo ratings of\n1578 and 1261, respectively, while other models struggle even with the easiest\nproblems, placing in the lowest 25 percent among all human participants.\nDetailed analysis experiments are also conducted to provide insights into\nperformance across algorithms and comparisons between using C++ and Python,\nwhich can suggest directions for future studies."
                },
                "authors": [
                    {
                        "name": "Shanghaoran Quan"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Xuancheng Ren"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yibo Miao"
                    },
                    {
                        "name": "Yunlong Feng"
                    },
                    {
                        "name": "Zekun Wang"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Yang Fan"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Junyang Lin"
                    }
                ],
                "author_detail": {
                    "name": "Junyang Lin"
                },
                "author": "Junyang Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14561v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14561v3",
                "updated": "2025-01-03T16:06:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    16,
                    6,
                    56,
                    4,
                    3,
                    0
                ],
                "published": "2024-07-18T17:59:01Z",
                "published_parsed": [
                    2024,
                    7,
                    18,
                    17,
                    59,
                    1,
                    3,
                    200,
                    0
                ],
                "title": "NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model\n  Internals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model\n  Internals"
                },
                "summary": "We introduce NNsight and NDIF, technologies that work in tandem to enable\nscientific study of very large neural networks. NNsight is an open-source\nsystem that extends PyTorch to introduce deferred remote execution. NDIF is a\nscalable inference service that executes NNsight requests, allowing users to\nshare GPU resources and pretrained models. These technologies are enabled by\nthe intervention graph, an architecture developed to decouple experiment design\nfrom model runtime. Together, this framework provides transparent and efficient\naccess to the internals of deep neural networks such as very large language\nmodels (LLMs) without imposing the cost or complexity of hosting customized\nmodels individually. We conduct a quantitative survey of the machine learning\nliterature that reveals a growing gap in the study of the internals of\nlarge-scale AI. We demonstrate the design and use of our framework to address\nthis gap by enabling a range of research methods on huge models. Finally, we\nconduct benchmarks to compare performance with previous approaches. Code\ndocumentation, and materials are available at https://nnsight.net/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce NNsight and NDIF, technologies that work in tandem to enable\nscientific study of very large neural networks. NNsight is an open-source\nsystem that extends PyTorch to introduce deferred remote execution. NDIF is a\nscalable inference service that executes NNsight requests, allowing users to\nshare GPU resources and pretrained models. These technologies are enabled by\nthe intervention graph, an architecture developed to decouple experiment design\nfrom model runtime. Together, this framework provides transparent and efficient\naccess to the internals of deep neural networks such as very large language\nmodels (LLMs) without imposing the cost or complexity of hosting customized\nmodels individually. We conduct a quantitative survey of the machine learning\nliterature that reveals a growing gap in the study of the internals of\nlarge-scale AI. We demonstrate the design and use of our framework to address\nthis gap by enabling a range of research methods on huge models. Finally, we\nconduct benchmarks to compare performance with previous approaches. Code\ndocumentation, and materials are available at https://nnsight.net/."
                },
                "authors": [
                    {
                        "name": "Jaden Fiotto-Kaufman"
                    },
                    {
                        "name": "Alexander R. Loftus"
                    },
                    {
                        "name": "Eric Todd"
                    },
                    {
                        "name": "Jannik Brinkmann"
                    },
                    {
                        "name": "Koyena Pal"
                    },
                    {
                        "name": "Dmitrii Troitskii"
                    },
                    {
                        "name": "Michael Ripa"
                    },
                    {
                        "name": "Adam Belfki"
                    },
                    {
                        "name": "Can Rager"
                    },
                    {
                        "name": "Caden Juang"
                    },
                    {
                        "name": "Aaron Mueller"
                    },
                    {
                        "name": "Samuel Marks"
                    },
                    {
                        "name": "Arnab Sen Sharma"
                    },
                    {
                        "name": "Francesca Lucchetti"
                    },
                    {
                        "name": "Nikhil Prakash"
                    },
                    {
                        "name": "Carla Brodley"
                    },
                    {
                        "name": "Arjun Guha"
                    },
                    {
                        "name": "Jonathan Bell"
                    },
                    {
                        "name": "Byron C. Wallace"
                    },
                    {
                        "name": "David Bau"
                    }
                ],
                "author_detail": {
                    "name": "David Bau"
                },
                "author": "David Bau",
                "arxiv_comment": "Code at https://nnsight.net",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14561v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14561v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13548v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13548v2",
                "updated": "2025-01-03T15:18:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    15,
                    18,
                    18,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-18T06:49:46Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    6,
                    49,
                    46,
                    2,
                    353,
                    0
                ],
                "title": "TelePreview: A User-Friendly Teleoperation System with Virtual Arm\n  Assistance for Enhanced Effectiveness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TelePreview: A User-Friendly Teleoperation System with Virtual Arm\n  Assistance for Enhanced Effectiveness"
                },
                "summary": "Teleoperation provides an effective way to collect robot data, which is\ncrucial for learning from demonstrations. In this field, teleoperation faces\nseveral key challenges: user-friendliness for new users, safety assurance, and\ntransferability across different platforms. While collecting real robot\ndexterous manipulation data by teleoperation to train robots has shown\nimpressive results on diverse tasks, due to the morphological differences\nbetween human and robot hands, it is not only hard for new users to understand\nthe action mapping but also raises potential safety concerns during operation.\nTo address these limitations, we introduce TelePreview. This teleoperation\nsystem offers real-time visual feedback on robot actions based on human user\ninputs, with a total hardware cost of less than $1,000. TelePreview allows the\nuser to see a virtual robot that represents the outcome of the user's next\nmovement. By enabling flexible switching between command visualization and\nactual execution, this system helps new users learn how to demonstrate quickly\nand safely. We demonstrate that it outperforms other teleoperation systems\nacross five tasks, emphasize its ease of use, and highlight its straightforward\ndeployment across diverse robotic platforms. We release our code and a\ndeployment document on our website https://nus-lins-lab.github.io/telepreview/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Teleoperation provides an effective way to collect robot data, which is\ncrucial for learning from demonstrations. In this field, teleoperation faces\nseveral key challenges: user-friendliness for new users, safety assurance, and\ntransferability across different platforms. While collecting real robot\ndexterous manipulation data by teleoperation to train robots has shown\nimpressive results on diverse tasks, due to the morphological differences\nbetween human and robot hands, it is not only hard for new users to understand\nthe action mapping but also raises potential safety concerns during operation.\nTo address these limitations, we introduce TelePreview. This teleoperation\nsystem offers real-time visual feedback on robot actions based on human user\ninputs, with a total hardware cost of less than $1,000. TelePreview allows the\nuser to see a virtual robot that represents the outcome of the user's next\nmovement. By enabling flexible switching between command visualization and\nactual execution, this system helps new users learn how to demonstrate quickly\nand safely. We demonstrate that it outperforms other teleoperation systems\nacross five tasks, emphasize its ease of use, and highlight its straightforward\ndeployment across diverse robotic platforms. We release our code and a\ndeployment document on our website https://nus-lins-lab.github.io/telepreview/."
                },
                "authors": [
                    {
                        "name": "Jingxiang Guo"
                    },
                    {
                        "name": "Jiayu Luo"
                    },
                    {
                        "name": "Zhenyu Wei"
                    },
                    {
                        "name": "Yiwen Hou"
                    },
                    {
                        "name": "Zhixuan Xu"
                    },
                    {
                        "name": "Xiaoyi Lin"
                    },
                    {
                        "name": "Chongkai Gao"
                    },
                    {
                        "name": "Lin Shao"
                    }
                ],
                "author_detail": {
                    "name": "Lin Shao"
                },
                "author": "Lin Shao",
                "arxiv_comment": "Submitted to RA-L",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13548v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13548v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01849v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01849v1",
                "updated": "2025-01-03T14:59:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    59,
                    38,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T14:59:38Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    59,
                    38,
                    4,
                    3,
                    0
                ],
                "title": "Multi-Agent Conversational Online Learning for Adaptive LLM Response\n  Identification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Agent Conversational Online Learning for Adaptive LLM Response\n  Identification"
                },
                "summary": "The remarkable generative capability of large language models (LLMs) has\nsparked a growing interest in automatically generating responses for different\napplications. Given the dynamic nature of user preferences and the uncertainty\nof LLM response performance, it is crucial to design efficient online learning\nalgorithms to identify optimal LLM responses (i.e., high-quality responses that\nalso meet user preferences). Most existing online algorithms adopt a\ncentralized approach and fail to leverage explicit user preferences for more\nefficient and personalized LLM response identification. In contrast, this paper\nintroduces \\textit{MACO} (\\underline{M}ulti-\\underline{A}gent\n\\underline{C}onversational \\underline{O}nline Learning for Adaptive LLM\nResponse Identification): 1) The online LLM response identification process is\naccelerated by multiple local agents (such as smartphones), while enhancing\ndata privacy; 2) A novel conversational mechanism is proposed to adaptively\nconduct conversations for soliciting user preferences (e.g., a preference for a\nhumorous tone over a serious one in generated responses), so to minimize\nuncertainty in preference estimation. Our theoretical analysis demonstrates\nthat \\cadi\\ is near-optimal regarding cumulative regret. Additionally, \\cadi\\\noffers reduced communication costs and computational complexity by eliminating\nthe traditional, computing-intensive ``G-optimal design\" found in previous\nworks. Extensive experiments with the open LLM \\textit{Llama}, coupled with two\ndifferent embedding models from Google and OpenAI for text vector\nrepresentation, demonstrate that \\cadi\\ significantly outperforms the current\nstate-of-the-art in online LLM response identification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The remarkable generative capability of large language models (LLMs) has\nsparked a growing interest in automatically generating responses for different\napplications. Given the dynamic nature of user preferences and the uncertainty\nof LLM response performance, it is crucial to design efficient online learning\nalgorithms to identify optimal LLM responses (i.e., high-quality responses that\nalso meet user preferences). Most existing online algorithms adopt a\ncentralized approach and fail to leverage explicit user preferences for more\nefficient and personalized LLM response identification. In contrast, this paper\nintroduces \\textit{MACO} (\\underline{M}ulti-\\underline{A}gent\n\\underline{C}onversational \\underline{O}nline Learning for Adaptive LLM\nResponse Identification): 1) The online LLM response identification process is\naccelerated by multiple local agents (such as smartphones), while enhancing\ndata privacy; 2) A novel conversational mechanism is proposed to adaptively\nconduct conversations for soliciting user preferences (e.g., a preference for a\nhumorous tone over a serious one in generated responses), so to minimize\nuncertainty in preference estimation. Our theoretical analysis demonstrates\nthat \\cadi\\ is near-optimal regarding cumulative regret. Additionally, \\cadi\\\noffers reduced communication costs and computational complexity by eliminating\nthe traditional, computing-intensive ``G-optimal design\" found in previous\nworks. Extensive experiments with the open LLM \\textit{Llama}, coupled with two\ndifferent embedding models from Google and OpenAI for text vector\nrepresentation, demonstrate that \\cadi\\ significantly outperforms the current\nstate-of-the-art in online LLM response identification."
                },
                "authors": [
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Yuejin Xie"
                    },
                    {
                        "name": "Maoli Liu"
                    },
                    {
                        "name": "Xuchuang Wang"
                    },
                    {
                        "name": "Zhuohua Li"
                    },
                    {
                        "name": "Huanyu Wang"
                    },
                    {
                        "name": "John C. S. Lui"
                    }
                ],
                "author_detail": {
                    "name": "John C. S. Lui"
                },
                "author": "John C. S. Lui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01849v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01849v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01834v1",
                "updated": "2025-01-03T14:38:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    38,
                    1,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T14:38:01Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    38,
                    1,
                    4,
                    3,
                    0
                ],
                "title": "MoColl: Agent-Based Specific and General Model Collaboration for Image\n  Captioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoColl: Agent-Based Specific and General Model Collaboration for Image\n  Captioning"
                },
                "summary": "Image captioning is a critical task at the intersection of computer vision\nand natural language processing, with wide-ranging applications across various\ndomains. For complex tasks such as diagnostic report generation, deep learning\nmodels require not only domain-specific image-caption datasets but also the\nincorporation of relevant general knowledge to provide contextual accuracy.\nExisting approaches exhibit inherent limitations: specialized models excel in\ncapturing domain-specific details but lack generalization, while\nvision-language models (VLMs) built on large language models (LLMs) leverage\ngeneral knowledge but struggle with domain-specific adaptation. To address\nthese limitations, this paper proposes a novel agent-enhanced model\ncollaboration framework, which we called \\textbf{MoColl}, designed to\neffectively integrate domain-specific and general knowledge. Specifically, our\napproach is to decompose complex image captioning tasks into a series of\ninterconnected question-answer subtasks. A trainable visual question answering\n(VQA) model is employed as a specialized tool to focus on domain-specific\nvisual analysis, answering task-specific questions based on image content.\nConcurrently, an LLM-based agent with general knowledge formulates these\nquestions and synthesizes the resulting question-answer pairs into coherent\ncaptions. Beyond its role in leveraging the VQA model, the agent further guides\nits training to enhance its domain-specific capabilities. Experimental results\non radiology report generation validate the effectiveness of the proposed\nframework, demonstrating significant improvements in the quality of generated\nreports.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Image captioning is a critical task at the intersection of computer vision\nand natural language processing, with wide-ranging applications across various\ndomains. For complex tasks such as diagnostic report generation, deep learning\nmodels require not only domain-specific image-caption datasets but also the\nincorporation of relevant general knowledge to provide contextual accuracy.\nExisting approaches exhibit inherent limitations: specialized models excel in\ncapturing domain-specific details but lack generalization, while\nvision-language models (VLMs) built on large language models (LLMs) leverage\ngeneral knowledge but struggle with domain-specific adaptation. To address\nthese limitations, this paper proposes a novel agent-enhanced model\ncollaboration framework, which we called \\textbf{MoColl}, designed to\neffectively integrate domain-specific and general knowledge. Specifically, our\napproach is to decompose complex image captioning tasks into a series of\ninterconnected question-answer subtasks. A trainable visual question answering\n(VQA) model is employed as a specialized tool to focus on domain-specific\nvisual analysis, answering task-specific questions based on image content.\nConcurrently, an LLM-based agent with general knowledge formulates these\nquestions and synthesizes the resulting question-answer pairs into coherent\ncaptions. Beyond its role in leveraging the VQA model, the agent further guides\nits training to enhance its domain-specific capabilities. Experimental results\non radiology report generation validate the effectiveness of the proposed\nframework, demonstrating significant improvements in the quality of generated\nreports."
                },
                "authors": [
                    {
                        "name": "Pu Yang"
                    },
                    {
                        "name": "Bin Dong"
                    }
                ],
                "author_detail": {
                    "name": "Bin Dong"
                },
                "author": "Bin Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01832v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01832v1",
                "updated": "2025-01-03T14:34:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    34,
                    30,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T14:34:30Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    34,
                    30,
                    4,
                    3,
                    0
                ],
                "title": "Time Series Language Model for Descriptive Caption Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Language Model for Descriptive Caption Generation"
                },
                "summary": "The automatic generation of representative natural language descriptions for\nobservable patterns in time series data enhances interpretability, simplifies\nanalysis and increases cross-domain utility of temporal data. While pre-trained\nfoundation models have made considerable progress in natural language\nprocessing (NLP) and computer vision (CV), their application to time series\nanalysis has been hindered by data scarcity. Although several large language\nmodel (LLM)-based methods have been proposed for time series forecasting, time\nseries captioning is under-explored in the context of LLMs. In this paper, we\nintroduce TSLM, a novel time series language model designed specifically for\ntime series captioning. TSLM operates as an encoder-decoder model, leveraging\nboth text prompts and time series data representations to capture subtle\ntemporal patterns across multiple phases and generate precise textual\ndescriptions of time series inputs. TSLM addresses the data scarcity problem in\ntime series captioning by first leveraging an in-context prompting synthetic\ndata generation, and second denoising the generated data via a novel\ncross-modal dense retrieval scoring applied to time series-caption pairs.\nExperimental findings on various time series captioning datasets demonstrate\nthat TSLM outperforms existing state-of-the-art approaches from multiple data\nmodalities by a significant margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The automatic generation of representative natural language descriptions for\nobservable patterns in time series data enhances interpretability, simplifies\nanalysis and increases cross-domain utility of temporal data. While pre-trained\nfoundation models have made considerable progress in natural language\nprocessing (NLP) and computer vision (CV), their application to time series\nanalysis has been hindered by data scarcity. Although several large language\nmodel (LLM)-based methods have been proposed for time series forecasting, time\nseries captioning is under-explored in the context of LLMs. In this paper, we\nintroduce TSLM, a novel time series language model designed specifically for\ntime series captioning. TSLM operates as an encoder-decoder model, leveraging\nboth text prompts and time series data representations to capture subtle\ntemporal patterns across multiple phases and generate precise textual\ndescriptions of time series inputs. TSLM addresses the data scarcity problem in\ntime series captioning by first leveraging an in-context prompting synthetic\ndata generation, and second denoising the generated data via a novel\ncross-modal dense retrieval scoring applied to time series-caption pairs.\nExperimental findings on various time series captioning datasets demonstrate\nthat TSLM outperforms existing state-of-the-art approaches from multiple data\nmodalities by a significant margin."
                },
                "authors": [
                    {
                        "name": "Mohamed Trabelsi"
                    },
                    {
                        "name": "Aidan Boyd"
                    },
                    {
                        "name": "Jin Cao"
                    },
                    {
                        "name": "Huseyin Uzunalioglu"
                    }
                ],
                "author_detail": {
                    "name": "Huseyin Uzunalioglu"
                },
                "author": "Huseyin Uzunalioglu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01832v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01832v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01830v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01830v1",
                "updated": "2025-01-03T14:30:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    30,
                    14,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T14:30:14Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    30,
                    14,
                    4,
                    3,
                    0
                ],
                "title": "Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-RT: Automatic Jailbreak Strategy Exploration for Red-Teaming Large\n  Language Models"
                },
                "summary": "Automated red-teaming has become a crucial approach for uncovering\nvulnerabilities in large language models (LLMs). However, most existing methods\nfocus on isolated safety flaws, limiting their ability to adapt to dynamic\ndefenses and uncover complex vulnerabilities efficiently. To address this\nchallenge, we propose Auto-RT, a reinforcement learning framework that\nautomatically explores and optimizes complex attack strategies to effectively\nuncover security vulnerabilities through malicious queries. Specifically, we\nintroduce two key mechanisms to reduce exploration complexity and improve\nstrategy optimization: 1) Early-terminated Exploration, which accelerate\nexploration by focusing on high-potential attack strategies; and 2) Progressive\nReward Tracking algorithm with intermediate downgrade models, which dynamically\nrefine the search trajectory toward successful vulnerability exploitation.\nExtensive experiments across diverse LLMs demonstrate that, by significantly\nimproving exploration efficiency and automatically optimizing attack\nstrategies, Auto-RT detects a boarder range of vulnerabilities, achieving a\nfaster detection speed and 16.63\\% higher success rates compared to existing\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated red-teaming has become a crucial approach for uncovering\nvulnerabilities in large language models (LLMs). However, most existing methods\nfocus on isolated safety flaws, limiting their ability to adapt to dynamic\ndefenses and uncover complex vulnerabilities efficiently. To address this\nchallenge, we propose Auto-RT, a reinforcement learning framework that\nautomatically explores and optimizes complex attack strategies to effectively\nuncover security vulnerabilities through malicious queries. Specifically, we\nintroduce two key mechanisms to reduce exploration complexity and improve\nstrategy optimization: 1) Early-terminated Exploration, which accelerate\nexploration by focusing on high-potential attack strategies; and 2) Progressive\nReward Tracking algorithm with intermediate downgrade models, which dynamically\nrefine the search trajectory toward successful vulnerability exploitation.\nExtensive experiments across diverse LLMs demonstrate that, by significantly\nimproving exploration efficiency and automatically optimizing attack\nstrategies, Auto-RT detects a boarder range of vulnerabilities, achieving a\nfaster detection speed and 16.63\\% higher success rates compared to existing\nmethods."
                },
                "authors": [
                    {
                        "name": "Yanjiang Liu"
                    },
                    {
                        "name": "Shuhen Zhou"
                    },
                    {
                        "name": "Yaojie Lu"
                    },
                    {
                        "name": "Huijia Zhu"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Hongyu Lin"
                    },
                    {
                        "name": "Ben He"
                    },
                    {
                        "name": "Xianpei Han"
                    },
                    {
                        "name": "Le Sun"
                    }
                ],
                "author_detail": {
                    "name": "Le Sun"
                },
                "author": "Le Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01830v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01830v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07031v2",
                "updated": "2025-01-03T14:19:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    19,
                    58,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-09T22:37:48Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    22,
                    37,
                    48,
                    0,
                    344,
                    0
                ],
                "title": "Large Language Models: An Applied Econometric Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models: An Applied Econometric Framework"
                },
                "summary": "How can we use the novel capacities of large language models (LLMs) in\nempirical research? And how can we do so while accounting for their\nlimitations, which are themselves only poorly understood? We develop an\neconometric framework to answer this question that distinguishes between two\ntypes of empirical tasks. Using LLMs for prediction problems (including\nhypothesis generation) is valid under one condition: no ``leakage'' between the\nLLM's training dataset and the researcher's sample. No leakage can be ensured\nby using open-source LLMs with documented training data and published weights.\nUsing LLM outputs for estimation problems to automate the measurement of some\neconomic concept (expressed either by some text or from human subjects)\nrequires the researcher to collect at least some validation data: without such\ndata, the errors of the LLM's automation cannot be assessed and accounted for.\nAs long as these steps are taken, LLM outputs can be used in empirical research\nwith the familiar econometric guarantees we desire. Using two illustrative\napplications to finance and political economy, we find that these requirements\nare stringent; when they are violated, the limitations of LLMs now result in\nunreliable empirical estimates. Our results suggest the excitement around the\nempirical uses of LLMs is warranted -- they allow researchers to effectively\nuse even small amounts of language data for both prediction and estimation --\nbut only with these safeguards in place.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How can we use the novel capacities of large language models (LLMs) in\nempirical research? And how can we do so while accounting for their\nlimitations, which are themselves only poorly understood? We develop an\neconometric framework to answer this question that distinguishes between two\ntypes of empirical tasks. Using LLMs for prediction problems (including\nhypothesis generation) is valid under one condition: no ``leakage'' between the\nLLM's training dataset and the researcher's sample. No leakage can be ensured\nby using open-source LLMs with documented training data and published weights.\nUsing LLM outputs for estimation problems to automate the measurement of some\neconomic concept (expressed either by some text or from human subjects)\nrequires the researcher to collect at least some validation data: without such\ndata, the errors of the LLM's automation cannot be assessed and accounted for.\nAs long as these steps are taken, LLM outputs can be used in empirical research\nwith the familiar econometric guarantees we desire. Using two illustrative\napplications to finance and political economy, we find that these requirements\nare stringent; when they are violated, the limitations of LLMs now result in\nunreliable empirical estimates. Our results suggest the excitement around the\nempirical uses of LLMs is warranted -- they allow researchers to effectively\nuse even small amounts of language data for both prediction and estimation --\nbut only with these safeguards in place."
                },
                "authors": [
                    {
                        "name": "Jens Ludwig"
                    },
                    {
                        "name": "Sendhil Mullainathan"
                    },
                    {
                        "name": "Ashesh Rambachan"
                    }
                ],
                "author_detail": {
                    "name": "Ashesh Rambachan"
                },
                "author": "Ashesh Rambachan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01821v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01821v1",
                "updated": "2025-01-03T14:09:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    9,
                    46,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T14:09:46Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    9,
                    46,
                    4,
                    3,
                    0
                ],
                "title": "SDPO: Segment-Level Direct Preference Optimization for Social Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDPO: Segment-Level Direct Preference Optimization for Social Agents"
                },
                "summary": "Social agents powered by large language models (LLMs) can simulate human\nsocial behaviors but fall short in handling complex goal-oriented social\ndialogues. Direct Preference Optimization (DPO) has proven effective in\naligning LLM behavior with human preferences across a variety of agent tasks.\nExisting DPO-based approaches for multi-turn interactions are divided into\nturn-level and session-level methods. The turn-level method is overly\nfine-grained, focusing exclusively on individual turns, while session-level\nmethods are too coarse-grained, often introducing training noise. To address\nthese limitations, we propose Segment-Level Direct Preference Optimization\n(SDPO), which focuses on specific key segments within interactions to optimize\nmulti-turn agent behavior while minimizing training noise. Evaluations on the\nSOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform\nboth existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring\nSDPO's potential to advance the social intelligence of LLM-based agents. We\nrelease our code and data at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social agents powered by large language models (LLMs) can simulate human\nsocial behaviors but fall short in handling complex goal-oriented social\ndialogues. Direct Preference Optimization (DPO) has proven effective in\naligning LLM behavior with human preferences across a variety of agent tasks.\nExisting DPO-based approaches for multi-turn interactions are divided into\nturn-level and session-level methods. The turn-level method is overly\nfine-grained, focusing exclusively on individual turns, while session-level\nmethods are too coarse-grained, often introducing training noise. To address\nthese limitations, we propose Segment-Level Direct Preference Optimization\n(SDPO), which focuses on specific key segments within interactions to optimize\nmulti-turn agent behavior while minimizing training noise. Evaluations on the\nSOTOPIA benchmark demonstrate that SDPO-tuned agents consistently outperform\nboth existing DPO-based methods and proprietary LLMs like GPT-4o, underscoring\nSDPO's potential to advance the social intelligence of LLM-based agents. We\nrelease our code and data at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/SDPO."
                },
                "authors": [
                    {
                        "name": "Aobo Kong"
                    },
                    {
                        "name": "Wentao Ma"
                    },
                    {
                        "name": "Shiwan Zhao"
                    },
                    {
                        "name": "Yongbin Li"
                    },
                    {
                        "name": "Yuchuan Wu"
                    },
                    {
                        "name": "Ke Wang"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Qicheng Li"
                    },
                    {
                        "name": "Yong Qin"
                    },
                    {
                        "name": "Fei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fei Huang"
                },
                "author": "Fei Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01821v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01821v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01818v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01818v1",
                "updated": "2025-01-03T14:03:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    3,
                    14,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T14:03:14Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    14,
                    3,
                    14,
                    4,
                    3,
                    0
                ],
                "title": "Rerouting LLM Routers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rerouting LLM Routers"
                },
                "summary": "LLM routers aim to balance quality and cost of generation by classifying\nqueries and routing them to a cheaper or more expensive LLM depending on their\ncomplexity. Routers represent one type of what we call LLM control planes:\nsystems that orchestrate use of one or more LLMs. In this paper, we investigate\nrouters' adversarial robustness.\n  We first define LLM control plane integrity, i.e., robustness of LLM\norchestration to adversarial inputs, as a distinct problem in AI safety. Next,\nwe demonstrate that an adversary can generate query-independent token sequences\nwe call ``confounder gadgets'' that, when added to any query, cause LLM routers\nto send the query to a strong LLM.\n  Our quantitative evaluation shows that this attack is successful both in\nwhite-box and black-box settings against a variety of open-source and\ncommercial routers, and that confounding queries do not affect the quality of\nLLM responses. Finally, we demonstrate that gadgets can be effective while\nmaintaining low perplexity, thus perplexity-based filtering is not an effective\ndefense. We finish by investigating alternative defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM routers aim to balance quality and cost of generation by classifying\nqueries and routing them to a cheaper or more expensive LLM depending on their\ncomplexity. Routers represent one type of what we call LLM control planes:\nsystems that orchestrate use of one or more LLMs. In this paper, we investigate\nrouters' adversarial robustness.\n  We first define LLM control plane integrity, i.e., robustness of LLM\norchestration to adversarial inputs, as a distinct problem in AI safety. Next,\nwe demonstrate that an adversary can generate query-independent token sequences\nwe call ``confounder gadgets'' that, when added to any query, cause LLM routers\nto send the query to a strong LLM.\n  Our quantitative evaluation shows that this attack is successful both in\nwhite-box and black-box settings against a variety of open-source and\ncommercial routers, and that confounding queries do not affect the quality of\nLLM responses. Finally, we demonstrate that gadgets can be effective while\nmaintaining low perplexity, thus perplexity-based filtering is not an effective\ndefense. We finish by investigating alternative defenses."
                },
                "authors": [
                    {
                        "name": "Avital Shafran"
                    },
                    {
                        "name": "Roei Schuster"
                    },
                    {
                        "name": "Thomas Ristenpart"
                    },
                    {
                        "name": "Vitaly Shmatikov"
                    }
                ],
                "author_detail": {
                    "name": "Vitaly Shmatikov"
                },
                "author": "Vitaly Shmatikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01818v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01818v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00958v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00958v2",
                "updated": "2025-01-03T13:25:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    25,
                    27,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-01T21:29:37Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    21,
                    29,
                    37,
                    2,
                    1,
                    0
                ],
                "title": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "2.5 Years in Class: A Multimodal Textbook for Vision-Language\n  Pretraining"
                },
                "summary": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving~\\footnote{Our code are\navailable at \\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to image-text pair data, interleaved corpora enable Vision-Language\nModels (VLMs) to understand the world more naturally like humans. However, such\nexisting datasets are crawled from webpage, facing challenges like low\nknowledge density, loose image-text relations, and poor logical coherence\nbetween images. On the other hand, the internet hosts vast instructional videos\n(e.g., online geometry courses) that are widely used by humans to learn\nfoundational subjects, yet these valuable resources remain underexplored in VLM\ntraining. In this paper, we introduce a high-quality \\textbf{multimodal\ntextbook} corpus with richer foundational knowledge for VLM pretraining. It\ncollects over 2.5 years of instructional videos, totaling 22,000 class hours.\nWe first use an LLM-proposed taxonomy to systematically gather instructional\nvideos. Then we progressively extract and refine visual (keyframes), audio\n(ASR), and textual knowledge (OCR) from the videos, and organize as an\nimage-text interleaved corpus based on temporal order. Compared to its\ncounterparts, our video-centric textbook offers more coherent context, richer\nknowledge, and better image-text alignment. Experiments demonstrate its superb\npretraining performance, particularly in knowledge- and reasoning-intensive\ntasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook\nexhibit outstanding interleaved context awareness, leveraging visual and\ntextual cues in their few-shot context for task solving~\\footnote{Our code are\navailable at \\url{https://github.com/DAMO-NLP-SG/multimodal_textbook}}."
                },
                "authors": [
                    {
                        "name": "Wenqi Zhang"
                    },
                    {
                        "name": "Hang Zhang"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Jiashuo Sun"
                    },
                    {
                        "name": "Yongliang Shen"
                    },
                    {
                        "name": "Weiming Lu"
                    },
                    {
                        "name": "Deli Zhao"
                    },
                    {
                        "name": "Yueting Zhuang"
                    },
                    {
                        "name": "Lidong Bing"
                    }
                ],
                "author_detail": {
                    "name": "Lidong Bing"
                },
                "author": "Lidong Bing",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00958v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00958v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04307v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04307v2",
                "updated": "2025-01-03T13:17:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    13,
                    17,
                    32,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-05T16:26:37Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    16,
                    26,
                    37,
                    3,
                    340,
                    0
                ],
                "title": "Feature Coding in the Era of Large Models: Dataset, Test Conditions, and\n  Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature Coding in the Era of Large Models: Dataset, Test Conditions, and\n  Benchmark"
                },
                "summary": "Large models have achieved remarkable performance across various tasks, yet\nthey incur significant computational costs and privacy concerns during both\ntraining and inference. Distributed deployment has emerged as a potential\nsolution, but it necessitates the exchange of intermediate information between\nmodel segments, with feature representations serving as crucial information\ncarriers. To optimize information exchange, feature coding methods are applied\nto reduce transmission and storage overhead. Despite its importance, feature\ncoding for large models remains an under-explored area. In this paper, we draw\nattention to large model feature coding and make three contributions to this\nfield. First, we introduce a comprehensive dataset encompassing diverse\nfeatures generated by three representative types of large models. Second, we\nestablish unified test conditions, enabling standardized evaluation pipelines\nand fair comparisons across future feature coding studies. Third, we introduce\ntwo baseline methods derived from widely used image coding techniques and\nbenchmark their performance on the proposed dataset. These contributions aim to\nadvance the field of feature coding, facilitating more efficient large model\ndeployment. All source code and the dataset are now available at\n\\href{https://github.com/chansongoal/FCM-LM/tree/master}{https://github.com/chansongoal/FCM-LM/tree/master}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large models have achieved remarkable performance across various tasks, yet\nthey incur significant computational costs and privacy concerns during both\ntraining and inference. Distributed deployment has emerged as a potential\nsolution, but it necessitates the exchange of intermediate information between\nmodel segments, with feature representations serving as crucial information\ncarriers. To optimize information exchange, feature coding methods are applied\nto reduce transmission and storage overhead. Despite its importance, feature\ncoding for large models remains an under-explored area. In this paper, we draw\nattention to large model feature coding and make three contributions to this\nfield. First, we introduce a comprehensive dataset encompassing diverse\nfeatures generated by three representative types of large models. Second, we\nestablish unified test conditions, enabling standardized evaluation pipelines\nand fair comparisons across future feature coding studies. Third, we introduce\ntwo baseline methods derived from widely used image coding techniques and\nbenchmark their performance on the proposed dataset. These contributions aim to\nadvance the field of feature coding, facilitating more efficient large model\ndeployment. All source code and the dataset are now available at\n\\href{https://github.com/chansongoal/FCM-LM/tree/master}{https://github.com/chansongoal/FCM-LM/tree/master}."
                },
                "authors": [
                    {
                        "name": "Changsheng Gao"
                    },
                    {
                        "name": "Yifan Ma"
                    },
                    {
                        "name": "Qiaoxi Chen"
                    },
                    {
                        "name": "Yenan Xu"
                    },
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Weisi Lin"
                    }
                ],
                "author_detail": {
                    "name": "Weisi Lin"
                },
                "author": "Weisi Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04307v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04307v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01793v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01793v1",
                "updated": "2025-01-03T12:52:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    52,
                    51,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:52:51Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    52,
                    51,
                    4,
                    3,
                    0
                ],
                "title": "Creating Artificial Students that Never Existed: Leveraging Large\n  Language Models and CTGANs for Synthetic Data Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Creating Artificial Students that Never Existed: Leveraging Large\n  Language Models and CTGANs for Synthetic Data Generation"
                },
                "summary": "In this study, we explore the growing potential of AI and deep learning\ntechnologies, particularly Generative Adversarial Networks (GANs) and Large\nLanguage Models (LLMs), for generating synthetic tabular data. Access to\nquality students data is critical for advancing learning analytics, but privacy\nconcerns and stricter data protection regulations worldwide limit their\navailability and usage. Synthetic data offers a promising alternative. We\ninvestigate whether synthetic data can be leveraged to create artificial\nstudents for serving learning analytics models. Using the popular GAN model\nCTGAN and three LLMs- GPT2, DistilGPT2, and DialoGPT, we generate synthetic\ntabular student data. Our results demonstrate the strong potential of these\nmethods to produce high-quality synthetic datasets that resemble real students\ndata. To validate our findings, we apply a comprehensive set of utility\nevaluation metrics to assess the statistical and predictive performance of the\nsynthetic data and compare the different generator models used, specially the\nperformance of LLMs. Our study aims to provide the learning analytics community\nwith valuable insights into the use of synthetic data, laying the groundwork\nfor expanding the field methodological toolbox with new innovative approaches\nfor learning analytics data generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we explore the growing potential of AI and deep learning\ntechnologies, particularly Generative Adversarial Networks (GANs) and Large\nLanguage Models (LLMs), for generating synthetic tabular data. Access to\nquality students data is critical for advancing learning analytics, but privacy\nconcerns and stricter data protection regulations worldwide limit their\navailability and usage. Synthetic data offers a promising alternative. We\ninvestigate whether synthetic data can be leveraged to create artificial\nstudents for serving learning analytics models. Using the popular GAN model\nCTGAN and three LLMs- GPT2, DistilGPT2, and DialoGPT, we generate synthetic\ntabular student data. Our results demonstrate the strong potential of these\nmethods to produce high-quality synthetic datasets that resemble real students\ndata. To validate our findings, we apply a comprehensive set of utility\nevaluation metrics to assess the statistical and predictive performance of the\nsynthetic data and compare the different generator models used, specially the\nperformance of LLMs. Our study aims to provide the learning analytics community\nwith valuable insights into the use of synthetic data, laying the groundwork\nfor expanding the field methodological toolbox with new innovative approaches\nfor learning analytics data generation."
                },
                "authors": [
                    {
                        "name": "Mohammad Khalil"
                    },
                    {
                        "name": "Farhad Vadiee"
                    },
                    {
                        "name": "Ronas Shakya"
                    },
                    {
                        "name": "Qinyi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qinyi Liu"
                },
                "author": "Qinyi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01793v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01793v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01792v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01792v1",
                "updated": "2025-01-03T12:51:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T12:51:37Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    51,
                    37,
                    4,
                    3,
                    0
                ],
                "title": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with Activation Checkpointing and Hybrid Caching"
                },
                "summary": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent large language models (LLMs) with enormous model sizes use many GPUs\nto meet memory capacity requirements incurring substantial costs for token\ngeneration. To provide cost-effective LLM inference with relaxed latency\nconstraints, extensive research has focused on expanding GPU memory by\nleveraging the host memory. However, LLM inference engines that utilize the\nhost memory often face underutilization of GPU compute units, as a considerable\nportion of inference time is spent in loading the model onto the GPU via\nhost-GPU interconnect. To tackle these challenges of the host memory offloading\nfor LLM, we introduce HybridServe, an LLM inference system with activation\ncheckpointing based on activation caching. The activation cache stores\nactivation checkpoints generated during intermediate inference stages, allowing\nthe fast recomputation of KV cache while model parameters are transferred to\nGPU from host memory. Unlike conventional methods that recompute the KV cache\nfrom scratch using token IDs, the activation cache allows bypassing projection\nand FFN operations. To balance between the activation recomputation and\nparameter loading overhead, this study proposes a KV-activation hybrid caching\nscheme which finds the best ratio of the key-value and activation caches to\nadjust the recomputation time. Our system achieves 2.19x throughput improvement\nover the state-of-the-art prior work for offloading both model weights and KV\ncache."
                },
                "authors": [
                    {
                        "name": "Sanghyeon Lee"
                    },
                    {
                        "name": "Hongbeen Kim"
                    },
                    {
                        "name": "Soojin Hwang"
                    },
                    {
                        "name": "Guseul Heo"
                    },
                    {
                        "name": "Minwoo Noh"
                    },
                    {
                        "name": "Jaehyuk Huh"
                    }
                ],
                "author_detail": {
                    "name": "Jaehyuk Huh"
                },
                "author": "Jaehyuk Huh",
                "arxiv_comment": "14 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01792v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01792v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.17837v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.17837v2",
                "updated": "2025-01-03T12:32:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    32,
                    35,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-17T07:42:39Z",
                "published_parsed": [
                    2024,
                    12,
                    17,
                    7,
                    42,
                    39,
                    1,
                    352,
                    0
                ],
                "title": "Evaluating the Capabilities of Large Language Models for Multi-label\n  Emotion Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Capabilities of Large Language Models for Multi-label\n  Emotion Understanding"
                },
                "summary": "Large Language Models (LLMs) show promising learning and reasoning abilities.\nCompared to other NLP tasks, multilingual and multi-label emotion evaluation\ntasks are under-explored in LLMs. In this paper, we present EthioEmo, a\nmulti-label emotion classification dataset for four Ethiopian languages,\nnamely, Amharic (amh), Afan Oromo (orm), Somali (som), and Tigrinya (tir). We\nperform extensive experiments with an additional English multi-label emotion\ndataset from SemEval 2018 Task 1. Our evaluation includes encoder-only,\nencoder-decoder, and decoder-only language models. We compare zero and few-shot\napproaches of LLMs to fine-tuning smaller language models. The results show\nthat accurate multi-label emotion classification is still insufficient even for\nhigh-resource languages such as English, and there is a large gap between the\nperformance of high-resource and low-resource languages. The results also show\nvarying performance levels depending on the language and model type. EthioEmo\nis available publicly to further improve the understanding of emotions in\nlanguage models and how people convey emotions through various languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show promising learning and reasoning abilities.\nCompared to other NLP tasks, multilingual and multi-label emotion evaluation\ntasks are under-explored in LLMs. In this paper, we present EthioEmo, a\nmulti-label emotion classification dataset for four Ethiopian languages,\nnamely, Amharic (amh), Afan Oromo (orm), Somali (som), and Tigrinya (tir). We\nperform extensive experiments with an additional English multi-label emotion\ndataset from SemEval 2018 Task 1. Our evaluation includes encoder-only,\nencoder-decoder, and decoder-only language models. We compare zero and few-shot\napproaches of LLMs to fine-tuning smaller language models. The results show\nthat accurate multi-label emotion classification is still insufficient even for\nhigh-resource languages such as English, and there is a large gap between the\nperformance of high-resource and low-resource languages. The results also show\nvarying performance levels depending on the language and model type. EthioEmo\nis available publicly to further improve the understanding of emotions in\nlanguage models and how people convey emotions through various languages."
                },
                "authors": [
                    {
                        "name": "Tadesse Destaw Belay"
                    },
                    {
                        "name": "Israel Abebe Azime"
                    },
                    {
                        "name": "Abinew Ali Ayele"
                    },
                    {
                        "name": "Grigori Sidorov"
                    },
                    {
                        "name": "Dietrich Klakow"
                    },
                    {
                        "name": "Philipp Slusallek"
                    },
                    {
                        "name": "Olga Kolesnikova"
                    },
                    {
                        "name": "Seid Muhie Yimam"
                    }
                ],
                "author_detail": {
                    "name": "Seid Muhie Yimam"
                },
                "author": "Seid Muhie Yimam",
                "arxiv_comment": "COLING 2025, main conference, long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.17837v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.17837v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09916v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09916v2",
                "updated": "2025-01-03T12:01:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    12,
                    1,
                    55,
                    4,
                    3,
                    0
                ],
                "published": "2024-08-19T11:44:40Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    11,
                    44,
                    40,
                    0,
                    232,
                    0
                ],
                "title": "Attribution Analysis Meets Model Editing: Advancing Knowledge Correction\n  in Vision Language Models with VisEdit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribution Analysis Meets Model Editing: Advancing Knowledge Correction\n  in Vision Language Models with VisEdit"
                },
                "summary": "Model editing aims to correct outdated or erroneous knowledge in large models\nwithout costly retraining. Recent research discovered that the mid-layer\nrepresentation of the subject's final token in a prompt has a strong influence\non factual predictions, and developed Large Language Model (LLM) editing\ntechniques based on this observation. However, for Vision-LLMs (VLLMs), how\nvisual representations impact the predictions from a decoder-only language\nmodel remains largely unexplored. To the best of our knowledge, model editing\nfor VLLMs has not been extensively studied in the literature. In this work, we\nemploy the contribution allocation and noise perturbation methods to measure\nthe contributions of visual representations for token predictions. Our\nattribution analysis shows that visual representations in mid-to-later layers\nthat are highly relevant to the prompt contribute significantly to predictions.\nBased on these insights, we propose VisEdit, a novel model editor for VLLMs\nthat effectively corrects knowledge by editing intermediate visual\nrepresentations in regions important to the edit prompt. We evaluated VisEdit\nusing multiple VLLM backbones and public VLLM editing benchmark datasets. The\nresults show the superiority of VisEdit over the strong baselines adapted from\nexisting state-of-the-art editors for LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model editing aims to correct outdated or erroneous knowledge in large models\nwithout costly retraining. Recent research discovered that the mid-layer\nrepresentation of the subject's final token in a prompt has a strong influence\non factual predictions, and developed Large Language Model (LLM) editing\ntechniques based on this observation. However, for Vision-LLMs (VLLMs), how\nvisual representations impact the predictions from a decoder-only language\nmodel remains largely unexplored. To the best of our knowledge, model editing\nfor VLLMs has not been extensively studied in the literature. In this work, we\nemploy the contribution allocation and noise perturbation methods to measure\nthe contributions of visual representations for token predictions. Our\nattribution analysis shows that visual representations in mid-to-later layers\nthat are highly relevant to the prompt contribute significantly to predictions.\nBased on these insights, we propose VisEdit, a novel model editor for VLLMs\nthat effectively corrects knowledge by editing intermediate visual\nrepresentations in regions important to the edit prompt. We evaluated VisEdit\nusing multiple VLLM backbones and public VLLM editing benchmark datasets. The\nresults show the superiority of VisEdit over the strong baselines adapted from\nexisting state-of-the-art editors for LLMs."
                },
                "authors": [
                    {
                        "name": "Qizhou Chen"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Chengyu Wang"
                    },
                    {
                        "name": "Xiaofeng He"
                    },
                    {
                        "name": "Dakan Wang"
                    },
                    {
                        "name": "Tingting Liu"
                    }
                ],
                "author_detail": {
                    "name": "Tingting Liu"
                },
                "author": "Tingting Liu",
                "arxiv_comment": "Accepted by AAAI-2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09916v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09916v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15204v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15204v2",
                "updated": "2025-01-03T11:44:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    11,
                    44,
                    51,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-19T18:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    18,
                    59,
                    17,
                    3,
                    354,
                    0
                ],
                "title": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic\n  Long-context Multitasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic\n  Long-context Multitasks"
                },
                "summary": "This paper introduces LongBench v2, a benchmark designed to assess the\nability of LLMs to handle long-context problems requiring deep understanding\nand reasoning across real-world multitasks. LongBench v2 consists of 503\nchallenging multiple-choice questions, with contexts ranging from 8k to 2M\nwords, across six major task categories: single-document QA, multi-document QA,\nlong in-context learning, long-dialogue history understanding, code repository\nunderstanding, and long structured data understanding. To ensure the breadth\nand the practicality, we collect data from nearly 100 highly educated\nindividuals with diverse professional backgrounds. We employ both automated and\nmanual review processes to maintain high quality and difficulty, resulting in\nhuman experts achieving only 53.7% accuracy under a 15-minute time constraint.\nOur evaluation reveals that the best-performing model, when directly answers\nthe questions, achieves only 50.1% accuracy. In contrast, the o1-preview model,\nwhich includes longer reasoning, achieves 57.7%, surpassing the human baseline\nby 4%. These results highlight the importance of enhanced reasoning ability and\nscaling inference-time compute to tackle the long-context challenges in\nLongBench v2. The project is available at https://longbench2.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces LongBench v2, a benchmark designed to assess the\nability of LLMs to handle long-context problems requiring deep understanding\nand reasoning across real-world multitasks. LongBench v2 consists of 503\nchallenging multiple-choice questions, with contexts ranging from 8k to 2M\nwords, across six major task categories: single-document QA, multi-document QA,\nlong in-context learning, long-dialogue history understanding, code repository\nunderstanding, and long structured data understanding. To ensure the breadth\nand the practicality, we collect data from nearly 100 highly educated\nindividuals with diverse professional backgrounds. We employ both automated and\nmanual review processes to maintain high quality and difficulty, resulting in\nhuman experts achieving only 53.7% accuracy under a 15-minute time constraint.\nOur evaluation reveals that the best-performing model, when directly answers\nthe questions, achieves only 50.1% accuracy. In contrast, the o1-preview model,\nwhich includes longer reasoning, achieves 57.7%, surpassing the human baseline\nby 4%. These results highlight the importance of enhanced reasoning ability and\nscaling inference-time compute to tackle the long-context challenges in\nLongBench v2. The project is available at https://longbench2.github.io."
                },
                "authors": [
                    {
                        "name": "Yushi Bai"
                    },
                    {
                        "name": "Shangqing Tu"
                    },
                    {
                        "name": "Jiajie Zhang"
                    },
                    {
                        "name": "Hao Peng"
                    },
                    {
                        "name": "Xiaozhi Wang"
                    },
                    {
                        "name": "Xin Lv"
                    },
                    {
                        "name": "Shulin Cao"
                    },
                    {
                        "name": "Jiazheng Xu"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Yuxiao Dong"
                    },
                    {
                        "name": "Jie Tang"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "26 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15204v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15204v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01765v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01765v1",
                "updated": "2025-01-03T11:34:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    11,
                    34,
                    28,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T11:34:28Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    11,
                    34,
                    28,
                    4,
                    3,
                    0
                ],
                "title": "SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SaLoRA: Safety-Alignment Preserved Low-Rank Adaptation"
                },
                "summary": "As advancements in large language models (LLMs) continue and the demand for\npersonalized models increases, parameter-efficient fine-tuning (PEFT) methods\n(e.g., LoRA) will become essential due to their efficiency in reducing\ncomputation costs. However, recent studies have raised alarming concerns that\nLoRA fine-tuning could potentially compromise the safety alignment in LLMs,\nposing significant risks for the model owner. In this paper, we first\ninvestigate the underlying mechanism by analyzing the changes in safety\nalignment related features before and after fine-tuning. Then, we propose a\nfixed safety module calculated by safety data and a task-specific\ninitialization for trainable parameters in low-rank adaptations, termed\nSafety-alignment preserved Low-Rank Adaptation (SaLoRA). Unlike previous LoRA\nmethods and their variants, SaLoRA enables targeted modifications to LLMs\nwithout disrupting their original alignments. Our experiments show that SaLoRA\noutperforms various adapters-based approaches across various evaluation metrics\nin different fine-tuning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As advancements in large language models (LLMs) continue and the demand for\npersonalized models increases, parameter-efficient fine-tuning (PEFT) methods\n(e.g., LoRA) will become essential due to their efficiency in reducing\ncomputation costs. However, recent studies have raised alarming concerns that\nLoRA fine-tuning could potentially compromise the safety alignment in LLMs,\nposing significant risks for the model owner. In this paper, we first\ninvestigate the underlying mechanism by analyzing the changes in safety\nalignment related features before and after fine-tuning. Then, we propose a\nfixed safety module calculated by safety data and a task-specific\ninitialization for trainable parameters in low-rank adaptations, termed\nSafety-alignment preserved Low-Rank Adaptation (SaLoRA). Unlike previous LoRA\nmethods and their variants, SaLoRA enables targeted modifications to LLMs\nwithout disrupting their original alignments. Our experiments show that SaLoRA\noutperforms various adapters-based approaches across various evaluation metrics\nin different fine-tuning tasks."
                },
                "authors": [
                    {
                        "name": "Mingjie Li"
                    },
                    {
                        "name": "Wai Man Si"
                    },
                    {
                        "name": "Michael Backes"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Yisen Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yisen Wang"
                },
                "author": "Yisen Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01765v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01765v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23242v2",
                "updated": "2025-01-03T11:29:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    11,
                    29,
                    35,
                    4,
                    3,
                    0
                ],
                "published": "2024-10-30T17:28:28Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    17,
                    28,
                    28,
                    2,
                    304,
                    0
                ],
                "title": "A little less conversation, a little more action, please: Investigating\n  the physical common-sense of LLMs in a 3D embodied environment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A little less conversation, a little more action, please: Investigating\n  the physical common-sense of LLMs in a 3D embodied environment"
                },
                "summary": "As general-purpose tools, Large Language Models (LLMs) must often reason\nabout everyday physical environments. In a question-and-answer capacity,\nunderstanding the interactions of physical objects may be necessary to give\nappropriate responses. Moreover, LLMs are increasingly used as reasoning\nengines in agentic systems, designing and controlling their action sequences.\nThe vast majority of research has tackled this issue using static benchmarks,\ncomprised of text or image-based questions about the physical world. However,\nthese benchmarks do not capture the complexity and nuance of real-life physical\nprocesses. Here we advocate for a second, relatively unexplored, approach:\n'embodying' the LLMs by granting them control of an agent within a 3D\nenvironment. We present the first embodied and cognitively meaningful\nevaluation of physical common-sense reasoning in LLMs. Our framework allows\ndirect comparison of LLMs with other embodied agents, such as those based on\nDeep Reinforcement Learning, and human and non-human animals. We employ the\nAnimal-AI (AAI) environment, a simulated 3D virtual laboratory, to study\nphysical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a\nsuite of experiments that replicate laboratory studies with non-human animals,\nto study physical reasoning capabilities including distance estimation,\ntracking out-of-sight objects, and tool use. We demonstrate that\nstate-of-the-art multi-modal models with no finetuning can complete this style\nof task, allowing meaningful comparison to the entrants of the 2019 Animal-AI\nOlympics competition and to human children. Our results show that LLMs are\ncurrently outperformed by human children on these tasks. We argue that this\napproach allows the study of physical reasoning using ecologically valid\nexperiments drawn directly from cognitive science, improving the predictability\nand reliability of LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As general-purpose tools, Large Language Models (LLMs) must often reason\nabout everyday physical environments. In a question-and-answer capacity,\nunderstanding the interactions of physical objects may be necessary to give\nappropriate responses. Moreover, LLMs are increasingly used as reasoning\nengines in agentic systems, designing and controlling their action sequences.\nThe vast majority of research has tackled this issue using static benchmarks,\ncomprised of text or image-based questions about the physical world. However,\nthese benchmarks do not capture the complexity and nuance of real-life physical\nprocesses. Here we advocate for a second, relatively unexplored, approach:\n'embodying' the LLMs by granting them control of an agent within a 3D\nenvironment. We present the first embodied and cognitively meaningful\nevaluation of physical common-sense reasoning in LLMs. Our framework allows\ndirect comparison of LLMs with other embodied agents, such as those based on\nDeep Reinforcement Learning, and human and non-human animals. We employ the\nAnimal-AI (AAI) environment, a simulated 3D virtual laboratory, to study\nphysical common-sense reasoning in LLMs. For this, we use the AAI Testbed, a\nsuite of experiments that replicate laboratory studies with non-human animals,\nto study physical reasoning capabilities including distance estimation,\ntracking out-of-sight objects, and tool use. We demonstrate that\nstate-of-the-art multi-modal models with no finetuning can complete this style\nof task, allowing meaningful comparison to the entrants of the 2019 Animal-AI\nOlympics competition and to human children. Our results show that LLMs are\ncurrently outperformed by human children on these tasks. We argue that this\napproach allows the study of physical reasoning using ecologically valid\nexperiments drawn directly from cognitive science, improving the predictability\nand reliability of LLMs."
                },
                "authors": [
                    {
                        "name": "Matteo G. Mecattaf"
                    },
                    {
                        "name": "Ben Slater"
                    },
                    {
                        "name": "Marko Tešić"
                    },
                    {
                        "name": "Jonathan Prunty"
                    },
                    {
                        "name": "Konstantinos Voudouris"
                    },
                    {
                        "name": "Lucy G. Cheke"
                    }
                ],
                "author_detail": {
                    "name": "Lucy G. Cheke"
                },
                "author": "Lucy G. Cheke",
                "arxiv_comment": "25 pages, 4 figures; v2: Added AFMR Acknowledgment",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.23242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07752v2",
                "updated": "2025-01-03T11:21:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    11,
                    21,
                    25,
                    4,
                    3,
                    0
                ],
                "published": "2024-10-10T09:28:36Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    9,
                    28,
                    36,
                    3,
                    284,
                    0
                ],
                "title": "TVBench: Redesigning Video-Language Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TVBench: Redesigning Video-Language Evaluation"
                },
                "summary": "Large language models have demonstrated impressive performance when\nintegrated with vision models even enabling video understanding. However,\nevaluating these video models presents its own unique challenges, for which\nseveral benchmarks have been proposed. In this paper, we show that the\ncurrently most used video-language benchmarks can be solved without requiring\nmuch temporal reasoning. We identified three main issues in existing datasets:\n(i) static information from single frames is often sufficient to solve the\ntasks (ii) the text of the questions and candidate answers is overly\ninformative, allowing models to answer correctly without relying on any visual\ninput (iii) world knowledge alone can answer many of the questions, making the\nbenchmarks a test of knowledge replication rather than visual reasoning. In\naddition, we found that open-ended question-answering benchmarks for video\nunderstanding suffer from similar issues while the automatic evaluation process\nwith LLMs is unreliable, making it an unsuitable alternative. As a solution, we\npropose TVBench, a novel open-source video multiple-choice question-answering\nbenchmark, and demonstrate through extensive evaluations that it requires a\nhigh level of temporal understanding. Surprisingly, we find that most recent\nstate-of-the-art video-language models perform similarly to random performance\non TVBench, with only a few models such as Qwen2-VL, and Tarsier clearly\nsurpassing this baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have demonstrated impressive performance when\nintegrated with vision models even enabling video understanding. However,\nevaluating these video models presents its own unique challenges, for which\nseveral benchmarks have been proposed. In this paper, we show that the\ncurrently most used video-language benchmarks can be solved without requiring\nmuch temporal reasoning. We identified three main issues in existing datasets:\n(i) static information from single frames is often sufficient to solve the\ntasks (ii) the text of the questions and candidate answers is overly\ninformative, allowing models to answer correctly without relying on any visual\ninput (iii) world knowledge alone can answer many of the questions, making the\nbenchmarks a test of knowledge replication rather than visual reasoning. In\naddition, we found that open-ended question-answering benchmarks for video\nunderstanding suffer from similar issues while the automatic evaluation process\nwith LLMs is unreliable, making it an unsuitable alternative. As a solution, we\npropose TVBench, a novel open-source video multiple-choice question-answering\nbenchmark, and demonstrate through extensive evaluations that it requires a\nhigh level of temporal understanding. Surprisingly, we find that most recent\nstate-of-the-art video-language models perform similarly to random performance\non TVBench, with only a few models such as Qwen2-VL, and Tarsier clearly\nsurpassing this baseline."
                },
                "authors": [
                    {
                        "name": "Daniel Cores"
                    },
                    {
                        "name": "Michael Dorkenwald"
                    },
                    {
                        "name": "Manuel Mucientes"
                    },
                    {
                        "name": "Cees G. M. Snoek"
                    },
                    {
                        "name": "Yuki M. Asano"
                    }
                ],
                "author_detail": {
                    "name": "Yuki M. Asano"
                },
                "author": "Yuki M. Asano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16597v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16597v2",
                "updated": "2025-01-03T10:57:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    10,
                    57,
                    17,
                    4,
                    3,
                    0
                ],
                "published": "2024-09-25T03:49:46Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    3,
                    49,
                    46,
                    2,
                    269,
                    0
                ],
                "title": "EventHallusion: Diagnosing Event Hallucinations in Video LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EventHallusion: Diagnosing Event Hallucinations in Video LLMs"
                },
                "summary": "Recently, Multimodal Large Language Models (MLLMs) have made significant\nprogress in the video comprehension field. Despite remarkable content reasoning\nand instruction following capabilities they demonstrated, the hallucination\nproblem of these VideoLLMs is less explored compared with its counterpart in\nthe image domain. To mitigate this gap, we propose EventHallusion, a novel\nbenchmark that focuses on assessing the VideoLLMs' hallucination toward event,\nthe crux of video analysis. From a hallucination attribution perspective, our\nEventHallusion benchmark is curated to assess a VideoLLM's susceptibility\ntoward language priors and vision-language biases. On the other hand, we also\npropose a simple yet effective method, called Temporal Contrastive Decoding\n(TCD), to tackle the hallucination problems of VideoLLMs. The proposed TCD\nmethod rectifies the model's bias toward its priors during the decoding stage\nby comparing the original video with a modified version, in which temporal cues\nare disrupted. Through comprehensive evaluation of eight open-source and two\nclosed-source VideoLLMs on the proposed EventHallusion benchmark, we observe\nthat the open-source models suffer significantly from hallucination problems,\nwhereas the closed-source ones perform markedly better. By further equipping\nopen-source VideoLLMs with the proposed TCD approach, evident performance\nimprovements are achieved across most metrics in the EventHallusion benchmark.\nOur codes and benchmark data are available at\nhttps://github.com/Stevetich/EventHallusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Multimodal Large Language Models (MLLMs) have made significant\nprogress in the video comprehension field. Despite remarkable content reasoning\nand instruction following capabilities they demonstrated, the hallucination\nproblem of these VideoLLMs is less explored compared with its counterpart in\nthe image domain. To mitigate this gap, we propose EventHallusion, a novel\nbenchmark that focuses on assessing the VideoLLMs' hallucination toward event,\nthe crux of video analysis. From a hallucination attribution perspective, our\nEventHallusion benchmark is curated to assess a VideoLLM's susceptibility\ntoward language priors and vision-language biases. On the other hand, we also\npropose a simple yet effective method, called Temporal Contrastive Decoding\n(TCD), to tackle the hallucination problems of VideoLLMs. The proposed TCD\nmethod rectifies the model's bias toward its priors during the decoding stage\nby comparing the original video with a modified version, in which temporal cues\nare disrupted. Through comprehensive evaluation of eight open-source and two\nclosed-source VideoLLMs on the proposed EventHallusion benchmark, we observe\nthat the open-source models suffer significantly from hallucination problems,\nwhereas the closed-source ones perform markedly better. By further equipping\nopen-source VideoLLMs with the proposed TCD approach, evident performance\nimprovements are achieved across most metrics in the EventHallusion benchmark.\nOur codes and benchmark data are available at\nhttps://github.com/Stevetich/EventHallusion."
                },
                "authors": [
                    {
                        "name": "Jiacheng Zhang"
                    },
                    {
                        "name": "Yang Jiao"
                    },
                    {
                        "name": "Shaoxiang Chen"
                    },
                    {
                        "name": "Na Zhao"
                    },
                    {
                        "name": "Jingjing Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jingjing Chen"
                },
                "author": "Jingjing Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16597v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16597v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01743v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01743v1",
                "updated": "2025-01-03T10:11:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    10,
                    11,
                    38,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T10:11:38Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    10,
                    11,
                    38,
                    4,
                    3,
                    0
                ],
                "title": "Automating Legal Concept Interpretation with LLMs: Retrieval,\n  Generation, and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automating Legal Concept Interpretation with LLMs: Retrieval,\n  Generation, and Evaluation"
                },
                "summary": "Legal articles often include vague concepts to adapt to the ever-changing\nsociety. Providing detailed interpretations of these concepts is a critical\ntask for legal practitioners, which requires meticulous and professional\nannotations by legal experts, admittedly time-consuming and expensive to\ncollect at scale. In this paper, we introduce a novel retrieval-augmented\ngeneration framework, ATRI, for AuTomatically Retrieving relevant information\nfrom past judicial precedents and Interpreting vague legal concepts. We further\npropose a new benchmark, Legal Concept Entailment, to automate the evaluation\nof generated concept interpretations without expert involvement. Automatic\nevaluations indicate that our generated interpretations can effectively assist\nlarge language models (LLMs) in understanding vague legal concepts.\nMulti-faceted evaluations by legal experts indicate that the quality of our\nconcept interpretations is comparable to those written by human experts. Our\nwork has strong implications for leveraging LLMs to support legal practitioners\nin interpreting vague legal concepts and beyond.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal articles often include vague concepts to adapt to the ever-changing\nsociety. Providing detailed interpretations of these concepts is a critical\ntask for legal practitioners, which requires meticulous and professional\nannotations by legal experts, admittedly time-consuming and expensive to\ncollect at scale. In this paper, we introduce a novel retrieval-augmented\ngeneration framework, ATRI, for AuTomatically Retrieving relevant information\nfrom past judicial precedents and Interpreting vague legal concepts. We further\npropose a new benchmark, Legal Concept Entailment, to automate the evaluation\nof generated concept interpretations without expert involvement. Automatic\nevaluations indicate that our generated interpretations can effectively assist\nlarge language models (LLMs) in understanding vague legal concepts.\nMulti-faceted evaluations by legal experts indicate that the quality of our\nconcept interpretations is comparable to those written by human experts. Our\nwork has strong implications for leveraging LLMs to support legal practitioners\nin interpreting vague legal concepts and beyond."
                },
                "authors": [
                    {
                        "name": "Kangcheng Luo"
                    },
                    {
                        "name": "Quzhe Huang"
                    },
                    {
                        "name": "Cong Jiang"
                    },
                    {
                        "name": "Yansong Feng"
                    }
                ],
                "author_detail": {
                    "name": "Yansong Feng"
                },
                "author": "Yansong Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01743v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01743v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01741v1",
                "updated": "2025-01-03T10:08:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    10,
                    8,
                    49,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T10:08:49Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    10,
                    8,
                    49,
                    4,
                    3,
                    0
                ],
                "title": "How Toxic Can You Get? Search-based Toxicity Testing for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Toxic Can You Get? Search-based Toxicity Testing for Large Language\n  Models"
                },
                "summary": "Language is a deep-rooted means of perpetration of stereotypes and\ndiscrimination. Large Language Models (LLMs), now a pervasive technology in our\neveryday lives, can cause extensive harm when prone to generating toxic\nresponses. The standard way to address this issue is to align the LLM, which,\nhowever, dampens the issue without constituting a definitive solution.\nTherefore, testing LLM even after alignment efforts remains crucial for\ndetecting any residual deviations with respect to ethical standards. We present\nEvoTox, an automated testing framework for LLMs' inclination to toxicity,\nproviding a way to quantitatively assess how much LLMs can be pushed towards\ntoxic responses even in the presence of alignment. The framework adopts an\niterative evolution strategy that exploits the interplay between two LLMs, the\nSystem Under Test (SUT) and the Prompt Generator steering SUT responses toward\nhigher toxicity. The toxicity level is assessed by an automated oracle based on\nan existing toxicity classifier. We conduct a quantitative and qualitative\nempirical evaluation using four state-of-the-art LLMs as evaluation subjects\nhaving increasing complexity (7-13 billion parameters). Our quantitative\nevaluation assesses the cost-effectiveness of four alternative versions of\nEvoTox against existing baseline methods, based on random search, curated\ndatasets of toxic prompts, and adversarial attacks. Our qualitative assessment\nengages human evaluators to rate the fluency of the generated prompts and the\nperceived toxicity of the responses collected during the testing sessions.\nResults indicate that the effectiveness, in terms of detected toxicity level,\nis significantly higher than the selected baseline methods (effect size up to\n1.0 against random search and up to 0.99 against adversarial attacks).\nFurthermore, EvoTox yields a limited cost overhead (from 22% to 35% on\naverage).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language is a deep-rooted means of perpetration of stereotypes and\ndiscrimination. Large Language Models (LLMs), now a pervasive technology in our\neveryday lives, can cause extensive harm when prone to generating toxic\nresponses. The standard way to address this issue is to align the LLM, which,\nhowever, dampens the issue without constituting a definitive solution.\nTherefore, testing LLM even after alignment efforts remains crucial for\ndetecting any residual deviations with respect to ethical standards. We present\nEvoTox, an automated testing framework for LLMs' inclination to toxicity,\nproviding a way to quantitatively assess how much LLMs can be pushed towards\ntoxic responses even in the presence of alignment. The framework adopts an\niterative evolution strategy that exploits the interplay between two LLMs, the\nSystem Under Test (SUT) and the Prompt Generator steering SUT responses toward\nhigher toxicity. The toxicity level is assessed by an automated oracle based on\nan existing toxicity classifier. We conduct a quantitative and qualitative\nempirical evaluation using four state-of-the-art LLMs as evaluation subjects\nhaving increasing complexity (7-13 billion parameters). Our quantitative\nevaluation assesses the cost-effectiveness of four alternative versions of\nEvoTox against existing baseline methods, based on random search, curated\ndatasets of toxic prompts, and adversarial attacks. Our qualitative assessment\nengages human evaluators to rate the fluency of the generated prompts and the\nperceived toxicity of the responses collected during the testing sessions.\nResults indicate that the effectiveness, in terms of detected toxicity level,\nis significantly higher than the selected baseline methods (effect size up to\n1.0 against random search and up to 0.99 against adversarial attacks).\nFurthermore, EvoTox yields a limited cost overhead (from 22% to 35% on\naverage)."
                },
                "authors": [
                    {
                        "name": "Simone Corbo"
                    },
                    {
                        "name": "Luca Bancale"
                    },
                    {
                        "name": "Valeria De Gennaro"
                    },
                    {
                        "name": "Livia Lestingi"
                    },
                    {
                        "name": "Vincenzo Scotti"
                    },
                    {
                        "name": "Matteo Camilli"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Camilli"
                },
                "author": "Matteo Camilli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01144v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01144v2",
                "updated": "2025-01-03T09:27:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    9,
                    27,
                    46,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-02T08:57:00Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    8,
                    57,
                    0,
                    3,
                    2,
                    0
                ],
                "title": "BlockDialect: Block-wise Fine-grained Mixed Format for Energy-Efficient\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BlockDialect: Block-wise Fine-grained Mixed Format for Energy-Efficient\n  LLM Inference"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success, but their\nincreasing size poses significant challenges in memory usage and computational\ncosts. Quantizing both weights and activations can address these issues, with\nfine-grained block-wise quantization emerging as a promising hardware-supported\nsolution to mitigate outliers. However, existing methods struggle to capture\nnuanced block data distributions. To address this, we propose BlockDialect, a\nblock-wise fine-grained mixed format technique that assigns a per-block optimal\nnumber format from formatbook for better data representation. Additionally, we\nintroduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that\nadapt to diverse data distributions. To leverage this efficiently, we propose a\ntwo-stage approach for online DialectFP4 activation quantization. Importantly,\nDialectFP4 ensures hardware efficiency by selecting representable values as\nscaled integers compatible with low-precision integer arithmetic. BlockDialect\nachieves 11.83% (7.56%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model\ncompared to MXFP4 format with lower bit usage per data, while being only 5.46%\n(2.65%) below full precision even when quantizing full-path matrix\nmultiplication. Focusing on how to represent over how to scale, our work\npresents a promising path for energy-efficient LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success, but their\nincreasing size poses significant challenges in memory usage and computational\ncosts. Quantizing both weights and activations can address these issues, with\nfine-grained block-wise quantization emerging as a promising hardware-supported\nsolution to mitigate outliers. However, existing methods struggle to capture\nnuanced block data distributions. To address this, we propose BlockDialect, a\nblock-wise fine-grained mixed format technique that assigns a per-block optimal\nnumber format from formatbook for better data representation. Additionally, we\nintroduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that\nadapt to diverse data distributions. To leverage this efficiently, we propose a\ntwo-stage approach for online DialectFP4 activation quantization. Importantly,\nDialectFP4 ensures hardware efficiency by selecting representable values as\nscaled integers compatible with low-precision integer arithmetic. BlockDialect\nachieves 11.83% (7.56%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model\ncompared to MXFP4 format with lower bit usage per data, while being only 5.46%\n(2.65%) below full precision even when quantizing full-path matrix\nmultiplication. Focusing on how to represent over how to scale, our work\npresents a promising path for energy-efficient LLM inference."
                },
                "authors": [
                    {
                        "name": "Wonsuk Jang"
                    },
                    {
                        "name": "Thierry Tambe"
                    }
                ],
                "author_detail": {
                    "name": "Thierry Tambe"
                },
                "author": "Thierry Tambe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01144v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01144v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01711v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01711v1",
                "updated": "2025-01-03T09:12:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    9,
                    12,
                    35,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T09:12:35Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    9,
                    12,
                    35,
                    4,
                    3,
                    0
                ],
                "title": "LLMs & Legal Aid: Understanding Legal Needs Exhibited Through User\n  Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs & Legal Aid: Understanding Legal Needs Exhibited Through User\n  Queries"
                },
                "summary": "The paper presents a preliminary analysis of an experiment conducted by Frank\nBold, a Czech expert group, to explore user interactions with GPT-4 for\naddressing legal queries. Between May 3, 2023, and July 25, 2023, 1,252 users\nsubmitted 3,847 queries. Unlike studies that primarily focus on the accuracy,\nfactuality, or hallucination tendencies of large language models (LLMs), our\nanalysis focuses on the user query dimension of the interaction. Using GPT-4o\nfor zero-shot classification, we categorized queries on (1) whether users\nprovided factual information about their issue (29.95%) or not (70.05%), (2)\nwhether they sought legal information (64.93%) or advice on the course of\naction (35.07\\%), and (3) whether they imposed requirements to shape or control\nthe model's answer (28.57%) or not (71.43%). We provide both quantitative and\nqualitative insight into user needs and contribute to a better understanding of\nuser engagement with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The paper presents a preliminary analysis of an experiment conducted by Frank\nBold, a Czech expert group, to explore user interactions with GPT-4 for\naddressing legal queries. Between May 3, 2023, and July 25, 2023, 1,252 users\nsubmitted 3,847 queries. Unlike studies that primarily focus on the accuracy,\nfactuality, or hallucination tendencies of large language models (LLMs), our\nanalysis focuses on the user query dimension of the interaction. Using GPT-4o\nfor zero-shot classification, we categorized queries on (1) whether users\nprovided factual information about their issue (29.95%) or not (70.05%), (2)\nwhether they sought legal information (64.93%) or advice on the course of\naction (35.07\\%), and (3) whether they imposed requirements to shape or control\nthe model's answer (28.57%) or not (71.43%). We provide both quantitative and\nqualitative insight into user needs and contribute to a better understanding of\nuser engagement with LLMs."
                },
                "authors": [
                    {
                        "name": "Michal Kuk"
                    },
                    {
                        "name": "Jakub Harasta"
                    }
                ],
                "author_detail": {
                    "name": "Jakub Harasta"
                },
                "author": "Jakub Harasta",
                "arxiv_comment": "Accepted at AI for Access to Justice Workshop at Jurix 2024, Brno,\n  Czechia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01711v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01711v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01705v1",
                "updated": "2025-01-03T09:04:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    9,
                    4,
                    45,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T09:04:45Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    9,
                    4,
                    45,
                    4,
                    3,
                    0
                ],
                "title": "The Essence of Contextual Understanding in Theory of Mind: A Study on\n  Question Answering with Story Characters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Essence of Contextual Understanding in Theory of Mind: A Study on\n  Question Answering with Story Characters"
                },
                "summary": "Theory-of-Mind (ToM) is a fundamental psychological capability that allows\nhumans to understand and interpret the mental states of others. Humans infer\nothers' thoughts by integrating causal cues and indirect clues from broad\ncontextual information, often derived from past interactions. In other words,\nhuman ToM heavily relies on the understanding about the backgrounds and life\nstories of others. Unfortunately, this aspect is largely overlooked in existing\nbenchmarks for evaluating machines' ToM capabilities, due to their usage of\nshort narratives without global backgrounds. In this paper, we verify the\nimportance of understanding long personal backgrounds in ToM and assess the\nperformance of LLMs in such realistic evaluation scenarios. To achieve this, we\nintroduce a novel benchmark, CharToM-QA, comprising 1,035 ToM questions based\non characters from classic novels. Our human study reveals a significant\ndisparity in performance: the same group of educated participants performs\ndramatically better when they have read the novels compared to when they have\nnot. In parallel, our experiments on state-of-the-art LLMs, including the very\nrecent o1 model, show that LLMs still perform notably worse than humans,\ndespite that they have seen these stories during pre-training. This highlights\nthe limitations of current LLMs in capturing the nuanced contextual information\nrequired for ToM reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Theory-of-Mind (ToM) is a fundamental psychological capability that allows\nhumans to understand and interpret the mental states of others. Humans infer\nothers' thoughts by integrating causal cues and indirect clues from broad\ncontextual information, often derived from past interactions. In other words,\nhuman ToM heavily relies on the understanding about the backgrounds and life\nstories of others. Unfortunately, this aspect is largely overlooked in existing\nbenchmarks for evaluating machines' ToM capabilities, due to their usage of\nshort narratives without global backgrounds. In this paper, we verify the\nimportance of understanding long personal backgrounds in ToM and assess the\nperformance of LLMs in such realistic evaluation scenarios. To achieve this, we\nintroduce a novel benchmark, CharToM-QA, comprising 1,035 ToM questions based\non characters from classic novels. Our human study reveals a significant\ndisparity in performance: the same group of educated participants performs\ndramatically better when they have read the novels compared to when they have\nnot. In parallel, our experiments on state-of-the-art LLMs, including the very\nrecent o1 model, show that LLMs still perform notably worse than humans,\ndespite that they have seen these stories during pre-training. This highlights\nthe limitations of current LLMs in capturing the nuanced contextual information\nrequired for ToM reasoning."
                },
                "authors": [
                    {
                        "name": "Chulun Zhou"
                    },
                    {
                        "name": "Qiujing Wang"
                    },
                    {
                        "name": "Mo Yu"
                    },
                    {
                        "name": "Xiaoqian Yue"
                    },
                    {
                        "name": "Rui Lu"
                    },
                    {
                        "name": "Jiangnan Li"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Shunchi Zhang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Wai Lam"
                    }
                ],
                "author_detail": {
                    "name": "Wai Lam"
                },
                "author": "Wai Lam",
                "arxiv_comment": "17 pages, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01702v1",
                "updated": "2025-01-03T08:55:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    8,
                    55,
                    19,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T08:55:19Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    8,
                    55,
                    19,
                    4,
                    3,
                    0
                ],
                "title": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AgentRefine: Enhancing Agent Generalization through Refinement Tuning"
                },
                "summary": "Large Language Model (LLM) based agents have proved their ability to perform\ncomplex tasks like humans. However, there is still a large gap between\nopen-sourced LLMs and commercial models like the GPT series. In this paper, we\nfocus on improving the agent generalization capabilities of LLMs via\ninstruction tuning. We first observe that the existing agent training corpus\nexhibits satisfactory results on held-in evaluation sets but fails to\ngeneralize to held-out sets. These agent-tuning works face severe formatting\nerrors and are frequently stuck in the same mistake for a long while. We\nanalyze that the poor generalization ability comes from overfitting to several\nmanual agent environments and a lack of adaptation to new situations. They\nstruggle with the wrong action steps and can not learn from the experience but\njust memorize existing observation-action relations. Inspired by the insight,\nwe propose a novel AgentRefine framework for agent-tuning. The core idea is to\nenable the model to learn to correct its mistakes via observation in the\ntrajectory. Specifically, we propose an agent synthesis framework to encompass\na diverse array of environments and tasks and prompt a strong LLM to refine its\nerror action according to the environment feedback. AgentRefine significantly\noutperforms state-of-the-art agent-tuning work in terms of generalization\nability on diverse agent tasks. It also has better robustness facing\nperturbation and can generate diversified thought in inference. Our findings\nestablish the correlation between agent generalization and self-refinement and\nprovide a new paradigm for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based agents have proved their ability to perform\ncomplex tasks like humans. However, there is still a large gap between\nopen-sourced LLMs and commercial models like the GPT series. In this paper, we\nfocus on improving the agent generalization capabilities of LLMs via\ninstruction tuning. We first observe that the existing agent training corpus\nexhibits satisfactory results on held-in evaluation sets but fails to\ngeneralize to held-out sets. These agent-tuning works face severe formatting\nerrors and are frequently stuck in the same mistake for a long while. We\nanalyze that the poor generalization ability comes from overfitting to several\nmanual agent environments and a lack of adaptation to new situations. They\nstruggle with the wrong action steps and can not learn from the experience but\njust memorize existing observation-action relations. Inspired by the insight,\nwe propose a novel AgentRefine framework for agent-tuning. The core idea is to\nenable the model to learn to correct its mistakes via observation in the\ntrajectory. Specifically, we propose an agent synthesis framework to encompass\na diverse array of environments and tasks and prompt a strong LLM to refine its\nerror action according to the environment feedback. AgentRefine significantly\noutperforms state-of-the-art agent-tuning work in terms of generalization\nability on diverse agent tasks. It also has better robustness facing\nperturbation and can generate diversified thought in inference. Our findings\nestablish the correlation between agent generalization and self-refinement and\nprovide a new paradigm for future research."
                },
                "authors": [
                    {
                        "name": "Dayuan Fu"
                    },
                    {
                        "name": "Keqing He"
                    },
                    {
                        "name": "Yejie Wang"
                    },
                    {
                        "name": "Wentao Hong"
                    },
                    {
                        "name": "Zhuoma Gongque"
                    },
                    {
                        "name": "Weihao Zeng"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Weiran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Weiran Xu"
                },
                "author": "Weiran Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11543v2",
                "updated": "2025-01-03T08:44:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    8,
                    44,
                    46,
                    4,
                    3,
                    0
                ],
                "published": "2024-11-18T13:01:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    13,
                    1,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Vision-Language Model Safety through Progressive\n  Concept-Bottleneck-Driven Alignment"
                },
                "summary": "Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benefiting from the powerful capabilities of Large Language Models (LLMs),\npre-trained visual encoder models connected to LLMs form Vision Language Models\n(VLMs). However, recent research shows that the visual modality in VLMs is\nhighly vulnerable, allowing attackers to bypass safety alignment in LLMs\nthrough visually transmitted content, launching harmful attacks. To address\nthis challenge, we propose a progressive concept-based alignment strategy,\nPSA-VLM, which incorporates safety modules as concept bottlenecks to enhance\nvisual modality safety alignment. By aligning model predictions with specific\nsafety concepts, we improve defenses against risky images, enhancing\nexplainability and controllability while minimally impacting general\nperformance. Our method is obtained through two-stage training. The low\ncomputational cost of the first stage brings very effective performance\nimprovement, and the fine-tuning of the language model in the second stage\nfurther improves the safety performance. Our method achieves state-of-the-art\nresults on popular VLM safety benchmark."
                },
                "authors": [
                    {
                        "name": "Zhendong Liu"
                    },
                    {
                        "name": "Yuanbi Nie"
                    },
                    {
                        "name": "Yingshui Tan"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Xiangyu Yue"
                    },
                    {
                        "name": "Qiushi Cui"
                    },
                    {
                        "name": "Chongjun Wang"
                    },
                    {
                        "name": "Xiaoyong Zhu"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2405.13581",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01306v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01306v2",
                "updated": "2025-01-03T08:29:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    8,
                    29,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-02T15:36:50Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    36,
                    50,
                    3,
                    2,
                    0
                ],
                "title": "Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process\n  of Fast and Slow Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Think More, Hallucinate Less: Mitigating Hallucinations via Dual Process\n  of Fast and Slow Thinking"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional capabilities, yet still\nface the hallucination issue. Typical text generation approaches adopt an\nauto-regressive generation without deliberate reasoning, which often results in\nuntrustworthy and factually inaccurate responses. In this paper, we propose\nHaluSearch, a novel framework that incorporates tree search-based algorithms\n(e.g. MCTS) to enable an explicit slow thinking generation process for\nmitigating hallucinations of LLMs during inference. Specifically, HaluSearch\nframes text generation as a step-by-step reasoning process, using a\nself-evaluation reward model to score each generation step and guide the tree\nsearch towards the most reliable generation pathway for fully exploiting the\ninternal knowledge of LLMs. To balance efficiency and quality, we introduce a\nhierarchical thinking system switch mechanism inspired by the dual process\ntheory in cognitive science, which dynamically alternates between fast and slow\nthinking modes at both the instance and step levels, adapting to the complexity\nof questions and reasoning states. We conduct extensive experiments on both\nEnglish and Chinese datasets and the results show that our approach\nsignificantly outperforms baseline approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional capabilities, yet still\nface the hallucination issue. Typical text generation approaches adopt an\nauto-regressive generation without deliberate reasoning, which often results in\nuntrustworthy and factually inaccurate responses. In this paper, we propose\nHaluSearch, a novel framework that incorporates tree search-based algorithms\n(e.g. MCTS) to enable an explicit slow thinking generation process for\nmitigating hallucinations of LLMs during inference. Specifically, HaluSearch\nframes text generation as a step-by-step reasoning process, using a\nself-evaluation reward model to score each generation step and guide the tree\nsearch towards the most reliable generation pathway for fully exploiting the\ninternal knowledge of LLMs. To balance efficiency and quality, we introduce a\nhierarchical thinking system switch mechanism inspired by the dual process\ntheory in cognitive science, which dynamically alternates between fast and slow\nthinking modes at both the instance and step levels, adapting to the complexity\nof questions and reasoning states. We conduct extensive experiments on both\nEnglish and Chinese datasets and the results show that our approach\nsignificantly outperforms baseline approaches."
                },
                "authors": [
                    {
                        "name": "Xiaoxue Cheng"
                    },
                    {
                        "name": "Junyi Li"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01306v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04798v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04798v4",
                "updated": "2025-01-03T08:05:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    8,
                    5,
                    30,
                    4,
                    3,
                    0
                ],
                "published": "2024-02-07T12:38:47Z",
                "published_parsed": [
                    2024,
                    2,
                    7,
                    12,
                    38,
                    47,
                    2,
                    38,
                    0
                ],
                "title": "Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with\n  Parallel Spike-driven Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking-PhysFormer: Camera-Based Remote Photoplethysmography with\n  Parallel Spike-driven Transformer"
                },
                "summary": "Artificial neural networks (ANNs) can help camera-based remote\nphotoplethysmography (rPPG) in measuring cardiac activity and physiological\nsignals from facial videos, such as pulse wave, heart rate and respiration rate\nwith better accuracy. However, most existing ANN-based methods require\nsubstantial computing resources, which poses challenges for effective\ndeployment on mobile devices. Spiking neural networks (SNNs), on the other\nhand, hold immense potential for energy-efficient deep learning owing to their\nbinary and event-driven architecture. To the best of our knowledge, we are the\nfirst to introduce SNNs into the realm of rPPG, proposing a hybrid neural\nnetwork (HNN) model, the Spiking-PhysFormer, aimed at reducing power\nconsumption. Specifically, the proposed Spiking-PhyFormer consists of an\nANN-based patch embedding block, SNN-based transformer blocks, and an ANN-based\npredictor head. First, to simplify the transformer block while preserving its\ncapacity to aggregate local and global spatio-temporal features, we design a\nparallel spike transformer block to replace sequential sub-blocks.\nAdditionally, we propose a simplified spiking self-attention mechanism that\nomits the value parameter without compromising the model's performance.\nExperiments conducted on four datasets-PURE, UBFC-rPPG, UBFC-Phys, and MMPD\ndemonstrate that the proposed model achieves a 12.4\\% reduction in power\nconsumption compared to PhysFormer. Additionally, the power consumption of the\ntransformer block is reduced by a factor of 12.2, while maintaining decent\nperformance as PhysFormer and other ANN-based models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial neural networks (ANNs) can help camera-based remote\nphotoplethysmography (rPPG) in measuring cardiac activity and physiological\nsignals from facial videos, such as pulse wave, heart rate and respiration rate\nwith better accuracy. However, most existing ANN-based methods require\nsubstantial computing resources, which poses challenges for effective\ndeployment on mobile devices. Spiking neural networks (SNNs), on the other\nhand, hold immense potential for energy-efficient deep learning owing to their\nbinary and event-driven architecture. To the best of our knowledge, we are the\nfirst to introduce SNNs into the realm of rPPG, proposing a hybrid neural\nnetwork (HNN) model, the Spiking-PhysFormer, aimed at reducing power\nconsumption. Specifically, the proposed Spiking-PhyFormer consists of an\nANN-based patch embedding block, SNN-based transformer blocks, and an ANN-based\npredictor head. First, to simplify the transformer block while preserving its\ncapacity to aggregate local and global spatio-temporal features, we design a\nparallel spike transformer block to replace sequential sub-blocks.\nAdditionally, we propose a simplified spiking self-attention mechanism that\nomits the value parameter without compromising the model's performance.\nExperiments conducted on four datasets-PURE, UBFC-rPPG, UBFC-Phys, and MMPD\ndemonstrate that the proposed model achieves a 12.4\\% reduction in power\nconsumption compared to PhysFormer. Additionally, the power consumption of the\ntransformer block is reduced by a factor of 12.2, while maintaining decent\nperformance as PhysFormer and other ANN-based models."
                },
                "authors": [
                    {
                        "name": "Mingxuan Liu"
                    },
                    {
                        "name": "Jiankai Tang"
                    },
                    {
                        "name": "Yongli Chen"
                    },
                    {
                        "name": "Haoxiang Li"
                    },
                    {
                        "name": "Jiahao Qi"
                    },
                    {
                        "name": "Siwei Li"
                    },
                    {
                        "name": "Kegang Wang"
                    },
                    {
                        "name": "Jie Gan"
                    },
                    {
                        "name": "Yuntao Wang"
                    },
                    {
                        "name": "Hong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Hong Chen"
                },
                "author": "Hong Chen",
                "arxiv_comment": "Mingxuan Liu and Jiankai Tang are co-first authors of the article.\n  Accepted by Neural Networks",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04798v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04798v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01679v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01679v1",
                "updated": "2025-01-03T07:47:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    47,
                    59,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T07:47:59Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    47,
                    59,
                    4,
                    3,
                    0
                ],
                "title": "Adaptive Few-shot Prompting for Machine Translation with Pre-trained\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Few-shot Prompting for Machine Translation with Pre-trained\n  Language Models"
                },
                "summary": "Recently, Large language models (LLMs) with in-context learning have\ndemonstrated remarkable potential in handling neural machine translation.\nHowever, existing evidence shows that LLMs are prompt-sensitive and it is\nsub-optimal to apply the fixed prompt to any input for downstream machine\ntranslation tasks. To address this issue, we propose an adaptive few-shot\nprompting (AFSP) framework to automatically select suitable translation\ndemonstrations for various source input sentences to further elicit the\ntranslation capability of an LLM for better machine translation. First, we\nbuild a translation demonstration retrieval module based on LLM's embedding to\nretrieve top-k semantic-similar translation demonstrations from aligned\nparallel translation corpus. Rather than using other embedding models for\nsemantic demonstration retrieval, we build a hybrid demonstration retrieval\nmodule based on the embedding layer of the deployed LLM to build better input\nrepresentation for retrieving more semantic-related translation demonstrations.\nThen, to ensure better semantic consistency between source inputs and target\noutputs, we force the deployed LLM itself to generate multiple output\ncandidates in the target language with the help of translation demonstrations\nand rerank these candidates. Besides, to better evaluate the effectiveness of\nour AFSP framework on the latest language and extend the research boundary of\nneural machine translation, we construct a high-quality diplomatic\nChinese-English parallel dataset that consists of 5,528 parallel\nChinese-English sentences. Finally, extensive experiments on the proposed\ndiplomatic Chinese-English parallel dataset and the United Nations Parallel\nCorpus (Chinese-English part) show the effectiveness and superiority of our\nproposed AFSP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large language models (LLMs) with in-context learning have\ndemonstrated remarkable potential in handling neural machine translation.\nHowever, existing evidence shows that LLMs are prompt-sensitive and it is\nsub-optimal to apply the fixed prompt to any input for downstream machine\ntranslation tasks. To address this issue, we propose an adaptive few-shot\nprompting (AFSP) framework to automatically select suitable translation\ndemonstrations for various source input sentences to further elicit the\ntranslation capability of an LLM for better machine translation. First, we\nbuild a translation demonstration retrieval module based on LLM's embedding to\nretrieve top-k semantic-similar translation demonstrations from aligned\nparallel translation corpus. Rather than using other embedding models for\nsemantic demonstration retrieval, we build a hybrid demonstration retrieval\nmodule based on the embedding layer of the deployed LLM to build better input\nrepresentation for retrieving more semantic-related translation demonstrations.\nThen, to ensure better semantic consistency between source inputs and target\noutputs, we force the deployed LLM itself to generate multiple output\ncandidates in the target language with the help of translation demonstrations\nand rerank these candidates. Besides, to better evaluate the effectiveness of\nour AFSP framework on the latest language and extend the research boundary of\nneural machine translation, we construct a high-quality diplomatic\nChinese-English parallel dataset that consists of 5,528 parallel\nChinese-English sentences. Finally, extensive experiments on the proposed\ndiplomatic Chinese-English parallel dataset and the United Nations Parallel\nCorpus (Chinese-English part) show the effectiveness and superiority of our\nproposed AFSP."
                },
                "authors": [
                    {
                        "name": "Lei Tang"
                    },
                    {
                        "name": "Jinghui Qin"
                    },
                    {
                        "name": "Wenxuan Ye"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Zhijing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zhijing Yang"
                },
                "author": "Zhijing Yang",
                "arxiv_comment": "published to AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01679v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01679v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00906v2",
                "updated": "2025-01-03T07:47:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    47,
                    36,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-01T17:38:40Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    17,
                    38,
                    40,
                    2,
                    1,
                    0
                ],
                "title": "Large Language Model Based Multi-Agent System Augmented Complex Event\n  Processing Pipeline for Internet of Multimedia Things",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Based Multi-Agent System Augmented Complex Event\n  Processing Pipeline for Internet of Multimedia Things"
                },
                "summary": "This paper presents the development and evaluation of a Large Language Model\n(LLM), also known as foundation models, based multi-agent system framework for\ncomplex event processing (CEP) with a focus on video query processing use\ncases. The primary goal is to create a proof-of-concept (POC) that integrates\nstate-of-the-art LLM orchestration frameworks with publish/subscribe (pub/sub)\ntools to address the integration of LLMs with current CEP systems. Utilizing\nthe Autogen framework in conjunction with Kafka message brokers, the system\ndemonstrates an autonomous CEP pipeline capable of handling complex workflows.\nExtensive experiments evaluate the system's performance across varying\nconfigurations, complexities, and video resolutions, revealing the trade-offs\nbetween functionality and latency. The results show that while higher agent\ncount and video complexities increase latency, the system maintains high\nconsistency in narrative coherence. This research builds upon and contributes\nto, existing novel approaches to distributed AI systems, offering detailed\ninsights into integrating such systems into existing infrastructures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the development and evaluation of a Large Language Model\n(LLM), also known as foundation models, based multi-agent system framework for\ncomplex event processing (CEP) with a focus on video query processing use\ncases. The primary goal is to create a proof-of-concept (POC) that integrates\nstate-of-the-art LLM orchestration frameworks with publish/subscribe (pub/sub)\ntools to address the integration of LLMs with current CEP systems. Utilizing\nthe Autogen framework in conjunction with Kafka message brokers, the system\ndemonstrates an autonomous CEP pipeline capable of handling complex workflows.\nExtensive experiments evaluate the system's performance across varying\nconfigurations, complexities, and video resolutions, revealing the trade-offs\nbetween functionality and latency. The results show that while higher agent\ncount and video complexities increase latency, the system maintains high\nconsistency in narrative coherence. This research builds upon and contributes\nto, existing novel approaches to distributed AI systems, offering detailed\ninsights into integrating such systems into existing infrastructures."
                },
                "authors": [
                    {
                        "name": "Talha Zeeshan"
                    },
                    {
                        "name": "Abhishek Kumar"
                    },
                    {
                        "name": "Susanna Pirttikangas"
                    },
                    {
                        "name": "Sasu Tarkoma"
                    }
                ],
                "author_detail": {
                    "name": "Sasu Tarkoma"
                },
                "author": "Sasu Tarkoma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04119v2",
                "updated": "2025-01-03T07:29:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    29,
                    3,
                    4,
                    3,
                    0
                ],
                "published": "2024-02-06T16:12:36Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    16,
                    12,
                    36,
                    1,
                    37,
                    0
                ],
                "title": "A quantitative analysis of knowledge-learning preferences in large\n  language models in molecular science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A quantitative analysis of knowledge-learning preferences in large\n  language models in molecular science"
                },
                "summary": "Deep learning has significantly advanced molecular modeling and design,\nenabling efficient understanding and discovery of novel molecules. In\nparticular, large language models (LLMs) introduce a fresh research paradigm to\ntackle scientific problems from a natural language processing (NLP)\nperspective. LLMs significantly enhance our understanding and generation of\nmolecules, often surpassing existing methods with their capabilities to decode\nand synthesize complex molecular patterns. However, two key issues remain: how\nto quantify the match between model and data modalities and how to identify the\nknowledge-learning preferences of models. To address these challenges, we\npropose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263\nexperiments to assess the model's compatibility with data modalities and\nknowledge acquisition. Through the modal transition probability matrix, we\nprovide insights into the most suitable modalities for tasks. Furthermore, we\nintroduce a statistically interpretable approach to discover context-specific\nknowledge mapping by localized feature filtering. Our analysis offers an\nexploration of the learning mechanism and paves the way for advancing LLMs in\nmolecular science.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning has significantly advanced molecular modeling and design,\nenabling efficient understanding and discovery of novel molecules. In\nparticular, large language models (LLMs) introduce a fresh research paradigm to\ntackle scientific problems from a natural language processing (NLP)\nperspective. LLMs significantly enhance our understanding and generation of\nmolecules, often surpassing existing methods with their capabilities to decode\nand synthesize complex molecular patterns. However, two key issues remain: how\nto quantify the match between model and data modalities and how to identify the\nknowledge-learning preferences of models. To address these challenges, we\npropose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263\nexperiments to assess the model's compatibility with data modalities and\nknowledge acquisition. Through the modal transition probability matrix, we\nprovide insights into the most suitable modalities for tasks. Furthermore, we\nintroduce a statistically interpretable approach to discover context-specific\nknowledge mapping by localized feature filtering. Our analysis offers an\nexploration of the learning mechanism and paves the way for advancing LLMs in\nmolecular science."
                },
                "authors": [
                    {
                        "name": "Pengfei Liu"
                    },
                    {
                        "name": "Jun Tao"
                    },
                    {
                        "name": "Zhixiang Ren"
                    }
                ],
                "author_detail": {
                    "name": "Zhixiang Ren"
                },
                "author": "Zhixiang Ren",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01672v1",
                "updated": "2025-01-03T07:19:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    19,
                    23,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T07:19:23Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    19,
                    23,
                    4,
                    3,
                    0
                ],
                "title": "Practical Secure Inference Algorithm for Fine-tuned Large Language Model\n  Based on Fully Homomorphic Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Secure Inference Algorithm for Fine-tuned Large Language Model\n  Based on Fully Homomorphic Encryption"
                },
                "summary": "Large language models(LLMs) are currently at the forefront of the machine\nlearning field, which show a broad application prospect but at the same time\nexpose some risks of privacy leakage. We combined Fully Homomorphic\nEncryption(FHE) and provable security theory with Parameter-Efficient\nFine-Tuning(PEFT) to propose an efficient and secure inference scheme for LLMs.\nMore specially, we focus on pre-trained LLMs who rely on open-sourced base\nmodel and then fine-tuned with the private datasets by LoRA. This is a popular\nroad-map for Vertical Domain Models such as LawGPT and BenTsao. We use two key\ntechnologies below. Firstly, we divide the whole model into the public part and\nthe private part. The weights of public part are publicly accessible(e.g. the\nopen-sourced base model) while the private part needs to be protected(e.g. the\nLoRA matrices). In this way, the overhead brought by computing on private data\ncan be greatly reduced. Secondly, we propose a general method to transform a\nlinear layer into another one which provides security against model extraction\nattacks and preserves its original functionality, which denotes as Private\nLinear Layer(PLL). Then we use this method on the LoRA matrices to make sure\nthat the server protects their private weights without restricting the user's\ninput. We also show that the difficulty of performing model extraction attacks\nfor PLL can be generalized to the well-known hard problem Learning with\nErrors(LWE). Combing this method with FHE, we can protect user's input at the\nsame time. This transform method can be applied to any linear layer to gain an\nextra protection against model extraction attacks. In this paper, we use the\nopen-source model ChatGLM2-6B as the base model which is fine-tuned by LoRA.\nExperimental results show the inference efficiency of our scheme reaches\n1.61s/token which shows that the scheme has good practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models(LLMs) are currently at the forefront of the machine\nlearning field, which show a broad application prospect but at the same time\nexpose some risks of privacy leakage. We combined Fully Homomorphic\nEncryption(FHE) and provable security theory with Parameter-Efficient\nFine-Tuning(PEFT) to propose an efficient and secure inference scheme for LLMs.\nMore specially, we focus on pre-trained LLMs who rely on open-sourced base\nmodel and then fine-tuned with the private datasets by LoRA. This is a popular\nroad-map for Vertical Domain Models such as LawGPT and BenTsao. We use two key\ntechnologies below. Firstly, we divide the whole model into the public part and\nthe private part. The weights of public part are publicly accessible(e.g. the\nopen-sourced base model) while the private part needs to be protected(e.g. the\nLoRA matrices). In this way, the overhead brought by computing on private data\ncan be greatly reduced. Secondly, we propose a general method to transform a\nlinear layer into another one which provides security against model extraction\nattacks and preserves its original functionality, which denotes as Private\nLinear Layer(PLL). Then we use this method on the LoRA matrices to make sure\nthat the server protects their private weights without restricting the user's\ninput. We also show that the difficulty of performing model extraction attacks\nfor PLL can be generalized to the well-known hard problem Learning with\nErrors(LWE). Combing this method with FHE, we can protect user's input at the\nsame time. This transform method can be applied to any linear layer to gain an\nextra protection against model extraction attacks. In this paper, we use the\nopen-source model ChatGLM2-6B as the base model which is fine-tuned by LoRA.\nExperimental results show the inference efficiency of our scheme reaches\n1.61s/token which shows that the scheme has good practicality."
                },
                "authors": [
                    {
                        "name": "Zhang Ruoyan"
                    },
                    {
                        "name": "Zheng Zhongxiang"
                    },
                    {
                        "name": "Bao Wankang"
                    }
                ],
                "author_detail": {
                    "name": "Bao Wankang"
                },
                "author": "Bao Wankang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16500v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16500v3",
                "updated": "2025-01-03T07:18:30Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    18,
                    30,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-21T06:16:04Z",
                "published_parsed": [
                    2024,
                    12,
                    21,
                    6,
                    16,
                    4,
                    5,
                    356,
                    0
                ],
                "title": "Speech Retrieval-Augmented Generation without Automatic Speech\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speech Retrieval-Augmented Generation without Automatic Speech\n  Recognition"
                },
                "summary": "One common approach for question answering over speech data is to first\ntranscribe speech using automatic speech recognition (ASR) and then employ\ntext-based retrieval-augmented generation (RAG) on the transcriptions. While\nthis cascaded pipeline has proven effective in many practical settings, ASR\nerrors can propagate to the retrieval and generation steps. To overcome this\nlimitation, we introduce SpeechRAG, a novel framework designed for\nopen-question answering over spoken data. Our proposed approach fine-tunes a\npre-trained speech encoder into a speech adapter fed into a frozen large\nlanguage model (LLM)--based retrieval model. By aligning the embedding spaces\nof text and speech, our speech retriever directly retrieves audio passages from\ntext-based queries, leveraging the retrieval capacity of the frozen text\nretriever. Our retrieval experiments on spoken question answering datasets show\nthat direct speech retrieval does not degrade over the text-based baseline, and\noutperforms the cascaded systems using ASR. For generation, we use a speech\nlanguage model (SLM) as a generator, conditioned on audio passages rather than\ntranscripts. Without fine-tuning of the SLM, this approach outperforms cascaded\ntext-based models when there is high WER in the transcripts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One common approach for question answering over speech data is to first\ntranscribe speech using automatic speech recognition (ASR) and then employ\ntext-based retrieval-augmented generation (RAG) on the transcriptions. While\nthis cascaded pipeline has proven effective in many practical settings, ASR\nerrors can propagate to the retrieval and generation steps. To overcome this\nlimitation, we introduce SpeechRAG, a novel framework designed for\nopen-question answering over spoken data. Our proposed approach fine-tunes a\npre-trained speech encoder into a speech adapter fed into a frozen large\nlanguage model (LLM)--based retrieval model. By aligning the embedding spaces\nof text and speech, our speech retriever directly retrieves audio passages from\ntext-based queries, leveraging the retrieval capacity of the frozen text\nretriever. Our retrieval experiments on spoken question answering datasets show\nthat direct speech retrieval does not degrade over the text-based baseline, and\noutperforms the cascaded systems using ASR. For generation, we use a speech\nlanguage model (SLM) as a generator, conditioned on audio passages rather than\ntranscripts. Without fine-tuning of the SLM, this approach outperforms cascaded\ntext-based models when there is high WER in the transcripts."
                },
                "authors": [
                    {
                        "name": "Do June Min"
                    },
                    {
                        "name": "Karel Mundnich"
                    },
                    {
                        "name": "Andy Lapastora"
                    },
                    {
                        "name": "Erfan Soltanmohammadi"
                    },
                    {
                        "name": "Srikanth Ronanki"
                    },
                    {
                        "name": "Kyu Han"
                    }
                ],
                "author_detail": {
                    "name": "Kyu Han"
                },
                "author": "Kyu Han",
                "arxiv_comment": "ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16500v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16500v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11814v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11814v2",
                "updated": "2025-01-03T07:18:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    7,
                    18,
                    19,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-16T14:29:49Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    29,
                    49,
                    0,
                    351,
                    0
                ],
                "title": "EventSum: A Large-Scale Event-Centric Summarization Dataset for Chinese\n  Multi-News Documents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EventSum: A Large-Scale Event-Centric Summarization Dataset for Chinese\n  Multi-News Documents"
                },
                "summary": "In real life, many dynamic events, such as major disasters and large-scale\nsports events, evolve continuously over time. Obtaining an overview of these\nevents can help people quickly understand the situation and respond more\neffectively. This is challenging because the key information of the event is\noften scattered across multiple documents, involving complex event knowledge\nunderstanding and reasoning, which is under-explored in previous work.\nTherefore, we proposed the Event-Centric Multi-Document Summarization (ECS)\ntask, which aims to generate concise and comprehensive summaries of a given\nevent based on multiple related news documents. Based on this, we constructed\nthe EventSum dataset, which was constructed using Baidu Baike entries and\nunderwent extensive human annotation, to facilitate relevant research. It is\nthe first large scale Chinese multi-document summarization dataset, containing\n5,100 events and a total of 57,984 news documents, with an average of 11.4\ninput news documents and 13,471 characters per event. To ensure data quality\nand mitigate potential data leakage, we adopted a multi-stage annotation\napproach for manually labeling the test set. Given the complexity of\nevent-related information, existing metrics struggle to comprehensively assess\nthe quality of generated summaries. We designed specific metrics including\nEvent Recall, Argument Recall, Causal Recall, and Temporal Recall along with\ncorresponding calculation methods for evaluation. We conducted comprehensive\nexperiments on EventSum to evaluate the performance of advanced long-context\nLarge Language Models (LLMs) on this task. Our experimental results indicate\nthat: 1) The event-centric multi-document summarization task remains\nchallenging for existing long-context LLMs; 2) The recall metrics we designed\nare crucial for evaluating the comprehensiveness of the summary information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In real life, many dynamic events, such as major disasters and large-scale\nsports events, evolve continuously over time. Obtaining an overview of these\nevents can help people quickly understand the situation and respond more\neffectively. This is challenging because the key information of the event is\noften scattered across multiple documents, involving complex event knowledge\nunderstanding and reasoning, which is under-explored in previous work.\nTherefore, we proposed the Event-Centric Multi-Document Summarization (ECS)\ntask, which aims to generate concise and comprehensive summaries of a given\nevent based on multiple related news documents. Based on this, we constructed\nthe EventSum dataset, which was constructed using Baidu Baike entries and\nunderwent extensive human annotation, to facilitate relevant research. It is\nthe first large scale Chinese multi-document summarization dataset, containing\n5,100 events and a total of 57,984 news documents, with an average of 11.4\ninput news documents and 13,471 characters per event. To ensure data quality\nand mitigate potential data leakage, we adopted a multi-stage annotation\napproach for manually labeling the test set. Given the complexity of\nevent-related information, existing metrics struggle to comprehensively assess\nthe quality of generated summaries. We designed specific metrics including\nEvent Recall, Argument Recall, Causal Recall, and Temporal Recall along with\ncorresponding calculation methods for evaluation. We conducted comprehensive\nexperiments on EventSum to evaluate the performance of advanced long-context\nLarge Language Models (LLMs) on this task. Our experimental results indicate\nthat: 1) The event-centric multi-document summarization task remains\nchallenging for existing long-context LLMs; 2) The recall metrics we designed\nare crucial for evaluating the comprehensiveness of the summary information."
                },
                "authors": [
                    {
                        "name": "Mengna Zhu"
                    },
                    {
                        "name": "Kaisheng Zeng"
                    },
                    {
                        "name": "Mao Wang"
                    },
                    {
                        "name": "Kaiming Xiao"
                    },
                    {
                        "name": "Lei Hou"
                    },
                    {
                        "name": "Hongbin Huang"
                    },
                    {
                        "name": "Juanzi Li"
                    }
                ],
                "author_detail": {
                    "name": "Juanzi Li"
                },
                "author": "Juanzi Li",
                "arxiv_comment": "Extended version for paper accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11814v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11814v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19878v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19878v3",
                "updated": "2025-01-03T06:55:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    55,
                    2,
                    4,
                    3,
                    0
                ],
                "published": "2024-09-30T02:23:31Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    2,
                    23,
                    31,
                    0,
                    274,
                    0
                ],
                "title": "HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic\n  Thresholds for Fine-Tuning LLM-based ASR Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HDMoLE: Mixture of LoRA Experts with Hierarchical Routing and Dynamic\n  Thresholds for Fine-Tuning LLM-based ASR Models"
                },
                "summary": "Recent advancements in integrating Large Language Models (LLM) with automatic\nspeech recognition (ASR) have performed remarkably in general domains. While\nsupervised fine-tuning (SFT) of all model parameters is often employed to adapt\npre-trained LLM-based ASR models to specific domains, it imposes high\ncomputational costs and notably reduces their performance in general domains.\nIn this paper, we propose a novel parameter-efficient multi-domain fine-tuning\nmethod for adapting pre-trained LLM-based ASR models to multi-accent domains\nwithout catastrophic forgetting named \\textit{HDMoLE}, which leverages\nhierarchical routing and dynamic thresholds based on combining low-rank\nadaptation (LoRA) with the mixer of experts (MoE) and can be generalized to any\nlinear layer. Hierarchical routing establishes a clear correspondence between\nLoRA experts and accent domains, improving cross-domain collaboration among the\nLoRA experts. Unlike the static Top-K strategy for activating LoRA experts,\ndynamic thresholds can adaptively activate varying numbers of LoRA experts at\neach MoE layer. Experiments on the multi-accent and standard Mandarin datasets\ndemonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model\nprojector module achieves similar performance to full fine-tuning in the target\nmulti-accent domains while using only 9.6% of the trainable parameters required\nfor full fine-tuning and minimal degradation in the source general domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in integrating Large Language Models (LLM) with automatic\nspeech recognition (ASR) have performed remarkably in general domains. While\nsupervised fine-tuning (SFT) of all model parameters is often employed to adapt\npre-trained LLM-based ASR models to specific domains, it imposes high\ncomputational costs and notably reduces their performance in general domains.\nIn this paper, we propose a novel parameter-efficient multi-domain fine-tuning\nmethod for adapting pre-trained LLM-based ASR models to multi-accent domains\nwithout catastrophic forgetting named \\textit{HDMoLE}, which leverages\nhierarchical routing and dynamic thresholds based on combining low-rank\nadaptation (LoRA) with the mixer of experts (MoE) and can be generalized to any\nlinear layer. Hierarchical routing establishes a clear correspondence between\nLoRA experts and accent domains, improving cross-domain collaboration among the\nLoRA experts. Unlike the static Top-K strategy for activating LoRA experts,\ndynamic thresholds can adaptively activate varying numbers of LoRA experts at\neach MoE layer. Experiments on the multi-accent and standard Mandarin datasets\ndemonstrate the efficacy of HDMoLE. Applying HDMoLE to an LLM-based ASR model\nprojector module achieves similar performance to full fine-tuning in the target\nmulti-accent domains while using only 9.6% of the trainable parameters required\nfor full fine-tuning and minimal degradation in the source general domain."
                },
                "authors": [
                    {
                        "name": "Bingshen Mu"
                    },
                    {
                        "name": "Kun Wei"
                    },
                    {
                        "name": "Qijie Shao"
                    },
                    {
                        "name": "Yong Xu"
                    },
                    {
                        "name": "Lei Xie"
                    }
                ],
                "author_detail": {
                    "name": "Lei Xie"
                },
                "author": "Lei Xie",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19878v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19878v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01668v1",
                "updated": "2025-01-03T06:50:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    50,
                    6,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T06:50:06Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    50,
                    6,
                    4,
                    3,
                    0
                ],
                "title": "CoT-based Synthesizer: Enhancing LLM Performance through Answer\n  Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoT-based Synthesizer: Enhancing LLM Performance through Answer\n  Synthesis"
                },
                "summary": "Current inference scaling methods, such as Self-consistency and Best-of-N,\nhave proven effective in improving the accuracy of LLMs on complex reasoning\ntasks. However, these methods rely heavily on the quality of candidate\nresponses and are unable to produce correct answers when all candidates are\nincorrect. In this paper, we propose a novel inference scaling strategy,\nCoT-based Synthesizer, which leverages CoT reasoning to synthesize superior\nanswers by analyzing complementary information from multiple candidate\nresponses, even when all candidate responses are flawed. To enable a\nlightweight and cost-effective implementation, we introduce an automated data\ngeneration pipeline that creates diverse training data. This allows smaller\nLLMs trained on this data to improve the inference accuracy of larger models,\nincluding API-based LLMs. Experimental results across four benchmark datasets\nwith seven policy models demonstrate that our method significantly enhances\nperformance, with gains of 11.8% for Llama3-8B and 10.3% for GPT-4o on the MATH\ndataset. The corresponding training data and code are publicly available on\nhttps://github.com/RUCKBReasoning/CoT-based-Synthesizer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current inference scaling methods, such as Self-consistency and Best-of-N,\nhave proven effective in improving the accuracy of LLMs on complex reasoning\ntasks. However, these methods rely heavily on the quality of candidate\nresponses and are unable to produce correct answers when all candidates are\nincorrect. In this paper, we propose a novel inference scaling strategy,\nCoT-based Synthesizer, which leverages CoT reasoning to synthesize superior\nanswers by analyzing complementary information from multiple candidate\nresponses, even when all candidate responses are flawed. To enable a\nlightweight and cost-effective implementation, we introduce an automated data\ngeneration pipeline that creates diverse training data. This allows smaller\nLLMs trained on this data to improve the inference accuracy of larger models,\nincluding API-based LLMs. Experimental results across four benchmark datasets\nwith seven policy models demonstrate that our method significantly enhances\nperformance, with gains of 11.8% for Llama3-8B and 10.3% for GPT-4o on the MATH\ndataset. The corresponding training data and code are publicly available on\nhttps://github.com/RUCKBReasoning/CoT-based-Synthesizer."
                },
                "authors": [
                    {
                        "name": "Bohan Zhang"
                    },
                    {
                        "name": "Xiaokang Zhang"
                    },
                    {
                        "name": "Jing Zhang"
                    },
                    {
                        "name": "Jifan Yu"
                    },
                    {
                        "name": "Sijia Luo"
                    },
                    {
                        "name": "Jie Tang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Tang"
                },
                "author": "Jie Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01664v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01664v1",
                "updated": "2025-01-03T06:37:39Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    37,
                    39,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T06:37:39Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    37,
                    39,
                    4,
                    3,
                    0
                ],
                "title": "BARTPredict: Empowering IoT Security with LLM-Driven Cyber Threat\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BARTPredict: Empowering IoT Security with LLM-Driven Cyber Threat\n  Prediction"
                },
                "summary": "The integration of Internet of Things (IoT) technology in various domains has\nled to operational advancements, but it has also introduced new vulnerabilities\nto cybersecurity threats, as evidenced by recent widespread cyberattacks on IoT\ndevices. Intrusion detection systems are often reactive, triggered by specific\npatterns or anomalies observed within the network. To address this challenge,\nthis work proposes a proactive approach to anticipate and preemptively mitigate\nmalicious activities, aiming to prevent potential damage before it occurs. This\npaper proposes an innovative intrusion prediction framework empowered by\nPre-trained Large Language Models (LLMs). The framework incorporates two LLMs:\na fine-tuned Bidirectional and AutoRegressive Transformers (BART) model for\npredicting network traffic and a fine-tuned Bidirectional Encoder\nRepresentations from Transformers (BERT) model for evaluating the predicted\ntraffic. By harnessing the bidirectional capabilities of BART the framework\nthen identifies malicious packets among these predictions. Evaluated using the\nCICIoT2023 IoT attack dataset, our framework showcases a notable enhancement in\npredictive performance, attaining an impressive 98% overall accuracy, providing\na powerful response to the cybersecurity challenges that confront IoT networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Internet of Things (IoT) technology in various domains has\nled to operational advancements, but it has also introduced new vulnerabilities\nto cybersecurity threats, as evidenced by recent widespread cyberattacks on IoT\ndevices. Intrusion detection systems are often reactive, triggered by specific\npatterns or anomalies observed within the network. To address this challenge,\nthis work proposes a proactive approach to anticipate and preemptively mitigate\nmalicious activities, aiming to prevent potential damage before it occurs. This\npaper proposes an innovative intrusion prediction framework empowered by\nPre-trained Large Language Models (LLMs). The framework incorporates two LLMs:\na fine-tuned Bidirectional and AutoRegressive Transformers (BART) model for\npredicting network traffic and a fine-tuned Bidirectional Encoder\nRepresentations from Transformers (BERT) model for evaluating the predicted\ntraffic. By harnessing the bidirectional capabilities of BART the framework\nthen identifies malicious packets among these predictions. Evaluated using the\nCICIoT2023 IoT attack dataset, our framework showcases a notable enhancement in\npredictive performance, attaining an impressive 98% overall accuracy, providing\na powerful response to the cybersecurity challenges that confront IoT networks."
                },
                "authors": [
                    {
                        "name": "Alaeddine Diaf"
                    },
                    {
                        "name": "Abdelaziz Amara Korba"
                    },
                    {
                        "name": "Nour Elislem Karabadji"
                    },
                    {
                        "name": "Yacine Ghamri-Doudane"
                    }
                ],
                "author_detail": {
                    "name": "Yacine Ghamri-Doudane"
                },
                "author": "Yacine Ghamri-Doudane",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01664v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01664v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20662v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20662v2",
                "updated": "2025-01-03T06:22:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    22,
                    52,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-30T02:40:19Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    2,
                    40,
                    19,
                    0,
                    365,
                    0
                ],
                "title": "Enhancing Table Recognition with Vision LLMs: A Benchmark and\n  Neighbor-Guided Toolchain Reasoner",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Table Recognition with Vision LLMs: A Benchmark and\n  Neighbor-Guided Toolchain Reasoner"
                },
                "summary": "Pre-trained foundation models have recently significantly progressed in\nstructured table understanding and reasoning. However, despite advancements in\nareas such as table semantic understanding and table question answering,\nrecognizing the structure and content of unstructured tables using Vision Large\nLanguage Models (VLLMs) remains under-explored. In this work, we address this\nresearch gap by employing VLLMs in a training-free reasoning paradigm. First,\nwe design a benchmark with various hierarchical dimensions relevant to table\nrecognition. Subsequently, we conduct in-depth evaluations using pre-trained\nVLLMs, finding that low-quality image input is a significant bottleneck in the\nrecognition process. Drawing inspiration from these findings, we propose the\nNeighbor-Guided Toolchain Reasoner (NGTR) framework, which is characterized by\nintegrating multiple lightweight models for low-level visual processing\noperations aimed at mitigating issues with low-quality input images.\nSpecifically, we utilize a neighbor retrieval mechanism to guide the generation\nof multiple tool invocation plans, transferring tool selection experiences from\nsimilar neighbors to the given input, thereby facilitating suitable tool\nselection. Additionally, we introduce a reflection module to supervise the tool\ninvocation process. Extensive experiments on public table recognition datasets\ndemonstrate that our approach significantly enhances the recognition\ncapabilities of the vanilla VLLMs. We believe that the designed benchmark and\nthe proposed NGTR framework could provide an alternative solution in table\nrecognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained foundation models have recently significantly progressed in\nstructured table understanding and reasoning. However, despite advancements in\nareas such as table semantic understanding and table question answering,\nrecognizing the structure and content of unstructured tables using Vision Large\nLanguage Models (VLLMs) remains under-explored. In this work, we address this\nresearch gap by employing VLLMs in a training-free reasoning paradigm. First,\nwe design a benchmark with various hierarchical dimensions relevant to table\nrecognition. Subsequently, we conduct in-depth evaluations using pre-trained\nVLLMs, finding that low-quality image input is a significant bottleneck in the\nrecognition process. Drawing inspiration from these findings, we propose the\nNeighbor-Guided Toolchain Reasoner (NGTR) framework, which is characterized by\nintegrating multiple lightweight models for low-level visual processing\noperations aimed at mitigating issues with low-quality input images.\nSpecifically, we utilize a neighbor retrieval mechanism to guide the generation\nof multiple tool invocation plans, transferring tool selection experiences from\nsimilar neighbors to the given input, thereby facilitating suitable tool\nselection. Additionally, we introduce a reflection module to supervise the tool\ninvocation process. Extensive experiments on public table recognition datasets\ndemonstrate that our approach significantly enhances the recognition\ncapabilities of the vanilla VLLMs. We believe that the designed benchmark and\nthe proposed NGTR framework could provide an alternative solution in table\nrecognition."
                },
                "authors": [
                    {
                        "name": "Yitong Zhou"
                    },
                    {
                        "name": "Mingyue Cheng"
                    },
                    {
                        "name": "Qingyang Mao"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Feiyang Xu"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Enhong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Enhong Chen"
                },
                "author": "Enhong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20662v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20662v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12247v2",
                "updated": "2025-01-03T06:19:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    19,
                    14,
                    4,
                    3,
                    0
                ],
                "published": "2024-10-16T05:17:49Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    5,
                    17,
                    49,
                    2,
                    290,
                    0
                ],
                "title": "EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient MoE Inference"
                },
                "summary": "The Mixture-of-Experts (MoE) model has emerged as a prominent architecture in\nthe field of Large Language Models (LLMs), providing a better balance between\nmodel performance and computational efficiency. However the General Matrix\nMultiply (GEMM) operations and large parameters introduce challenges related to\ncomputational efficiency and communication overhead, which become throughput\nbottlenecks during inference. Applying a single parallelism strategy like EP,\nDP, TP or a straightforward combination of them to MoE usually achieves\nsub-optimal inference throughput. This paper introduces EPS-MoE, a novel expert\npipeline scheduler for MoE that surpasses the existing parallelism schemes. Our\napproach optimizes the computation of MoE FeedForward Network (FFN) modules by\ndynamically selecting the best kernel implementation of GroupGemm and DenseGemm\nfor different loads and adaptively overlapping these computations with\ncommunication, leading to a substantial increase in throughput. Our\nexperimental results demonstrate at most 52.4\\% improvement in prefill\nthroughput compared to existing parallel inference methods. Specifically, our\nmethod accelerated the highly optimized DeepSeekV2 model from a claimed 100K\ntokens per second to at least 120K tokens per second.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture-of-Experts (MoE) model has emerged as a prominent architecture in\nthe field of Large Language Models (LLMs), providing a better balance between\nmodel performance and computational efficiency. However the General Matrix\nMultiply (GEMM) operations and large parameters introduce challenges related to\ncomputational efficiency and communication overhead, which become throughput\nbottlenecks during inference. Applying a single parallelism strategy like EP,\nDP, TP or a straightforward combination of them to MoE usually achieves\nsub-optimal inference throughput. This paper introduces EPS-MoE, a novel expert\npipeline scheduler for MoE that surpasses the existing parallelism schemes. Our\napproach optimizes the computation of MoE FeedForward Network (FFN) modules by\ndynamically selecting the best kernel implementation of GroupGemm and DenseGemm\nfor different loads and adaptively overlapping these computations with\ncommunication, leading to a substantial increase in throughput. Our\nexperimental results demonstrate at most 52.4\\% improvement in prefill\nthroughput compared to existing parallel inference methods. Specifically, our\nmethod accelerated the highly optimized DeepSeekV2 model from a claimed 100K\ntokens per second to at least 120K tokens per second."
                },
                "authors": [
                    {
                        "name": "Yulei Qian"
                    },
                    {
                        "name": "Fengcun Li"
                    },
                    {
                        "name": "Xiangyang Ji"
                    },
                    {
                        "name": "Xiaoyu Zhao"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "arxiv_comment": "14 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17799v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17799v2",
                "updated": "2025-01-03T06:15:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    15,
                    58,
                    4,
                    3,
                    0
                ],
                "published": "2024-10-23T11:58:58Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    11,
                    58,
                    58,
                    2,
                    297,
                    0
                ],
                "title": "OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniFlatten: An End-to-end GPT Model for Seamless Voice Conversation"
                },
                "summary": "Full-duplex spoken dialogue systems significantly surpass traditional\nturn-based dialogue systems, as they allow simultaneous bidirectional\ncommunication, closely mirroring human-human interactions. However, achieving\nlow latency and natural interactions in full-duplex dialogue systems remains a\nsignificant challenge, especially considering human conversation dynamics such\nas interruptions, backchannels, and overlapping speech. In this paper, we\nintroduce a novel End-to-End GPT-based model OmniFlatten for full-duplex\nconversation, capable of effectively modeling the complex behaviors inherent to\nnatural conversations with low latency. To achieve full-duplex conversation\ncapabilities, we propose a multi-stage post-training scheme that progressively\nadapts a text large language model (LLM) backbone into a speech-text dialogue\nLLM, capable of generating text and speech in real time, without modifying the\narchitecture of the backbone LLM. The training process comprises three stages:\nmodality alignment, half-duplex dialogue learning, and full-duplex dialogue\nlearning. In all training stages, we standardize the data using a flattening\noperation, which enables unifying the training methods and the GPT backbone\nacross different modalities and tasks. Our approach offers a simple modeling\ntechnique and a promising research direction for developing efficient and\nnatural end-to-end full-duplex spoken dialogue systems. Audio samples of\ndialogues generated by OmniFlatten can be found at this web site\n(https://omniflatten.github.io/).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Full-duplex spoken dialogue systems significantly surpass traditional\nturn-based dialogue systems, as they allow simultaneous bidirectional\ncommunication, closely mirroring human-human interactions. However, achieving\nlow latency and natural interactions in full-duplex dialogue systems remains a\nsignificant challenge, especially considering human conversation dynamics such\nas interruptions, backchannels, and overlapping speech. In this paper, we\nintroduce a novel End-to-End GPT-based model OmniFlatten for full-duplex\nconversation, capable of effectively modeling the complex behaviors inherent to\nnatural conversations with low latency. To achieve full-duplex conversation\ncapabilities, we propose a multi-stage post-training scheme that progressively\nadapts a text large language model (LLM) backbone into a speech-text dialogue\nLLM, capable of generating text and speech in real time, without modifying the\narchitecture of the backbone LLM. The training process comprises three stages:\nmodality alignment, half-duplex dialogue learning, and full-duplex dialogue\nlearning. In all training stages, we standardize the data using a flattening\noperation, which enables unifying the training methods and the GPT backbone\nacross different modalities and tasks. Our approach offers a simple modeling\ntechnique and a promising research direction for developing efficient and\nnatural end-to-end full-duplex spoken dialogue systems. Audio samples of\ndialogues generated by OmniFlatten can be found at this web site\n(https://omniflatten.github.io/)."
                },
                "authors": [
                    {
                        "name": "Qinglin Zhang"
                    },
                    {
                        "name": "Luyao Cheng"
                    },
                    {
                        "name": "Chong Deng"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Wen Wang"
                    },
                    {
                        "name": "Siqi Zheng"
                    },
                    {
                        "name": "Jiaqing Liu"
                    },
                    {
                        "name": "Hai Yu"
                    },
                    {
                        "name": "Chaohong Tan"
                    },
                    {
                        "name": "Zhihao Du"
                    },
                    {
                        "name": "Shiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shiliang Zhang"
                },
                "author": "Shiliang Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17799v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17799v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01652v1",
                "updated": "2025-01-03T06:07:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    7,
                    48,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T06:07:48Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    6,
                    7,
                    48,
                    4,
                    3,
                    0
                ],
                "title": "MIRAGE: Exploring How Large Language Models Perform in Complex Social\n  Interactive Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: Exploring How Large Language Models Perform in Complex Social\n  Interactive Environments"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in\nenvironmental perception, reasoning-based decision-making, and simulating\ncomplex human behaviors, particularly in interactive role-playing contexts.\nThis paper introduces the Multiverse Interactive Role-play Ability General\nEvaluation (MIRAGE), a comprehensive framework designed to assess LLMs'\nproficiency in portraying advanced human behaviors through murder mystery\ngames. MIRAGE features eight intricately crafted scripts encompassing diverse\nthemes and styles, providing a rich simulation. To evaluate LLMs' performance,\nMIRAGE employs four distinct methods: the Trust Inclination Index (TII) to\nmeasure dynamics of trust and suspicion, the Clue Investigation Capability\n(CIC) to measure LLMs' capability of conducting information, the Interactivity\nCapability Index (ICI) to assess role-playing capabilities and the Script\nCompliance Index (SCI) to assess LLMs' capability of understanding and\nfollowing instructions. Our experiments indicate that even popular models like\nGPT-4 face significant challenges in navigating the complexities presented by\nthe MIRAGE. The datasets and simulation codes are available in\n\\href{https://github.com/lime728/MIRAGE}{github}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in\nenvironmental perception, reasoning-based decision-making, and simulating\ncomplex human behaviors, particularly in interactive role-playing contexts.\nThis paper introduces the Multiverse Interactive Role-play Ability General\nEvaluation (MIRAGE), a comprehensive framework designed to assess LLMs'\nproficiency in portraying advanced human behaviors through murder mystery\ngames. MIRAGE features eight intricately crafted scripts encompassing diverse\nthemes and styles, providing a rich simulation. To evaluate LLMs' performance,\nMIRAGE employs four distinct methods: the Trust Inclination Index (TII) to\nmeasure dynamics of trust and suspicion, the Clue Investigation Capability\n(CIC) to measure LLMs' capability of conducting information, the Interactivity\nCapability Index (ICI) to assess role-playing capabilities and the Script\nCompliance Index (SCI) to assess LLMs' capability of understanding and\nfollowing instructions. Our experiments indicate that even popular models like\nGPT-4 face significant challenges in navigating the complexities presented by\nthe MIRAGE. The datasets and simulation codes are available in\n\\href{https://github.com/lime728/MIRAGE}{github}."
                },
                "authors": [
                    {
                        "name": "Cai Yin"
                    },
                    {
                        "name": "Gu Zhouhong"
                    },
                    {
                        "name": "Du Zhaohan"
                    },
                    {
                        "name": "Ye Zheyu"
                    },
                    {
                        "name": "Cao Shaosheng"
                    },
                    {
                        "name": "Xu Yiqian"
                    },
                    {
                        "name": "Feng Hongwei"
                    },
                    {
                        "name": "Chen Ping"
                    }
                ],
                "author_detail": {
                    "name": "Chen Ping"
                },
                "author": "Chen Ping",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01643v1",
                "updated": "2025-01-03T05:28:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    5,
                    28,
                    19,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T05:28:19Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    5,
                    28,
                    19,
                    4,
                    3,
                    0
                ],
                "title": "A Polarimetry-based Field-deployable Non-interruptive Mirror Soiling\n  Detection Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Polarimetry-based Field-deployable Non-interruptive Mirror Soiling\n  Detection Method"
                },
                "summary": "The soiling level of heliostat mirrors in Concentrated Solar Power (CSP)\nfields is one of the key factors that significantly influences optical\nefficiency. State-of-the-art methods of monitoring heliostats soiling levels\nhave limitations such as slow, labor-intensive, high-cost installation, and\ninterruptive to solar field operations. Here we present a rapid,\ncost-effective, user-friendly and non-intrusive Polarimetric Imaging-based\nMirror Soiling (PIMS) detection method. The PIMS imaging device is very compact\nand can be integrated on an unmanned aerial vehicle (UAV) for single-shot\nmeasurement of large area measurement on Heliostat mirrors for fast soiling\ndetection without labor-intensive inspection of each heliostat with a\nreflectometer. With skylight as a natural light source, we developed a\nmethodology to correlate Degree of Linear Polarization (DoLP) image of mirrors\nto their soiling levels using an experimentally calibrated model based on Mie\nScattering Theory and Monte-Carlo simulation. For field deployment of the PIMS\nmethod, minimal pre-installation is required, and the field operation is not\ninterrupted by the UAV imaging process. The PIMS method has significant\npotential for deployment in various concentration solar-thermal power (CSP)\nplants, offering high speed, non-interruptive mirror soiling detection.\nMoreover, the method can be further developed for other types of solar fields,\nsuch as parabolic troughs, solar panels, etc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The soiling level of heliostat mirrors in Concentrated Solar Power (CSP)\nfields is one of the key factors that significantly influences optical\nefficiency. State-of-the-art methods of monitoring heliostats soiling levels\nhave limitations such as slow, labor-intensive, high-cost installation, and\ninterruptive to solar field operations. Here we present a rapid,\ncost-effective, user-friendly and non-intrusive Polarimetric Imaging-based\nMirror Soiling (PIMS) detection method. The PIMS imaging device is very compact\nand can be integrated on an unmanned aerial vehicle (UAV) for single-shot\nmeasurement of large area measurement on Heliostat mirrors for fast soiling\ndetection without labor-intensive inspection of each heliostat with a\nreflectometer. With skylight as a natural light source, we developed a\nmethodology to correlate Degree of Linear Polarization (DoLP) image of mirrors\nto their soiling levels using an experimentally calibrated model based on Mie\nScattering Theory and Monte-Carlo simulation. For field deployment of the PIMS\nmethod, minimal pre-installation is required, and the field operation is not\ninterrupted by the UAV imaging process. The PIMS method has significant\npotential for deployment in various concentration solar-thermal power (CSP)\nplants, offering high speed, non-interruptive mirror soiling detection.\nMoreover, the method can be further developed for other types of solar fields,\nsuch as parabolic troughs, solar panels, etc."
                },
                "authors": [
                    {
                        "name": "Mo Tian"
                    },
                    {
                        "name": "Md Zubair Ebne Rafique"
                    },
                    {
                        "name": "Kolappan Chidambaranathan"
                    },
                    {
                        "name": "Randy Brost"
                    },
                    {
                        "name": "Daniel Small"
                    },
                    {
                        "name": "David Novick"
                    },
                    {
                        "name": "Julius Yellowhair"
                    },
                    {
                        "name": "Yu Yao"
                    }
                ],
                "author_detail": {
                    "name": "Yu Yao"
                },
                "author": "Yu Yao",
                "arxiv_comment": "38 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01641v1",
                "updated": "2025-01-03T05:20:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    5,
                    20,
                    48,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T05:20:48Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    5,
                    20,
                    48,
                    4,
                    3,
                    0
                ],
                "title": "Fiber-based mid-infrared frequency-swept laser at 50 MScans/s via\n  frequency down-conversion of time-stretched pulses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fiber-based mid-infrared frequency-swept laser at 50 MScans/s via\n  frequency down-conversion of time-stretched pulses"
                },
                "summary": "Increasing the sweep rate of mid-infrared (MIR) frequency-swept sources\noffers significant potential for various high-speed spectroscopy-based\napplications. While continuous-wave frequency-swept lasers have achieved sweep\nrates up to 1 MHz, a recently demonstrated time-stretched ultrashort pulsed\nlaser has reached a significantly higher sweep rate, up to tens of MHz.\nHowever, the previous system relied on a bulky femtosecond optical parametric\noscillator and produced only ~30 discrete spectral elements due to the use of a\nfree-space time stretcher. In this work, we present a fiber-based\nfrequency-swept MIR source that utilizes the frequency down-conversion of\ntime-stretched near-infrared pulses, employing a compact mode-locked fiber\nlaser and telecommunication fiber. As a proof-of-concept demonstration, we\nperformed MIR spectroscopy of methane gas around 3.4 um at a rate of 50\nMSpectra/s, capturing 220 spectral elements over a range of 19.0 cm-1. This\ncompact and robust high-speed MIR frequency-swept laser system holds the\npotential for deployment in field applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Increasing the sweep rate of mid-infrared (MIR) frequency-swept sources\noffers significant potential for various high-speed spectroscopy-based\napplications. While continuous-wave frequency-swept lasers have achieved sweep\nrates up to 1 MHz, a recently demonstrated time-stretched ultrashort pulsed\nlaser has reached a significantly higher sweep rate, up to tens of MHz.\nHowever, the previous system relied on a bulky femtosecond optical parametric\noscillator and produced only ~30 discrete spectral elements due to the use of a\nfree-space time stretcher. In this work, we present a fiber-based\nfrequency-swept MIR source that utilizes the frequency down-conversion of\ntime-stretched near-infrared pulses, employing a compact mode-locked fiber\nlaser and telecommunication fiber. As a proof-of-concept demonstration, we\nperformed MIR spectroscopy of methane gas around 3.4 um at a rate of 50\nMSpectra/s, capturing 220 spectral elements over a range of 19.0 cm-1. This\ncompact and robust high-speed MIR frequency-swept laser system holds the\npotential for deployment in field applications."
                },
                "authors": [
                    {
                        "name": "Makoto Shoshin"
                    },
                    {
                        "name": "Takahiro Kageyama"
                    },
                    {
                        "name": "Takuma Nakamura"
                    },
                    {
                        "name": "Kazuki Hashimoto"
                    },
                    {
                        "name": "Takuro Ideguchi"
                    }
                ],
                "author_detail": {
                    "name": "Takuro Ideguchi"
                },
                "author": "Takuro Ideguchi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10819v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10819v2",
                "updated": "2025-01-03T04:12:32Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    4,
                    12,
                    32,
                    4,
                    3,
                    0
                ],
                "published": "2024-08-20T13:13:41Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    13,
                    41,
                    1,
                    233,
                    0
                ],
                "title": "GS-KGC: A Generative Subgraph-based Framework for Knowledge Graph\n  Completion with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GS-KGC: A Generative Subgraph-based Framework for Knowledge Graph\n  Completion with Large Language Models"
                },
                "summary": "Knowledge graph completion (KGC) focuses on identifying missing triples in a\nknowledge graph (KG) , which is crucial for many downstream applications. Given\nthe rapid development of large language models (LLMs), some LLM-based methods\nare proposed for KGC task. However, most of them focus on prompt engineering\nwhile overlooking the fact that finer-grained subgraph information can aid LLMs\nin generating more accurate answers. In this paper, we propose a novel\ncompletion framework called \\textbf{G}enerative \\textbf{S}ubgraph-based KGC\n(GS-KGC), which utilizes subgraph information as contextual reasoning and\nemploys a QA approach to achieve the KGC task. This framework primarily\nincludes a subgraph partitioning algorithm designed to generate negatives and\nneighbors. Specifically, negatives can encourage LLMs to generate a broader\nrange of answers, while neighbors provide additional contextual insights for\nLLM reasoning. Furthermore, we found that GS-KGC can discover potential triples\nwithin the KGs and new facts beyond the KGs. Experiments conducted on four\ncommon KGC datasets highlight the advantages of the proposed GS-KGC, e.g., it\nshows a 5.6\\% increase in Hits@3 compared to the LLM-based model CP-KGC on the\nFB15k-237N, and a 9.3\\% increase over the LLM-based model TECHS on the ICEWS14.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge graph completion (KGC) focuses on identifying missing triples in a\nknowledge graph (KG) , which is crucial for many downstream applications. Given\nthe rapid development of large language models (LLMs), some LLM-based methods\nare proposed for KGC task. However, most of them focus on prompt engineering\nwhile overlooking the fact that finer-grained subgraph information can aid LLMs\nin generating more accurate answers. In this paper, we propose a novel\ncompletion framework called \\textbf{G}enerative \\textbf{S}ubgraph-based KGC\n(GS-KGC), which utilizes subgraph information as contextual reasoning and\nemploys a QA approach to achieve the KGC task. This framework primarily\nincludes a subgraph partitioning algorithm designed to generate negatives and\nneighbors. Specifically, negatives can encourage LLMs to generate a broader\nrange of answers, while neighbors provide additional contextual insights for\nLLM reasoning. Furthermore, we found that GS-KGC can discover potential triples\nwithin the KGs and new facts beyond the KGs. Experiments conducted on four\ncommon KGC datasets highlight the advantages of the proposed GS-KGC, e.g., it\nshows a 5.6\\% increase in Hits@3 compared to the LLM-based model CP-KGC on the\nFB15k-237N, and a 9.3\\% increase over the LLM-based model TECHS on the ICEWS14."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Jiahao Zhu"
                    },
                    {
                        "name": "Jianping Man"
                    },
                    {
                        "name": "Hongze Liu"
                    },
                    {
                        "name": "Li Fang"
                    },
                    {
                        "name": "Yi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zhou"
                },
                "author": "Yi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10819v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10819v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01625v1",
                "updated": "2025-01-03T03:46:51Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    3,
                    46,
                    51,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T03:46:51Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    3,
                    46,
                    51,
                    4,
                    3,
                    0
                ],
                "title": "ICPC: In-context Prompt Compression with Faster Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ICPC: In-context Prompt Compression with Faster Inference"
                },
                "summary": "Despite the recent success of Large Language Models (LLMs), it remains\nchallenging to feed LLMs with long prompts due to the fixed size of LLM inputs.\nAs a remedy, prompt compression becomes a promising solution by removing\nredundant tokens in the prompt. However, using LLM in the existing works\nrequires additional computation resources and leads to memory overheads. To\naddress it, we propose ICPC (In-context Prompt Compression), a novel and\nscalable prompt compression method that adaptively reduces the prompt length.\nThe key idea of ICPC is to calculate the probability of each word appearing in\nthe prompt using encoders and calculate information carried by each word\nthrough the information function, which effectively reduces the information\nloss during prompt compression and increases the speed of compression.\nEmpirically, we demonstrate that ICPC can effectively compress long texts of\ndifferent categories and thus achieve better performance and speed on different\ntypes of NLP tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of Large Language Models (LLMs), it remains\nchallenging to feed LLMs with long prompts due to the fixed size of LLM inputs.\nAs a remedy, prompt compression becomes a promising solution by removing\nredundant tokens in the prompt. However, using LLM in the existing works\nrequires additional computation resources and leads to memory overheads. To\naddress it, we propose ICPC (In-context Prompt Compression), a novel and\nscalable prompt compression method that adaptively reduces the prompt length.\nThe key idea of ICPC is to calculate the probability of each word appearing in\nthe prompt using encoders and calculate information carried by each word\nthrough the information function, which effectively reduces the information\nloss during prompt compression and increases the speed of compression.\nEmpirically, we demonstrate that ICPC can effectively compress long texts of\ndifferent categories and thus achieve better performance and speed on different\ntypes of NLP tasks."
                },
                "authors": [
                    {
                        "name": "Ziyang Yu"
                    },
                    {
                        "name": "Yuyu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yuyu Liu"
                },
                "author": "Yuyu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01620v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01620v1",
                "updated": "2025-01-03T03:28:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    3,
                    28,
                    33,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T03:28:33Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    3,
                    28,
                    33,
                    4,
                    3,
                    0
                ],
                "title": "Adaptive Meta-learning-based Adversarial Training for Robust Automatic\n  Modulation Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Meta-learning-based Adversarial Training for Robust Automatic\n  Modulation Classification"
                },
                "summary": "DL-based automatic modulation classification (AMC) models are highly\nsusceptible to adversarial attacks, where even minimal input perturbations can\ncause severe misclassifications. While adversarially training an AMC model\nbased on an adversarial attack significantly increases its robustness against\nthat attack, the AMC model will still be defenseless against other adversarial\nattacks. The theoretically infinite possibilities for adversarial perturbations\nmean that an AMC model will inevitably encounter new unseen adversarial attacks\nif it is ever to be deployed to a real-world communication system. Moreover,\nthe computational limitations and challenges of obtaining new data in real-time\nwill not allow a full training process for the AMC model to adapt to the new\nattack when it is online. To this end, we propose a meta-learning-based\nadversarial training framework for AMC models that substantially enhances\nrobustness against unseen adversarial attacks and enables fast adaptation to\nthese attacks using just a few new training samples, if any are available. Our\nresults demonstrate that this training framework provides superior robustness\nand accuracy with much less online training time than conventional adversarial\ntraining of AMC models, making it highly efficient for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DL-based automatic modulation classification (AMC) models are highly\nsusceptible to adversarial attacks, where even minimal input perturbations can\ncause severe misclassifications. While adversarially training an AMC model\nbased on an adversarial attack significantly increases its robustness against\nthat attack, the AMC model will still be defenseless against other adversarial\nattacks. The theoretically infinite possibilities for adversarial perturbations\nmean that an AMC model will inevitably encounter new unseen adversarial attacks\nif it is ever to be deployed to a real-world communication system. Moreover,\nthe computational limitations and challenges of obtaining new data in real-time\nwill not allow a full training process for the AMC model to adapt to the new\nattack when it is online. To this end, we propose a meta-learning-based\nadversarial training framework for AMC models that substantially enhances\nrobustness against unseen adversarial attacks and enables fast adaptation to\nthese attacks using just a few new training samples, if any are available. Our\nresults demonstrate that this training framework provides superior robustness\nand accuracy with much less online training time than conventional adversarial\ntraining of AMC models, making it highly efficient for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Amirmohammad Bamdad"
                    },
                    {
                        "name": "Ali Owfi"
                    },
                    {
                        "name": "Fatemeh Afghah"
                    }
                ],
                "author_detail": {
                    "name": "Fatemeh Afghah"
                },
                "author": "Fatemeh Afghah",
                "arxiv_comment": "Submitted to IEEE International Conference on Communications (ICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01620v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01620v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01616v1",
                "updated": "2025-01-03T03:20:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    3,
                    20,
                    27,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T03:20:27Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    3,
                    20,
                    27,
                    4,
                    3,
                    0
                ],
                "title": "Digital-Analog Transmission based Emergency Semantic Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital-Analog Transmission based Emergency Semantic Communications"
                },
                "summary": "Emergency Wireless Communication (EWC) networks adopt the User Datagram\nProtocol (UDP) to transmit scene images in real time for quickly assessing the\nextent of the damage. However, existing UDP-based EWC exhibits suboptimal\nperformance under poor channel conditions since UDP lacks an Automatic Repeat\nreQuest (ARQ) mechanism. In addition, future EWC systems must not only enhance\nhuman decisionmaking during emergency response operations but also support\nArtificial Intelligence (AI)-driven approaches to improve rescue efficiency.\nThe Deep Learning-based Semantic Communication (DL-based SemCom) emerges as a\nrobust, efficient, and taskoriented transmission scheme, suitable for\ndeployment in UDP based EWC. Due to the constraints in hardware capabilities\nand transmission resources, the EWC transmitter is unable to integrate\nsufficiently powerful NN model, thereby failing to achieve ideal performance\nunder EWC scene. For EWC scene, we propose a performance-constrained semantic\ncoding model, which considers the effects of the semantic noise and the channel\nnoise. Then, we derive Cramer-Rao lower bound of the proposed semantic coding\nmodel, as guidance for the design of semantic codec to enhance its adaptability\nto semantic noise as well as channel noise. To further improve the system\nperformance, we propose Digital-Analog transmission based Emergency Semantic\nCommunication (DAESemCom) framework, which integrates the analog DL-based\nsemantic coding and the digital Distributed Source Coding (DSC) schemes to\nleverage their respective advantages. The simulation results show that the\nproposed DA-ESemCom framework outperforms the classical Separated\nSource-Channel Coding (SSCC) and other DL-based Joint Source-Channel Coding\n(DL-based JSCC) schemes in terms of fidelity and detection performances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergency Wireless Communication (EWC) networks adopt the User Datagram\nProtocol (UDP) to transmit scene images in real time for quickly assessing the\nextent of the damage. However, existing UDP-based EWC exhibits suboptimal\nperformance under poor channel conditions since UDP lacks an Automatic Repeat\nreQuest (ARQ) mechanism. In addition, future EWC systems must not only enhance\nhuman decisionmaking during emergency response operations but also support\nArtificial Intelligence (AI)-driven approaches to improve rescue efficiency.\nThe Deep Learning-based Semantic Communication (DL-based SemCom) emerges as a\nrobust, efficient, and taskoriented transmission scheme, suitable for\ndeployment in UDP based EWC. Due to the constraints in hardware capabilities\nand transmission resources, the EWC transmitter is unable to integrate\nsufficiently powerful NN model, thereby failing to achieve ideal performance\nunder EWC scene. For EWC scene, we propose a performance-constrained semantic\ncoding model, which considers the effects of the semantic noise and the channel\nnoise. Then, we derive Cramer-Rao lower bound of the proposed semantic coding\nmodel, as guidance for the design of semantic codec to enhance its adaptability\nto semantic noise as well as channel noise. To further improve the system\nperformance, we propose Digital-Analog transmission based Emergency Semantic\nCommunication (DAESemCom) framework, which integrates the analog DL-based\nsemantic coding and the digital Distributed Source Coding (DSC) schemes to\nleverage their respective advantages. The simulation results show that the\nproposed DA-ESemCom framework outperforms the classical Separated\nSource-Channel Coding (SSCC) and other DL-based Joint Source-Channel Coding\n(DL-based JSCC) schemes in terms of fidelity and detection performances."
                },
                "authors": [
                    {
                        "name": "Yuzhou Fu"
                    },
                    {
                        "name": "Wenchi Cheng"
                    },
                    {
                        "name": "Jingqing Wang"
                    },
                    {
                        "name": "Liuguo Yin"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01614v1",
                "updated": "2025-01-03T03:18:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    3,
                    18,
                    28,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T03:18:28Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    3,
                    18,
                    28,
                    4,
                    3,
                    0
                ],
                "title": "Evaluation of Rail Decarbonization Alternatives: Framework and\n  Application",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluation of Rail Decarbonization Alternatives: Framework and\n  Application"
                },
                "summary": "The Northwestern University Freight Rail Infrastructure and Energy Network\nDecarbonization (NUFRIEND) framework is a comprehensive industry-oriented tool\nfor simulating the deployment of new energy technologies including biofuels,\ne-fuels, battery-electric, and hydrogen locomotives. By classifying fuel types\ninto two categories based on deployment requirements, the associated optimal\ncharging/fueling facility location and sizing problem are solved with a\nfive-step framework. Life cycle analyses (LCA) and techno-economic analyses\n(TEA) are used to estimate carbon reduction, capital investments, cost of\ncarbon reduction, and operational impacts, enabling sensitivity analysis with\noperational and technological parameters. The framework is illustrated on\nlower-carbon drop-in fuels as well as battery-electric technology deployments\nfor US Eastern and Western Class I railroad networks. Drop-in fuel deployments\nare modeled as admixtures with diesel in existing locomotives, while\nbattery-electric deployments are shown for varying technology penetration\nlevels and locomotive ranges. When mixed in a 50 percent ratio with diesel,\nresults show biodiesel's capacity to reduce emissions at 36 percent with a cost\nof 0.13 USD per kilogram of CO2 reduced, while e-fuels offer a 50 percent\nemissions reduction potential at a cost of 0.22 USD per kilogram of CO2\nreduced. Battery-electric results for 50 percent deployment over all ton-miles\nhighlight the value of future innovations in battery energy densities as\nscenarios assuming 800-mile range locomotives show an estimated emissions\nreduction of 46 percent with a cost of 0.06 USD per kilogram of CO2 reduced,\ncompared to 16 percent emissions reduction at a cost of 0.11 USD per kilogram\nof CO2 reduced for 400-mile range locomotives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Northwestern University Freight Rail Infrastructure and Energy Network\nDecarbonization (NUFRIEND) framework is a comprehensive industry-oriented tool\nfor simulating the deployment of new energy technologies including biofuels,\ne-fuels, battery-electric, and hydrogen locomotives. By classifying fuel types\ninto two categories based on deployment requirements, the associated optimal\ncharging/fueling facility location and sizing problem are solved with a\nfive-step framework. Life cycle analyses (LCA) and techno-economic analyses\n(TEA) are used to estimate carbon reduction, capital investments, cost of\ncarbon reduction, and operational impacts, enabling sensitivity analysis with\noperational and technological parameters. The framework is illustrated on\nlower-carbon drop-in fuels as well as battery-electric technology deployments\nfor US Eastern and Western Class I railroad networks. Drop-in fuel deployments\nare modeled as admixtures with diesel in existing locomotives, while\nbattery-electric deployments are shown for varying technology penetration\nlevels and locomotive ranges. When mixed in a 50 percent ratio with diesel,\nresults show biodiesel's capacity to reduce emissions at 36 percent with a cost\nof 0.13 USD per kilogram of CO2 reduced, while e-fuels offer a 50 percent\nemissions reduction potential at a cost of 0.22 USD per kilogram of CO2\nreduced. Battery-electric results for 50 percent deployment over all ton-miles\nhighlight the value of future innovations in battery energy densities as\nscenarios assuming 800-mile range locomotives show an estimated emissions\nreduction of 46 percent with a cost of 0.06 USD per kilogram of CO2 reduced,\ncompared to 16 percent emissions reduction at a cost of 0.11 USD per kilogram\nof CO2 reduced for 400-mile range locomotives."
                },
                "authors": [
                    {
                        "name": "Adrian Hernandez"
                    },
                    {
                        "name": "Max TM Ng"
                    },
                    {
                        "name": "Nazib Siddique"
                    },
                    {
                        "name": "Pablo L. Durango-Cohen"
                    },
                    {
                        "name": "Amgad Elgowainy"
                    },
                    {
                        "name": "Hani S. Mahmassani"
                    },
                    {
                        "name": "Michael Wang"
                    },
                    {
                        "name": "Yan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhou"
                },
                "author": "Yan Zhou",
                "arxiv_doi": "10.1177/03611981231170182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1177/03611981231170182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.01614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "29 pages, 17 figures. This is the accepted version of a work that was\n  published in Transportation Research Record",
                "arxiv_journal_ref": "Transportation Research Record 2678.1 (2024): 102-121",
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01028v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01028v2",
                "updated": "2025-01-03T03:16:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    3,
                    16,
                    10,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-02T03:17:51Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    3,
                    17,
                    51,
                    3,
                    2,
                    0
                ],
                "title": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KaLM-Embedding: Superior Training Data Brings A Stronger Embedding Model"
                },
                "summary": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As retrieval-augmented generation prevails in large language models,\nembedding models are becoming increasingly crucial. Despite the growing number\nof general embedding models, prior work often overlooks the critical role of\ntraining data quality. In this work, we introduce KaLM-Embedding, a general\nmultilingual embedding model that leverages a large quantity of cleaner, more\ndiverse, and domain-specific training data. Our model has been trained with key\ntechniques proven to enhance performance: (1) persona-based synthetic data to\ncreate diversified examples distilled from LLMs, (2) ranking consistency\nfiltering to remove less informative samples, and (3) semi-homogeneous task\nbatch sampling to improve training efficacy. Departing from traditional\nBERT-like architectures, we adopt Qwen2-0.5B as the pre-trained model,\nfacilitating the adaptation of auto-regressive language models for general\nembedding tasks. Extensive evaluations of the MTEB benchmark across multiple\nlanguages show that our model outperforms others of comparable size, setting a\nnew standard for multilingual embedding models with <1B parameters."
                },
                "authors": [
                    {
                        "name": "Xinshuo Hu"
                    },
                    {
                        "name": "Zifei Shan"
                    },
                    {
                        "name": "Xinping Zhao"
                    },
                    {
                        "name": "Zetian Sun"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Dongfang Li"
                    },
                    {
                        "name": "Shaolin Ye"
                    },
                    {
                        "name": "Xinyuan Wei"
                    },
                    {
                        "name": "Qian Chen"
                    },
                    {
                        "name": "Baotian Hu"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "Technical Report. 23 pages, 6 figures, 10 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01028v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01028v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01608v1",
                "updated": "2025-01-03T02:58:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    2,
                    58,
                    22,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T02:58:22Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    2,
                    58,
                    22,
                    4,
                    3,
                    0
                ],
                "title": "Online Meta-Learning Channel Autoencoder for Dynamic End-to-end Physical\n  Layer Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Meta-Learning Channel Autoencoder for Dynamic End-to-end Physical\n  Layer Optimization"
                },
                "summary": "Channel Autoencoders (CAEs) have shown significant potential in optimizing\nthe physical layer of a wireless communication system for a specific channel\nthrough joint end-to-end training. However, the practical implementation of\nCAEs faces several challenges, particularly in realistic and dynamic scenarios.\nChannels in communication systems are dynamic and change with time. Still, most\nproposed CAE designs assume stationary scenarios, meaning they are trained and\ntested for only one channel realization without regard for the dynamic nature\nof wireless communication systems. Moreover, conventional CAEs are designed\nbased on the assumption of having access to a large number of pilot signals,\nwhich act as training samples in the context of CAEs. However, in real-world\napplications, it is not feasible for a CAE operating in real-time to acquire\nlarge amounts of training samples for each new channel realization. Hence, the\nCAE has to be deployable in few-shot learning scenarios where only limited\ntraining samples are available. Furthermore, most proposed conventional CAEs\nlack fast adaptability to new channel realizations, which becomes more\npronounced when dealing with a limited number of pilots. To address these\nchallenges, this paper proposes the Online Meta Learning channel AE (OML-CAE)\nframework for few-shot CAE scenarios with dynamic channels. The OML-CAE\nframework enhances adaptability to varying channel conditions in an online\nmanner, allowing for dynamic adjustments in response to evolving communication\nscenarios. Moreover, it can adapt to new channel conditions using only a few\npilots, drastically increasing pilot efficiency and making the CAE design\nfeasible in realistic scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Channel Autoencoders (CAEs) have shown significant potential in optimizing\nthe physical layer of a wireless communication system for a specific channel\nthrough joint end-to-end training. However, the practical implementation of\nCAEs faces several challenges, particularly in realistic and dynamic scenarios.\nChannels in communication systems are dynamic and change with time. Still, most\nproposed CAE designs assume stationary scenarios, meaning they are trained and\ntested for only one channel realization without regard for the dynamic nature\nof wireless communication systems. Moreover, conventional CAEs are designed\nbased on the assumption of having access to a large number of pilot signals,\nwhich act as training samples in the context of CAEs. However, in real-world\napplications, it is not feasible for a CAE operating in real-time to acquire\nlarge amounts of training samples for each new channel realization. Hence, the\nCAE has to be deployable in few-shot learning scenarios where only limited\ntraining samples are available. Furthermore, most proposed conventional CAEs\nlack fast adaptability to new channel realizations, which becomes more\npronounced when dealing with a limited number of pilots. To address these\nchallenges, this paper proposes the Online Meta Learning channel AE (OML-CAE)\nframework for few-shot CAE scenarios with dynamic channels. The OML-CAE\nframework enhances adaptability to varying channel conditions in an online\nmanner, allowing for dynamic adjustments in response to evolving communication\nscenarios. Moreover, it can adapt to new channel conditions using only a few\npilots, drastically increasing pilot efficiency and making the CAE design\nfeasible in realistic scenarios."
                },
                "authors": [
                    {
                        "name": "Ali Owfi"
                    },
                    {
                        "name": "Jonathan Ashdown"
                    },
                    {
                        "name": "Kurt Turck"
                    }
                ],
                "author_detail": {
                    "name": "Kurt Turck"
                },
                "author": "Kurt Turck",
                "arxiv_comment": "To be published in IEEE Wireless Communications and Networking\n  Conference (WCNC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00430v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00430v2",
                "updated": "2025-01-03T02:50:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    2,
                    50,
                    59,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-31T13:11:20Z",
                "published_parsed": [
                    2024,
                    12,
                    31,
                    13,
                    11,
                    20,
                    1,
                    366,
                    0
                ],
                "title": "Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and\n  Reflection agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing LLM Reasoning with Multi-Path Collaborative Reactive and\n  Reflection agents"
                },
                "summary": "Agents have demonstrated their potential in scientific reasoning tasks\nthrough large language models. However, they often face challenges such as\ninsufficient accuracy and degeneration of thought when handling complex\nreasoning tasks, which impede their performance. To overcome these issues, we\npropose the Reactive and Reflection agents with Multi-Path Reasoning (RR-MP)\nFramework, aimed at enhancing the reasoning capabilities of LLMs. Our approach\nimproves scientific reasoning accuracy by employing a multi-path reasoning\nmechanism where each path consists of a reactive agent and a reflection agent\nthat collaborate to prevent degeneration of thought inherent in single-agent\nreliance. Additionally, the RR-MP framework does not require additional\ntraining; it utilizes multiple dialogue instances for each reasoning path and a\nseparate summarizer to consolidate insights from all paths. This design\nintegrates diverse perspectives and strengthens reasoning across each path. We\nconducted zero-shot and few-shot evaluations on tasks involving moral\nscenarios, college-level physics, and mathematics. Experimental results\ndemonstrate that our method outperforms baseline approaches, highlighting the\neffectiveness and advantages of the RR-MP framework in managing complex\nscientific reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents have demonstrated their potential in scientific reasoning tasks\nthrough large language models. However, they often face challenges such as\ninsufficient accuracy and degeneration of thought when handling complex\nreasoning tasks, which impede their performance. To overcome these issues, we\npropose the Reactive and Reflection agents with Multi-Path Reasoning (RR-MP)\nFramework, aimed at enhancing the reasoning capabilities of LLMs. Our approach\nimproves scientific reasoning accuracy by employing a multi-path reasoning\nmechanism where each path consists of a reactive agent and a reflection agent\nthat collaborate to prevent degeneration of thought inherent in single-agent\nreliance. Additionally, the RR-MP framework does not require additional\ntraining; it utilizes multiple dialogue instances for each reasoning path and a\nseparate summarizer to consolidate insights from all paths. This design\nintegrates diverse perspectives and strengthens reasoning across each path. We\nconducted zero-shot and few-shot evaluations on tasks involving moral\nscenarios, college-level physics, and mathematics. Experimental results\ndemonstrate that our method outperforms baseline approaches, highlighting the\neffectiveness and advantages of the RR-MP framework in managing complex\nscientific reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Chengbo He"
                    },
                    {
                        "name": "Bochao Zou"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Jiansheng Chen"
                    },
                    {
                        "name": "Junliang Xing"
                    },
                    {
                        "name": "Huimin Ma"
                    }
                ],
                "author_detail": {
                    "name": "Huimin Ma"
                },
                "author": "Huimin Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00430v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00430v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02318v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02318v3",
                "updated": "2025-01-03T02:19:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    2,
                    19,
                    3,
                    4,
                    3,
                    0
                ],
                "published": "2024-11-04T17:44:11Z",
                "published_parsed": [
                    2024,
                    11,
                    4,
                    17,
                    44,
                    11,
                    0,
                    309,
                    0
                ],
                "title": "Evaluating the Ability of Large Language Models to Generate Verifiable\n  Specifications in VeriFast",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Ability of Large Language Models to Generate Verifiable\n  Specifications in VeriFast"
                },
                "summary": "Static verification is a powerful method for enhancing software quality, but\nit demands significant human labor and resources. This is particularly true of\nstatic verifiers that reason about heap manipulating programs using an\nownership logic. LLMs have shown promise in a number of software engineering\nactivities, including code generation, test generation, proof generation for\ntheorem provers, and specification generation for static verifiers. However,\nprior work has not explored how well LLMs can perform specification generation\nfor specifications based in an ownership logic, such as separation logic. To\naddress this gap, this paper explores OpenAI's GPT-4o model's effectiveness in\ngenerating specifications on C programs that are verifiable with VeriFast, a\nseparation logic based static verifier. Our experiment employs three different\ntypes of user inputs as well as basic and Chain-of-Thought (CoT) prompting to\nassess GPT's capabilities. Our results indicate that the specifications\ngenerated by GPT-4o preserve functional behavior, but struggle to be\nverifiable. When the specifications are verifiable they contain redundancies.\nFuture directions are discussed to improve the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static verification is a powerful method for enhancing software quality, but\nit demands significant human labor and resources. This is particularly true of\nstatic verifiers that reason about heap manipulating programs using an\nownership logic. LLMs have shown promise in a number of software engineering\nactivities, including code generation, test generation, proof generation for\ntheorem provers, and specification generation for static verifiers. However,\nprior work has not explored how well LLMs can perform specification generation\nfor specifications based in an ownership logic, such as separation logic. To\naddress this gap, this paper explores OpenAI's GPT-4o model's effectiveness in\ngenerating specifications on C programs that are verifiable with VeriFast, a\nseparation logic based static verifier. Our experiment employs three different\ntypes of user inputs as well as basic and Chain-of-Thought (CoT) prompting to\nassess GPT's capabilities. Our results indicate that the specifications\ngenerated by GPT-4o preserve functional behavior, but struggle to be\nverifiable. When the specifications are verifiable they contain redundancies.\nFuture directions are discussed to improve the performance."
                },
                "authors": [
                    {
                        "name": "Wen Fan"
                    },
                    {
                        "name": "Marilyn Rego"
                    },
                    {
                        "name": "Xin Hu"
                    },
                    {
                        "name": "Sanya Dod"
                    },
                    {
                        "name": "Zhaorui Ni"
                    },
                    {
                        "name": "Danning Xie"
                    },
                    {
                        "name": "Jenna DiVincenzo"
                    },
                    {
                        "name": "Lin Tan"
                    }
                ],
                "author_detail": {
                    "name": "Lin Tan"
                },
                "author": "Lin Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02318v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02318v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15115v2",
                "updated": "2025-01-03T02:18:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    2,
                    18,
                    21,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-19T17:56:09Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    56,
                    9,
                    3,
                    354,
                    0
                ],
                "title": "Qwen2.5 Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qwen2.5 Technical Report"
                },
                "summary": "In this report, we introduce Qwen2.5, a comprehensive series of large\nlanguage models (LLMs) designed to meet diverse needs. Compared to previous\niterations, Qwen 2.5 has been significantly improved during both the\npre-training and post-training stages. In terms of pre-training, we have scaled\nthe high-quality pre-training datasets from the previous 7 trillion tokens to\n18 trillion tokens. This provides a strong foundation for common sense, expert\nknowledge, and reasoning capabilities. In terms of post-training, we implement\nintricate supervised finetuning with over 1 million samples, as well as\nmultistage reinforcement learning. Post-training techniques enhance human\npreference, and notably improve long text generation, structural data analysis,\nand instruction following. To handle diverse and varied use cases effectively,\nwe present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base\nand instruction-tuned models, with quantized versions available. In addition,\nfor hosted solutions, the proprietary models currently include two\nmixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both\navailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier\nperformance on a wide range of benchmarks evaluating language understanding,\nreasoning, mathematics, coding, human preference alignment, etc. Specifically,\nthe open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and\nproprietary models and demonstrates competitive performance to the\nstate-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5\ntimes larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness\nwhile performing competitively against GPT-4o-mini and GPT-4o respectively.\nAdditionally, as the foundation, Qwen2.5 models have been instrumental in\ntraining specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and\nmultimodal models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this report, we introduce Qwen2.5, a comprehensive series of large\nlanguage models (LLMs) designed to meet diverse needs. Compared to previous\niterations, Qwen 2.5 has been significantly improved during both the\npre-training and post-training stages. In terms of pre-training, we have scaled\nthe high-quality pre-training datasets from the previous 7 trillion tokens to\n18 trillion tokens. This provides a strong foundation for common sense, expert\nknowledge, and reasoning capabilities. In terms of post-training, we implement\nintricate supervised finetuning with over 1 million samples, as well as\nmultistage reinforcement learning. Post-training techniques enhance human\npreference, and notably improve long text generation, structural data analysis,\nand instruction following. To handle diverse and varied use cases effectively,\nwe present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base\nand instruction-tuned models, with quantized versions available. In addition,\nfor hosted solutions, the proprietary models currently include two\nmixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both\navailable from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier\nperformance on a wide range of benchmarks evaluating language understanding,\nreasoning, mathematics, coding, human preference alignment, etc. Specifically,\nthe open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and\nproprietary models and demonstrates competitive performance to the\nstate-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5\ntimes larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness\nwhile performing competitively against GPT-4o-mini and GPT-4o respectively.\nAdditionally, as the foundation, Qwen2.5 models have been instrumental in\ntraining specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and\nmultimodal models."
                },
                "authors": [
                    {
                        "name": "Qwen"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "An Yang"
                    },
                    {
                        "name": "Baosong Yang"
                    },
                    {
                        "name": "Beichen Zhang"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Bowen Yu"
                    },
                    {
                        "name": "Chengyuan Li"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Huan Lin"
                    },
                    {
                        "name": "Jian Yang"
                    },
                    {
                        "name": "Jianhong Tu"
                    },
                    {
                        "name": "Jianwei Zhang"
                    },
                    {
                        "name": "Jianxin Yang"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Jingren Zhou"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Kai Dang"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Keqin Bao"
                    },
                    {
                        "name": "Kexin Yang"
                    },
                    {
                        "name": "Le Yu"
                    },
                    {
                        "name": "Mei Li"
                    },
                    {
                        "name": "Mingfeng Xue"
                    },
                    {
                        "name": "Pei Zhang"
                    },
                    {
                        "name": "Qin Zhu"
                    },
                    {
                        "name": "Rui Men"
                    },
                    {
                        "name": "Runji Lin"
                    },
                    {
                        "name": "Tianhao Li"
                    },
                    {
                        "name": "Tianyi Tang"
                    },
                    {
                        "name": "Tingyu Xia"
                    },
                    {
                        "name": "Xingzhang Ren"
                    },
                    {
                        "name": "Xuancheng Ren"
                    },
                    {
                        "name": "Yang Fan"
                    },
                    {
                        "name": "Yang Su"
                    },
                    {
                        "name": "Yichang Zhang"
                    },
                    {
                        "name": "Yu Wan"
                    },
                    {
                        "name": "Yuqiong Liu"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Zhenru Zhang"
                    },
                    {
                        "name": "Zihan Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Zihan Qiu"
                },
                "arxiv_affiliation": "additional authors not shown",
                "author": "Zihan Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10622v2",
                "updated": "2025-01-03T02:12:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    2,
                    12,
                    37,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-14T00:05:42Z",
                "published_parsed": [
                    2024,
                    12,
                    14,
                    0,
                    5,
                    42,
                    5,
                    349,
                    0
                ],
                "title": "A recent evaluation on the performance of LLMs on radiation oncology\n  physics using questions of randomly shuffled options",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent evaluation on the performance of LLMs on radiation oncology\n  physics using questions of randomly shuffled options"
                },
                "summary": "Purpose: We present an updated study evaluating the performance of large\nlanguage models (LLMs) in answering radiation oncology physics questions,\nfocusing on the recently released models.\n  Methods: A set of 100 multiple choice radiation oncology physics questions,\npreviously created by a well-experienced physicist, was used for this study.\nThe answer options of the questions were randomly shuffled to create \"new\" exam\nsets. Five LLMs (OpenAI o1-preview, GPT-4o, LLaMA 3.1 (405B), Gemini 1.5 Pro,\nand Claude 3.5 Sonnet) with the versions released before September 30, 2024,\nwere queried using these new exam sets. To evaluate their deductive reasoning\ncapabilities, the correct answers in the questions were replaced with \"None of\nthe above.\" Then, the explaining-first and step-by-step instruction prompts\nwere used to test if this strategy improved their reasoning capabilities. The\nperformance of the LLMs was compared with the answers from medical physicists.\n  Results: All models demonstrated expert-level performance on these questions,\nwith o1-preview even surpassing medical physicists with a majority vote. When\nreplacing the correct answers with \"None of the above,\" all models exhibited a\nconsiderable decline in performance, suggesting room for improvement. The\nexplaining-first and step-by-step instruction prompts helped enhance the\nreasoning capabilities of the LLaMA 3.1 (405B), Gemini 1.5 Pro, and Claude 3.5\nSonnet models.\n  Conclusion: These recently released LLMs demonstrated expert-level\nperformance in answering radiation oncology physics questions, exhibiting great\npotential to assist in radiation oncology physics training and education.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Purpose: We present an updated study evaluating the performance of large\nlanguage models (LLMs) in answering radiation oncology physics questions,\nfocusing on the recently released models.\n  Methods: A set of 100 multiple choice radiation oncology physics questions,\npreviously created by a well-experienced physicist, was used for this study.\nThe answer options of the questions were randomly shuffled to create \"new\" exam\nsets. Five LLMs (OpenAI o1-preview, GPT-4o, LLaMA 3.1 (405B), Gemini 1.5 Pro,\nand Claude 3.5 Sonnet) with the versions released before September 30, 2024,\nwere queried using these new exam sets. To evaluate their deductive reasoning\ncapabilities, the correct answers in the questions were replaced with \"None of\nthe above.\" Then, the explaining-first and step-by-step instruction prompts\nwere used to test if this strategy improved their reasoning capabilities. The\nperformance of the LLMs was compared with the answers from medical physicists.\n  Results: All models demonstrated expert-level performance on these questions,\nwith o1-preview even surpassing medical physicists with a majority vote. When\nreplacing the correct answers with \"None of the above,\" all models exhibited a\nconsiderable decline in performance, suggesting room for improvement. The\nexplaining-first and step-by-step instruction prompts helped enhance the\nreasoning capabilities of the LLaMA 3.1 (405B), Gemini 1.5 Pro, and Claude 3.5\nSonnet models.\n  Conclusion: These recently released LLMs demonstrated expert-level\nperformance in answering radiation oncology physics questions, exhibiting great\npotential to assist in radiation oncology physics training and education."
                },
                "authors": [
                    {
                        "name": "Peilong Wang"
                    },
                    {
                        "name": "Jason Holmes"
                    },
                    {
                        "name": "Zhengliang Liu"
                    },
                    {
                        "name": "Dequan Chen"
                    },
                    {
                        "name": "Tianming Liu"
                    },
                    {
                        "name": "Jiajian Shen"
                    },
                    {
                        "name": "Wei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Wei Liu"
                },
                "author": "Wei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02642v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02642v2",
                "updated": "2025-01-03T02:00:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    2,
                    0,
                    1,
                    4,
                    3,
                    0
                ],
                "published": "2024-06-04T10:59:43Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    10,
                    59,
                    43,
                    1,
                    156,
                    0
                ],
                "title": "E-ICL: Enhancing Fine-Grained Emotion Recognition through the Lens of\n  Prototype Theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E-ICL: Enhancing Fine-Grained Emotion Recognition through the Lens of\n  Prototype Theory"
                },
                "summary": "In-context learning (ICL) achieves remarkable performance in various domains\nsuch as knowledge acquisition, commonsense reasoning, and semantic\nunderstanding. However, its performance significantly deteriorates for emotion\ndetection tasks, especially fine-grained emotion recognition. The underlying\nreasons for this remain unclear. In this paper, we identify the reasons behind\nICL's poor performance from the perspective of prototype theory and propose a\nmethod to address this issue. Specifically, we conduct extensive pilot\nexperiments and find that ICL conforms to the prototype theory on fine-grained\nemotion recognition. Based on this theory, we uncover the following\ndeficiencies in ICL: (1) It relies on prototypes (example-label pairs) that are\nsemantically similar but emotionally inaccurate to predict emotions. (2) It is\nprone to interference from irrelevant categories, affecting the accuracy and\nrobustness of the predictions. To address these issues, we propose an Emotion\nContext Learning method (E-ICL) on fine-grained emotion recognition. E-ICL\nrelies on more emotionally accurate prototypes to predict categories by\nreferring to emotionally similar examples with dynamic labels. Simultaneously,\nE-ICL employs an exclusionary emotion prediction strategy to avoid interference\nfrom irrelevant categories, thereby increasing its accuracy and robustness.\nNote that the entire process is accomplished with the assistance of a\nplug-and-play emotion auxiliary model, without additional training. Experiments\non the fine-grained emotion datasets EDOS, Empathetic-Dialogues,\nEmpatheticIntent, and GoEmotions show that E-ICL achieves superior emotion\nprediction performance. Furthermore, even when the emotion auxiliary model used\nis lower than 10% of the LLMs, E-ICL can still boost the performance of LLMs by\nover 4% on multiple datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) achieves remarkable performance in various domains\nsuch as knowledge acquisition, commonsense reasoning, and semantic\nunderstanding. However, its performance significantly deteriorates for emotion\ndetection tasks, especially fine-grained emotion recognition. The underlying\nreasons for this remain unclear. In this paper, we identify the reasons behind\nICL's poor performance from the perspective of prototype theory and propose a\nmethod to address this issue. Specifically, we conduct extensive pilot\nexperiments and find that ICL conforms to the prototype theory on fine-grained\nemotion recognition. Based on this theory, we uncover the following\ndeficiencies in ICL: (1) It relies on prototypes (example-label pairs) that are\nsemantically similar but emotionally inaccurate to predict emotions. (2) It is\nprone to interference from irrelevant categories, affecting the accuracy and\nrobustness of the predictions. To address these issues, we propose an Emotion\nContext Learning method (E-ICL) on fine-grained emotion recognition. E-ICL\nrelies on more emotionally accurate prototypes to predict categories by\nreferring to emotionally similar examples with dynamic labels. Simultaneously,\nE-ICL employs an exclusionary emotion prediction strategy to avoid interference\nfrom irrelevant categories, thereby increasing its accuracy and robustness.\nNote that the entire process is accomplished with the assistance of a\nplug-and-play emotion auxiliary model, without additional training. Experiments\non the fine-grained emotion datasets EDOS, Empathetic-Dialogues,\nEmpatheticIntent, and GoEmotions show that E-ICL achieves superior emotion\nprediction performance. Furthermore, even when the emotion auxiliary model used\nis lower than 10% of the LLMs, E-ICL can still boost the performance of LLMs by\nover 4% on multiple datasets."
                },
                "authors": [
                    {
                        "name": "Zhaochun Ren"
                    },
                    {
                        "name": "Zhou Yang"
                    },
                    {
                        "name": "Chenglong Ye"
                    },
                    {
                        "name": "Yufeng Wang"
                    },
                    {
                        "name": "Haizhou Sun"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Xiaofei Zhu"
                    },
                    {
                        "name": "Yunbing Wu"
                    },
                    {
                        "name": "Xiangwen Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangwen Liao"
                },
                "author": "Xiangwen Liao",
                "arxiv_comment": "16 pages, 7 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02642v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02642v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15221v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15221v2",
                "updated": "2025-01-03T01:55:35Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    1,
                    55,
                    35,
                    4,
                    3,
                    0
                ],
                "published": "2024-11-20T23:08:01Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    23,
                    8,
                    1,
                    2,
                    325,
                    0
                ],
                "title": "Reflections from the 2024 Large Language Model (LLM) Hackathon for\n  Applications in Materials Science and Chemistry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflections from the 2024 Large Language Model (LLM) Hackathon for\n  Applications in Materials Science and Chemistry"
                },
                "summary": "Here, we present the outcomes from the second Large Language Model (LLM)\nHackathon for Applications in Materials Science and Chemistry, which engaged\nparticipants across global hybrid locations, resulting in 34 team submissions.\nThe submissions spanned seven key application areas and demonstrated the\ndiverse utility of LLMs for applications in (1) molecular and material property\nprediction; (2) molecular and material design; (3) automation and novel\ninterfaces; (4) scientific communication and education; (5) research data\nmanagement and automation; (6) hypothesis generation and evaluation; and (7)\nknowledge extraction and reasoning from scientific literature. Each team\nsubmission is presented in a summary table with links to the code and as brief\npapers in the appendix. Beyond team results, we discuss the hackathon event and\nits hybrid format, which included physical hubs in Toronto, Montreal, San\nFrancisco, Berlin, Lausanne, and Tokyo, alongside a global online hub to enable\nlocal and virtual collaboration. Overall, the event highlighted significant\nimprovements in LLM capabilities since the previous year's hackathon,\nsuggesting continued expansion of LLMs for applications in materials science\nand chemistry research. These outcomes demonstrate the dual utility of LLMs as\nboth multipurpose models for diverse machine learning tasks and platforms for\nrapid prototyping custom applications in scientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Here, we present the outcomes from the second Large Language Model (LLM)\nHackathon for Applications in Materials Science and Chemistry, which engaged\nparticipants across global hybrid locations, resulting in 34 team submissions.\nThe submissions spanned seven key application areas and demonstrated the\ndiverse utility of LLMs for applications in (1) molecular and material property\nprediction; (2) molecular and material design; (3) automation and novel\ninterfaces; (4) scientific communication and education; (5) research data\nmanagement and automation; (6) hypothesis generation and evaluation; and (7)\nknowledge extraction and reasoning from scientific literature. Each team\nsubmission is presented in a summary table with links to the code and as brief\npapers in the appendix. Beyond team results, we discuss the hackathon event and\nits hybrid format, which included physical hubs in Toronto, Montreal, San\nFrancisco, Berlin, Lausanne, and Tokyo, alongside a global online hub to enable\nlocal and virtual collaboration. Overall, the event highlighted significant\nimprovements in LLM capabilities since the previous year's hackathon,\nsuggesting continued expansion of LLMs for applications in materials science\nand chemistry research. These outcomes demonstrate the dual utility of LLMs as\nboth multipurpose models for diverse machine learning tasks and platforms for\nrapid prototyping custom applications in scientific research."
                },
                "authors": [
                    {
                        "name": "Yoel Zimmermann"
                    },
                    {
                        "name": "Adib Bazgir"
                    },
                    {
                        "name": "Zartashia Afzal"
                    },
                    {
                        "name": "Fariha Agbere"
                    },
                    {
                        "name": "Qianxiang Ai"
                    },
                    {
                        "name": "Nawaf Alampara"
                    },
                    {
                        "name": "Alexander Al-Feghali"
                    },
                    {
                        "name": "Mehrad Ansari"
                    },
                    {
                        "name": "Dmytro Antypov"
                    },
                    {
                        "name": "Amro Aswad"
                    },
                    {
                        "name": "Jiaru Bai"
                    },
                    {
                        "name": "Viktoriia Baibakova"
                    },
                    {
                        "name": "Devi Dutta Biswajeet"
                    },
                    {
                        "name": "Erik Bitzek"
                    },
                    {
                        "name": "Joshua D. Bocarsly"
                    },
                    {
                        "name": "Anna Borisova"
                    },
                    {
                        "name": "Andres M Bran"
                    },
                    {
                        "name": "L. Catherine Brinson"
                    },
                    {
                        "name": "Marcel Moran Calderon"
                    },
                    {
                        "name": "Alessandro Canalicchio"
                    },
                    {
                        "name": "Victor Chen"
                    },
                    {
                        "name": "Yuan Chiang"
                    },
                    {
                        "name": "Defne Circi"
                    },
                    {
                        "name": "Benjamin Charmes"
                    },
                    {
                        "name": "Vikrant Chaudhary"
                    },
                    {
                        "name": "Zizhang Chen"
                    },
                    {
                        "name": "Min-Hsueh Chiu"
                    },
                    {
                        "name": "Judith Clymo"
                    },
                    {
                        "name": "Kedar Dabhadkar"
                    },
                    {
                        "name": "Nathan Daelman"
                    },
                    {
                        "name": "Archit Datar"
                    },
                    {
                        "name": "Wibe A. de Jong"
                    },
                    {
                        "name": "Matthew L. Evans"
                    },
                    {
                        "name": "Maryam Ghazizade Fard"
                    },
                    {
                        "name": "Giuseppe Fisicaro"
                    },
                    {
                        "name": "Abhijeet Sadashiv Gangan"
                    },
                    {
                        "name": "Janine George"
                    },
                    {
                        "name": "Jose D. Cojal Gonzalez"
                    },
                    {
                        "name": "Michael Götte"
                    },
                    {
                        "name": "Ankur K. Gupta"
                    },
                    {
                        "name": "Hassan Harb"
                    },
                    {
                        "name": "Pengyu Hong"
                    },
                    {
                        "name": "Abdelrahman Ibrahim"
                    },
                    {
                        "name": "Ahmed Ilyas"
                    },
                    {
                        "name": "Alishba Imran"
                    },
                    {
                        "name": "Kevin Ishimwe"
                    },
                    {
                        "name": "Ramsey Issa"
                    },
                    {
                        "name": "Kevin Maik Jablonka"
                    },
                    {
                        "name": "Colin Jones"
                    },
                    {
                        "name": "Tyler R. Josephson"
                    },
                    {
                        "name": "Greg Juhasz"
                    },
                    {
                        "name": "Sarthak Kapoor"
                    },
                    {
                        "name": "Rongda Kang"
                    },
                    {
                        "name": "Ghazal Khalighinejad"
                    },
                    {
                        "name": "Sartaaj Khan"
                    },
                    {
                        "name": "Sascha Klawohn"
                    },
                    {
                        "name": "Suneel Kuman"
                    },
                    {
                        "name": "Alvin Noe Ladines"
                    },
                    {
                        "name": "Sarom Leang"
                    },
                    {
                        "name": "Magdalena Lederbauer"
                    },
                    {
                        "name": "Sheng-Lun"
                    },
                    {
                        "name": "Liao"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Xuefeng Liu"
                    },
                    {
                        "name": "Stanley Lo"
                    },
                    {
                        "name": "Sandeep Madireddy"
                    },
                    {
                        "name": "Piyush Ranjan Maharana"
                    },
                    {
                        "name": "Shagun Maheshwari"
                    },
                    {
                        "name": "Soroush Mahjoubi"
                    },
                    {
                        "name": "José A. Márquez"
                    },
                    {
                        "name": "Rob Mills"
                    },
                    {
                        "name": "Trupti Mohanty"
                    },
                    {
                        "name": "Bernadette Mohr"
                    },
                    {
                        "name": "Seyed Mohamad Moosavi"
                    },
                    {
                        "name": "Alexander Moßhammer"
                    },
                    {
                        "name": "Amirhossein D. Naghdi"
                    },
                    {
                        "name": "Aakash Naik"
                    },
                    {
                        "name": "Oleksandr Narykov"
                    },
                    {
                        "name": "Hampus Näsström"
                    },
                    {
                        "name": "Xuan Vu Nguyen"
                    },
                    {
                        "name": "Xinyi Ni"
                    },
                    {
                        "name": "Dana O'Connor"
                    },
                    {
                        "name": "Teslim Olayiwola"
                    },
                    {
                        "name": "Federico Ottomano"
                    },
                    {
                        "name": "Aleyna Beste Ozhan"
                    },
                    {
                        "name": "Sebastian Pagel"
                    },
                    {
                        "name": "Chiku Parida"
                    },
                    {
                        "name": "Jaehee Park"
                    },
                    {
                        "name": "Vraj Patel"
                    },
                    {
                        "name": "Elena Patyukova"
                    },
                    {
                        "name": "Martin Hoffmann Petersen"
                    },
                    {
                        "name": "Luis Pinto"
                    },
                    {
                        "name": "José M. Pizarro"
                    },
                    {
                        "name": "Dieter Plessers"
                    },
                    {
                        "name": "Tapashree Pradhan"
                    },
                    {
                        "name": "Utkarsh Pratiush"
                    },
                    {
                        "name": "Charishma Puli"
                    },
                    {
                        "name": "Andrew Qin"
                    },
                    {
                        "name": "Mahyar Rajabi"
                    },
                    {
                        "name": "Francesco Ricci"
                    },
                    {
                        "name": "Elliot Risch"
                    },
                    {
                        "name": "Martiño Ríos-García"
                    },
                    {
                        "name": "Aritra Roy"
                    },
                    {
                        "name": "Tehseen Rug"
                    },
                    {
                        "name": "Hasan M Sayeed"
                    },
                    {
                        "name": "Markus Scheidgen"
                    },
                    {
                        "name": "Mara Schilling-Wilhelmi"
                    },
                    {
                        "name": "Marcel Schloz"
                    },
                    {
                        "name": "Fabian Schöppach"
                    },
                    {
                        "name": "Julia Schumann"
                    },
                    {
                        "name": "Philippe Schwaller"
                    },
                    {
                        "name": "Marcus Schwarting"
                    },
                    {
                        "name": "Samiha Sharlin"
                    },
                    {
                        "name": "Kevin Shen"
                    },
                    {
                        "name": "Jiale Shi"
                    },
                    {
                        "name": "Pradip Si"
                    },
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Taylor Sparks"
                    },
                    {
                        "name": "Suraj Sudhakar"
                    },
                    {
                        "name": "Leopold Talirz"
                    },
                    {
                        "name": "Dandan Tang"
                    },
                    {
                        "name": "Olga Taran"
                    },
                    {
                        "name": "Carla Terboven"
                    },
                    {
                        "name": "Mark Tropin"
                    },
                    {
                        "name": "Anastasiia Tsymbal"
                    },
                    {
                        "name": "Katharina Ueltzen"
                    },
                    {
                        "name": "Pablo Andres Unzueta"
                    },
                    {
                        "name": "Archit Vasan"
                    },
                    {
                        "name": "Tirtha Vinchurkar"
                    },
                    {
                        "name": "Trung Vo"
                    },
                    {
                        "name": "Gabriel Vogel"
                    },
                    {
                        "name": "Christoph Völker"
                    },
                    {
                        "name": "Jan Weinreich"
                    },
                    {
                        "name": "Faradawn Yang"
                    },
                    {
                        "name": "Mohd Zaki"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Sylvester Zhang"
                    },
                    {
                        "name": "Weijie Zhang"
                    },
                    {
                        "name": "Ruijie Zhu"
                    },
                    {
                        "name": "Shang Zhu"
                    },
                    {
                        "name": "Jan Janssen"
                    },
                    {
                        "name": "Calvin Li"
                    },
                    {
                        "name": "Ian Foster"
                    },
                    {
                        "name": "Ben Blaiszik"
                    }
                ],
                "author_detail": {
                    "name": "Ben Blaiszik"
                },
                "arxiv_affiliation": "Mark",
                "author": "Ben Blaiszik",
                "arxiv_comment": "Updating author information, the submission remains largely\n  unchanged. 98 pages total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15221v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15221v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01594v1",
                "updated": "2025-01-03T01:38:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    1,
                    38,
                    46,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T01:38:46Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    1,
                    38,
                    46,
                    4,
                    3,
                    0
                ],
                "title": "PSYCHE: A Multi-faceted Patient Simulation Framework for Evaluation of\n  Psychiatric Assessment Conversational Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PSYCHE: A Multi-faceted Patient Simulation Framework for Evaluation of\n  Psychiatric Assessment Conversational Agents"
                },
                "summary": "Recent advances in large language models (LLMs) have accelerated the\ndevelopment of conversational agents capable of generating human-like\nresponses. Since psychiatric assessments typically involve complex\nconversational interactions between psychiatrists and patients, there is\ngrowing interest in developing LLM-based psychiatric assessment conversational\nagents (PACAs) that aim to simulate the role of psychiatrists in clinical\nevaluations. However, standardized methods for benchmarking the clinical\nappropriateness of PACAs' interaction with patients still remain underexplored.\nHere, we propose PSYCHE, a novel framework designed to enable the 1) clinically\nrelevant, 2) ethically safe, 3) cost-efficient, and 4) quantitative evaluation\nof PACAs. This is achieved by simulating psychiatric patients based on a\nmulti-faceted psychiatric construct that defines the simulated patients'\nprofiles, histories, and behaviors, which PACAs are expected to assess. We\nvalidate the effectiveness of PSYCHE through a study with 10 board-certified\npsychiatrists, supported by an in-depth analysis of the simulated patient\nutterances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have accelerated the\ndevelopment of conversational agents capable of generating human-like\nresponses. Since psychiatric assessments typically involve complex\nconversational interactions between psychiatrists and patients, there is\ngrowing interest in developing LLM-based psychiatric assessment conversational\nagents (PACAs) that aim to simulate the role of psychiatrists in clinical\nevaluations. However, standardized methods for benchmarking the clinical\nappropriateness of PACAs' interaction with patients still remain underexplored.\nHere, we propose PSYCHE, a novel framework designed to enable the 1) clinically\nrelevant, 2) ethically safe, 3) cost-efficient, and 4) quantitative evaluation\nof PACAs. This is achieved by simulating psychiatric patients based on a\nmulti-faceted psychiatric construct that defines the simulated patients'\nprofiles, histories, and behaviors, which PACAs are expected to assess. We\nvalidate the effectiveness of PSYCHE through a study with 10 board-certified\npsychiatrists, supported by an in-depth analysis of the simulated patient\nutterances."
                },
                "authors": [
                    {
                        "name": "Jingoo Lee"
                    },
                    {
                        "name": "Kyungho Lim"
                    },
                    {
                        "name": "Young-Chul Jung"
                    },
                    {
                        "name": "Byung-Hoon Kim"
                    }
                ],
                "author_detail": {
                    "name": "Byung-Hoon Kim"
                },
                "author": "Byung-Hoon Kim",
                "arxiv_comment": "The first two authors contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01588v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01588v1",
                "updated": "2025-01-03T00:56:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    0,
                    56,
                    46,
                    4,
                    3,
                    0
                ],
                "published": "2025-01-03T00:56:46Z",
                "published_parsed": [
                    2025,
                    1,
                    3,
                    0,
                    56,
                    46,
                    4,
                    3,
                    0
                ],
                "title": "(WhyPHI) Fine-Tuning PHI-3 for Multiple-Choice Question Answering:\n  Methodology, Results, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "(WhyPHI) Fine-Tuning PHI-3 for Multiple-Choice Question Answering:\n  Methodology, Results, and Challenges"
                },
                "summary": "Large Language Models (LLMs) have become essential tools across various\ndomains due to their impressive capabilities in understanding and generating\nhuman-like text. The ability to accurately answer multiple-choice questions\n(MCQs) holds significant value in education, particularly in automated tutoring\nsystems and assessment platforms. However, adapting LLMs to handle MCQ tasks\neffectively remains challenging due to the hallucinations and unclear prompts.\nThis work explores the potential of Microsoft's PHI-3\\cite{Abdin2024}, a\ncompact yet efficient LLM, for MCQ answering. Our contributions include\nfine-tuning the model on the TruthfulQA dataset, designing optimized prompts to\nenhance model performance, and evaluating using perplexity and traditional\nmetrics like accuracy and F1 score. Results show a remarkable improvement in\nPHI-3.5's MCQ handling post-fine-tuning, with perplexity decreasing from 4.68\nto 2.27, and accuracy rising from 62\\% to 90.8\\%. This research underlines the\nimportance of efficient models in adaptive learning systems and educational\nassessments, paving the way for broader integration into the classroom,\nparticularly in fields like test preparation, student feedback, and\npersonalized learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become essential tools across various\ndomains due to their impressive capabilities in understanding and generating\nhuman-like text. The ability to accurately answer multiple-choice questions\n(MCQs) holds significant value in education, particularly in automated tutoring\nsystems and assessment platforms. However, adapting LLMs to handle MCQ tasks\neffectively remains challenging due to the hallucinations and unclear prompts.\nThis work explores the potential of Microsoft's PHI-3\\cite{Abdin2024}, a\ncompact yet efficient LLM, for MCQ answering. Our contributions include\nfine-tuning the model on the TruthfulQA dataset, designing optimized prompts to\nenhance model performance, and evaluating using perplexity and traditional\nmetrics like accuracy and F1 score. Results show a remarkable improvement in\nPHI-3.5's MCQ handling post-fine-tuning, with perplexity decreasing from 4.68\nto 2.27, and accuracy rising from 62\\% to 90.8\\%. This research underlines the\nimportance of efficient models in adaptive learning systems and educational\nassessments, paving the way for broader integration into the classroom,\nparticularly in fields like test preparation, student feedback, and\npersonalized learning."
                },
                "authors": [
                    {
                        "name": "Mohamed Hisham Abdellatif"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Hisham Abdellatif"
                },
                "author": "Mohamed Hisham Abdellatif",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01588v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01588v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.18947v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.18947v2",
                "updated": "2025-01-03T00:16:52Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    0,
                    16,
                    52,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-25T16:51:29Z",
                "published_parsed": [
                    2024,
                    12,
                    25,
                    16,
                    51,
                    29,
                    2,
                    360,
                    0
                ],
                "title": "MedHallBench: A New Benchmark for Assessing Hallucination in Medical\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedHallBench: A New Benchmark for Assessing Hallucination in Medical\n  Large Language Models"
                },
                "summary": "Medical Large Language Models (MLLMs) have demonstrated potential in\nhealthcare applications, yet their propensity for hallucinations -- generating\nmedically implausible or inaccurate information -- presents substantial risks\nto patient care. This paper introduces MedHallBench, a comprehensive benchmark\nframework for evaluating and mitigating hallucinations in MLLMs. Our\nmethodology integrates expert-validated medical case scenarios with established\nmedical databases to create a robust evaluation dataset. The framework employs\na sophisticated measurement system that combines automated ACHMI (Automatic\nCaption Hallucination Measurement in Medical Imaging) scoring with rigorous\nclinical expert evaluations and utilizes reinforcement learning methods to\nachieve automatic annotation. Through an optimized reinforcement learning from\nhuman feedback (RLHF) training pipeline specifically designed for medical\napplications, MedHallBench enables thorough evaluation of MLLMs across diverse\nclinical contexts while maintaining stringent accuracy standards. We conducted\ncomparative experiments involving various models, utilizing the benchmark to\nestablish a baseline for widely adopted large language models (LLMs). Our\nfindings indicate that ACHMI provides a more nuanced understanding of the\neffects of hallucinations compared to traditional metrics, thereby highlighting\nits advantages in hallucination assessment. This research establishes a\nfoundational framework for enhancing MLLMs' reliability in healthcare settings\nand presents actionable strategies for addressing the critical challenge of AI\nhallucinations in medical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medical Large Language Models (MLLMs) have demonstrated potential in\nhealthcare applications, yet their propensity for hallucinations -- generating\nmedically implausible or inaccurate information -- presents substantial risks\nto patient care. This paper introduces MedHallBench, a comprehensive benchmark\nframework for evaluating and mitigating hallucinations in MLLMs. Our\nmethodology integrates expert-validated medical case scenarios with established\nmedical databases to create a robust evaluation dataset. The framework employs\na sophisticated measurement system that combines automated ACHMI (Automatic\nCaption Hallucination Measurement in Medical Imaging) scoring with rigorous\nclinical expert evaluations and utilizes reinforcement learning methods to\nachieve automatic annotation. Through an optimized reinforcement learning from\nhuman feedback (RLHF) training pipeline specifically designed for medical\napplications, MedHallBench enables thorough evaluation of MLLMs across diverse\nclinical contexts while maintaining stringent accuracy standards. We conducted\ncomparative experiments involving various models, utilizing the benchmark to\nestablish a baseline for widely adopted large language models (LLMs). Our\nfindings indicate that ACHMI provides a more nuanced understanding of the\neffects of hallucinations compared to traditional metrics, thereby highlighting\nits advantages in hallucination assessment. This research establishes a\nfoundational framework for enhancing MLLMs' reliability in healthcare settings\nand presents actionable strategies for addressing the critical challenge of AI\nhallucinations in medical applications."
                },
                "authors": [
                    {
                        "name": "Kaiwen Zuo"
                    },
                    {
                        "name": "Yirui Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Yirui Jiang"
                },
                "author": "Yirui Jiang",
                "arxiv_comment": "Published to AAAI-25 Bridge Program",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.18947v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.18947v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16833v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16833v2",
                "updated": "2025-01-03T00:07:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    3,
                    0,
                    7,
                    9,
                    4,
                    3,
                    0
                ],
                "published": "2024-12-22T02:40:59Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    2,
                    40,
                    59,
                    6,
                    357,
                    0
                ],
                "title": "KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge\n  Graph Enhancement for Medical Diagnosis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KG4Diagnosis: A Hierarchical Multi-Agent LLM Framework with Knowledge\n  Graph Enhancement for Medical Diagnosis"
                },
                "summary": "Integrating Large Language Models (LLMs) in healthcare diagnosis demands\nsystematic frameworks that can handle complex medical scenarios while\nmaintaining specialized expertise. We present KG4Diagnosis, a novel\nhierarchical multi-agent framework that combines LLMs with automated knowledge\ngraph construction, encompassing 362 common diseases across medical\nspecialties. Our framework mirrors real-world medical systems through a\ntwo-tier architecture: a general practitioner (GP) agent for initial assessment\nand triage, coordinating with specialized agents for in-depth diagnosis in\nspecific domains. The core innovation lies in our end-to-end knowledge graph\ngeneration methodology, incorporating: (1) semantic-driven entity and relation\nextraction optimized for medical terminology, (2) multi-dimensional decision\nrelationship reconstruction from unstructured medical texts, and (3)\nhuman-guided reasoning for knowledge expansion. KG4Diagnosis serves as an\nextensible foundation for specialized medical diagnosis systems, with\ncapabilities to incorporate new diseases and medical knowledge. The framework's\nmodular design enables seamless integration of domain-specific enhancements,\nmaking it valuable for developing targeted medical diagnosis systems. We\nprovide architectural guidelines and protocols to facilitate adoption across\nmedical contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Large Language Models (LLMs) in healthcare diagnosis demands\nsystematic frameworks that can handle complex medical scenarios while\nmaintaining specialized expertise. We present KG4Diagnosis, a novel\nhierarchical multi-agent framework that combines LLMs with automated knowledge\ngraph construction, encompassing 362 common diseases across medical\nspecialties. Our framework mirrors real-world medical systems through a\ntwo-tier architecture: a general practitioner (GP) agent for initial assessment\nand triage, coordinating with specialized agents for in-depth diagnosis in\nspecific domains. The core innovation lies in our end-to-end knowledge graph\ngeneration methodology, incorporating: (1) semantic-driven entity and relation\nextraction optimized for medical terminology, (2) multi-dimensional decision\nrelationship reconstruction from unstructured medical texts, and (3)\nhuman-guided reasoning for knowledge expansion. KG4Diagnosis serves as an\nextensible foundation for specialized medical diagnosis systems, with\ncapabilities to incorporate new diseases and medical knowledge. The framework's\nmodular design enables seamless integration of domain-specific enhancements,\nmaking it valuable for developing targeted medical diagnosis systems. We\nprovide architectural guidelines and protocols to facilitate adoption across\nmedical contexts."
                },
                "authors": [
                    {
                        "name": "Kaiwen Zuo"
                    },
                    {
                        "name": "Yirui Jiang"
                    },
                    {
                        "name": "Fan Mo"
                    },
                    {
                        "name": "Pietro Lio"
                    }
                ],
                "author_detail": {
                    "name": "Pietro Lio"
                },
                "author": "Pietro Lio",
                "arxiv_comment": "10 pages,5 figures,published to AAAI-25 Bridge Program",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16833v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16833v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.20302v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.20302v3",
                "updated": "2025-01-02T23:08:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    23,
                    8,
                    47,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-27T00:50:30Z",
                "published_parsed": [
                    2024,
                    10,
                    27,
                    0,
                    50,
                    30,
                    6,
                    301,
                    0
                ],
                "title": "Sequential Large Language Model-Based Hyper-parameter Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Large Language Model-Based Hyper-parameter Optimization"
                },
                "summary": "This study introduces SLLMBO, an innovative framework leveraging large\nlanguage models (LLMs) for hyperparameter optimization (HPO), incorporating\ndynamic search space adaptability, enhanced parameter space exploitation, and a\nnovel LLM-tree-structured parzen estimator (LLM-TPE) sampler. By addressing\nlimitations in recent fully LLM-based methods and traditional bayesian\noptimization (BO), SLLMBO achieves more robust optimization. This comprehensive\nbenchmarking evaluates multiple LLMs, including GPT-3.5-Turbo, GPT-4o,\nClaude-Sonnet-3.5, and Gemini-1.5-Flash, extending prior work and establishing\nSLLMBO as the first framework to benchmark a diverse set of LLMs for HPO. By\nintegrating LLMs' established strengths in parameter initialization with the\nexploitation abilities demonstrated in this study, alongside TPE's exploration\ncapabilities, the LLM-TPE sampler achieves a balanced exploration-exploitation\ntrade-off, reduces API costs, and mitigates premature early stoppings for more\neffective parameter searches. Across 14 tabular tasks in classification and\nregression, the LLM-TPE sampler outperformed fully LLM-based methods and\nachieved superior results over BO methods in 9 tasks. Testing early stopping in\nbudget-constrained scenarios demonstrated competitive performance, indicating\nthat LLM-based methods generally benefit from extended iterations for optimal\nresults. This work lays the foundation for future research exploring\nopen-source LLMs, reproducibility of LLM results in HPO, and benchmarking\nSLLMBO on complex datasets, such as image classification, segmentation, and\nmachine translation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces SLLMBO, an innovative framework leveraging large\nlanguage models (LLMs) for hyperparameter optimization (HPO), incorporating\ndynamic search space adaptability, enhanced parameter space exploitation, and a\nnovel LLM-tree-structured parzen estimator (LLM-TPE) sampler. By addressing\nlimitations in recent fully LLM-based methods and traditional bayesian\noptimization (BO), SLLMBO achieves more robust optimization. This comprehensive\nbenchmarking evaluates multiple LLMs, including GPT-3.5-Turbo, GPT-4o,\nClaude-Sonnet-3.5, and Gemini-1.5-Flash, extending prior work and establishing\nSLLMBO as the first framework to benchmark a diverse set of LLMs for HPO. By\nintegrating LLMs' established strengths in parameter initialization with the\nexploitation abilities demonstrated in this study, alongside TPE's exploration\ncapabilities, the LLM-TPE sampler achieves a balanced exploration-exploitation\ntrade-off, reduces API costs, and mitigates premature early stoppings for more\neffective parameter searches. Across 14 tabular tasks in classification and\nregression, the LLM-TPE sampler outperformed fully LLM-based methods and\nachieved superior results over BO methods in 9 tasks. Testing early stopping in\nbudget-constrained scenarios demonstrated competitive performance, indicating\nthat LLM-based methods generally benefit from extended iterations for optimal\nresults. This work lays the foundation for future research exploring\nopen-source LLMs, reproducibility of LLM results in HPO, and benchmarking\nSLLMBO on complex datasets, such as image classification, segmentation, and\nmachine translation."
                },
                "authors": [
                    {
                        "name": "Kanan Mahammadli"
                    },
                    {
                        "name": "Seyda Ertekin"
                    }
                ],
                "author_detail": {
                    "name": "Seyda Ertekin"
                },
                "author": "Seyda Ertekin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.20302v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.20302v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01568v1",
                "updated": "2025-01-02T23:03:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    23,
                    3,
                    3,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T23:03:03Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    23,
                    3,
                    3,
                    3,
                    2,
                    0
                ],
                "title": "Interruption Handling for Conversational Robots",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interruption Handling for Conversational Robots"
                },
                "summary": "Interruptions, a fundamental component of human communication, can enhance\nthe dynamism and effectiveness of conversations, but only when effectively\nmanaged by all parties involved. Despite advancements in robotic systems,\nstate-of-the-art systems still have limited capabilities in handling\nuser-initiated interruptions in real-time. Prior research has primarily focused\non post hoc analysis of interruptions. To address this gap, we present a system\nthat detects user-initiated interruptions and manages them in real-time based\non the interrupter's intent (i.e., cooperative agreement, cooperative\nassistance, cooperative clarification, or disruptive interruption). The system\nwas designed based on interaction patterns identified from human-human\ninteraction data. We integrated our system into an LLM-powered social robot and\nvalidated its effectiveness through a timed decision-making task and a\ncontentious discussion task with 21 participants. Our system successfully\nhandled 93.69% (n=104/111) of user-initiated interruptions. We discuss our\nlearnings and their implications for designing interruption-handling behaviors\nin conversational robots.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interruptions, a fundamental component of human communication, can enhance\nthe dynamism and effectiveness of conversations, but only when effectively\nmanaged by all parties involved. Despite advancements in robotic systems,\nstate-of-the-art systems still have limited capabilities in handling\nuser-initiated interruptions in real-time. Prior research has primarily focused\non post hoc analysis of interruptions. To address this gap, we present a system\nthat detects user-initiated interruptions and manages them in real-time based\non the interrupter's intent (i.e., cooperative agreement, cooperative\nassistance, cooperative clarification, or disruptive interruption). The system\nwas designed based on interaction patterns identified from human-human\ninteraction data. We integrated our system into an LLM-powered social robot and\nvalidated its effectiveness through a timed decision-making task and a\ncontentious discussion task with 21 participants. Our system successfully\nhandled 93.69% (n=104/111) of user-initiated interruptions. We discuss our\nlearnings and their implications for designing interruption-handling behaviors\nin conversational robots."
                },
                "authors": [
                    {
                        "name": "Shiye Cao"
                    },
                    {
                        "name": "Jiwon Moon"
                    },
                    {
                        "name": "Amama Mahmood"
                    },
                    {
                        "name": "Victor Nikhil Antony"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Anqi Liu"
                    },
                    {
                        "name": "Chien-Ming Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chien-Ming Huang"
                },
                "author": "Chien-Ming Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01558v1",
                "updated": "2025-01-02T22:26:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    22,
                    26,
                    54,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T22:26:54Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    22,
                    26,
                    54,
                    3,
                    2,
                    0
                ],
                "title": "Predicting the Performance of Black-box LLMs through Self-Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predicting the Performance of Black-box LLMs through Self-Queries"
                },
                "summary": "As large language models (LLMs) are increasingly relied on in AI systems,\npredicting when they make mistakes is crucial. While a great deal of work in\nthe field uses internal representations to interpret model behavior, these\nrepresentations are inaccessible when given solely black-box access through an\nAPI. In this paper, we extract features of LLMs in a black-box manner by using\nfollow-up prompts and taking the probabilities of different responses as\nrepresentations to train reliable predictors of model behavior. We demonstrate\nthat training a linear model on these low-dimensional representations produces\nreliable and generalizable predictors of model performance at the instance\nlevel (e.g., if a particular generation correctly answers a question).\nRemarkably, these can often outperform white-box linear predictors that operate\nover a model's hidden state or the full distribution over its vocabulary. In\naddition, we demonstrate that these extracted features can be used to evaluate\nmore nuanced aspects of a language model's state. For instance, they can be\nused to distinguish between a clean version of GPT-4o-mini and a version that\nhas been influenced via an adversarial system prompt that answers\nquestion-answering tasks incorrectly or introduces bugs into generated code.\nFurthermore, they can reliably distinguish between different model\narchitectures and sizes, enabling the detection of misrepresented models\nprovided through an API (e.g., identifying if GPT-3.5 is supplied instead of\nGPT-4o-mini).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) are increasingly relied on in AI systems,\npredicting when they make mistakes is crucial. While a great deal of work in\nthe field uses internal representations to interpret model behavior, these\nrepresentations are inaccessible when given solely black-box access through an\nAPI. In this paper, we extract features of LLMs in a black-box manner by using\nfollow-up prompts and taking the probabilities of different responses as\nrepresentations to train reliable predictors of model behavior. We demonstrate\nthat training a linear model on these low-dimensional representations produces\nreliable and generalizable predictors of model performance at the instance\nlevel (e.g., if a particular generation correctly answers a question).\nRemarkably, these can often outperform white-box linear predictors that operate\nover a model's hidden state or the full distribution over its vocabulary. In\naddition, we demonstrate that these extracted features can be used to evaluate\nmore nuanced aspects of a language model's state. For instance, they can be\nused to distinguish between a clean version of GPT-4o-mini and a version that\nhas been influenced via an adversarial system prompt that answers\nquestion-answering tasks incorrectly or introduces bugs into generated code.\nFurthermore, they can reliably distinguish between different model\narchitectures and sizes, enabling the detection of misrepresented models\nprovided through an API (e.g., identifying if GPT-3.5 is supplied instead of\nGPT-4o-mini)."
                },
                "authors": [
                    {
                        "name": "Dylan Sam"
                    },
                    {
                        "name": "Marc Finzi"
                    },
                    {
                        "name": "J. Zico Kolter"
                    }
                ],
                "author_detail": {
                    "name": "J. Zico Kolter"
                },
                "author": "J. Zico Kolter",
                "arxiv_comment": "28 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01545v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01545v1",
                "updated": "2025-01-02T21:31:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    21,
                    31,
                    56,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T21:31:56Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    21,
                    31,
                    56,
                    3,
                    2,
                    0
                ],
                "title": "Enhancing User Engagement in Large-Scale Social Annotation Platforms:\n  Community-Based Design Interventions and Implications for Large Language\n  Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing User Engagement in Large-Scale Social Annotation Platforms:\n  Community-Based Design Interventions and Implications for Large Language\n  Models (LLMs)"
                },
                "summary": "Social annotation platforms enable student engagement by integrating\ndiscussions directly into course materials. However, in large online courses,\nthe sheer volume of comments can overwhelm students and impede learning. This\npaper investigates community-based design interventions on a social annotation\nplatform (NB) to address this challenge and foster more meaningful online\neducational discussions. By examining student preferences and reactions to\ndifferent curation strategies, this research aims to optimize the utility of\nsocial annotations in educational contexts. A key emphasis is placed on how the\nvisibility of comments shapes group interactions, guides conversational flows,\nand enriches learning experiences.\n  The study combined iterative design and development with two large-scale\nexperiments to create and refine comment curation strategies, involving\nthousands of students. The study introduced specific features of the platform,\nsuch as targeted comment visibility controls, which demonstrably improved peer\ninteractions and reduced discussion overload. These findings inform the design\nof next-generation social annotation systems and highlight opportunities to\nintegrate Large Language Models (LLMs) for key activities like summarizing\nannotations, improving clarity in student writing, and assisting instructors\nwith efficient comment curation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Social annotation platforms enable student engagement by integrating\ndiscussions directly into course materials. However, in large online courses,\nthe sheer volume of comments can overwhelm students and impede learning. This\npaper investigates community-based design interventions on a social annotation\nplatform (NB) to address this challenge and foster more meaningful online\neducational discussions. By examining student preferences and reactions to\ndifferent curation strategies, this research aims to optimize the utility of\nsocial annotations in educational contexts. A key emphasis is placed on how the\nvisibility of comments shapes group interactions, guides conversational flows,\nand enriches learning experiences.\n  The study combined iterative design and development with two large-scale\nexperiments to create and refine comment curation strategies, involving\nthousands of students. The study introduced specific features of the platform,\nsuch as targeted comment visibility controls, which demonstrably improved peer\ninteractions and reduced discussion overload. These findings inform the design\nof next-generation social annotation systems and highlight opportunities to\nintegrate Large Language Models (LLMs) for key activities like summarizing\nannotations, improving clarity in student writing, and assisting instructors\nwith efficient comment curation."
                },
                "authors": [
                    {
                        "name": "Jumana Almahmoud"
                    },
                    {
                        "name": "Marc Facciotti"
                    },
                    {
                        "name": "Michele Igo"
                    },
                    {
                        "name": "Kamali Sripathi"
                    },
                    {
                        "name": "David Karger"
                    }
                ],
                "author_detail": {
                    "name": "David Karger"
                },
                "author": "David Karger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01545v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01545v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01544v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01544v1",
                "updated": "2025-01-02T21:31:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    21,
                    31,
                    38,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T21:31:38Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    21,
                    31,
                    38,
                    3,
                    2,
                    0
                ],
                "title": "Many of Your DPOs are Secretly One: Attempting Unification Through\n  Mutual Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many of Your DPOs are Secretly One: Attempting Unification Through\n  Mutual Information"
                },
                "summary": "Post-alignment of large language models (LLMs) is critical in improving their\nutility, safety, and alignment with human intentions. Direct preference\noptimisation (DPO) has become one of the most widely used algorithms for\nachieving this alignment, given its ability to optimise models based on human\nfeedback directly. However, the vast number of DPO variants in the literature\nhas made it increasingly difficult for researchers to navigate and fully grasp\nthe connections between these approaches. This paper introduces a unifying\nframework inspired by mutual information, which proposes a new loss function\nwith flexible priors. By carefully specifying these priors, we demonstrate that\nmany existing algorithms, such as SimPO, TDPO, SparsePO, and others, can be\nderived from our framework. This unification offers a clearer and more\nstructured approach, allowing researchers to understand the relationships\nbetween different DPO variants better. We aim to simplify the landscape of DPO\nalgorithms, making it easier for the research community to gain insights and\nfoster further advancements in LLM alignment. Ultimately, we hope our framework\ncan be a foundation for developing more robust and interpretable alignment\ntechniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-alignment of large language models (LLMs) is critical in improving their\nutility, safety, and alignment with human intentions. Direct preference\noptimisation (DPO) has become one of the most widely used algorithms for\nachieving this alignment, given its ability to optimise models based on human\nfeedback directly. However, the vast number of DPO variants in the literature\nhas made it increasingly difficult for researchers to navigate and fully grasp\nthe connections between these approaches. This paper introduces a unifying\nframework inspired by mutual information, which proposes a new loss function\nwith flexible priors. By carefully specifying these priors, we demonstrate that\nmany existing algorithms, such as SimPO, TDPO, SparsePO, and others, can be\nderived from our framework. This unification offers a clearer and more\nstructured approach, allowing researchers to understand the relationships\nbetween different DPO variants better. We aim to simplify the landscape of DPO\nalgorithms, making it easier for the research community to gain insights and\nfoster further advancements in LLM alignment. Ultimately, we hope our framework\ncan be a foundation for developing more robust and interpretable alignment\ntechniques."
                },
                "authors": [
                    {
                        "name": "Rasul Tutnov"
                    },
                    {
                        "name": "Antoine Grosnit"
                    },
                    {
                        "name": "Haitham Bou-Ammar"
                    }
                ],
                "author_detail": {
                    "name": "Haitham Bou-Ammar"
                },
                "author": "Haitham Bou-Ammar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01544v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01544v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01540v1",
                "updated": "2025-01-02T21:15:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    21,
                    15,
                    57,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T21:15:57Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    21,
                    15,
                    57,
                    3,
                    2,
                    0
                ],
                "title": "BoxingGym: Benchmarking Progress in Automated Experimental Design and\n  Model Discovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BoxingGym: Benchmarking Progress in Automated Experimental Design and\n  Model Discovery"
                },
                "summary": "Understanding the world and explaining it with scientific theories is a\ncentral aspiration of artificial intelligence research. Proposing theories,\ndesigning experiments to test them, and then revising them based on data are\nfundamental to scientific discovery. Despite the significant promise of\nLLM-based scientific agents, no benchmarks systematically test LLM's ability to\npropose scientific models, collect experimental data, and revise them in light\nof new data. We introduce BoxingGym, a benchmark with 10 environments for\nsystematically evaluating both experimental design (e.g. collecting data to\ntest a scientific theory) and model discovery (e.g. proposing and revising\nscientific theories). To enable tractable and quantitative evaluation, we\nimplement each environment as a generative probabilistic model with which a\nscientific agent can run interactive experiments. These probabilistic models\nare drawn from various real-world scientific domains ranging from psychology to\necology. To quantitatively evaluate a scientific agent's ability to collect\ninformative experimental data, we compute the expected information gain (EIG),\nan information-theoretic quantity which measures how much an experiment reduces\nuncertainty about the parameters of a generative model. A good scientific\ntheory is a concise and predictive explanation. Therefore, to quantitatively\nevaluate model discovery, we ask a scientific agent to explain their model and\nthen assess whether this explanation enables another scientific agent to make\nreliable predictions about this environment. In addition to this\nexplanation-based evaluation, we compute standard model evaluation metrics such\nas prediction errors. We find that current LLMs, such as GPT-4o, struggle with\nboth experimental design and model discovery. We find that augmenting the\nLLM-based agent with an explicit statistical model does not reliably improve\nthese results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the world and explaining it with scientific theories is a\ncentral aspiration of artificial intelligence research. Proposing theories,\ndesigning experiments to test them, and then revising them based on data are\nfundamental to scientific discovery. Despite the significant promise of\nLLM-based scientific agents, no benchmarks systematically test LLM's ability to\npropose scientific models, collect experimental data, and revise them in light\nof new data. We introduce BoxingGym, a benchmark with 10 environments for\nsystematically evaluating both experimental design (e.g. collecting data to\ntest a scientific theory) and model discovery (e.g. proposing and revising\nscientific theories). To enable tractable and quantitative evaluation, we\nimplement each environment as a generative probabilistic model with which a\nscientific agent can run interactive experiments. These probabilistic models\nare drawn from various real-world scientific domains ranging from psychology to\necology. To quantitatively evaluate a scientific agent's ability to collect\ninformative experimental data, we compute the expected information gain (EIG),\nan information-theoretic quantity which measures how much an experiment reduces\nuncertainty about the parameters of a generative model. A good scientific\ntheory is a concise and predictive explanation. Therefore, to quantitatively\nevaluate model discovery, we ask a scientific agent to explain their model and\nthen assess whether this explanation enables another scientific agent to make\nreliable predictions about this environment. In addition to this\nexplanation-based evaluation, we compute standard model evaluation metrics such\nas prediction errors. We find that current LLMs, such as GPT-4o, struggle with\nboth experimental design and model discovery. We find that augmenting the\nLLM-based agent with an explicit statistical model does not reliably improve\nthese results."
                },
                "authors": [
                    {
                        "name": "Kanishk Gandhi"
                    },
                    {
                        "name": "Michael Y. Li"
                    },
                    {
                        "name": "Lyle Goodyear"
                    },
                    {
                        "name": "Louise Li"
                    },
                    {
                        "name": "Aditi Bhaskar"
                    },
                    {
                        "name": "Mohammed Zaman"
                    },
                    {
                        "name": "Noah D. Goodman"
                    }
                ],
                "author_detail": {
                    "name": "Noah D. Goodman"
                },
                "author": "Noah D. Goodman",
                "arxiv_comment": "KG and MYL contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.04587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.04587v2",
                "updated": "2025-01-02T19:30:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    19,
                    30,
                    9,
                    3,
                    2,
                    0
                ],
                "published": "2024-08-08T16:56:07Z",
                "published_parsed": [
                    2024,
                    8,
                    8,
                    16,
                    56,
                    7,
                    3,
                    221,
                    0
                ],
                "title": "FORGE: Force-Guided Exploration for Robust Contact-Rich Manipulation\n  under Uncertainty",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FORGE: Force-Guided Exploration for Robust Contact-Rich Manipulation\n  under Uncertainty"
                },
                "summary": "We present FORGE, a method for sim-to-real transfer of force-aware\nmanipulation policies in the presence of significant pose uncertainty. During\nsimulation-based policy learning, FORGE combines a force threshold mechanism\nwith a dynamics randomization scheme to enable robust transfer of the learned\npolicies to the real robot. At deployment, FORGE policies, conditioned on a\nmaximum allowable force, adaptively perform contact-rich tasks while avoiding\naggressive and unsafe behaviour, regardless of the controller gains.\nAdditionally, FORGE policies predict task success, enabling efficient\ntermination and autonomous tuning of the force threshold. We show that FORGE\ncan be used to learn a variety of robust contact-rich policies, including the\nforceful insertion of snap-fit connectors. We further demonstrate the\nmultistage assembly of a planetary gear system, which requires success across\nthree assembly tasks: nut threading, insertion, and gear meshing. Project\nwebsite can be accessed at https://noseworm.github.io/forge/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present FORGE, a method for sim-to-real transfer of force-aware\nmanipulation policies in the presence of significant pose uncertainty. During\nsimulation-based policy learning, FORGE combines a force threshold mechanism\nwith a dynamics randomization scheme to enable robust transfer of the learned\npolicies to the real robot. At deployment, FORGE policies, conditioned on a\nmaximum allowable force, adaptively perform contact-rich tasks while avoiding\naggressive and unsafe behaviour, regardless of the controller gains.\nAdditionally, FORGE policies predict task success, enabling efficient\ntermination and autonomous tuning of the force threshold. We show that FORGE\ncan be used to learn a variety of robust contact-rich policies, including the\nforceful insertion of snap-fit connectors. We further demonstrate the\nmultistage assembly of a planetary gear system, which requires success across\nthree assembly tasks: nut threading, insertion, and gear meshing. Project\nwebsite can be accessed at https://noseworm.github.io/forge/."
                },
                "authors": [
                    {
                        "name": "Michael Noseworthy"
                    },
                    {
                        "name": "Bingjie Tang"
                    },
                    {
                        "name": "Bowen Wen"
                    },
                    {
                        "name": "Ankur Handa"
                    },
                    {
                        "name": "Chad Kessens"
                    },
                    {
                        "name": "Nicholas Roy"
                    },
                    {
                        "name": "Dieter Fox"
                    },
                    {
                        "name": "Fabio Ramos"
                    },
                    {
                        "name": "Yashraj Narang"
                    },
                    {
                        "name": "Iretiayo Akinola"
                    }
                ],
                "author_detail": {
                    "name": "Iretiayo Akinola"
                },
                "author": "Iretiayo Akinola",
                "arxiv_comment": "IndustReal comparisons and snap-fit task added (v2)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.04587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.04587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06708v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06708v2",
                "updated": "2025-01-02T19:27:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    19,
                    27,
                    1,
                    3,
                    2,
                    0
                ],
                "published": "2024-10-09T09:27:07Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    9,
                    27,
                    7,
                    2,
                    283,
                    0
                ],
                "title": "Do Developers Adopt Green Architectural Tactics for ML-Enabled Systems?\n  A Mining Software Repository Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do Developers Adopt Green Architectural Tactics for ML-Enabled Systems?\n  A Mining Software Repository Study"
                },
                "summary": "As machine learning (ML) and artificial intelligence (AI) technologies become\nmore widespread, concerns about their environmental impact are increasing due\nto the resource-intensive nature of training and inference processes. Green AI\nadvocates for reducing computational demands while still maintaining accuracy.\nAlthough various strategies for creating sustainable ML systems have been\nidentified, their real-world implementation is still underexplored. This paper\naddresses this gap by studying 168 open-source ML projects on GitHub. It\nemploys a novel large language model (LLM)-based mining mechanism to identify\nand analyze green strategies. The findings reveal the adoption of established\ntactics that offer significant environmental benefits. This provides practical\ninsights for developers and paves the way for future automation of sustainable\npractices in ML systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As machine learning (ML) and artificial intelligence (AI) technologies become\nmore widespread, concerns about their environmental impact are increasing due\nto the resource-intensive nature of training and inference processes. Green AI\nadvocates for reducing computational demands while still maintaining accuracy.\nAlthough various strategies for creating sustainable ML systems have been\nidentified, their real-world implementation is still underexplored. This paper\naddresses this gap by studying 168 open-source ML projects on GitHub. It\nemploys a novel large language model (LLM)-based mining mechanism to identify\nand analyze green strategies. The findings reveal the adoption of established\ntactics that offer significant environmental benefits. This provides practical\ninsights for developers and paves the way for future automation of sustainable\npractices in ML systems."
                },
                "authors": [
                    {
                        "name": "Vincenzo De Martino"
                    },
                    {
                        "name": "Silverio Martínez-Fernández"
                    },
                    {
                        "name": "Fabio Palomba"
                    }
                ],
                "author_detail": {
                    "name": "Fabio Palomba"
                },
                "author": "Fabio Palomba",
                "arxiv_comment": "Accepted at the 2025 IEEE/ACM 47th International Conference on\n  Software Engineering: Software Engineering in Society (ICSE-SEIS)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06708v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06708v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01426v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01426v1",
                "updated": "2025-01-02T18:59:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    45,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:59:45Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    59,
                    45,
                    3,
                    2,
                    0
                ],
                "title": "Unifying Specialized Visual Encoders for Video Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Specialized Visual Encoders for Video Language Models"
                },
                "summary": "The recent advent of Large Language Models (LLMs) has ushered sophisticated\nreasoning capabilities into the realm of video through Video Large Language\nModels (VideoLLMs). However, VideoLLMs currently rely on a single vision\nencoder for all of their visual processing, which limits the amount and type of\nvisual information that can be conveyed to the LLM. Our method, MERV,\nMulti-Encoder Representation of Videos, instead leverages multiple frozen\nvisual encoders to create a unified representation of a video, providing the\nVideoLLM with a comprehensive set of specialized visual knowledge.\nSpatio-temporally aligning the features from each encoder allows us to tackle a\nwider range of open-ended and multiple-choice video understanding questions and\noutperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy\nthan Video-LLaVA across the standard suite video understanding benchmarks,\nwhile also having a better Video-ChatGPT score. We also improve upon SeViLA,\nthe previous best on zero-shot Perception Test accuracy, by 2.2%. MERV\nintroduces minimal extra parameters and trains faster than equivalent\nsingle-encoder methods while parallelizing the visual processing. Finally, we\nprovide qualitative evidence that MERV successfully captures domain knowledge\nfrom each of its encoders. Our results offer promising directions in utilizing\nmultiple vision encoders for comprehensive video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent advent of Large Language Models (LLMs) has ushered sophisticated\nreasoning capabilities into the realm of video through Video Large Language\nModels (VideoLLMs). However, VideoLLMs currently rely on a single vision\nencoder for all of their visual processing, which limits the amount and type of\nvisual information that can be conveyed to the LLM. Our method, MERV,\nMulti-Encoder Representation of Videos, instead leverages multiple frozen\nvisual encoders to create a unified representation of a video, providing the\nVideoLLM with a comprehensive set of specialized visual knowledge.\nSpatio-temporally aligning the features from each encoder allows us to tackle a\nwider range of open-ended and multiple-choice video understanding questions and\noutperform prior state-of-the-art works. MERV is up to 3.7% better in accuracy\nthan Video-LLaVA across the standard suite video understanding benchmarks,\nwhile also having a better Video-ChatGPT score. We also improve upon SeViLA,\nthe previous best on zero-shot Perception Test accuracy, by 2.2%. MERV\nintroduces minimal extra parameters and trains faster than equivalent\nsingle-encoder methods while parallelizing the visual processing. Finally, we\nprovide qualitative evidence that MERV successfully captures domain knowledge\nfrom each of its encoders. Our results offer promising directions in utilizing\nmultiple vision encoders for comprehensive video understanding."
                },
                "authors": [
                    {
                        "name": "Jihoon Chung"
                    },
                    {
                        "name": "Tyler Zhu"
                    },
                    {
                        "name": "Max Gonzalez Saez-Diez"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "Honglu Zhou"
                    },
                    {
                        "name": "Olga Russakovsky"
                    }
                ],
                "author_detail": {
                    "name": "Olga Russakovsky"
                },
                "author": "Olga Russakovsky",
                "arxiv_comment": "Project page: https://tylerzhu.com/merv/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01426v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01426v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01417v1",
                "updated": "2025-01-02T18:58:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    58,
                    3,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T18:58:03Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    58,
                    3,
                    3,
                    2,
                    0
                ],
                "title": "The Bayesian Global Sky Model (B-GSM): Validation of a Data Driven\n  Bayesian Simultaneous Component Separation and Calibration Algorithm for EoR\n  Foreground Modelling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bayesian Global Sky Model (B-GSM): Validation of a Data Driven\n  Bayesian Simultaneous Component Separation and Calibration Algorithm for EoR\n  Foreground Modelling"
                },
                "summary": "We introduce the Bayesian Global Sky Model (B-GSM), a novel data-driven\nBayesian approach to modelling radio foregrounds at frequencies <400~MHz. B-GSM\naims to address the limitations of previous models by incorporating robust\nerror quantification and calibration. Using nested sampling, we compute\nBayesian evidence and posterior distributions for the spectral behaviour and\nspatial amplitudes of diffuse emission components. Bayesian model comparison is\nused to determine the optimal number of emission components and their spectral\nparametrisation. Posterior sky predictions are conditioned on both diffuse\nemission and absolute temperature datasets, enabling simultaneous component\nseparation and calibration. B-GSM is validated against a synthetic dataset\ndesigned to mimic the partial sky coverage, thermal noise, and calibration\nuncertainties present in real observations of the diffuse sky at low\nfrequencies. B-GSM correctly identifies a model parametrisation with two\nemission components featuring curved power-law spectra. The posterior sky\npredictions agree with the true synthetic sky within statistical uncertainty.\nWe find that the root-mean-square (RMS) residuals between the true and\nposterior predictions for the sky temperature as a function of LST are\nsignificantly reduced, when compared to the uncalibrated dataset. This\nindicates that B-GSM is able to correctly calibrate its posterior sky\nprediction to the independent absolute temperature dataset. We find that while\nthe spectral parameters and component amplitudes exhibit some sensitivity to\nprior assumptions, the posterior sky predictions remain robust across a\nselection of different priors. This is the first of two papers, and is focused\non validation of B-GSMs Bayesian framework, the second paper will present\nresults of deployment on real data and introduce the low-frequency sky model\nwhich will be available for public download.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Bayesian Global Sky Model (B-GSM), a novel data-driven\nBayesian approach to modelling radio foregrounds at frequencies <400~MHz. B-GSM\naims to address the limitations of previous models by incorporating robust\nerror quantification and calibration. Using nested sampling, we compute\nBayesian evidence and posterior distributions for the spectral behaviour and\nspatial amplitudes of diffuse emission components. Bayesian model comparison is\nused to determine the optimal number of emission components and their spectral\nparametrisation. Posterior sky predictions are conditioned on both diffuse\nemission and absolute temperature datasets, enabling simultaneous component\nseparation and calibration. B-GSM is validated against a synthetic dataset\ndesigned to mimic the partial sky coverage, thermal noise, and calibration\nuncertainties present in real observations of the diffuse sky at low\nfrequencies. B-GSM correctly identifies a model parametrisation with two\nemission components featuring curved power-law spectra. The posterior sky\npredictions agree with the true synthetic sky within statistical uncertainty.\nWe find that the root-mean-square (RMS) residuals between the true and\nposterior predictions for the sky temperature as a function of LST are\nsignificantly reduced, when compared to the uncalibrated dataset. This\nindicates that B-GSM is able to correctly calibrate its posterior sky\nprediction to the independent absolute temperature dataset. We find that while\nthe spectral parameters and component amplitudes exhibit some sensitivity to\nprior assumptions, the posterior sky predictions remain robust across a\nselection of different priors. This is the first of two papers, and is focused\non validation of B-GSMs Bayesian framework, the second paper will present\nresults of deployment on real data and introduce the low-frequency sky model\nwhich will be available for public download."
                },
                "authors": [
                    {
                        "name": "George Carter"
                    },
                    {
                        "name": "Will Handley"
                    },
                    {
                        "name": "Mark Ashdown"
                    },
                    {
                        "name": "Nima Razavi-Ghods"
                    }
                ],
                "author_detail": {
                    "name": "Nima Razavi-Ghods"
                },
                "author": "Nima Razavi-Ghods",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19260v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19260v2",
                "updated": "2025-01-02T18:46:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    18,
                    46,
                    5,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-26T15:54:10Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    54,
                    10,
                    3,
                    361,
                    0
                ],
                "title": "MEDEC: A Benchmark for Medical Error Detection and Correction in\n  Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MEDEC: A Benchmark for Medical Error Detection and Correction in\n  Clinical Notes"
                },
                "summary": "Several studies showed that Large Language Models (LLMs) can answer medical\nquestions correctly, even outperforming the average human score in some medical\nexams. However, to our knowledge, no study has been conducted to assess the\nability of language models to validate existing or generated medical text for\ncorrectness and consistency. In this paper, we introduce MEDEC\n(https://github.com/abachaa/MEDEC), the first publicly available benchmark for\nmedical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal\nOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes\nfrom three US hospital systems that were not previously seen by any LLM. The\ndataset has been used for the MEDIQA-CORR shared task to evaluate seventeen\nparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe the\ndata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,\nClaude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and\ncorrecting medical errors requiring both medical knowledge and reasoning\ncapabilities. We also conducted a comparative study where two medical doctors\nperformed the same task on the MEDEC test set. The results showed that MEDEC is\na sufficiently challenging benchmark to assess the ability of models to\nvalidate existing or generated notes and to correct medical errors. We also\nfound that although recent LLMs have a good performance in error detection and\ncorrection, they are still outperformed by medical doctors in these tasks. We\ndiscuss the potential factors behind this gap, the insights from our\nexperiments, the limitations of current evaluation metrics, and share potential\npointers for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several studies showed that Large Language Models (LLMs) can answer medical\nquestions correctly, even outperforming the average human score in some medical\nexams. However, to our knowledge, no study has been conducted to assess the\nability of language models to validate existing or generated medical text for\ncorrectness and consistency. In this paper, we introduce MEDEC\n(https://github.com/abachaa/MEDEC), the first publicly available benchmark for\nmedical error detection and correction in clinical notes, covering five types\nof errors (Diagnosis, Management, Treatment, Pharmacotherapy, and Causal\nOrganism). MEDEC consists of 3,848 clinical texts, including 488 clinical notes\nfrom three US hospital systems that were not previously seen by any LLM. The\ndataset has been used for the MEDIQA-CORR shared task to evaluate seventeen\nparticipating systems [Ben Abacha et al., 2024]. In this paper, we describe the\ndata creation methods and we evaluate recent LLMs (e.g., o1-preview, GPT-4,\nClaude 3.5 Sonnet, and Gemini 2.0 Flash) for the tasks of detecting and\ncorrecting medical errors requiring both medical knowledge and reasoning\ncapabilities. We also conducted a comparative study where two medical doctors\nperformed the same task on the MEDEC test set. The results showed that MEDEC is\na sufficiently challenging benchmark to assess the ability of models to\nvalidate existing or generated notes and to correct medical errors. We also\nfound that although recent LLMs have a good performance in error detection and\ncorrection, they are still outperformed by medical doctors in these tasks. We\ndiscuss the potential factors behind this gap, the insights from our\nexperiments, the limitations of current evaluation metrics, and share potential\npointers for future research."
                },
                "authors": [
                    {
                        "name": "Asma Ben Abacha"
                    },
                    {
                        "name": "Wen-wai Yim"
                    },
                    {
                        "name": "Yujuan Fu"
                    },
                    {
                        "name": "Zhaoyi Sun"
                    },
                    {
                        "name": "Meliha Yetisgen"
                    },
                    {
                        "name": "Fei Xia"
                    },
                    {
                        "name": "Thomas Lin"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Lin"
                },
                "author": "Thomas Lin",
                "arxiv_comment": "This version has been updated with further clarification regarding\n  the model size estimates that were mined from public articles only and\n  provided to aid in contextualizing model performance. The authors cannot\n  vouch for the accuracy of those estimates",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19260v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19260v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04655v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04655v2",
                "updated": "2025-01-02T17:21:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    17,
                    21,
                    22,
                    3,
                    2,
                    0
                ],
                "published": "2024-12-05T22:59:26Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    59,
                    26,
                    3,
                    340,
                    0
                ],
                "title": "From Models to Systems: A Comprehensive Fairness Framework for\n  Compositional Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Models to Systems: A Comprehensive Fairness Framework for\n  Compositional Recommender Systems"
                },
                "summary": "Fairness research in machine learning often centers on ensuring equitable\nperformance of individual models. However, real-world recommendation systems\nare built on multiple models and even multiple stages, from candidate retrieval\nto scoring and serving, which raises challenges for responsible development and\ndeployment. This system-level view, as highlighted by regulations like the EU\nAI Act, necessitates moving beyond auditing individual models as independent\nentities. We propose a holistic framework for modeling system-level fairness,\nfocusing on the end-utility delivered to diverse user groups, and consider\ninteractions between components such as retrieval and scoring models. We\nprovide formal insights on the limitations of focusing solely on model-level\nfairness and highlight the need for alternative tools that account for\nheterogeneity in user preferences. To mitigate system-level disparities, we\nadapt closed-box optimization tools (e.g., BayesOpt) to jointly optimize\nutility and equity. We empirically demonstrate the effectiveness of our\nproposed framework on synthetic and real datasets, underscoring the need for a\nsystem-level framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fairness research in machine learning often centers on ensuring equitable\nperformance of individual models. However, real-world recommendation systems\nare built on multiple models and even multiple stages, from candidate retrieval\nto scoring and serving, which raises challenges for responsible development and\ndeployment. This system-level view, as highlighted by regulations like the EU\nAI Act, necessitates moving beyond auditing individual models as independent\nentities. We propose a holistic framework for modeling system-level fairness,\nfocusing on the end-utility delivered to diverse user groups, and consider\ninteractions between components such as retrieval and scoring models. We\nprovide formal insights on the limitations of focusing solely on model-level\nfairness and highlight the need for alternative tools that account for\nheterogeneity in user preferences. To mitigate system-level disparities, we\nadapt closed-box optimization tools (e.g., BayesOpt) to jointly optimize\nutility and equity. We empirically demonstrate the effectiveness of our\nproposed framework on synthetic and real datasets, underscoring the need for a\nsystem-level framework."
                },
                "authors": [
                    {
                        "name": "Brian Hsu"
                    },
                    {
                        "name": "Cyrus DiCiccio"
                    },
                    {
                        "name": "Natesh Sivasubramoniapillai"
                    },
                    {
                        "name": "Hongseok Namkoong"
                    }
                ],
                "author_detail": {
                    "name": "Hongseok Namkoong"
                },
                "author": "Hongseok Namkoong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04655v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04655v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01366v1",
                "updated": "2025-01-02T17:20:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    17,
                    20,
                    41,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T17:20:41Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    17,
                    20,
                    41,
                    3,
                    2,
                    0
                ],
                "title": "ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ViGiL3D: A Linguistically Diverse Dataset for 3D Visual Grounding"
                },
                "summary": "3D visual grounding (3DVG) involves localizing entities in a 3D scene\nreferred to by natural language text. Such models are useful for embodied AI\nand scene retrieval applications, which involve searching for objects or\npatterns using natural language descriptions. While recent works have focused\non LLM-based scaling of 3DVG datasets, these datasets do not capture the full\nrange of potential prompts which could be specified in the English language. To\nensure that we are scaling up and testing against a useful and representative\nset of prompts, we propose a framework for linguistically analyzing 3DVG\nprompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a\ndiagnostic dataset for evaluating visual grounding methods against a diverse\nset of language patterns. We evaluate existing open-vocabulary 3DVG methods to\ndemonstrate that these methods are not yet proficient in understanding and\nidentifying the targets of more challenging, out-of-distribution prompts,\ntoward real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D visual grounding (3DVG) involves localizing entities in a 3D scene\nreferred to by natural language text. Such models are useful for embodied AI\nand scene retrieval applications, which involve searching for objects or\npatterns using natural language descriptions. While recent works have focused\non LLM-based scaling of 3DVG datasets, these datasets do not capture the full\nrange of potential prompts which could be specified in the English language. To\nensure that we are scaling up and testing against a useful and representative\nset of prompts, we propose a framework for linguistically analyzing 3DVG\nprompts and introduce Visual Grounding with Diverse Language in 3D (ViGiL3D), a\ndiagnostic dataset for evaluating visual grounding methods against a diverse\nset of language patterns. We evaluate existing open-vocabulary 3DVG methods to\ndemonstrate that these methods are not yet proficient in understanding and\nidentifying the targets of more challenging, out-of-distribution prompts,\ntoward real-world applications."
                },
                "authors": [
                    {
                        "name": "Austin T. Wang"
                    },
                    {
                        "name": "ZeMing Gong"
                    },
                    {
                        "name": "Angel X. Chang"
                    }
                ],
                "author_detail": {
                    "name": "Angel X. Chang"
                },
                "author": "Angel X. Chang",
                "arxiv_comment": "20 pages with 5 figures and 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10848v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10848v3",
                "updated": "2025-01-02T17:17:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    17,
                    17,
                    28,
                    3,
                    2,
                    0
                ],
                "published": "2024-08-20T13:40:25Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    13,
                    40,
                    25,
                    1,
                    233,
                    0
                ],
                "title": "Perception-guided Jailbreak against Text-to-Image Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception-guided Jailbreak against Text-to-Image Models"
                },
                "summary": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Text-to-Image (T2I) models have garnered significant\nattention due to their remarkable advancements. However, security concerns have\nemerged due to their potential to generate inappropriate or Not-Safe-For-Work\n(NSFW) images. In this paper, inspired by the observation that texts with\ndifferent semantics can lead to similar human perceptions, we propose an\nLLM-driven perception-guided jailbreak method, termed PGJ. It is a black-box\njailbreak method that requires no specific T2I model (model-free) and generates\nhighly natural attack prompts. Specifically, we propose identifying a safe\nphrase that is similar in human perception yet inconsistent in text semantics\nwith the target unsafe word and using it as a substitution. The experiments\nconducted on six open-source models and commercial online services with\nthousands of prompts have verified the effectiveness of PGJ."
                },
                "authors": [
                    {
                        "name": "Yihao Huang"
                    },
                    {
                        "name": "Le Liang"
                    },
                    {
                        "name": "Tianlin Li"
                    },
                    {
                        "name": "Xiaojun Jia"
                    },
                    {
                        "name": "Run Wang"
                    },
                    {
                        "name": "Weikai Miao"
                    },
                    {
                        "name": "Geguang Pu"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "arxiv_comment": "9 pages, accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10848v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10848v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01336v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01336v1",
                "updated": "2025-01-02T16:38:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    38,
                    21,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T16:38:21Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    38,
                    21,
                    3,
                    2,
                    0
                ],
                "title": "Aligning Large Language Models for Faithful Integrity Against Opposing\n  Argument",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models for Faithful Integrity Against Opposing\n  Argument"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncomplex reasoning tasks. However, they can be easily misled by unfaithful\narguments during conversations, even when their original statements are\ncorrect. To this end, we investigate the problem of maintaining faithful\nintegrity in LLMs. This involves ensuring that LLMs adhere to their faithful\nstatements in the face of opposing arguments and are able to correct their\nincorrect statements when presented with faithful arguments. In this work, we\npropose a novel framework, named Alignment for Faithful Integrity with\nConfidence Estimation (AFICE), which aims to align the LLM responses with\nfaithful integrity. Specifically, AFICE first designs a Bilateral Confidence\nEstimation (BCE) approach for estimating the uncertainty of each response\ngenerated by the LLM given a specific context, which simultaneously estimate\nthe model's confidence to the question based on the internal states during\ndecoding as well as to the answer based on cumulative probability ratios. With\nthe BCE, we construct a conversational preference dataset composed of context,\noriginal statement, and argument, which is adopted for aligning the LLM for\nfaithful integrity using Direct Preference Optimization (DPO). Extensive\nexperimental results on a wide range of benchmarks demonstrate significant\nimprovements in the LLM's ability to maintain faithful responses when\nencountering opposing arguments, ensuring both the practical utility and\ntrustworthiness of LLMs in complex interactive settings. Code and data will be\nreleased via https://github.com/zhaoy777/AFICE.git",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncomplex reasoning tasks. However, they can be easily misled by unfaithful\narguments during conversations, even when their original statements are\ncorrect. To this end, we investigate the problem of maintaining faithful\nintegrity in LLMs. This involves ensuring that LLMs adhere to their faithful\nstatements in the face of opposing arguments and are able to correct their\nincorrect statements when presented with faithful arguments. In this work, we\npropose a novel framework, named Alignment for Faithful Integrity with\nConfidence Estimation (AFICE), which aims to align the LLM responses with\nfaithful integrity. Specifically, AFICE first designs a Bilateral Confidence\nEstimation (BCE) approach for estimating the uncertainty of each response\ngenerated by the LLM given a specific context, which simultaneously estimate\nthe model's confidence to the question based on the internal states during\ndecoding as well as to the answer based on cumulative probability ratios. With\nthe BCE, we construct a conversational preference dataset composed of context,\noriginal statement, and argument, which is adopted for aligning the LLM for\nfaithful integrity using Direct Preference Optimization (DPO). Extensive\nexperimental results on a wide range of benchmarks demonstrate significant\nimprovements in the LLM's ability to maintain faithful responses when\nencountering opposing arguments, ensuring both the practical utility and\ntrustworthiness of LLMs in complex interactive settings. Code and data will be\nreleased via https://github.com/zhaoy777/AFICE.git"
                },
                "authors": [
                    {
                        "name": "Yong Zhao"
                    },
                    {
                        "name": "Yang Deng"
                    },
                    {
                        "name": "See-Kiong Ng"
                    },
                    {
                        "name": "Tat-Seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-Seng Chua"
                },
                "author": "Tat-Seng Chua",
                "arxiv_comment": "17 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01336v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01336v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01335v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01335v1",
                "updated": "2025-01-02T16:37:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    37,
                    4,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T16:37:04Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    37,
                    4,
                    3,
                    2,
                    0
                ],
                "title": "CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for\n  Benchmarking Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CySecBench: Generative AI-based CyberSecurity-focused Prompt Dataset for\n  Benchmarking Large Language Models"
                },
                "summary": "Numerous studies have investigated methods for jailbreaking Large Language\nModels (LLMs) to generate harmful content. Typically, these methods are\nevaluated using datasets of malicious prompts designed to bypass security\npolicies established by LLM providers. However, the generally broad scope and\nopen-ended nature of existing datasets can complicate the assessment of\njailbreaking effectiveness, particularly in specific domains, notably\ncybersecurity. To address this issue, we present and publicly release\nCySecBench, a comprehensive dataset containing 12662 prompts specifically\ndesigned to evaluate jailbreaking techniques in the cybersecurity domain. The\ndataset is organized into 10 distinct attack-type categories, featuring\nclose-ended prompts to enable a more consistent and accurate assessment of\njailbreaking attempts. Furthermore, we detail our methodology for dataset\ngeneration and filtration, which can be adapted to create similar datasets in\nother domains. To demonstrate the utility of CySecBench, we propose and\nevaluate a jailbreaking approach based on prompt obfuscation. Our experimental\nresults show that this method successfully elicits harmful content from\ncommercial black-box LLMs, achieving Success Rates (SRs) of 65% with ChatGPT\nand 88% with Gemini; in contrast, Claude demonstrated greater resilience with a\njailbreaking SR of 17%. Compared to existing benchmark approaches, our method\nshows superior performance, highlighting the value of domain-specific\nevaluation datasets for assessing LLM security measures. Moreover, when\nevaluated using prompts from a widely used dataset (i.e., AdvBench), it\nachieved an SR of 78.5%, higher than the state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Numerous studies have investigated methods for jailbreaking Large Language\nModels (LLMs) to generate harmful content. Typically, these methods are\nevaluated using datasets of malicious prompts designed to bypass security\npolicies established by LLM providers. However, the generally broad scope and\nopen-ended nature of existing datasets can complicate the assessment of\njailbreaking effectiveness, particularly in specific domains, notably\ncybersecurity. To address this issue, we present and publicly release\nCySecBench, a comprehensive dataset containing 12662 prompts specifically\ndesigned to evaluate jailbreaking techniques in the cybersecurity domain. The\ndataset is organized into 10 distinct attack-type categories, featuring\nclose-ended prompts to enable a more consistent and accurate assessment of\njailbreaking attempts. Furthermore, we detail our methodology for dataset\ngeneration and filtration, which can be adapted to create similar datasets in\nother domains. To demonstrate the utility of CySecBench, we propose and\nevaluate a jailbreaking approach based on prompt obfuscation. Our experimental\nresults show that this method successfully elicits harmful content from\ncommercial black-box LLMs, achieving Success Rates (SRs) of 65% with ChatGPT\nand 88% with Gemini; in contrast, Claude demonstrated greater resilience with a\njailbreaking SR of 17%. Compared to existing benchmark approaches, our method\nshows superior performance, highlighting the value of domain-specific\nevaluation datasets for assessing LLM security measures. Moreover, when\nevaluated using prompts from a widely used dataset (i.e., AdvBench), it\nachieved an SR of 78.5%, higher than the state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Johan Wahréus"
                    },
                    {
                        "name": "Ahmed Mohamed Hussain"
                    },
                    {
                        "name": "Panos Papadimitratos"
                    }
                ],
                "author_detail": {
                    "name": "Panos Papadimitratos"
                },
                "author": "Panos Papadimitratos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01335v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01335v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01332v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01332v1",
                "updated": "2025-01-02T16:34:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    34,
                    10,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T16:34:10Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    34,
                    10,
                    3,
                    2,
                    0
                ],
                "title": "Decoding Knowledge in Large Language Models: A Framework for\n  Categorization and Comprehension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoding Knowledge in Large Language Models: A Framework for\n  Categorization and Comprehension"
                },
                "summary": "Understanding how large language models (LLMs) acquire, retain, and apply\nknowledge remains an open challenge. This paper introduces a novel framework,\nK-(CSA)^2, which categorizes LLM knowledge along two dimensions: correctness\nand confidence. The framework defines six categories of knowledge, ranging from\nhighly confident correctness to confidently held misconceptions, enabling a\nnuanced evaluation of model comprehension beyond binary accuracy. Using this\nframework, we demonstrate how techniques like chain-of-thought prompting and\nreinforcement learning with human feedback fundamentally alter the knowledge\nstructures of internal (pre-trained) and external (context-dependent) knowledge\nin LLMs. CoT particularly enhances base model performance and shows synergistic\nbenefits when applied to aligned LLMs. Moreover, our layer-wise analysis\nreveals that higher layers in LLMs encode more high-confidence knowledge, while\nlow-confidence knowledge tends to emerge in middle-to-lower layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding how large language models (LLMs) acquire, retain, and apply\nknowledge remains an open challenge. This paper introduces a novel framework,\nK-(CSA)^2, which categorizes LLM knowledge along two dimensions: correctness\nand confidence. The framework defines six categories of knowledge, ranging from\nhighly confident correctness to confidently held misconceptions, enabling a\nnuanced evaluation of model comprehension beyond binary accuracy. Using this\nframework, we demonstrate how techniques like chain-of-thought prompting and\nreinforcement learning with human feedback fundamentally alter the knowledge\nstructures of internal (pre-trained) and external (context-dependent) knowledge\nin LLMs. CoT particularly enhances base model performance and shows synergistic\nbenefits when applied to aligned LLMs. Moreover, our layer-wise analysis\nreveals that higher layers in LLMs encode more high-confidence knowledge, while\nlow-confidence knowledge tends to emerge in middle-to-lower layers."
                },
                "authors": [
                    {
                        "name": "Yanbo Fang"
                    },
                    {
                        "name": "Ruixiang Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruixiang Tang"
                },
                "author": "Ruixiang Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01332v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01332v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01329v1",
                "updated": "2025-01-02T16:30:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    30,
                    5,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T16:30:05Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    30,
                    5,
                    3,
                    2,
                    0
                ],
                "title": "The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for\n  Test Case Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Prompt Alchemist: Automated LLM-Tailored Prompt Optimization for\n  Test Case Generation"
                },
                "summary": "Test cases are essential for validating the reliability and quality of\nsoftware applications. Recent studies have demonstrated the capability of Large\nLanguage Models (LLMs) to generate useful test cases for given source code.\nHowever, the existing work primarily relies on human-written plain prompts,\nwhich often leads to suboptimal results since the performance of LLMs can be\nhighly influenced by the prompts. Moreover, these approaches use the same\nprompt for all LLMs, overlooking the fact that different LLMs might be best\nsuited to different prompts. Given the wide variety of possible prompt\nformulations, automatically discovering the optimal prompt for each LLM\npresents a significant challenge. Although there are methods on automated\nprompt optimization in the natural language processing field, they are hard to\nproduce effective prompts for the test case generation task. First, the methods\niteratively optimize prompts by simply combining and mutating existing ones\nwithout proper guidance, resulting in prompts that lack diversity and tend to\nrepeat the same errors in the generated test cases. Second, the prompts are\ngenerally lack of domain contextual knowledge, limiting LLMs' performance in\nthe task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test cases are essential for validating the reliability and quality of\nsoftware applications. Recent studies have demonstrated the capability of Large\nLanguage Models (LLMs) to generate useful test cases for given source code.\nHowever, the existing work primarily relies on human-written plain prompts,\nwhich often leads to suboptimal results since the performance of LLMs can be\nhighly influenced by the prompts. Moreover, these approaches use the same\nprompt for all LLMs, overlooking the fact that different LLMs might be best\nsuited to different prompts. Given the wide variety of possible prompt\nformulations, automatically discovering the optimal prompt for each LLM\npresents a significant challenge. Although there are methods on automated\nprompt optimization in the natural language processing field, they are hard to\nproduce effective prompts for the test case generation task. First, the methods\niteratively optimize prompts by simply combining and mutating existing ones\nwithout proper guidance, resulting in prompts that lack diversity and tend to\nrepeat the same errors in the generated test cases. Second, the prompts are\ngenerally lack of domain contextual knowledge, limiting LLMs' performance in\nthe task."
                },
                "authors": [
                    {
                        "name": "Shuzheng Gao"
                    },
                    {
                        "name": "Chaozheng Wang"
                    },
                    {
                        "name": "Cuiyun Gao"
                    },
                    {
                        "name": "Xiaoqian Jiao"
                    },
                    {
                        "name": "Chun Yong Chong"
                    },
                    {
                        "name": "Shan Gao"
                    },
                    {
                        "name": "Michael Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Michael Lyu"
                },
                "author": "Michael Lyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.06083v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.06083v2",
                "updated": "2025-01-02T16:14:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    16,
                    14,
                    16,
                    3,
                    2,
                    0
                ],
                "published": "2024-07-04T09:50:50Z",
                "published_parsed": [
                    2024,
                    7,
                    4,
                    9,
                    50,
                    50,
                    3,
                    186,
                    0
                ],
                "title": "A Survey of Controllable Learning: Methods and Applications in\n  Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Controllable Learning: Methods and Applications in\n  Information Retrieval"
                },
                "summary": "Controllability has become a crucial aspect of trustworthy machine learning,\nenabling learners to meet predefined targets and adapt dynamically at test time\nwithout requiring retraining as the targets shift. We provide a formal\ndefinition of controllable learning (CL), and discuss its applications in\ninformation retrieval (IR) where information needs are often complex and\ndynamic. The survey categorizes CL according to what is controllable (e.g.,\nmultiple objectives, user portrait, scenario adaptation), who controls (users\nor platforms), how control is implemented (e.g., rule-based method, Pareto\noptimization, hypernetwork and others), and where to implement control (e.g.,\npre-processing, in-processing, post-processing methods). Then, we identify\nchallenges faced by CL across training, evaluation, task setting, and\ndeployment in online environments. Additionally, we outline promising\ndirections for CL in theoretical analysis, efficient computation, empowering\nlarge language models, application scenarios and evaluation frameworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Controllability has become a crucial aspect of trustworthy machine learning,\nenabling learners to meet predefined targets and adapt dynamically at test time\nwithout requiring retraining as the targets shift. We provide a formal\ndefinition of controllable learning (CL), and discuss its applications in\ninformation retrieval (IR) where information needs are often complex and\ndynamic. The survey categorizes CL according to what is controllable (e.g.,\nmultiple objectives, user portrait, scenario adaptation), who controls (users\nor platforms), how control is implemented (e.g., rule-based method, Pareto\noptimization, hypernetwork and others), and where to implement control (e.g.,\npre-processing, in-processing, post-processing methods). Then, we identify\nchallenges faced by CL across training, evaluation, task setting, and\ndeployment in online environments. Additionally, we outline promising\ndirections for CL in theoretical analysis, efficient computation, empowering\nlarge language models, application scenarios and evaluation frameworks."
                },
                "authors": [
                    {
                        "name": "Chenglei Shen"
                    },
                    {
                        "name": "Xiao Zhang"
                    },
                    {
                        "name": "Teng Shi"
                    },
                    {
                        "name": "Changshuo Zhang"
                    },
                    {
                        "name": "Guofu Xie"
                    },
                    {
                        "name": "Jun Xu"
                    }
                ],
                "author_detail": {
                    "name": "Jun Xu"
                },
                "author": "Jun Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.06083v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.06083v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01312v1",
                "updated": "2025-01-02T15:53:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    53,
                    25,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T15:53:25Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    53,
                    25,
                    3,
                    2,
                    0
                ],
                "title": "Learning Spectral Methods by Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Spectral Methods by Transformers"
                },
                "summary": "Transformers demonstrate significant advantages as the building block of\nmodern LLMs. In this work, we study the capacities of Transformers in\nperforming unsupervised learning. We show that multi-layered Transformers,\ngiven a sufficiently large set of pre-training instances, are able to learn the\nalgorithms themselves and perform statistical estimation tasks given new\ninstances. This learning paradigm is distinct from the in-context learning\nsetup and is similar to the learning procedure of human brains where skills are\nlearned through past experience. Theoretically, we prove that pre-trained\nTransformers can learn the spectral methods and use the classification of\nbi-class Gaussian mixture model as an example. Our proof is constructive using\nalgorithmic design techniques. Our results are built upon the similarities of\nmulti-layered Transformer architecture with the iterative recovery algorithms\nused in practice. Empirically, we verify the strong capacity of the\nmulti-layered (pre-trained) Transformer on unsupervised learning through the\nlens of both the PCA and the Clustering tasks performed on the synthetic and\nreal-world datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers demonstrate significant advantages as the building block of\nmodern LLMs. In this work, we study the capacities of Transformers in\nperforming unsupervised learning. We show that multi-layered Transformers,\ngiven a sufficiently large set of pre-training instances, are able to learn the\nalgorithms themselves and perform statistical estimation tasks given new\ninstances. This learning paradigm is distinct from the in-context learning\nsetup and is similar to the learning procedure of human brains where skills are\nlearned through past experience. Theoretically, we prove that pre-trained\nTransformers can learn the spectral methods and use the classification of\nbi-class Gaussian mixture model as an example. Our proof is constructive using\nalgorithmic design techniques. Our results are built upon the similarities of\nmulti-layered Transformer architecture with the iterative recovery algorithms\nused in practice. Empirically, we verify the strong capacity of the\nmulti-layered (pre-trained) Transformer on unsupervised learning through the\nlens of both the PCA and the Clustering tasks performed on the synthetic and\nreal-world datasets."
                },
                "authors": [
                    {
                        "name": "Yihan He"
                    },
                    {
                        "name": "Yuan Cao"
                    },
                    {
                        "name": "Hong-Yu Chen"
                    },
                    {
                        "name": "Dennis Wu"
                    },
                    {
                        "name": "Jianqing Fan"
                    },
                    {
                        "name": "Han Liu"
                    }
                ],
                "author_detail": {
                    "name": "Han Liu"
                },
                "author": "Han Liu",
                "arxiv_comment": "77 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01305v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01305v1",
                "updated": "2025-01-02T15:34:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    34,
                    2,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T15:34:02Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    34,
                    2,
                    3,
                    2,
                    0
                ],
                "title": "Large Language Models for Mental Health Diagnostic Assessments:\n  Exploring The Potential of Large Language Models for Assisting with Mental\n  Health Diagnostic Assessments -- The Depression and Anxiety Case",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Mental Health Diagnostic Assessments:\n  Exploring The Potential of Large Language Models for Assisting with Mental\n  Health Diagnostic Assessments -- The Depression and Anxiety Case"
                },
                "summary": "Large language models (LLMs) are increasingly attracting the attention of\nhealthcare professionals for their potential to assist in diagnostic\nassessments, which could alleviate the strain on the healthcare system caused\nby a high patient load and a shortage of providers. For LLMs to be effective in\nsupporting diagnostic assessments, it is essential that they closely replicate\nthe standard diagnostic procedures used by clinicians. In this paper, we\nspecifically examine the diagnostic assessment processes described in the\nPatient Health Questionnaire-9 (PHQ-9) for major depressive disorder (MDD) and\nthe Generalized Anxiety Disorder-7 (GAD-7) questionnaire for generalized\nanxiety disorder (GAD). We investigate various prompting and fine-tuning\ntechniques to guide both proprietary and open-source LLMs in adhering to these\nprocesses, and we evaluate the agreement between LLM-generated diagnostic\noutcomes and expert-validated ground truth. For fine-tuning, we utilize the\nMentalllama and Llama models, while for prompting, we experiment with\nproprietary models like GPT-3.5 and GPT-4o, as well as open-source models such\nas llama-3.1-8b and mixtral-8x7b.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly attracting the attention of\nhealthcare professionals for their potential to assist in diagnostic\nassessments, which could alleviate the strain on the healthcare system caused\nby a high patient load and a shortage of providers. For LLMs to be effective in\nsupporting diagnostic assessments, it is essential that they closely replicate\nthe standard diagnostic procedures used by clinicians. In this paper, we\nspecifically examine the diagnostic assessment processes described in the\nPatient Health Questionnaire-9 (PHQ-9) for major depressive disorder (MDD) and\nthe Generalized Anxiety Disorder-7 (GAD-7) questionnaire for generalized\nanxiety disorder (GAD). We investigate various prompting and fine-tuning\ntechniques to guide both proprietary and open-source LLMs in adhering to these\nprocesses, and we evaluate the agreement between LLM-generated diagnostic\noutcomes and expert-validated ground truth. For fine-tuning, we utilize the\nMentalllama and Llama models, while for prompting, we experiment with\nproprietary models like GPT-3.5 and GPT-4o, as well as open-source models such\nas llama-3.1-8b and mixtral-8x7b."
                },
                "authors": [
                    {
                        "name": "Kaushik Roy"
                    },
                    {
                        "name": "Harshul Surana"
                    },
                    {
                        "name": "Darssan Eswaramoorthi"
                    },
                    {
                        "name": "Yuxin Zi"
                    },
                    {
                        "name": "Vedant Palit"
                    },
                    {
                        "name": "Ritvik Garimella"
                    },
                    {
                        "name": "Amit Sheth"
                    }
                ],
                "author_detail": {
                    "name": "Amit Sheth"
                },
                "author": "Amit Sheth",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01305v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01305v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01303v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01303v1",
                "updated": "2025-01-02T15:32:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    32,
                    50,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T15:32:50Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    32,
                    50,
                    3,
                    2,
                    0
                ],
                "title": "Citations and Trust in LLM Generated Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Citations and Trust in LLM Generated Responses"
                },
                "summary": "Question answering systems are rapidly advancing, but their opaque nature may\nimpact user trust. We explored trust through an anti-monitoring framework,\nwhere trust is predicted to be correlated with presence of citations and\ninversely related to checking citations. We tested this hypothesis with a live\nquestion-answering experiment that presented text responses generated using a\ncommercial Chatbot along with varying citations (zero, one, or five), both\nrelevant and random, and recorded if participants checked the citations and\ntheir self-reported trust in the generated responses. We found a significant\nincrease in trust when citations were present, a result that held true even\nwhen the citations were random; we also found a significant decrease in trust\nwhen participants checked the citations. These results highlight the importance\nof citations in enhancing trust in AI-generated content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question answering systems are rapidly advancing, but their opaque nature may\nimpact user trust. We explored trust through an anti-monitoring framework,\nwhere trust is predicted to be correlated with presence of citations and\ninversely related to checking citations. We tested this hypothesis with a live\nquestion-answering experiment that presented text responses generated using a\ncommercial Chatbot along with varying citations (zero, one, or five), both\nrelevant and random, and recorded if participants checked the citations and\ntheir self-reported trust in the generated responses. We found a significant\nincrease in trust when citations were present, a result that held true even\nwhen the citations were random; we also found a significant decrease in trust\nwhen participants checked the citations. These results highlight the importance\nof citations in enhancing trust in AI-generated content."
                },
                "authors": [
                    {
                        "name": "Yifan Ding"
                    },
                    {
                        "name": "Matthew Facciani"
                    },
                    {
                        "name": "Amrit Poudel"
                    },
                    {
                        "name": "Ellen Joyce"
                    },
                    {
                        "name": "Salvador Aguinaga"
                    },
                    {
                        "name": "Balaji Veeramani"
                    },
                    {
                        "name": "Sanmitra Bhattacharya"
                    },
                    {
                        "name": "Tim Weninger"
                    }
                ],
                "author_detail": {
                    "name": "Tim Weninger"
                },
                "author": "Tim Weninger",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01303v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01303v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01293v1",
                "updated": "2025-01-02T15:19:16Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    19,
                    16,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T15:19:16Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    15,
                    19,
                    16,
                    3,
                    2,
                    0
                ],
                "title": "LEO-Split: A Semi-Supervised Split Learning Framework over LEO Satellite\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LEO-Split: A Semi-Supervised Split Learning Framework over LEO Satellite\n  Networks"
                },
                "summary": "Recently, the increasing deployment of LEO satellite systems has enabled\nvarious space analytics (e.g., crop and climate monitoring), which heavily\nrelies on the advancements in deep learning (DL). However, the intermittent\nconnectivity between LEO satellites and ground station (GS) significantly\nhinders the timely transmission of raw data to GS for centralized learning,\nwhile the scaled-up DL models hamper distributed learning on\nresource-constrained LEO satellites. Though split learning (SL) can be a\npotential solution to these problems by partitioning a model and offloading\nprimary training workload to GS, the labor-intensive labeling process remains\nan obstacle, with intermittent connectivity and data heterogeneity being other\nchallenges. In this paper, we propose LEO-Split, a semi-supervised (SS) SL\ndesign tailored for satellite networks to combat these challenges. Leveraging\nSS learning to handle (labeled) data scarcity, we construct an auxiliary model\nto tackle the training failure of the satellite-GS non-contact time. Moreover,\nwe propose a pseudo-labeling algorithm to rectify data imbalances across\nsatellites. Lastly, an adaptive activation interpolation scheme is devised to\nprevent the overfitting of server-side sub-model training at GS. Extensive\nexperiments with real-world LEO satellite traces (e.g., Starlink) demonstrate\nthat our LEO-Split framework achieves superior performance compared to\nstate-ofthe-art benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the increasing deployment of LEO satellite systems has enabled\nvarious space analytics (e.g., crop and climate monitoring), which heavily\nrelies on the advancements in deep learning (DL). However, the intermittent\nconnectivity between LEO satellites and ground station (GS) significantly\nhinders the timely transmission of raw data to GS for centralized learning,\nwhile the scaled-up DL models hamper distributed learning on\nresource-constrained LEO satellites. Though split learning (SL) can be a\npotential solution to these problems by partitioning a model and offloading\nprimary training workload to GS, the labor-intensive labeling process remains\nan obstacle, with intermittent connectivity and data heterogeneity being other\nchallenges. In this paper, we propose LEO-Split, a semi-supervised (SS) SL\ndesign tailored for satellite networks to combat these challenges. Leveraging\nSS learning to handle (labeled) data scarcity, we construct an auxiliary model\nto tackle the training failure of the satellite-GS non-contact time. Moreover,\nwe propose a pseudo-labeling algorithm to rectify data imbalances across\nsatellites. Lastly, an adaptive activation interpolation scheme is devised to\nprevent the overfitting of server-side sub-model training at GS. Extensive\nexperiments with real-world LEO satellite traces (e.g., Starlink) demonstrate\nthat our LEO-Split framework achieves superior performance compared to\nstate-ofthe-art benchmarks."
                },
                "authors": [
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Zihan Fang"
                    },
                    {
                        "name": "Cong Wu"
                    },
                    {
                        "name": "Xianhao Chen"
                    },
                    {
                        "name": "Yue Gao"
                    },
                    {
                        "name": "Jun Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jun Luo"
                },
                "author": "Jun Luo",
                "arxiv_comment": "13 pages, 15 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.07259v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.07259v2",
                "updated": "2025-01-02T14:57:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    14,
                    57,
                    3,
                    3,
                    2,
                    0
                ],
                "published": "2024-02-11T18:04:06Z",
                "published_parsed": [
                    2024,
                    2,
                    11,
                    18,
                    4,
                    6,
                    6,
                    42,
                    0
                ],
                "title": "RIS-Augmented Millimeter-Wave MIMO Systems for Passive Drone Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RIS-Augmented Millimeter-Wave MIMO Systems for Passive Drone Detection"
                },
                "summary": "In the past decade, the number of amateur drones is increasing, and this\ntrend is expected to continue in the future. The security issues brought by\nabuse and misconduct of drones become more and more severe and may incur a\nnegative impact to the society. In this paper, we leverage existing cellular\nmultiple-input multiple-output (MIMO) base station (BS) infrastructure,\noperating at millimeter wave (mmWave) frequency bands, for drone detection in a\ndevice-free manner with the aid of one reconfigurable intelligent surface\n(RIS), deployed in the proximity of the BS. We theoretically examine the\nfeasibility of drone detection with the aid of the generalized likelihood ratio\ntest (GLRT) and validate via simulations that, the optimized deployment of an\nRIS can bring added benefits compared to RIS-free systems. In addition, the\neffect of RIS training beams, training overhead, and radar cross section, is\ninvestigated in order to offer theoretical design guidance for the proposed\ncellular RIS-based passive drone detection system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the past decade, the number of amateur drones is increasing, and this\ntrend is expected to continue in the future. The security issues brought by\nabuse and misconduct of drones become more and more severe and may incur a\nnegative impact to the society. In this paper, we leverage existing cellular\nmultiple-input multiple-output (MIMO) base station (BS) infrastructure,\noperating at millimeter wave (mmWave) frequency bands, for drone detection in a\ndevice-free manner with the aid of one reconfigurable intelligent surface\n(RIS), deployed in the proximity of the BS. We theoretically examine the\nfeasibility of drone detection with the aid of the generalized likelihood ratio\ntest (GLRT) and validate via simulations that, the optimized deployment of an\nRIS can bring added benefits compared to RIS-free systems. In addition, the\neffect of RIS training beams, training overhead, and radar cross section, is\ninvestigated in order to offer theoretical design guidance for the proposed\ncellular RIS-based passive drone detection system."
                },
                "authors": [
                    {
                        "name": "Jiguang He"
                    },
                    {
                        "name": "Aymen Fakhreddine"
                    },
                    {
                        "name": "George C. Alexandropoulos"
                    }
                ],
                "author_detail": {
                    "name": "George C. Alexandropoulos"
                },
                "author": "George C. Alexandropoulos",
                "arxiv_comment": "6 pages, 6 figures, accepted by IEEE PIMRC 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.07259v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.07259v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01285v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01285v1",
                "updated": "2025-01-02T14:53:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    14,
                    53,
                    11,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T14:53:11Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    14,
                    53,
                    11,
                    3,
                    2,
                    0
                ],
                "title": "SARA: A Microservice-Based Architecture for Cross-Platform Collaborative\n  Augmented Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SARA: A Microservice-Based Architecture for Cross-Platform Collaborative\n  Augmented Reality"
                },
                "summary": "Augmented Reality (AR) functionalities may be effectively leveraged in\ncollaborative service scenarios (e.g., remote maintenance, on-site building,\nstreet gaming, etc.). Standard development cycles for collaborative AR require\nto code for each specific visualization platform and implement the necessary\ncontrol mechanisms over the shared assets. This paper describes SARA, an\narchitecture to support cross-platform collaborative Augmented Reality\napplications based on microservices. The architecture is designed to work over\nthe concept of collaboration models (turn, layer, ownership,hierarchy-based and\nunconstrained examples) which regulate the interaction and permissions of each\nuser over the AR assets. Thanks to the reusability of its components, during\nthe development of an application, SARA enables focusing on the application\nlogic while avoiding the implementation of the communication protocol, data\nmodel handling and orchestration between the different, possibly\nheterogeneous,devices involved in the collaboration (i.e., mobile or wearable\nAR devices using different operating systems). To describe how to build an\napplication based on SARA, a prototype for HoloLens and iOS devices has been\nimplemented. the prototype is a collaborative voxel-based game in which several\nplayers work real time together on a piece of land, adding or eliminating cubes\nin a collaborative manner to create buildings and landscapes. Turn-based and\nunconstrained collaboration models are applied to regulate the interaction, the\ndevelopment workflow for this case study shows how the architecture serves as a\nframework to support the deployment of collaborative AR services, enabling the\nreuse of collaboration model components, agnostically handling client\ntechnologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Augmented Reality (AR) functionalities may be effectively leveraged in\ncollaborative service scenarios (e.g., remote maintenance, on-site building,\nstreet gaming, etc.). Standard development cycles for collaborative AR require\nto code for each specific visualization platform and implement the necessary\ncontrol mechanisms over the shared assets. This paper describes SARA, an\narchitecture to support cross-platform collaborative Augmented Reality\napplications based on microservices. The architecture is designed to work over\nthe concept of collaboration models (turn, layer, ownership,hierarchy-based and\nunconstrained examples) which regulate the interaction and permissions of each\nuser over the AR assets. Thanks to the reusability of its components, during\nthe development of an application, SARA enables focusing on the application\nlogic while avoiding the implementation of the communication protocol, data\nmodel handling and orchestration between the different, possibly\nheterogeneous,devices involved in the collaboration (i.e., mobile or wearable\nAR devices using different operating systems). To describe how to build an\napplication based on SARA, a prototype for HoloLens and iOS devices has been\nimplemented. the prototype is a collaborative voxel-based game in which several\nplayers work real time together on a piece of land, adding or eliminating cubes\nin a collaborative manner to create buildings and landscapes. Turn-based and\nunconstrained collaboration models are applied to regulate the interaction, the\ndevelopment workflow for this case study shows how the architecture serves as a\nframework to support the deployment of collaborative AR services, enabling the\nreuse of collaboration model components, agnostically handling client\ntechnologies."
                },
                "authors": [
                    {
                        "name": "Diego Vaquero-Melchor"
                    },
                    {
                        "name": "Ana M. Bernardos"
                    },
                    {
                        "name": "Luca Bergesio"
                    }
                ],
                "author_detail": {
                    "name": "Luca Bergesio"
                },
                "author": "Luca Bergesio",
                "arxiv_doi": "10.3390/app10062074",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/app10062074",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.01285v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01285v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Applied Sciences 2020, 10(6), 2074",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01273v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01273v1",
                "updated": "2025-01-02T14:13:44Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    14,
                    13,
                    44,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T14:13:44Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    14,
                    13,
                    44,
                    3,
                    2,
                    0
                ],
                "title": "Does a Large Language Model Really Speak in Human-Like Language?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Does a Large Language Model Really Speak in Human-Like Language?"
                },
                "summary": "Large Language Models (LLMs) have recently emerged, attracting considerable\nattention due to their ability to generate highly natural, human-like text.\nThis study compares the latent community structures of LLM-generated text and\nhuman-written text within a hypothesis testing procedure. Specifically, we\nanalyze three text sets: original human-written texts ($\\mathcal{O}$), their\nLLM-paraphrased versions ($\\mathcal{G}$), and a twice-paraphrased set\n($\\mathcal{S}$) derived from $\\mathcal{G}$. Our analysis addresses two key\nquestions: (1) Is the difference in latent community structures between\n$\\mathcal{O}$ and $\\mathcal{G}$ the same as that between $\\mathcal{G}$ and\n$\\mathcal{S}$? (2) Does $\\mathcal{G}$ become more similar to $\\mathcal{O}$ as\nthe LLM parameter controlling text variability is adjusted? The first question\nis based on the assumption that if LLM-generated text truly resembles human\nlanguage, then the gap between the pair ($\\mathcal{O}$, $\\mathcal{G}$) should\nbe similar to that between the pair ($\\mathcal{G}$, $\\mathcal{S}$), as both\npairs consist of an original text and its paraphrase. The second question\nexamines whether the degree of similarity between LLM-generated and human text\nvaries with changes in the breadth of text generation. To address these\nquestions, we propose a statistical hypothesis testing framework that leverages\nthe fact that each text has corresponding parts across all datasets due to\ntheir paraphrasing relationship. This relationship enables the mapping of one\ndataset's relative position to another, allowing two datasets to be mapped to a\nthird dataset. As a result, both mapped datasets can be quantified with respect\nto the space characterized by the third dataset, facilitating a direct\ncomparison between them. Our results indicate that GPT-generated text remains\ndistinct from human-authored text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently emerged, attracting considerable\nattention due to their ability to generate highly natural, human-like text.\nThis study compares the latent community structures of LLM-generated text and\nhuman-written text within a hypothesis testing procedure. Specifically, we\nanalyze three text sets: original human-written texts ($\\mathcal{O}$), their\nLLM-paraphrased versions ($\\mathcal{G}$), and a twice-paraphrased set\n($\\mathcal{S}$) derived from $\\mathcal{G}$. Our analysis addresses two key\nquestions: (1) Is the difference in latent community structures between\n$\\mathcal{O}$ and $\\mathcal{G}$ the same as that between $\\mathcal{G}$ and\n$\\mathcal{S}$? (2) Does $\\mathcal{G}$ become more similar to $\\mathcal{O}$ as\nthe LLM parameter controlling text variability is adjusted? The first question\nis based on the assumption that if LLM-generated text truly resembles human\nlanguage, then the gap between the pair ($\\mathcal{O}$, $\\mathcal{G}$) should\nbe similar to that between the pair ($\\mathcal{G}$, $\\mathcal{S}$), as both\npairs consist of an original text and its paraphrase. The second question\nexamines whether the degree of similarity between LLM-generated and human text\nvaries with changes in the breadth of text generation. To address these\nquestions, we propose a statistical hypothesis testing framework that leverages\nthe fact that each text has corresponding parts across all datasets due to\ntheir paraphrasing relationship. This relationship enables the mapping of one\ndataset's relative position to another, allowing two datasets to be mapped to a\nthird dataset. As a result, both mapped datasets can be quantified with respect\nto the space characterized by the third dataset, facilitating a direct\ncomparison between them. Our results indicate that GPT-generated text remains\ndistinct from human-authored text."
                },
                "authors": [
                    {
                        "name": "Mose Park"
                    },
                    {
                        "name": "Yunjin Choi"
                    },
                    {
                        "name": "Jong-June Jeon"
                    }
                ],
                "author_detail": {
                    "name": "Jong-June Jeon"
                },
                "author": "Jong-June Jeon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01273v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01273v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.01264v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.01264v1",
                "updated": "2025-01-02T13:59:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    59,
                    20,
                    3,
                    2,
                    0
                ],
                "published": "2025-01-02T13:59:20Z",
                "published_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    59,
                    20,
                    3,
                    2,
                    0
                ],
                "title": "ProgCo: Program Helps Self-Correction of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProgCo: Program Helps Self-Correction of Large Language Models"
                },
                "summary": "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Correction aims to enable large language models (LLMs) to self-verify\nand self-refine their initial responses without external feedback. However,\nLLMs often fail to effectively self-verify and generate correct feedback,\nfurther misleading refinement and leading to the failure of self-correction,\nespecially in complex reasoning tasks. In this paper, we propose Program-driven\nSelf-Correction (ProgCo). First, program-driven verification (ProgVe) achieves\ncomplex verification logic and extensive validation through self-generated,\nself-executing verification pseudo-programs. Then, program-driven refinement\n(ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement\non both responses and verification programs to mitigate misleading of incorrect\nfeedback in complex reasoning tasks. Experiments on three instruction-following\nand mathematical benchmarks indicate that ProgCo achieves effective\nself-correction, and can be further enhance performance when combined with real\nprogram tools."
                },
                "authors": [
                    {
                        "name": "Xiaoshuai Song"
                    },
                    {
                        "name": "Yanan Wu"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Jiaheng Liu"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zheng"
                },
                "author": "Bo Zheng",
                "arxiv_comment": "Working in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.01264v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.01264v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.19530v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.19530v2",
                "updated": "2025-01-02T13:54:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    54,
                    17,
                    3,
                    2,
                    0
                ],
                "published": "2024-03-28T16:06:06Z",
                "published_parsed": [
                    2024,
                    3,
                    28,
                    16,
                    6,
                    6,
                    3,
                    88,
                    0
                ],
                "title": "Detecting Financial Bots on the Ethereum Blockchain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Financial Bots on the Ethereum Blockchain"
                },
                "summary": "The integration of bots in Distributed Ledger Technologies (DLTs) fosters\nefficiency and automation. However, their use is also associated with predatory\ntrading and market manipulation, and can pose threats to system integrity. It\nis therefore essential to understand the extent of bot deployment in DLTs;\ndespite this, current detection systems are predominantly rule-based and lack\nflexibility. In this study, we present a novel approach that utilizes machine\nlearning for the detection of financial bots on the Ethereum platform. First,\nwe systematize existing scientific literature and collect anecdotal evidence to\nestablish a taxonomy for financial bots, comprising 7 categories and 24\nsubcategories. Next, we create a ground-truth dataset consisting of 133 human\nand 137 bot addresses. Third, we employ both unsupervised and supervised\nmachine learning algorithms to detect bots deployed on Ethereum. The\nhighest-performing clustering algorithm is a Gaussian Mixture Model with an\naverage cluster purity of 82.6%, while the highest-performing model for binary\nclassification is a Random Forest with an accuracy of 83%. Our machine\nlearning-based detection mechanism contributes to understanding the Ethereum\necosystem dynamics by providing additional insights into the current bot\nlandscape.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of bots in Distributed Ledger Technologies (DLTs) fosters\nefficiency and automation. However, their use is also associated with predatory\ntrading and market manipulation, and can pose threats to system integrity. It\nis therefore essential to understand the extent of bot deployment in DLTs;\ndespite this, current detection systems are predominantly rule-based and lack\nflexibility. In this study, we present a novel approach that utilizes machine\nlearning for the detection of financial bots on the Ethereum platform. First,\nwe systematize existing scientific literature and collect anecdotal evidence to\nestablish a taxonomy for financial bots, comprising 7 categories and 24\nsubcategories. Next, we create a ground-truth dataset consisting of 133 human\nand 137 bot addresses. Third, we employ both unsupervised and supervised\nmachine learning algorithms to detect bots deployed on Ethereum. The\nhighest-performing clustering algorithm is a Gaussian Mixture Model with an\naverage cluster purity of 82.6%, while the highest-performing model for binary\nclassification is a Random Forest with an accuracy of 83%. Our machine\nlearning-based detection mechanism contributes to understanding the Ethereum\necosystem dynamics by providing additional insights into the current bot\nlandscape."
                },
                "authors": [
                    {
                        "name": "Thomas Niedermayer"
                    },
                    {
                        "name": "Pietro Saggese"
                    },
                    {
                        "name": "Bernhard Haslhofer"
                    }
                ],
                "author_detail": {
                    "name": "Bernhard Haslhofer"
                },
                "author": "Bernhard Haslhofer",
                "arxiv_doi": "10.1145/3589335.3651959",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3589335.3651959",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.19530v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.19530v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.01887v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.01887v3",
                "updated": "2025-01-02T13:49:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    2,
                    13,
                    49,
                    59,
                    3,
                    2,
                    0
                ],
                "published": "2024-07-02T02:18:14Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    2,
                    18,
                    14,
                    1,
                    184,
                    0
                ],
                "title": "Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Numeric Awards: In-Context Dueling Bandits with LLM Agents"
                },
                "summary": "In-context reinforcement learning (ICRL) is a frontier paradigm for solving\nreinforcement learning problems in the foundation model era. While ICRL\ncapabilities have been demonstrated in transformers through task-specific\ntraining, the potential of Large Language Models (LLMs) out-of-the-box remains\nlargely unexplored. Recent findings highlight that LLMs often face challenges\nwhen dealing with numerical contexts, and limited attention has been paid to\nevaluating their performance through preference feedback generated by the\nenvironment. This paper is the first to investigate LLMs as in-context\ndecision-makers under the problem of Dueling Bandits (DB), a stateless\npreference-based reinforcement learning setting that extends the classic\nMulti-Armed Bandit (MAB) model by querying for preference feedback. We compare\nGPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-Preview against nine\nwell-established DB algorithms. Our results reveal that our top-performing LLM,\nGPT-4 Turbo, has the zero-shot relative decision-making ability to achieve\nsurprisingly low weak regret across all the DB environment instances by quickly\nincluding the best arm in duels. However, an optimality gap exists between LLMs\nand classic DB algorithms in terms of strong regret. LLMs struggle to converge\nand consistently exploit even when explicitly prompted to do so, and are\nsensitive to prompt variations. To bridge this gap, we propose an agentic flow\nframework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates\noff-the-shelf DB algorithms with LLM agents through fine-grained adaptive\ninterplay. We show that LEAD has theoretical guarantees inherited from classic\nDB algorithms on both weak and strong regret. We validate its efficacy and\nrobustness even with noisy and adversarial prompts. The design of our framework\nsheds light on how to enhance the trustworthiness of LLMs used for in-context\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context reinforcement learning (ICRL) is a frontier paradigm for solving\nreinforcement learning problems in the foundation model era. While ICRL\ncapabilities have been demonstrated in transformers through task-specific\ntraining, the potential of Large Language Models (LLMs) out-of-the-box remains\nlargely unexplored. Recent findings highlight that LLMs often face challenges\nwhen dealing with numerical contexts, and limited attention has been paid to\nevaluating their performance through preference feedback generated by the\nenvironment. This paper is the first to investigate LLMs as in-context\ndecision-makers under the problem of Dueling Bandits (DB), a stateless\npreference-based reinforcement learning setting that extends the classic\nMulti-Armed Bandit (MAB) model by querying for preference feedback. We compare\nGPT-3.5 Turbo, GPT-4, GPT-4 Turbo, Llama 3.1, and o1-Preview against nine\nwell-established DB algorithms. Our results reveal that our top-performing LLM,\nGPT-4 Turbo, has the zero-shot relative decision-making ability to achieve\nsurprisingly low weak regret across all the DB environment instances by quickly\nincluding the best arm in duels. However, an optimality gap exists between LLMs\nand classic DB algorithms in terms of strong regret. LLMs struggle to converge\nand consistently exploit even when explicitly prompted to do so, and are\nsensitive to prompt variations. To bridge this gap, we propose an agentic flow\nframework: LLM with Enhanced Algorithmic Dueling (LEAD), which integrates\noff-the-shelf DB algorithms with LLM agents through fine-grained adaptive\ninterplay. We show that LEAD has theoretical guarantees inherited from classic\nDB algorithms on both weak and strong regret. We validate its efficacy and\nrobustness even with noisy and adversarial prompts. The design of our framework\nsheds light on how to enhance the trustworthiness of LLMs used for in-context\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Fanzeng Xia"
                    },
                    {
                        "name": "Hao Liu"
                    },
                    {
                        "name": "Yisong Yue"
                    },
                    {
                        "name": "Tongxin Li"
                    }
                ],
                "author_detail": {
                    "name": "Tongxin Li"
                },
                "author": "Tongxin Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.01887v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.01887v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]