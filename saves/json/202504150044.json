[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2411.17459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v3",
                "updated": "2025-04-11T12:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    31,
                    7,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v1",
                "updated": "2025-04-11T09:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08334v1",
                "updated": "2025-04-11T07:59:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T07:59:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    59,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Efficient Architecture for RISC-V Vector Memory Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Architecture for RISC-V Vector Memory Access"
                },
                "summary": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector processors frequently suffer from inefficient memory accesses,\nparticularly for strided and segment patterns. While coalescing strided\naccesses is a natural solution, effectively gathering or scattering elements at\nfixed strides remains challenging. Naive approaches rely on high-overhead\ncrossbars that remap any byte between memory and registers, leading to physical\ndesign issues. Segment operations require row-column transpositions, typically\nhandled using either element-level in-place transposition (degrading\nperformance) or large buffer-based bulk transposition (incurring high area\noverhead). In this paper, we present EARTH, a novel vector memory access\narchitecture designed to overcome these challenges through shifting-based\noptimizations. For strided accesses, EARTH integrates specialized shift\nnetworks for gathering and scattering elements. After coalescing multiple\naccesses within the same cache line, data is routed between memory and\nregisters through the shifting network with minimal overhead. For segment\noperations, EARTH employs a shifted register bank enabling direct column-wise\naccess, eliminating dedicated segment buffers while providing high-performance,\nin-place bulk transposition. Implemented on FPGA with Chisel HDL based on an\nopen-source RISC-V vector unit, EARTH enhances performance for strided memory\naccesses, achieving 4x-8x speedups in benchmarks dominated by strided\noperations. Compared to conventional designs, EARTH reduces hardware area by 9%\nand power consumption by 41%, significantly advancing both performance and\nefficiency of vector processors."
                },
                "authors": [
                    {
                        "name": "Hongyi Guan"
                    },
                    {
                        "name": "Yichuan Gao"
                    },
                    {
                        "name": "Chenlu Miao"
                    },
                    {
                        "name": "Haoyang Wu"
                    },
                    {
                        "name": "Hang Zhu"
                    },
                    {
                        "name": "Mingfeng Lin"
                    },
                    {
                        "name": "Huayue Liang"
                    }
                ],
                "author_detail": {
                    "name": "Huayue Liang"
                },
                "author": "Huayue Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08204v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08204v1",
                "updated": "2025-04-11T02:10:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:10:02Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    10,
                    2,
                    4,
                    101,
                    0
                ],
                "title": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "II-NVM: Enhancing Map Accuracy and Consistency with Normal\n  Vector-Assisted Mapping"
                },
                "summary": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\".",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLAM technology plays a crucial role in indoor mapping and localization. A\ncommon challenge in indoor environments is the \"double-sided mapping issue\",\nwhere closely positioned walls, doors, and other surfaces are mistakenly\nidentified as a single plane, significantly hindering map accuracy and\nconsistency. To address this issue this paper introduces a SLAM approach that\nensures accurate mapping using normal vector consistency. We enhance the voxel\nmap structure to store both point cloud data and normal vector information,\nenabling the system to evaluate consistency during nearest neighbor searches\nand map updates. This process distinguishes between the front and back sides of\nsurfaces, preventing incorrect point-to-plane constraints. Moreover, we\nimplement an adaptive radius KD-tree search method that dynamically adjusts the\nsearch radius based on the local density of the point cloud, thereby enhancing\nthe accuracy of normal vector calculations. To further improve realtime\nperformance and storage efficiency, we incorporate a Least Recently Used (LRU)\ncache strategy, which facilitates efficient incremental updates of the voxel\nmap. The code is released as open-source and validated in both simulated\nenvironments and real indoor scenarios. Experimental results demonstrate that\nthis approach effectively resolves the \"double-sided mapping issue\" and\nsignificantly improves mapping precision. Additionally, we have developed and\nopen-sourced the first simulation and real world dataset specifically tailored\nfor the \"double-sided mapping issue\"."
                },
                "authors": [
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yina Jian"
                    },
                    {
                        "name": "Jie Xu"
                    },
                    {
                        "name": "Linji Wang"
                    },
                    {
                        "name": "Yongxin Ma"
                    },
                    {
                        "name": "Xinglai Jin"
                    }
                ],
                "author_detail": {
                    "name": "Xinglai Jin"
                },
                "author": "Xinglai Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08204v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08204v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v2",
                "updated": "2025-04-11T02:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    5,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07815v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07815v1",
                "updated": "2025-04-10T14:52:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T14:52:03Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    14,
                    52,
                    3,
                    3,
                    100,
                    0
                ],
                "title": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Siren Federate: Bridging document, relational, and graph models for\n  exploratory graph analysis"
                },
                "summary": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigative workflows require interactive exploratory analysis on large\nheterogeneous knowledge graphs. Current databases show limitations in enabling\nsuch task. This paper discusses the architecture of Siren Federate, a system\nthat efficiently supports exploratory graph analysis by bridging\ndocument-oriented, relational and graph models. Technical contributions include\ndistributed join algorithms, adaptive query planning, query plan folding,\nsemantic caching, and semi-join decomposition for path query. Semi-join\ndecomposition addresses the exponential growth of intermediate results in\npath-based queries. Experiments show that Siren Federate exhibits low latency\nand scales well with the amount of data, the number of users, and the number of\ncomputing nodes."
                },
                "authors": [
                    {
                        "name": "Georgeta Bordea"
                    },
                    {
                        "name": "Stephane Campinas"
                    },
                    {
                        "name": "Matteo Catena"
                    },
                    {
                        "name": "Renaud Delbru"
                    }
                ],
                "author_detail": {
                    "name": "Renaud Delbru"
                },
                "author": "Renaud Delbru",
                "arxiv_comment": "36 pages, 16 figures, submitted to the ComSIS journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07815v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07815v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.11; E.1; H.2.4; H.3.3; H.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07642v1",
                "updated": "2025-04-10T10:43:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T10:43:42Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    10,
                    43,
                    42,
                    3,
                    100,
                    0
                ],
                "title": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-a-lot: Pushing the Limits of Unsatisfiable Core Reuse in SMT-Based\n  Program Analysis"
                },
                "summary": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satisfiability Modulo Theories (SMT) solvers are integral to program analysis\ntechniques like concolic and symbolic execution, where they help assess the\nsatisfiability of logical formulae to explore execution paths of the program\nunder test. However, frequent solver invocations are still the main performance\nbottleneck of these techniques. One way to mitigate this challenge is through\noptimizations such as caching and reusing solver results. While current methods\ntypically focus on reusing results from fully equivalent or closely related\nformulas, they often miss broader opportunities for reuse. In this paper, we\npropose a novel approach, Cache-a-lot, that extends the reuse of unsatisfiable\n(unsat) results by systematically considering all possible variable\nsubstitutions. This enables more extensive reuse of results, thereby reducing\nthe number of SMT solver invocations and improving the overall efficiency of\nconcolic and symbolic execution. Our evaluation, conducted against the\nstate-of-the-art Utopia solution using two benchmark sets, shows significant\nimprovements, particularly with more complex formulas. Our method achieves up\nto 74% unsat core reuse, compared to Utopia's 41%, and significant increase in\nthe time savings. These results demonstrate that, despite the additional\ncomputational complexity, the broader reuse of unsat results significantly\nenhances performance, offering valuable advancements for formal verification\nand program analysis."
                },
                "authors": [
                    {
                        "name": "Rustam Sadykov"
                    },
                    {
                        "name": "Azat Abdullin"
                    },
                    {
                        "name": "Marat Akhin"
                    }
                ],
                "author_detail": {
                    "name": "Marat Akhin"
                },
                "author": "Marat Akhin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07494v1",
                "updated": "2025-04-10T06:51:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:51:23Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    51,
                    23,
                    3,
                    100,
                    0
                ],
                "title": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apt-Serve: Adaptive Request Scheduling on Hybrid Cache for Scalable LLM\n  Inference Serving"
                },
                "summary": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference serving systems are essential to various\nLLM-based applications. As demand for LLM services continues to grow, scaling\nthese systems to handle high request rates while meeting latency Service-Level\nObjectives (SLOs), referred to as effective throughput, becomes critical.\nHowever, existing systems often struggle to improve effective throughput,\nprimarily due to a significant decline in Time To First Token (TTFT) SLO\nattainment. We identify two major causes of this bottleneck: (1)\nmemory-intensive KV cache that limits batch size expansion under GPU memory\nconstraints, and (2) rigid batch composition enforced by the default\nFirst-Come-First-Serve scheduling policy. In this paper, we introduce\nApt-Serve, a scalable framework designed to enhance effective throughput in LLM\ninference serving. Apt-Serve features a new hybrid cache scheme that combines\nKV cache with a memory-efficient hidden cache for reusable input hidden state\nvectors, allowing large batch sizes and improving request concurrency. Based on\nthe hybrid cache, Apt-Serve employs an adaptive runtime scheduling mechanism\nthat dynamically optimizes batch composition. We formally define the adaptive\nscheduling optimization problem and propose an efficient algorithm with\ntheoretical guarantees. Extensive evaluations on three real-world datasets and\nLLMs ranging from 13B to 66B parameters demonstrate that Apt-Serve achieves up\nto 8.8x improvement in effective throughput compared to the state-of-the-art\ninference serving systems."
                },
                "authors": [
                    {
                        "name": "Shihong Gao"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Yanyan Shen"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_doi": "10.1145/3725394",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725394",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.07494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07479v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07479v1",
                "updated": "2025-04-10T06:13:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T06:13:30Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    6,
                    13,
                    30,
                    3,
                    100,
                    0
                ],
                "title": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAIM: A Unified CAM/CIM Architecture with Static-Dynamic KV Cache\n  Pruning for Efficient Long-Context LLM Inference"
                },
                "summary": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) have achieved impressive\nperformance in various natural language processing (NLP) applications. However,\nthe high memory and computation cost induced by the KV cache limits the\ninference efficiency, especially for long input sequences. Compute-in-memory\n(CIM)-based accelerators have been proposed for LLM acceleration with KV cache\npruning. However, as existing accelerators only support static pruning with a\nfixed pattern or dynamic pruning with primitive implementations, they suffer\nfrom either high accuracy degradation or low efficiency. In this paper, we\npropose a ferroelectric FET (FeFET)-based unified content addressable memory\n(CAM) and CIM architecture, dubbed as UniCAIM. UniCAIM features simultaneous\nsupport for static and dynamic pruning with 3 computation modes: 1) in the CAM\nmode, UniCAIM enables approximate similarity measurement in O(1) time for\ndynamic KV cache pruning with high energy efficiency; 2) in the charge-domain\nCIM mode, static pruning can be supported based on accumulative similarity\nscore, which is much more flexible compared to fixed patterns; 3) in the\ncurrent-domain mode, exact attention computation can be conducted with a subset\nof selected KV cache. We further propose a novel CAM/CIM cell design that\nleverages the multi-level characteristics of FeFETs for signed multibit storage\nof the KV cache and in-place attention computation. With extensive experimental\nresults, we demonstrate UniCAIM can reduce the area-energy-delay product (AEDP)\nby 8.2-831x over the state-ofthe-art CIM-based LLM accelerators at the circuit\nlevel, along with high accuracy comparable with dense attention at the\napplication level, showing its great potential for efficient long-context LLM\ninference."
                },
                "authors": [
                    {
                        "name": "Weikai Xu"
                    },
                    {
                        "name": "Wenxuan Zeng"
                    },
                    {
                        "name": "Qianqian Huang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Ru Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ru Huang"
                },
                "author": "Ru Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07479v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07479v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v3",
                "updated": "2025-04-10T05:06:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    5,
                    6,
                    29,
                    3,
                    100,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "arxiv_comment": "MLSys 2025 camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v2",
                "updated": "2025-04-09T21:47:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    21,
                    47,
                    31,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAGNUS: Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse general matrix-matrix multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines, including Intel MKL, for a variety of different matrices on three\nIntel architectures. For matrices from the SuiteSparse collection, MAGNUS is\nfaster than all the baselines in most cases and is often an order of magnitude\nfaster than at least one baseline. For massive random matrices, MAGNUS scales\nto the largest matrix sizes, while the baselines do not. Furthermore, MAGNUS is\nclose to the optimal bound for these matrices, regardless of the matrix size,\nstructure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "arxiv_comment": "Accepted to ICS25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06425v3",
                "updated": "2025-04-09T20:51:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    20,
                    51,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-01-11T03:37:10Z",
                "published_parsed": [
                    2025,
                    1,
                    11,
                    3,
                    37,
                    10,
                    5,
                    11,
                    0
                ],
                "title": "Tensor Product Attention Is All You Need",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tensor Product Attention Is All You Need"
                },
                "summary": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models to handle longer input sequences typically\nnecessitates large key-value (KV) caches, resulting in substantial memory\noverhead during inference. In this paper, we propose Tensor Product Attention\n(TPA), a novel attention mechanism that uses tensor decompositions to represent\nqueries, keys, and values compactly, significantly shrinking KV cache size at\ninference time. By factorizing these representations into contextual low-rank\ncomponents (contextual factorization) and seamlessly integrating with RoPE, TPA\nachieves improved model quality alongside memory efficiency. Based on TPA, we\nintroduce the Tensor ProducT ATTenTion Transformer (T6), a new model\narchitecture for sequence modeling. Through extensive empirical evaluation of\nlanguage modeling tasks, we demonstrate that T6 exceeds the performance of\nstandard Transformer baselines including MHA, MQA, GQA, and MLA across various\nmetrics, including perplexity and a range of renowned evaluation benchmarks.\nNotably, TPA's memory efficiency enables the processing of significantly longer\nsequences under fixed resource constraints, addressing a critical scalability\nchallenge in modern language models. The code is available at\nhttps://github.com/tensorgi/T6."
                },
                "authors": [
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Yifeng Liu"
                    },
                    {
                        "name": "Huizhuo Yuan"
                    },
                    {
                        "name": "Zhen Qin"
                    },
                    {
                        "name": "Yang Yuan"
                    },
                    {
                        "name": "Quanquan Gu"
                    },
                    {
                        "name": "Andrew Chi-Chih Yao"
                    }
                ],
                "author_detail": {
                    "name": "Andrew Chi-Chih Yao"
                },
                "author": "Andrew Chi-Chih Yao",
                "arxiv_comment": "31 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06261v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06261v2",
                "updated": "2025-04-09T17:56:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    17,
                    56,
                    8,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-08T17:59:41Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    17,
                    59,
                    41,
                    1,
                    98,
                    0
                ],
                "title": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hogwild! Inference: Parallel LLM Generation via Concurrent Attention"
                },
                "summary": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated the ability to tackle\nincreasingly complex tasks through advanced reasoning, long-form content\ngeneration, and tool use. Solving these tasks often involves long\ninference-time computations. In human problem solving, a common strategy to\nexpedite work is collaboration: by dividing the problem into sub-tasks,\nexploring different strategies concurrently, etc. Recent research has shown\nthat LLMs can also operate in parallel by implementing explicit cooperation\nframeworks, such as voting mechanisms or the explicit creation of independent\nsub-tasks that can be executed in parallel. However, each of these frameworks\nmay not be suitable for all types of tasks, which can hinder their\napplicability. In this work, we propose a different design approach: we run LLM\n\"workers\" in parallel , allowing them to synchronize via a concurrently-updated\nattention cache and prompt these workers to decide how best to collaborate. Our\napproach allows the instances to come up with their own collaboration strategy\nfor the problem at hand, all the while \"seeing\" each other's partial progress\nin the concurrent cache. We implement this approach via Hogwild! Inference: a\nparallel LLM inference engine where multiple instances of the same LLM run in\nparallel with the same attention cache, with \"instant\" access to each other's\ngenerated tokens. Hogwild! inference takes advantage of Rotary Position\nEmbeddings (RoPE) to avoid recomputation while improving parallel hardware\nutilization. We find that modern reasoning-capable LLMs can perform inference\nwith shared Key-Value cache out of the box, without additional fine-tuning."
                },
                "authors": [
                    {
                        "name": "Gleb Rodionov"
                    },
                    {
                        "name": "Roman Garipov"
                    },
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "George Yakushev"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Anton Sinitsin"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06261v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06261v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04514v2",
                "updated": "2025-04-09T14:36:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    36,
                    19,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-06T15:15:07Z",
                "published_parsed": [
                    2025,
                    4,
                    6,
                    15,
                    15,
                    7,
                    6,
                    96,
                    0
                ],
                "title": "Saliency-driven Dynamic Token Pruning for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Saliency-driven Dynamic Token Pruning for Large Language Models"
                },
                "summary": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the recent success of large language models (LLMs), LLMs are\nparticularly challenging in long-sequence inference scenarios due to the\nquadratic computational complexity of the attention mechanism. Inspired by the\ninterpretability theory of feature attribution in neural network models, we\nobserve that not all tokens have the same contribution. Based on this\nobservation, we propose a novel token pruning framework, namely Saliency-driven\nDynamic Token Pruning (SDTP), to gradually and dynamically prune redundant\ntokens based on the input context. Specifically, a lightweight saliency-driven\nprediction module is designed to estimate the importance score of each token\nwith its hidden state, which is added to different layers of the LLM to\nhierarchically prune redundant tokens. Furthermore, a ranking-based\noptimization strategy is proposed to minimize the ranking divergence of the\nsaliency score and the predicted importance score. Extensive experiments have\nshown that our framework is generalizable to various models and datasets. By\nhierarchically pruning 65\\% of the input tokens, our method greatly reduces\n33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during\ninference, while maintaining comparable performance. We further demonstrate\nthat SDTP can be combined with KV cache compression method for further\ncompression."
                },
                "authors": [
                    {
                        "name": "Yao Tao"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Yun Wang"
                    },
                    {
                        "name": "Mingjian Zhu"
                    },
                    {
                        "name": "Hailin Hu"
                    },
                    {
                        "name": "Yunhe Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhe Wang"
                },
                "author": "Yunhe Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06813v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06813v1",
                "updated": "2025-04-09T12:07:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "published": "2025-04-09T12:07:26Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    12,
                    7,
                    26,
                    2,
                    99,
                    0
                ],
                "title": "Introducing the Arm-membench Throughput Benchmark",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Introducing the Arm-membench Throughput Benchmark"
                },
                "summary": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application performance of modern day processors is often limited by the\nmemory subsystem rather than actual compute capabilities. Therefore, data\nthroughput specifications play a key role in modeling application performance\nand determining possible bottlenecks. However, while peak instruction\nthroughputs and bandwidths for local caches are often documented, the\nachievable throughput can also depend on the relation between memory access and\ncompute instructions. In this paper, we present an Arm version of the well\nestablished x86-membench throughput benchmark, which we have adapted to support\nall current SIMD extensions of the Armv8 instruction set architecture. We\ndescribe aspects of the Armv8 ISA that need to be considered in the portable\ndesign of this benchmark. We use the benchmark to analyze the memory subsystem\nat a fine spatial granularity and to unveil microarchitectural details of three\nprocessors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the\nresulting performance information, we show that instruction fetch and decoder\nwidths become a potential bottleneck for cache-bandwidth-sensitive workloads\ndue to the load-store concept of the Arm ISA."
                },
                "authors": [
                    {
                        "name": "Cyrill Burth"
                    },
                    {
                        "name": "Markus Velten"
                    },
                    {
                        "name": "Robert Schne"
                    }
                ],
                "author_detail": {
                    "name": "Robert Schne"
                },
                "author": "Robert Schne",
                "arxiv_doi": "10.1007/978-3-031-85697-6_7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-031-85697-6_7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.06813v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06813v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14 pages, 6 figures, published in Parallel Processing and Applied\n  Mathematics (PPAM 2024), see https://doi.org/10.1007/978-3-031-85697-6_7",
                "arxiv_journal_ref": "Parallel Processing and Applied Mathematics. PPAM 2024. Lecture\n  Notes in Computer Science, vol 15579. Springer, Cham",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05821v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05821v2",
                "updated": "2025-04-09T10:23:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    10,
                    23,
                    39,
                    2,
                    99,
                    0
                ],
                "published": "2024-03-09T07:01:44Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    7,
                    1,
                    44,
                    5,
                    69,
                    0
                ],
                "title": "Optimizing LLM Queries in Relational Data Analytics Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing LLM Queries in Relational Data Analytics Workloads"
                },
                "summary": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch data analytics is a growing application for Large Language Models\n(LLMs). LLMs enable users to perform a wide range of natural language tasks,\nsuch as classification, entity extraction, and translation, over large\ndatasets. However, LLM inference is highly costly and slow: for example, an\nNVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second,\ntaking about a day to handle 15 GB of data; processing a similar amount of data\ncosts around $10K on OpenAI's GPT-4o. In this paper, we propose novel\ntechniques that can significantly reduce the cost of LLM calls for relational\ndata analytics workloads. Our key contribution is developing efficient\nalgorithms for reordering the rows and the fields within each row of an input\ntable to maximize key-value (KV) cache reuse when performing LLM serving. As\nsuch, our approach can be easily applied to existing analytics systems and\nserving platforms. Our evaluation shows that our solution can yield up to 3.4x\nimprovement in job completion time on a benchmark of diverse LLM-based queries\nusing Llama 3 models. Our solution also achieves a 32% cost savings under\nOpenAI and Anthropic pricing models."
                },
                "authors": [
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Asim Biswal"
                    },
                    {
                        "name": "Amog Kamsetty"
                    },
                    {
                        "name": "Audrey Cheng"
                    },
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Liana Patel"
                    },
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Xiangxi Mo"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    },
                    {
                        "name": "Matei Zaharia"
                    }
                ],
                "author_detail": {
                    "name": "Matei Zaharia"
                },
                "author": "Matei Zaharia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05821v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05821v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05591v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05591v3",
                "updated": "2025-04-09T09:09:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    9,
                    9,
                    37,
                    2,
                    99,
                    0
                ],
                "published": "2024-09-09T13:20:31Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    13,
                    20,
                    31,
                    0,
                    253,
                    0
                ],
                "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced\n  Retrieval Augmentation"
                },
                "summary": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long contexts presents a significant challenge for large language\nmodels (LLMs). While recent advancements allow LLMs to handle much longer\ncontexts than before (e.g., 32K or 128K tokens), it is computationally\nexpensive and can still be insufficient for many applications.\nRetrieval-Augmented Generation (RAG) is considered a promising strategy to\naddress this problem. However, conventional RAG methods face inherent\nlimitations because of two underlying requirements: 1) explicitly stated\nqueries, and 2) well-structured knowledge. These conditions, however, do not\nhold in general long-context processing tasks.\n  In this work, we propose MemoRAG, a novel RAG framework empowered by global\nmemory-augmented retrieval. MemoRAG features a dual-system architecture. First,\nit employs a light but long-range system to create a global memory of the long\ncontext. Once a task is presented, it generates draft answers, providing useful\nclues for the retrieval tools to locate relevant information within the long\ncontext. Second, it leverages an expensive but expressive system, which\ngenerates the final answer based on the retrieved information. Building upon\nthis fundamental framework, we realize the memory module in the form of KV\ncompression, and reinforce its memorization and cluing capacity from the\nGeneration quality's Feedback (a.k.a. RLGF). In our experiments, MemoRAG\nachieves superior performances across a variety of long-context evaluation\ntasks, not only complex scenarios where traditional RAG methods struggle, but\nalso simpler ones where RAG is typically applied."
                },
                "authors": [
                    {
                        "name": "Hongjin Qian"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Kelong Mao"
                    },
                    {
                        "name": "Defu Lian"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Tiejun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Tiejun Huang"
                },
                "author": "Tiejun Huang",
                "arxiv_comment": "theWebConf 2025. Codes and models are in\n  https://github.com/qhjqhj00/MemoRAG",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05591v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05591v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v5",
                "updated": "2025-04-09T07:55:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    7,
                    55,
                    43,
                    2,
                    99,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.14396v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.14396v5",
                "updated": "2025-04-09T03:49:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    49,
                    16,
                    2,
                    99,
                    0
                ],
                "published": "2023-12-22T02:52:59Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    2,
                    52,
                    59,
                    4,
                    356,
                    0
                ],
                "title": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for\n  Dynamic Graph Processing"
                },
                "summary": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An efficient data structure is fundamental to meeting the growing demands in\ndynamic graph processing. However, the dual requirements for graph computation\nefficiency (with contiguous structures) and graph update efficiency (with\nlinked list-like structures) present a conflict in the design principles of\ngraph structures. After experimental studies of existing state-of-the-art\ndynamic graph structures, we observe that the overhead of cache misses accounts\nfor a major portion of the graph computation time. This paper presents\nGastCoCo, a system with graph storage and coroutine-based prefetch co-design.\nBy employing software prefetching via stackless coroutines and introducing a\nprefetch-friendly data structure CBList, GastCoCo significantly alleviates the\nperformance degradation caused by cache misses. Our results show that GastCoCo\noutperforms state-of-the-art graph storage systems by 1.3x - 180x in graph\nupdates and 1.4x - 41.1x in graph computation."
                },
                "authors": [
                    {
                        "name": "Hongfu Li"
                    },
                    {
                        "name": "Qian Tao"
                    },
                    {
                        "name": "Song Yu"
                    },
                    {
                        "name": "Shufeng Gong"
                    },
                    {
                        "name": "Yanfeng Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Ge Yu"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "This paper was accepted by VLDB2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.14396v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.14396v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06419v1",
                "updated": "2025-04-08T20:39:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:39:20Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    39,
                    20,
                    1,
                    98,
                    0
                ],
                "title": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPIRe: Boosting LLM Inference Throughput with Speculative Decoding"
                },
                "summary": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding (SD) has been shown to reduce the latency of\nautoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing\nthroughput and therefore reducing the cost per token requires decoding with\nlarge batch sizes. Recent work shows that SD can accelerate decoding with large\nbatch sizes too if the context is sufficiently long and the draft model's KV\ncache is sparse. We introduce SPIRe, a draft model that combines static sparse\nattention, pruned initialization, and feedback memory to increase the modeled\nthroughput of speculative decoding by over 100% compared to speculation with a\nmuch smaller draft model and by over 35% compared to the strong baseline of\nsparse self-speculation. Our approach is particularly effective when context\nlengths vary significantly across requests."
                },
                "authors": [
                    {
                        "name": "Sanjit Neelam"
                    },
                    {
                        "name": "Daniel Heinlein"
                    },
                    {
                        "name": "Vaclav Cvicek"
                    },
                    {
                        "name": "Akshay Mishra"
                    },
                    {
                        "name": "Reiner Pope"
                    }
                ],
                "author_detail": {
                    "name": "Reiner Pope"
                },
                "author": "Reiner Pope",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v1",
                "updated": "2025-04-08T20:32:10Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation."
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-Andr Nol"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Andr Nol"
                },
                "author": "Pierre-Andr Nol",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.17692v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.17692v2",
                "updated": "2025-04-08T19:26:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    19,
                    26,
                    41,
                    1,
                    98,
                    0
                ],
                "published": "2024-04-26T20:44:36Z",
                "published_parsed": [
                    2024,
                    4,
                    26,
                    20,
                    44,
                    36,
                    4,
                    117,
                    0
                ],
                "title": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walking on Spheres and Talking to Neighbors: Variance Reduction for\n  Laplace's Equation"
                },
                "summary": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Walk on Spheres algorithms leverage properties of Brownian Motion to create\nMonte Carlo estimates of solutions to a class of elliptic partial differential\nequations. We propose a new caching strategy which leverages the continuity of\npaths of Brownian Motion. In the case of Laplace's equation with Dirichlet\nboundary conditions, our algorithm has improved asymptotic runtime compared to\nprevious approaches. Until recently, estimates were constructed pointwise and\ndid not use the relationship between solutions at nearby points within a\ndomain. Instead, our results are achieved by passing information from a cache\nof fixed size. We also provide bounds on the performance of our algorithm and\ndemonstrate its performance on example problems of increasing complexity."
                },
                "authors": [
                    {
                        "name": "Michael Czekanski"
                    },
                    {
                        "name": "Benjamin Faber"
                    },
                    {
                        "name": "Margaret Fairborn"
                    },
                    {
                        "name": "Adelle Wright"
                    },
                    {
                        "name": "David Bindel"
                    }
                ],
                "author_detail": {
                    "name": "David Bindel"
                },
                "author": "David Bindel",
                "arxiv_comment": "21 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.17692v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.17692v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.comp-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06067v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06067v1",
                "updated": "2025-04-08T14:09:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T14:09:23Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    9,
                    23,
                    1,
                    98,
                    0
                ],
                "title": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU-accelerated Evolutionary Many-objective Optimization Using\n  Tensorized NSGA-III"
                },
                "summary": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NSGA-III is one of the most widely adopted algorithms for tackling\nmany-objective optimization problems. However, its CPU-based design severely\nlimits scalability and computational efficiency. To address the limitations, we\npropose {TensorNSGA-III}, a fully tensorized implementation of NSGA-III that\nleverages GPU parallelism for large-scale many-objective optimization. Unlike\nconventional GPU-accelerated evolutionary algorithms that rely on heuristic\napproximations to improve efficiency, TensorNSGA-III maintains the exact\nselection and variation mechanisms of NSGA-III while achieving significant\nacceleration. By reformulating the selection process with tensorized data\nstructures and an optimized caching strategy, our approach effectively\neliminates computational bottlenecks inherent in traditional CPU-based and\nna\\\"ive GPU implementations. Experimental results on widely used numerical\nbenchmarks show that TensorNSGA-III achieves speedups of up to $3629\\times$\nover the CPU version of NSGA-III. Additionally, we validate its effectiveness\nin multiobjective robotic control tasks, where it discovers diverse and\nhigh-quality behavioral solutions. Furthermore, we investigate the critical\nrole of large population sizes in many-objective optimization and demonstrate\nthe scalability of TensorNSGA-III in such scenarios. The source code is\navailable at https://github.com/EMI-Group/evomo"
                },
                "authors": [
                    {
                        "name": "Hao Li"
                    },
                    {
                        "name": "Zhenyu Liang"
                    },
                    {
                        "name": "Ran Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Ran Cheng"
                },
                "author": "Ran Cheng",
                "arxiv_comment": "Accepted by IEEE CEC 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06067v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06067v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v2",
                "updated": "2025-04-08T14:05:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    14,
                    5,
                    12,
                    1,
                    98,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit exceptional performance but incur\nsignificant serving costs due to their substantial memory requirements, with\nthe key-value (KV) cache being a primary bottleneck. Existing KV cache\ncompression techniques, such as quantization and pruning, apply uniform\ntreatment to both keys and values, and discard unimportant tokens entirely,\noverlooking the fine-grained differences in significance of various components\nwithin the KV cache. To address these limitations, we introduce LeanKV, a\nframework that advances KV cache compression by exploiting three levels of\ndifferentiation in the KV cache: (1) the differing impact of keys and values on\nattention computation, (2) the varying importance of tokens, and (3) the\ndiverse dynamic sparsity patterns across attention heads. At the core of LeanKV\nis an on-GPU memory manager that compacts fragmented free memory list into\ncontiguous regions in parallel, effectively translating sparsity in the KV\ncache into performance gains. We evaluate LeanKV on several mainstream models,\nincluding the recent \"thinking model\". LeanKV is able to compress the KV cache\nby $2.7\\times$ to $5.7\\times$ with near-lossless accuracy on complex workloads\nrequiring sophisticated reasoning and long-generation capabilities, and\nenhances throughput by $1.9\\times$ to $5.4\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04760v2",
                "updated": "2025-04-08T12:46:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    12,
                    46,
                    45,
                    1,
                    98,
                    0
                ],
                "published": "2025-02-07T08:48:06Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    8,
                    48,
                    6,
                    4,
                    38,
                    0
                ],
                "title": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Federated Learning Based Proactive Content Caching in Edge\n  Computing"
                },
                "summary": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid growth of mobile data traffic and the increasing prevalence of\nvideo streaming, proactive content caching in edge computing has become crucial\nfor reducing latency and alleviating network congestion. However, traditional\ncaching strategies such as FIFO, LRU, and LFU fail to effectively predict\nfuture content popularity, while existing proactive caching approaches often\nrequire users to upload data to a central server, raising concerns regarding\nprivacy and scalability. To address these challenges, this paper proposes a\nGraph Federated Learning-based Proactive Content Caching (GFPCC) scheme that\nenhances caching efficiency while preserving user privacy. The proposed\napproach integrates federated learning and graph neural networks, enabling\nusers to locally train Light Graph Convolutional Networks (LightGCN) to capture\nuser-item relationships and predict content popularity. Instead of sharing raw\ndata, only the trained model parameters are transmitted to the central server,\nwhere a federated averaging algorithm aggregates updates, refines the global\nmodel, and selects the most popular files for proactive caching. Experimental\nevaluations on real-world datasets, such as MovieLens, demonstrate that GFPCC\noutperforms baseline caching algorithms by achieving higher cache efficiency\nthrough more accurate content popularity predictions. Moreover, the federated\nlearning framework strengthens privacy protection while maintaining efficient\nmodel training; however, scalability remains a challenge in large-scale\nnetworks with dynamic user preferences."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Rui Wang"
                },
                "author": "Rui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05897v1",
                "updated": "2025-04-08T10:47:37Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T10:47:37Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    10,
                    47,
                    37,
                    1,
                    98,
                    0
                ],
                "title": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient\n  MoE Inference"
                },
                "summary": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mixture of Experts (MoE) architecture has demonstrated significant\nadvantages as it enables to increase the model capacity without a proportional\nincrease in computation. However, the large MoE model size still introduces\nsubstantial memory demands, which usually requires expert offloading on\nresource-constrained platforms and incurs significant overhead. Hybrid CPU-GPU\ninference has been proposed to leverage CPU computation to reduce expert\nloading overhead but faces major challenges: on one hand, the expert activation\npatterns of MoE models are highly unstable, rendering the fixed mapping\nstrategies in existing works inefficient; on the other hand, the hybrid CPU-GPU\nschedule for MoE is inherently complex due to the diverse expert sizes,\nstructures, uneven workload distribution, etc. To address these challenges, in\nthis paper, we propose HybriMoE, a hybrid CPU-GPU inference framework that\nimproves resource utilization through a novel CPU-GPU scheduling and cache\nmanagement system. HybriMoE introduces (i) a dynamic intra-layer scheduling\nstrategy to balance workloads across CPU and GPU, (ii) an impact-driven\ninter-layer prefetching algorithm, and (iii) a score-based caching algorithm to\nmitigate expert activation instability. We implement HybriMoE on top of the\nkTransformers framework and evaluate it on three widely used MoE-based LLMs.\nExperimental results demonstrate that HybriMoE achieves an average speedup of\n1.33$\\times$ in the prefill stage and 1.70$\\times$ in the decode stage compared\nto state-of-the-art hybrid MoE inference framework. Our code is available at:\nhttps://github.com/PKU-SEC-Lab/HybriMoE."
                },
                "authors": [
                    {
                        "name": "Shuzhang Zhong"
                    },
                    {
                        "name": "Yanfan Sun"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Ru Huang"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "arxiv_comment": "Accepted by DAC 25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06319v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06319v1",
                "updated": "2025-04-08T09:17:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T09:17:35Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    9,
                    17,
                    35,
                    1,
                    98,
                    0
                ],
                "title": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating LLM Inference Throughput via Asynchronous KV Cache\n  Prefetching"
                },
                "summary": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit pronounced memory-bound characteristics\nduring inference due to High Bandwidth Memory (HBM) bandwidth constraints. In\nthis paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching\nmethod to break through the memory bandwidth bottleneck in LLM inference\nthrough computation-load overlap. By strategically scheduling idle memory\nbandwidth during active computation windows, our method proactively prefetches\nrequired KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for\nsubsequent accesses and effectively hiding HBM access latency within\ncomputational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that\nthe proposed method achieves 2.15x improvement in attention kernel efficiency\nand up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art\nbaseline FlashAttention-3. Notably, our solution maintains orthogonality to\nexisting optimization techniques and can be integrated with current inference\nframeworks, providing a scalable latency-hiding solution for next-generation\nLLM inference engines."
                },
                "authors": [
                    {
                        "name": "Yanhao Dong"
                    },
                    {
                        "name": "Yubo Miao"
                    },
                    {
                        "name": "Weinan Li"
                    },
                    {
                        "name": "Xiao Zheng"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Feng Lyu"
                    }
                ],
                "author_detail": {
                    "name": "Feng Lyu"
                },
                "author": "Feng Lyu",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06319v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06319v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05807v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05807v1",
                "updated": "2025-04-08T08:40:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T08:40:36Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    8,
                    40,
                    36,
                    1,
                    98,
                    0
                ],
                "title": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Complexity AoI-Optimal Status Update Control with Partial Battery\n  State Information in Energy Harvesting IoT Networks"
                },
                "summary": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a two-hop IoT system consisting of multiple energy harvesting sensors, a\ncache-enabled edge node, and multiple monitors, the status update control at\nthe edge node, which has partial battery state information (pBSI) of the\nsensors, is formulated as a pBSI problem. The concept of inferred pBSI is\nintroduced to reduce the noiseless single-sensor pBSI problem to a Markov\ndecision process with a moderate state-space size, enabling the optimal policy\nto be obtained through a value iteration algorithm. A lower bound on the\nexpected time-average on-demand age of information performance is established\nfor the general single-sensor status update problem. For the single-sensor pBSI\nproblem, a semi-closed-form policy called the current-next (CN) policy is\nproposed, along with an efficient post-update value iteration algorithm with a\nper-iteration time complexity proportional to the square of the battery\ncapacity. A weighted-update-gain-competition (WUGC) approach is further\nleveraged to extend the CN policy to the multi-sensor case. Numerical results\nin the single-sensor case demonstrate the near-optimal performance of the CN\npolicy across various energy arrival processes. Simulations for an IoT system\nwith $100$ sensors reveal that the WUGC-CN policy outperforms the\nmaximum-age-first policy and the random-scheduling-based CN policy under\nBernoulli energy arrival processes."
                },
                "authors": [
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Shengtian Yang"
                    },
                    {
                        "name": "Jun Chen"
                    },
                    {
                        "name": "Chao Chen"
                    },
                    {
                        "name": "Anding Wang"
                    }
                ],
                "author_detail": {
                    "name": "Anding Wang"
                },
                "author": "Anding Wang",
                "arxiv_comment": "18 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05807v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05807v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05718v1",
                "updated": "2025-04-08T06:38:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T06:38:27Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    6,
                    38,
                    27,
                    1,
                    98,
                    0
                ],
                "title": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6-VMRT: A Modular Approach Towards Time-Predictable Virtual Memory in\n  a 64-bit Application Class RISC-V Processor"
                },
                "summary": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing complexity of autonomous systems has driven a shift to\nintegrated heterogeneous SoCs with real-time and safety demands. Ensuring\ndeterministic WCETs and low-latency for critical tasks requires minimizing\ninterference on shared resources like virtual memory. Existing techniques, such\nas software coloring and memory replication, introduce significant area and\nperformance overhead, especially with virtualized memory where address\ntranslation adds latency uncertainty. To address these limitations, we propose\nCVA6-VMRT, an extension of the open-source RISC-V CVA6 core, adding hardware\nsupport for predictability in virtual memory access with minimal area overhead.\nCVA6-VMRT features dynamically partitioned Translation Look-aside Buffers\n(TLBs) and hybrid L1 cache/scratchpad memory (SPM) functionality. It allows\nfine-grained per-thread control of resources, enabling the operating system to\nmanage TLB replacements, including static overwrites, to ensure single-cycle\naddress translation for critical memory regions. Additionally, CVA6-VMRT\nenables runtime partitioning of data and instruction caches into cache and SPM\nsections, providing low and predictable access times for critical data without\nimpacting other accesses. In a virtualized setting, CVA6-VMRT enhances\nexecution time determinism for critical guests by 94% during interference from\nnon-critical guests, with minimal impact on their average absolute execution\ntime compared to isolated execution of the critical guests only. This\ninterference-aware behaviour is achieved with just a 4% area overhead and no\ntiming penalty compared to the baseline CVA6 core."
                },
                "authors": [
                    {
                        "name": "Christopher Reinwardt"
                    },
                    {
                        "name": "Robert Balas"
                    },
                    {
                        "name": "Alessandro Ottaviano"
                    },
                    {
                        "name": "Angelo Garofalo"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "8 pages, 7 figures, accepted at the 22nd ACM International Conference\n  on Computing Frontiers 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22926v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22926v2",
                "updated": "2025-04-08T05:27:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    5,
                    27,
                    15,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-29T01:06:54Z",
                "published_parsed": [
                    2025,
                    3,
                    29,
                    1,
                    6,
                    54,
                    5,
                    88,
                    0
                ],
                "title": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SR-LIO++: Efficient LiDAR-Inertial Odometry and Quantized Mapping with\n  Sweep Reconstruction"
                },
                "summary": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing the inherent low acquisition frequency limitation of 3D LiDAR to\nachieve high-frequency output has become a critical research focus in the\nLiDAR-Inertial Odometry (LIO) domain. To ensure real-time performance,\nfrequency-enhanced LIO systems must process each sweep within significantly\nreduced timeframe, which presents substantial challenges for deployment on\nlow-computational-power platforms. To address these limitations, we introduce\nSR-LIO++, an innovative LIO system capable of achieving doubled output\nfrequency relative to input frequency on resource-constrained hardware\nplatforms, including the Raspberry Pi 4B. Our system employs a sweep\nreconstruction methodology to enhance LiDAR sweep frequency, generating\nhigh-frequency reconstructed sweeps. Building upon this foundation, we propose\na caching mechanism for intermediate results (i.e., surface parameters) of the\nmost recent segments, effectively minimizing redundant processing of common\nsegments in adjacent reconstructed sweeps. This method decouples processing\ntime from the traditionally linear dependence on reconstructed sweep frequency.\nFurthermore, we present a quantized map point management based on index table\nmapping, significantly reducing memory usage by converting global 3D point\nstorage from 64-bit double precision to 8-bit char representation. This method\nalso converts the computationally intensive Euclidean distance calculations in\nnearest neighbor searches from 64-bit double precision to 16-bit short and\n32-bit integer formats, significantly reducing both memory and computational\ncost. Extensive experimental evaluations across three distinct computing\nplatforms and four public datasets demonstrate that SR-LIO++ maintains\nstate-of-the-art accuracy while substantially enhancing efficiency. Notably,\nour system successfully achieves 20Hz state output on Raspberry Pi 4B hardware."
                },
                "authors": [
                    {
                        "name": "Zikang Yuan"
                    },
                    {
                        "name": "Ruiye Ming"
                    },
                    {
                        "name": "Chengwei Zhao"
                    },
                    {
                        "name": "Yonghao Tan"
                    },
                    {
                        "name": "Pingcheng Dong"
                    },
                    {
                        "name": "Hongcheng Luo"
                    },
                    {
                        "name": "Yuzhong Jiao"
                    },
                    {
                        "name": "Xin Yang"
                    },
                    {
                        "name": "Kwang-Ting Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Kwang-Ting Cheng"
                },
                "author": "Kwang-Ting Cheng",
                "arxiv_comment": "10 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22926v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22926v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03661v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03661v2",
                "updated": "2025-04-08T04:34:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    4,
                    34,
                    44,
                    1,
                    98,
                    0
                ],
                "published": "2025-03-12T13:32:50Z",
                "published_parsed": [
                    2025,
                    3,
                    12,
                    13,
                    32,
                    50,
                    2,
                    71,
                    0
                ],
                "title": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MILLION: Mastering Long-Context LLM Inference Via Outlier-Immunized KV\n  Product Quantization"
                },
                "summary": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly utilized for complex tasks\nrequiring longer context lengths, with some models supporting up to 128K or 1M\ntokens. This trend, however, presents significant challenges in inference speed\nand memory management. Quantization emerges as a promising approach to address\nthe widening gap between LLM size and memory capacity. However, traditional\nquantization schemes often yield suboptimal compression results for KV caches\ndue to two key factors: i) On-the-fly quantization and de-quantization, causing\nsignificant performance overhead; ii) Prevalence of outliers in KV values,\nchallenging low-bitwidth uniform quantization. To this end, we propose MILLION,\na novel quantization framework achieving low-bitwidth KV cache through product\nquantization. First, we conduct a thorough analysis of KV cache distribution,\nrevealing the limitations of existing quantization schemes. Second, we\nintroduce a non-uniform quantization algorithm based on product quantization,\nwhich efficiently compresses data while preserving accuracy. Third, we develop\na high-performance GPU inference framework with efficient attention kernel and\npipeline design for MILLION that leverages sparse computation and asynchronous\nquantization, significantly enhancing inference speed. Comprehensive evaluation\nresults demonstrate that MILLION can achieve 4 bits quantization with trivial\nperplexity and accuracy loss, and achieve 2.09x end-to-end performance gains at\n32K context length. Code is released at https://github.com/ZongwuWang/MILLION."
                },
                "authors": [
                    {
                        "name": "Zongwu Wang"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Yiwei Hu"
                    },
                    {
                        "name": "Qingxiao Sun"
                    },
                    {
                        "name": "Gezi Li"
                    },
                    {
                        "name": "Cheng Li"
                    },
                    {
                        "name": "Xuan Wang"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Haibing Guan"
                    }
                ],
                "author_detail": {
                    "name": "Haibing Guan"
                },
                "author": "Haibing Guan",
                "arxiv_comment": "7 pages, 7 figures and 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03661v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03661v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05646v1",
                "updated": "2025-04-08T03:48:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "published": "2025-04-08T03:48:43Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    48,
                    43,
                    1,
                    98,
                    0
                ],
                "title": "Lattice: Learning to Efficiently Compress the Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lattice: Learning to Efficiently Compress the Memory"
                },
                "summary": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms have revolutionized sequence learning but suffer from\nquadratic computational complexity. This paper introduces Lattice, a novel\nrecurrent neural network (RNN) mechanism that leverages the inherent low-rank\nstructure of K-V matrices to efficiently compress the cache into a fixed number\nof memory slots, achieving sub-quadratic complexity. We formulate this\ncompression as an online optimization problem and derive a dynamic memory\nupdate rule based on a single gradient descent step. The resulting recurrence\nfeatures a state- and input-dependent gating mechanism, offering an\ninterpretable memory update process. The core innovation is the orthogonal\nupdate: each memory slot is updated exclusively with information orthogonal to\nits current state hence incorporation of only novel, non-redundant data, which\nminimizes the interference with previously stored information. The experimental\nresults show that Lattice achieves the best perplexity compared to all\nbaselines across diverse context lengths, with performance improvement becoming\nmore pronounced as the context length increases."
                },
                "authors": [
                    {
                        "name": "Mahdi Karami"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02533v3",
                "updated": "2025-04-07T22:48:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    22,
                    48,
                    33,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-03T12:36:01Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    12,
                    36,
                    1,
                    3,
                    93,
                    0
                ],
                "title": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARCANE: Adaptive RISC-V Cache Architecture for Near-memory Extensions"
                },
                "summary": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern data-driven applications expose limitations of von Neumann\narchitectures - extensive data movement, low throughput, and poor energy\nefficiency. Accelerators improve performance but lack flexibility and require\ndata transfers. Existing compute in- and near-memory solutions mitigate these\nissues but face usability challenges due to data placement constraints. We\npropose a novel cache architecture that doubles as a tightly-coupled\ncompute-near-memory coprocessor. Our RISC-V cache controller executes custom\ninstructions from the host CPU using vector operations dispatched to\nnear-memory vector processing units within the cache memory subsystem. This\narchitecture abstracts memory synchronization and data mapping from application\nsoftware while offering software-based Instruction Set Architecture\nextensibility. Our implementation shows $30\\times$ to $84\\times$ performance\nimprovement when operating on 8-bit data over the same system with a\ntraditional cache when executing a worst-case 32-bit CNN workload, with only\n$41.3\\%$ area overhead."
                },
                "authors": [
                    {
                        "name": "Vincenzo Petrolo"
                    },
                    {
                        "name": "Flavia Guella"
                    },
                    {
                        "name": "Michele Caon"
                    },
                    {
                        "name": "Pasquale Davide Schiavone"
                    },
                    {
                        "name": "Guido Masera"
                    },
                    {
                        "name": "Maurizio Martina"
                    }
                ],
                "author_detail": {
                    "name": "Maurizio Martina"
                },
                "author": "Maurizio Martina",
                "arxiv_comment": "6 pages, 4 figures, accepted at the Design Automation Conference\n  (DAC) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v2",
                "updated": "2025-04-07T20:52:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    20,
                    52,
                    4,
                    0,
                    97,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unseen Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for ULAD, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\nin F1 score while using 62.87 percentage points less labeled data. When trained\non the same amount of data as the baselines, FlexLog achieves up to a 13\npercentage points increase in F1 score on ADFA-U across varying training\ndataset sizes. Additionally, FlexLog maintains inference time under one second\nper log sequence, making it suitable for most applications except\nlatency-sensitive systems. Further analysis reveals the positive impact of\nFlexLog's key components: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.07467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05097v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05097v1",
                "updated": "2025-04-07T14:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T14:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    4,
                    30,
                    0,
                    97,
                    0
                ],
                "title": "State Tuning: State-based Test-Time Scaling on RWKV-7",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Tuning: State-based Test-Time Scaling on RWKV-7"
                },
                "summary": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling has emerged as a prominent research direction in machine\nlearning, enabling models to enhance their expressive capabilities during\ninference.Transformers, renowned for striking a delicate balance between\nefficiency and expressiveness, have benefited from test-time scaling techniques\nthat leverage an expanding key-value (KV) cache to significantly improve\nperformance.In this paper, we introduce a novel state-based approach to\ntest-time scaling, which we term state tuning, tailored to the RNN-based RWKV-7\nmodel.By exploiting the unique strengths of RWKV-7, our method achieves\nstate-of-the-art performance on the target task without altering the model's\npre-trained weights. Our approach centers on three key innovations. First, we\ndevelop an observer framework that allows a smaller model to replicate and\nlearn the state dynamics of the RWKV-7 model. Second, we employ a kernel method\nto dynamically upscale the state size, enhancing the model's capacity to\ncapture intricate patterns. Third, we integrate Decorrelated Backpropagation\n(DBP) to optimize the upscaled state matrix, thereby improving convergence and\nexpressivity. By tuning only the state matrix, we demonstrate that a smaller\nmodel can outperform larger models on the given task. This method preserves the\nefficiency of the original RWKV-7 architecture while harnessing the power of\ntest-time scaling to deliver superior results. Our findings underscore the\npotential of state tuning as an effective strategy for advancing model\nperformance in resource-constrained settings. Our code is\nhttps://github.com/TorchRWKV/flash-linear-attention."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05097v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05097v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04823v1",
                "updated": "2025-04-07T08:22:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T08:22:45Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    8,
                    22,
                    45,
                    0,
                    97,
                    0
                ],
                "title": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning\n  Models"
                },
                "summary": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in reasoning language models have demonstrated remarkable\nperformance in complex tasks, but their extended chain-of-thought reasoning\nprocess increases inference overhead. While quantization has been widely\nadopted to reduce the inference cost of large language models, its impact on\nreasoning models remains understudied. In this study, we conduct the first\nsystematic study on quantized reasoning models, evaluating the open-sourced\nDeepSeek-R1-Distilled Qwen and LLaMA families ranging from 1.5B to 70B\nparameters, and QwQ-32B. Our investigation covers weight, KV cache, and\nactivation quantization using state-of-the-art algorithms at varying\nbit-widths, with extensive evaluation across mathematical (AIME, MATH-500),\nscientific (GPQA), and programming (LiveCodeBench) reasoning benchmarks. Our\nfindings reveal that while lossless quantization can be achieved with W8A8 or\nW4A16 quantization, lower bit-widths introduce significant accuracy risks. We\nfurther identify model size, model origin, and task difficulty as critical\ndeterminants of performance. Contrary to expectations, quantized models do not\nexhibit increased output lengths. In addition, strategically scaling the model\nsizes or reasoning steps can effectively enhance the performance. All quantized\nmodels and codes will be open-sourced in\nhttps://github.com/ruikangliu/Quantized-Reasoning-Models."
                },
                "authors": [
                    {
                        "name": "Ruikang Liu"
                    },
                    {
                        "name": "Yuxuan Sun"
                    },
                    {
                        "name": "Manyi Zhang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Xianzhi Yu"
                    },
                    {
                        "name": "Tiezheng Yu"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Lu Hou"
                    }
                ],
                "author_detail": {
                    "name": "Lu Hou"
                },
                "author": "Lu Hou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.00414v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.00414v3",
                "updated": "2025-04-07T06:27:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    6,
                    27,
                    48,
                    0,
                    97,
                    0
                ],
                "published": "2024-10-01T05:46:22Z",
                "published_parsed": [
                    2024,
                    10,
                    1,
                    5,
                    46,
                    22,
                    1,
                    275,
                    0
                ],
                "title": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Parsing with Candidate Expressions for Knowledge Base Question\n  Answering"
                },
                "summary": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic parsers convert natural language to logical forms, which can be\nevaluated on knowledge bases (KBs) to produce denotations. Recent semantic\nparsers have been developed with sequence-to-sequence (seq2seq) pre-trained\nlanguage models (PLMs) or large language models, where the models treat logical\nforms as sequences of tokens. For syntactic and semantic validity, the semantic\nparsers use grammars that enable constrained decoding. However, the grammars\nlack the ability to utilize large information of KBs, although logical forms\ncontain representations of KB elements, such as entities or relations. In this\nwork, we propose a grammar augmented with candidate expressions for semantic\nparsing on a large KB with a seq2seq PLM. The grammar defines actions as\nproduction rules, and our semantic parser predicts actions during inference\nunder the constraints by types and candidate expressions. We apply the grammar\nto knowledge base question answering, where the constraints by candidate\nexpressions assist a semantic parser to generate valid KB elements. We also\nintroduce two special rules, sub-type inference and union types, and a mask\ncaching algorithm. In particular, sub-type inference and the mask caching\nalgorithm greatly increase the decoding speed of our semantic parser. We\nexperimented on two benchmarks, KQA Pro and Overnight, where the constraints by\ncandidate expressions increased the accuracy of our semantic parser, whether it\nwas trained with strong supervision or weak supervision. In addition, our\nsemantic parser had a fast decoding speed in the experiments."
                },
                "authors": [
                    {
                        "name": "Daehwan Nam"
                    },
                    {
                        "name": "Gary Geunbae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Gary Geunbae Lee"
                },
                "author": "Gary Geunbae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.00414v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.00414v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v1",
                "updated": "2025-04-07T03:22:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modifiation of the inference infrastructure and\nsignificant computation overhead. Base on the fact that the Large Lanuage\nmodels are autoregresssive models, we propose {\\it LagKV}, a KV allocation\nstrategy only relying on straight forward comparison among KV themself. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on LongBench and PasskeyRetrieval show that, our\napproach achieves nearly zero loss when the ratio is $2\\times$ and $\\approx\n90\\%$ of the original model performance for $8\\times$. Especially in the\n64-digit passkey retrieval task, our mehod outperforms the attention weight\nbased method $H_2O$ over $60\\%$ with same compression ratios. Our code is\navailable at \\url{https://github.com/AI-Lab-China-Merchants-Bank/LagKV}."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23367v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23367v2",
                "updated": "2025-04-07T01:35:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    7,
                    1,
                    35,
                    39,
                    0,
                    97,
                    0
                ],
                "published": "2025-03-30T08:51:19Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    51,
                    19,
                    6,
                    89,
                    0
                ],
                "title": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastVAR: Linear Visual Autoregressive Modeling via Cached Token Pruning"
                },
                "summary": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Autoregressive (VAR) modeling has gained popularity for its shift\ntowards next-scale prediction. However, existing VAR paradigms process the\nentire token map at each scale step, leading to the complexity and runtime\nscaling dramatically with image resolution. To address this challenge, we\npropose FastVAR, a post-training acceleration method for efficient resolution\nscaling with VARs. Our key finding is that the majority of latency arises from\nthe large-scale step where most tokens have already converged. Leveraging this\nobservation, we develop the cached token pruning strategy that only forwards\npivotal tokens for scale-specific modeling while using cached tokens from\nprevious scale steps to restore the pruned slots. This significantly reduces\nthe number of forwarded tokens and improves the efficiency at larger\nresolutions. Experiments show the proposed FastVAR can further speedup\nFlashAttention-accelerated VAR by 2.7$\\times$ with negligible performance drop\nof <1%. We further extend FastVAR to zero-shot generation of higher resolution\nimages. In particular, FastVAR can generate one 2K image with 15GB memory\nfootprints in 1.5s on a single NVIDIA 3090 GPU. Code is available at\nhttps://github.com/csguoh/FastVAR."
                },
                "authors": [
                    {
                        "name": "Hang Guo"
                    },
                    {
                        "name": "Yawei Li"
                    },
                    {
                        "name": "Taolin Zhang"
                    },
                    {
                        "name": "Jiangshan Wang"
                    },
                    {
                        "name": "Tao Dai"
                    },
                    {
                        "name": "Shu-Tao Xia"
                    },
                    {
                        "name": "Luca Benini"
                    }
                ],
                "author_detail": {
                    "name": "Luca Benini"
                },
                "author": "Luca Benini",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23367v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23367v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10714v2",
                "updated": "2025-04-06T12:20:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    6,
                    12,
                    20,
                    25,
                    6,
                    96,
                    0
                ],
                "published": "2025-03-13T03:36:03Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    3,
                    36,
                    3,
                    3,
                    72,
                    0
                ],
                "title": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ZSMerge: Zero-Shot KV Cache Compression for Memory-Efficient\n  Long-Context LLMs"
                },
                "summary": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The linear growth of key-value (KV) cache memory and quadratic computational\nin attention mechanisms complexity pose significant bottlenecks for large\nlanguage models (LLMs) in long-context processing. While existing KV cache\noptimization methods address these challenges through token pruning or feature\nmerging, they often incur irreversible information loss or require costly\nparameter retraining. To this end, we propose ZSMerge, a dynamic KV cache\ncompression framework designed for efficient cache management, featuring three\nkey operations: (1) fine-grained memory allocation guided by multi-dimensional\ntoken importance metrics at head-level granularity, (2) a residual merging\nmechanism that preserves critical context through compensated attention\nscoring, and (3) a zero-shot adaptation mechanism compatible with diverse LLM\narchitectures without requiring retraining. ZSMerge significantly enhances\nmemory efficiency and inference speed with negligible performance degradation\nacross LLMs. When applied to LLaMA2-7B, it demonstrates a 20:1 compression\nratio for key-value cache retention (reducing memory footprint to 5\\% of\nbaseline) while sustaining comparable generation quality, coupled with triple\nthroughput gains at extreme 54k-token contexts that eliminate out-of-memory\nfailures. The code is available at https://github.com/SusCom-Lab/ZSMerge."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Pei Liu"
                    },
                    {
                        "name": "Guoming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Guoming Tang"
                },
                "author": "Guoming Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04005v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04005v1",
                "updated": "2025-04-05T00:59:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "published": "2025-04-05T00:59:52Z",
                "published_parsed": [
                    2025,
                    4,
                    5,
                    0,
                    59,
                    52,
                    5,
                    95,
                    0
                ],
                "title": "Learning Cache Coherence Traffic for NoC Routing Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Cache Coherence Traffic for NoC Routing Design"
                },
                "summary": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid growth of multi-core systems highlights the need for efficient\nNetwork-on-Chip (NoC) design to ensure seamless communication. Cache coherence,\nessential for data consistency, substantially reduces task computation time by\nenabling data sharing among caches. As a result, routing serves two roles:\nfacilitating data sharing (influenced by topology) and managing NoC-level\ncommunication. However, cache coherence is often overlooked in routing, causing\nmismatches between design expectations and evaluation outcomes. Two main\nchallenges are the lack of specialized tools to assess cache coherence's impact\nand the neglect of topology selection in routing. In this work, we propose a\ncache coherence-aware routing approach with integrated topology selection,\nguided by our Cache Coherence Traffic Analyzer (CCTA). Our method achieves up\nto 10.52% lower packet latency, 55.51% faster execution time, and 49.02% total\nenergy savings, underscoring the critical role of cache coherence in NoC design\nand enabling effective co-design."
                },
                "authors": [
                    {
                        "name": "Guochu Xiong"
                    },
                    {
                        "name": "Xiangzhong Luo"
                    },
                    {
                        "name": "Weichen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Weichen Liu"
                },
                "author": "Weichen Liu",
                "arxiv_comment": "7 pages, 14 figures. Preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04005v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04005v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03632v1",
                "updated": "2025-04-04T17:56:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T17:56:44Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    17,
                    56,
                    44,
                    4,
                    94,
                    0
                ],
                "title": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Analysis of HPC applications on the Aurora Supercomputer:\n  Exploring the Impact of HBM-Enabled Intel Xeon Max CPUs"
                },
                "summary": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Aurora supercomputer is an exascale-class system designed to tackle some\nof the most demanding computational workloads. Equipped with both High\nBandwidth Memory (HBM) and DDR memory, it provides unique trade-offs in\nperformance, latency, and capacity. This paper presents a comprehensive\nanalysis of the memory systems on the Aurora supercomputer, with a focus on\nevaluating the trade-offs between HBM and DDR memory systems. We explore how\ndifferent memory configurations, including memory modes (Flat and Cache) and\nclustering modes (Quad and SNC4), influence key system performance metrics such\nas memory bandwidth, latency, CPU-GPU PCIe bandwidth, and MPI communication\nbandwidth. Additionally, we examine the performance of three representative HPC\napplications -- HACC, QMCPACK, and BFS -- each illustrating the impact of\nmemory configurations on performance. By using microbenchmarks and\napplication-level analysis, we provide insights into how to select the optimal\nmemory system and configuration to maximize performance based on the\napplication characteristics. The findings presented in this paper offer\nguidance for users of the Aurora system and similar exascale systems."
                },
                "authors": [
                    {
                        "name": "Huda Ibeid"
                    },
                    {
                        "name": "Vikram Narayana"
                    },
                    {
                        "name": "Jeongnim Kim"
                    },
                    {
                        "name": "Anthony Nguyen"
                    },
                    {
                        "name": "Vitali Morozov"
                    },
                    {
                        "name": "Ye Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ye Luo"
                },
                "author": "Ye Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.03771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.03771v2",
                "updated": "2025-04-04T16:51:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    16,
                    51,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-02-06T04:16:20Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    4,
                    16,
                    20,
                    3,
                    37,
                    0
                ],
                "title": "Adaptive Semantic Prompt Caching with VectorQ",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Semantic Prompt Caching with VectorQ"
                },
                "summary": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic prompt caches reduce the latency and cost of large language model\n(LLM) inference by reusing cached LLM-generated responses for semantically\nsimilar prompts. Vector similarity metrics assign a numerical score to quantify\nthe similarity between an embedded prompt and its nearest neighbor in the\ncache. Existing systems rely on a static threshold to classify whether the\nsimilarity score is sufficiently high to result in a cache hit. We show that\nthis one-size-fits-all threshold is insufficient across different embeddings.\nWe propose VectorQ, an online framework with a threshold convergence guarantee\nto learn embedding-specific threshold regions that adapt to the uncertainty of\nan embedding. Through evaluations on a combination of three diverse datasets,\nwe show that VectorQ consistently outperforms state-of-the-art systems across\nall static thresholds, achieving up to 26x increases in cache hit rate and\nerror rate reductions up to 74%."
                },
                "authors": [
                    {
                        "name": "Luis Gaspar Schroeder"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Alejandro Cuadron"
                    },
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Stephan Krusche"
                    },
                    {
                        "name": "Alfons Kemper"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Joseph E. Gonzalez"
                    }
                ],
                "author_detail": {
                    "name": "Joseph E. Gonzalez"
                },
                "author": "Joseph E. Gonzalez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.03771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.03771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.02073v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.02073v2",
                "updated": "2025-04-04T15:30:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    15,
                    30,
                    20,
                    4,
                    94,
                    0
                ],
                "published": "2023-07-05T07:30:53Z",
                "published_parsed": [
                    2023,
                    7,
                    5,
                    7,
                    30,
                    53,
                    2,
                    186,
                    0
                ],
                "title": "Performance Modeling of Data Storage Systems using Generative Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Modeling of Data Storage Systems using Generative Models"
                },
                "summary": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-precision modeling of systems is one of the main areas of industrial\ndata analysis. Models of systems, their digital twins, are used to predict\ntheir behavior under various conditions. We have developed several models of a\nstorage system using machine learning-based generative models. The system\nconsists of several components: hard disk drive (HDD) and solid-state drive\n(SSD) storage pools with different RAID schemes and cache. Each storage\ncomponent is represented by a probabilistic model that describes the\nprobability distribution of the component performance in terms of IOPS and\nlatency, depending on their configuration and external data load parameters.\nThe results of the experiments demonstrate the errors of 4-10 % for IOPS and\n3-16 % for latency predictions depending on the components and models of the\nsystem. The predictions show up to 0.99 Pearson correlation with Little's law,\nwhich can be used for unsupervised reliability checks of the models. In\naddition, we present novel data sets that can be used for benchmarking\nregression algorithms, conditional generative models, and uncertainty\nestimation methods in machine learning."
                },
                "authors": [
                    {
                        "name": "Abdalaziz Rashid Al-Maeeni"
                    },
                    {
                        "name": "Aziz Temirkhanov"
                    },
                    {
                        "name": "Artem Ryzhikov"
                    },
                    {
                        "name": "Mikhail Hushchyn"
                    }
                ],
                "author_detail": {
                    "name": "Mikhail Hushchyn"
                },
                "author": "Mikhail Hushchyn",
                "arxiv_doi": "10.1109/ACCESS.2025.3552409",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ACCESS.2025.3552409",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2307.02073v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.02073v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Access 2025 ( Volume: 13) 49643 - 49658",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03499v1",
                "updated": "2025-04-04T14:55:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T14:55:27Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    14,
                    55,
                    27,
                    4,
                    94,
                    0
                ],
                "title": "Optimistic Learning for Communication Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimistic Learning for Communication Networks"
                },
                "summary": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI/ML-based tools are at the forefront of resource management solutions for\ncommunication networks. Deep learning, in particular, is highly effective in\nfacilitating fast and high-performing decision-making whenever representative\ntraining data is available to build offline accurate models. Conversely, online\nlearning solutions do not require training and enable adaptive decisions based\non runtime observations, alas are often overly conservative. This extensive\ntutorial proposes the use of optimistic learning (OpL) as a decision engine for\nresource management frameworks in modern communication systems. When properly\ndesigned, such solutions can achieve fast and high-performing decisions --\ncomparable to offline-trained models -- while preserving the robustness and\nperformance guarantees of the respective online learning approaches. We\nintroduce the fundamental concepts, algorithms and results of OpL, discuss the\nroots of this theory and present different approaches to defining and achieving\noptimism. We proceed to showcase how OpL can enhance resource management in\ncommunication networks for several key problems such as caching, edge\ncomputing, network slicing, and workload assignment in decentralized O-RAN\nplatforms. Finally, we discuss the open challenges that must be addressed to\nunlock the full potential of this new resource management approach."
                },
                "authors": [
                    {
                        "name": "George Iosifidis"
                    },
                    {
                        "name": "Naram Mhaisen"
                    },
                    {
                        "name": "Douglas J. Leith"
                    }
                ],
                "author_detail": {
                    "name": "Douglas J. Leith"
                },
                "author": "Douglas J. Leith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10153v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10153v3",
                "updated": "2025-04-04T13:27:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    13,
                    27,
                    49,
                    4,
                    94,
                    0
                ],
                "published": "2024-12-13T14:11:42Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    14,
                    11,
                    42,
                    4,
                    348,
                    0
                ],
                "title": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EVOS: Efficient Implicit Neural Training via EVOlutionary Selector"
                },
                "summary": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose EVOlutionary Selector (EVOS), an efficient training paradigm for\naccelerating Implicit Neural Representation (INR). Unlike conventional INR\ntraining that feeds all samples through the neural network in each iteration,\nour approach restricts training to strategically selected points, reducing\ncomputational overhead by eliminating redundant forward passes. Specifically,\nwe treat each sample as an individual in an evolutionary process, where only\nthose fittest ones survive and merit inclusion in training, adaptively evolving\nwith the neural network dynamics. While this is conceptually similar to\nEvolutionary Algorithms, their distinct objectives (selection for acceleration\nvs. iterative solution optimization) require a fundamental redefinition of\nevolutionary mechanisms for our context. In response, we design sparse fitness\nevaluation, frequency-guided crossover, and augmented unbiased mutation to\ncomprise EVOS. These components respectively guide sample selection with\nreduced computational cost, enhance performance through frequency-domain\nbalance, and mitigate selection bias from cached evaluation. Extensive\nexperiments demonstrate that our method achieves approximately 48%-66%\nreduction in training time while ensuring superior convergence without\nadditional cost, establishing state-of-the-art acceleration among recent\nsampling-based strategies."
                },
                "authors": [
                    {
                        "name": "Weixiang Zhang"
                    },
                    {
                        "name": "Shuzhao Xie"
                    },
                    {
                        "name": "Chengwei Ren"
                    },
                    {
                        "name": "Siyi Xie"
                    },
                    {
                        "name": "Chen Tang"
                    },
                    {
                        "name": "Shijia Ge"
                    },
                    {
                        "name": "Mingzi Wang"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10153v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10153v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v1",
                "updated": "2025-04-04T03:30:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.16444v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.16444v3",
                "updated": "2025-04-03T22:49:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    22,
                    49,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2024-05-26T06:00:17Z",
                "published_parsed": [
                    2024,
                    5,
                    26,
                    6,
                    0,
                    17,
                    6,
                    147,
                    0
                ],
                "title": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheBlend: Fast Large Language Model Serving for RAG with Cached\n  Knowledge Fusion"
                },
                "summary": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) often incorporate multiple text chunks in their\ninputs to provide the necessary contexts. To speed up the prefill of the long\nLLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache\nwhen the context is reused as the prefix of another LLM input. However, the\nreused text chunks are not always the input prefix, which makes precomputed KV\ncaches not directly usable since they ignore the text's cross-attention with\nthe preceding texts. Thus, the benefits of reusing KV caches remain largely\nunrealized.\n  This paper tackles just one challenge: when an LLM input contains multiple\ntext chunks, how to quickly combine their precomputed KV caches in order to\nachieve the same generation quality as the expensive full prefill (i.e.,\nwithout reusing KV cache)? This challenge naturally arises in\nretrieval-augmented generation (RAG) where the input is supplemented with\nmultiple retrieved texts as the context. We present CacheBlend, a scheme that\nreuses the precomputed KV caches, regardless prefix or not, and selectively\nrecomputes the KV values of a small subset of tokens to partially update each\nreused KV cache. In the meantime, the small extra delay for recomputing some\ntokens can be pipelined with the retrieval of KV caches within the same job,\nallowing CacheBlend to store KV caches in slower devices with more storage\ncapacity while retrieving them without increasing the inference delay. By\ncomparing CacheBlend with the state-of-the-art KV cache reusing schemes on\nthree open-source LLMs of various sizes and four popular benchmark datasets of\ndifferent tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by\n2.2-3.3x and increases the inference throughput by 2.8-5x from full KV\nrecompute without compromising generation quality. The code is available at\nhttps://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Siddhant Ray"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.16444v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.16444v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03048v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03048v1",
                "updated": "2025-04-03T21:53:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T21:53:51Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    21,
                    53,
                    51,
                    3,
                    93,
                    0
                ],
                "title": "LLM Library Learning Fails: A LEGO-Prover Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Library Learning Fails: A LEGO-Prover Case Study"
                },
                "summary": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in the coding, reasoning, and tool-using abilities of\nLLMs have spurred interest in library learning (i.e., online learning through\nthe creation, storage, and retrieval of reusable and composable functions,\nknowledge, checklists, or lemmas). Such systems often promise improved task\nperformance through the automatic creation of broadly applicable tools, as well\nas superior computational performance through the caching of reasoning (i.e.,\nthe storage of generated tools). However, we find strong reason to be\nskeptical. We perform a deep dive into one such system, LEGO-Prover, which\npurports to learn reusable lemmas for mathematical reasoning. We find no\nevidence of the direct reuse of learned lemmas, and find evidence against the\nsoft reuse of learned lemmas (i.e., reuse by modifying relevant examples).\nCrucially, we find that LEGO-Prover does not in fact improve over the simple\nbaseline of prompting the model - the improvements in task accuracy vanish once\ncomputational cost is accounted for. Our findings suggest that serious\nmisconceptions exist as to the effectiveness of these techniques, that a\nserious re-examination of the state of LLM-based library learning is required,\nand that we require much stronger standards for evaluation including\nbehavioural analysis and ensuring that an equal computational budget is used\nfor baselines."
                },
                "authors": [
                    {
                        "name": "Ian Berlot-Attwell"
                    },
                    {
                        "name": "Frank Rudzicz"
                    },
                    {
                        "name": "Xujie Si"
                    }
                ],
                "author_detail": {
                    "name": "Xujie Si"
                },
                "author": "Xujie Si",
                "arxiv_comment": "24 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03048v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03048v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02976v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02976v1",
                "updated": "2025-04-03T18:54:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:54:50Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    54,
                    50,
                    3,
                    93,
                    0
                ],
                "title": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Localized Definitions and Distributed Reasoning: A Proof-of-Concept\n  Mechanistic Interpretability Study via Activation Patching"
                },
                "summary": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the localization of knowledge representation in\nfine-tuned GPT-2 models using Causal Layer Attribution via Activation Patching\n(CLAP), a method that identifies critical neural layers responsible for correct\nanswer generation. The model was fine-tuned on 9,958 PubMed abstracts\n(epilepsy: 20,595 mentions, EEG: 11,674 mentions, seizure: 13,921 mentions)\nusing two configurations with validation loss monitoring for early stopping.\nCLAP involved (1) caching clean (correct answer) and corrupted (incorrect\nanswer) activations, (2) computing logit difference to quantify model\npreference, and (3) patching corrupted activations with clean ones to assess\nrecovery. Results revealed three findings: First, patching the first\nfeedforward layer recovered 56% of correct preference, demonstrating that\nassociative knowledge is distributed across multiple layers. Second, patching\nthe final output layer completely restored accuracy (100% recovery), indicating\nthat definitional knowledge is localised. The stronger clean logit difference\nfor definitional questions further supports this localized representation.\nThird, minimal recovery from convolutional layer patching (13.6%) suggests\nlow-level features contribute marginally to high-level reasoning. Statistical\nanalysis confirmed significant layer-specific effects (p<0.01). These findings\ndemonstrate that factual knowledge is more localized and associative knowledge\ndepends on distributed representations. We also showed that editing efficacy\ndepends on task type. Our findings not only reconcile conflicting observations\nabout localization in model editing but also emphasize on using task-adaptive\ntechniques for reliable, interpretable updates."
                },
                "authors": [
                    {
                        "name": "Nooshin Bahador"
                    }
                ],
                "author_detail": {
                    "name": "Nooshin Bahador"
                },
                "author": "Nooshin Bahador",
                "arxiv_comment": "15 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02976v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02976v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02972v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02972v1",
                "updated": "2025-04-03T18:47:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T18:47:26Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    18,
                    47,
                    26,
                    3,
                    93,
                    0
                ],
                "title": "Improved Compact Genetic Algorithms with Efficient Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Compact Genetic Algorithms with Efficient Caching"
                },
                "summary": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compact Genetic Algorithms (cGAs) are condensed variants of classical Genetic\nAlgorithms (GAs) that use a probability vector representation of the population\ninstead of the complete population. cGAs have been shown to significantly\nreduce the number of function evaluations required while producing outcomes\nsimilar to those of classical GAs. However, cGAs have a tendency to repeatedly\ngenerate the same chromosomes as they approach convergence, resulting in\nunnecessary evaluations of identical chromosomes. This article introduces the\nconcept of caching in cGAs as a means of avoiding redundant evaluations of the\nsame chromosomes. Our proposed approach operates equivalently to cGAs, but\nenhances the algorithm's time efficiency by reducing the number of function\nevaluations. We also present a data structure for efficient cache maintenance\nto ensure low overhead. The proposed caching approach has an asymptotically\nconstant time complexity on average. The proposed method further generalizes\nthe caching mechanism with higher selection pressure for elitism-based cGAs. We\nconduct a rigorous analysis based on experiments on benchmark optimization\nproblems using two well-known cache replacement strategies. The results\ndemonstrate that caching significantly reduces the number of function\nevaluations required while maintaining the same level of performance accuracy."
                },
                "authors": [
                    {
                        "name": "Prasanta Dutta"
                    },
                    {
                        "name": "Anirban Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Anirban Mukhopadhyay"
                },
                "author": "Anirban Mukhopadhyay",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02972v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02972v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02921v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02921v1",
                "updated": "2025-04-03T17:08:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T17:08:42Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    17,
                    8,
                    42,
                    3,
                    93,
                    0
                ],
                "title": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyperRAG: Enhancing Quality-Efficiency Tradeoffs in Retrieval-Augmented\n  Generation with Reranker KV-Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing the performance of large language models (LLMs) by integrating\nexternal knowledge into the generation process. A key component of RAG\npipelines is the reranker, which selects the most relevant documents from a\npool of retrieved candidates and significantly improves the quality of the\ngenerated responses. While rerankers refine the selection of retrieved\ndocuments in RAG pipelines, they introduce computational challenges that hinder\nhigh throughput and low latency. To address this problem, we propose HyperRAG,\na system that optimizes the trade-off between quality and efficiency in RAG\npipelines by leveraging KV-cache reuse for efficient reranker inference. By\nreusing document-side KV-cache, HyperRAG achieves both high-quality generation\nand system-level efficiency. To fully realize the benefits of KV-cache reuse,\nHyperRAG incorporates a range of system-level optimizations designed to enhance\nefficiency and scalability. Experiments show that HyperRAG achieves a 2 - 3\nthroughput improvement with decoder-only rerankers while also delivering higher\ndownstream performance compared with traditional RAG service."
                },
                "authors": [
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Seo Jin Park"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02921v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02921v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v2",
                "updated": "2025-04-03T13:28:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    13,
                    28,
                    51,
                    3,
                    93,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which results in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46\\% reduction in memory and 40\\% increase in throughput with $<$\n0.1 loss in perplexity when compared to streaming the dense model from Flash.\nThe open source code for HW simulator, methods, and experiments in this paper\nis available at https://github.com/Qualcomm-AI-research/dynamic-sparsity ."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 6 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v1",
                "updated": "2025-04-03T09:58:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03775v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03775v1",
                "updated": "2025-04-03T08:58:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T08:58:05Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    8,
                    58,
                    5,
                    3,
                    93,
                    0
                ],
                "title": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: A Disaggregated Inference Framework with Low-Latency KV Cache\n  Transfer and Load-Aware Scheduling"
                },
                "summary": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregated inference has become an essential framework that separates the\nprefill (P) and decode (D) stages in large language model inference to improve\nthroughput. However, the KV cache transfer faces significant delays between\nprefill and decode nodes. The block-wise calling method and discontinuous KV\ncache memory allocation increase the number of calls to the transmission\nkernel. Additionally, existing frameworks often fix the roles of P and D nodes,\nleading to computational imbalances. In this paper, we propose FlowKV, a novel\ndisaggregated inference framework, which reduces the average transmission\nlatency of KV cache by 96%, from 0.944s to 0.053s, almost eliminating the\ntransfer time relative to the total request latency by optimizing the KV cache\ntransfer. FlowKV introduces the Load-Aware Scheduler for balanced request\nscheduling and flexible PD node allocation. This design maximizes hardware\nresource utilization, achieving peak system throughput across various\nscenarios, including normal, computational imbalance, and extreme overload\nconditions. Experimental results demonstrate that FlowKV significantly\naccelerates inference by 15.2%-48.9% on LongBench dataset compared to the\nbaseline and supports applications with heterogeneous GPUs."
                },
                "authors": [
                    {
                        "name": "Weiqing Li"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Xiangyong Ding"
                    },
                    {
                        "name": "Zhangcheng Tao"
                    },
                    {
                        "name": "Chuzhan Hao"
                    },
                    {
                        "name": "Chenfeng Xu"
                    },
                    {
                        "name": "Yuewei Zhang"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03775v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03775v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02268v1",
                "updated": "2025-04-03T04:27:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T04:27:02Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    4,
                    27,
                    2,
                    3,
                    93,
                    0
                ],
                "title": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Semantic Caching for LLMs with Domain-Specific Embeddings and\n  Synthetic Data"
                },
                "summary": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This report investigates enhancing semantic caching effectiveness by\nemploying specialized, fine-tuned embedding models. Semantic caching relies on\nembedding similarity rather than exact key matching, presenting unique\nchallenges in balancing precision, query latency, and computational efficiency.\nWe propose leveraging smaller, domain-specific embedding models, fine-tuned\nwith targeted real-world and synthetically generated datasets. Our empirical\nevaluations demonstrate that compact embedding models fine-tuned for just one\nepoch on specialized datasets significantly surpass both state-of-the-art\nopen-source and proprietary alternatives in precision and recall. Moreover, we\nintroduce a novel synthetic data generation pipeline for the semantic cache\nthat mitigates the challenge of limited domain-specific annotated data, further\nboosting embedding performance. Our approach effectively balances computational\noverhead and accuracy, establishing a viable and efficient strategy for\npractical semantic caching implementations."
                },
                "authors": [
                    {
                        "name": "Waris Gill"
                    },
                    {
                        "name": "Justin Cechmanek"
                    },
                    {
                        "name": "Tyler Hutcherson"
                    },
                    {
                        "name": "Srijith Rajamohan"
                    },
                    {
                        "name": "Jen Agarwal"
                    },
                    {
                        "name": "Muhammad Ali Gulzar"
                    },
                    {
                        "name": "Manvinder Singh"
                    },
                    {
                        "name": "Benoit Dion"
                    }
                ],
                "author_detail": {
                    "name": "Benoit Dion"
                },
                "arxiv_affiliation": "Redis",
                "author": "Benoit Dion",
                "arxiv_comment": "Initial study on embedding fine tuning for semantic cache. It also\n  explores synthetic data. Total pages are 12, including refrences",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02220v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02220v1",
                "updated": "2025-04-03T02:24:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-03T02:24:21Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    2,
                    24,
                    21,
                    3,
                    93,
                    0
                ],
                "title": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Distributed Caching Algorithms: Performance\n  Metrics and Implementation Considerations"
                },
                "summary": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive comparison of distributed caching\nalgorithms employed in modern distributed systems. We evaluate various caching\nstrategies including Least Recently Used (LRU), Least Frequently Used (LFU),\nAdaptive Replacement Cache (ARC), and Time-Aware Least Recently Used (TLRU)\nagainst metrics such as hit ratio, latency reduction, memory overhead, and\nscalability. Our analysis reveals that while traditional algorithms like LRU\nremain prevalent, hybrid approaches incorporating machine learning techniques\ndemonstrate superior performance in dynamic environments. Additionally, we\nanalyze implementation patterns across different distributed architectures and\nprovide recommendations for algorithm selection based on specific workload\ncharacteristics."
                },
                "authors": [
                    {
                        "name": "Helen Mayer"
                    },
                    {
                        "name": "James Richards"
                    }
                ],
                "author_detail": {
                    "name": "James Richards"
                },
                "author": "James Richards",
                "arxiv_comment": "International Conference on Computing Technologies and Artificial\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02220v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02220v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01281v2",
                "updated": "2025-04-03T01:23:22Z",
                "updated_parsed": [
                    2025,
                    4,
                    3,
                    1,
                    23,
                    22,
                    3,
                    93,
                    0
                ],
                "published": "2025-04-02T01:16:10Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    16,
                    10,
                    2,
                    92,
                    0
                ],
                "title": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Test-Time Inference with Policy-Optimized, Dynamic\n  Retrieval-Augmented Generation via KV Caching and Decoding"
                },
                "summary": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a comprehensive framework for enhancing Retrieval-Augmented\nGeneration (RAG) systems through dynamic retrieval strategies and reinforcement\nfine-tuning. This approach significantly improves large language models on\nknowledge-intensive tasks, including opendomain question answering and complex\nreasoning. Our framework integrates two complementary techniques:\nPolicy-Optimized RetrievalAugmented Generation (PORAG), which optimizes the use\nof retrieved information, and Adaptive Token-Layer Attention Scoring (ATLAS),\nwhich dynamically determines retrieval timing and content based on contextual\nneeds. Together, these techniques enhance both the utilization and relevance of\nretrieved content, improving factual accuracy and response quality. Designed as\na lightweight solution compatible with any Transformer-based LLM without\nrequiring additional training, our framework excels in knowledge-intensive\ntasks, boosting output accuracy in RAG settings. We further propose CRITIC, a\nnovel method to selectively compress key-value caches by token importance,\nmitigating memory bottlenecks in long-context applications. The framework also\nincorporates test-time scaling techniques to dynamically balance reasoning\ndepth and computational resources, alongside optimized decoding strategies for\nfaster inference. Experiments on benchmark datasets show that our framework\nreduces hallucinations, strengthens domain-specific reasoning, and achieves\nsignificant efficiency and scalability gains over traditional RAG systems. This\nintegrated approach advances the development of robust, efficient, and scalable\nRAG systems across diverse applications."
                },
                "authors": [
                    {
                        "name": "Sakhinana Sagar Srinivas"
                    },
                    {
                        "name": "Venkataramana Runkana"
                    }
                ],
                "author_detail": {
                    "name": "Venkataramana Runkana"
                },
                "author": "Venkataramana Runkana",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22875v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22875v2",
                "updated": "2025-04-02T18:51:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    18,
                    51,
                    53,
                    2,
                    92,
                    0
                ],
                "published": "2025-03-28T21:02:32Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    21,
                    2,
                    32,
                    4,
                    87,
                    0
                ],
                "title": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Pilot Study on Tunable Precision Emulation via Automatic BLAS\n  Offloading"
                },
                "summary": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study explores the use of automatic BLAS offloading and INT8-based\nemulation for accelerating traditional HPC workloads on modern GPU\narchitectures. Through the use of low-bitwidth integer units and cache-coherent\nUnified Memory Architecture, we emulate double-precision matrix multiplications\nin the MuST application without code changes. We find that accuracy depends on\nboth arithmetic precision and the properties of the operator, which can be\ndealt with through tunable precision emulation. Unlike traditional\nmixed-precision approaches, this method preserves original algorithms while\noptimizing hardware utilization. We showcases the potential of improving\naccuracy and performance at the same time. This work highlights the potential\nof AI-driven hardware to transform HPC, advocating for adaptive precision\nstrategies in future scientific computing."
                },
                "authors": [
                    {
                        "name": "Hang Liu"
                    },
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Yinzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhi Wang"
                },
                "author": "Yinzhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22875v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22875v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01582v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01582v1",
                "updated": "2025-04-02T10:38:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T10:38:25Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    10,
                    38,
                    25,
                    2,
                    92,
                    0
                ],
                "title": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MERE: Hardware-Software Co-Design for Masking Cache Miss Latency in\n  Embedded Processors"
                },
                "summary": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Runahead execution is a technique to mask memory latency caused by irregular\nmemory accesses. By pre-executing the application code during occurrences of\nlong-latency operations and prefetching anticipated cache-missed data into the\ncache hierarchy, runahead effectively masks memory latency for subsequent cache\nmisses and achieves high prefetching accuracy; however, this technique has been\nlimited to superscalar out-of-order and superscalar in-order cores. For\nimplementation in scalar in-order cores, the challenges of\narea-/energy-constraint and severe cache contention remain.\n  Here, we build the first full-stack system featuring runahead, MERE, from SoC\nand a dedicated ISA to the OS and programming model. Through this deployment,\nwe show that enabling runahead in scalar in-order cores is possible, with\nminimal area and power overheads, while still achieving high performance. By\nre-constructing the sequential runahead employing a hardware/software co-design\napproach, the system can be implemented on a mature processor and SoC. Building\non this, an adaptive runahead mechanism is proposed to mitigate the severe\ncache contention in scalar in-order cores. Combining this, we provide a\ncomprehensive solution for embedded processors managing irregular workloads.\nOur evaluation demonstrates that the proposed MERE attains 93.5% of a 2-wide\nout-of-order core's performance while constraining area and power overheads\nbelow 5%, with the adaptive runahead mechanism delivering an additional 20.1%\nperformance gain through mitigating the severe cache contention issues."
                },
                "authors": [
                    {
                        "name": "Dean You"
                    },
                    {
                        "name": "Jieyu Jiang"
                    },
                    {
                        "name": "Xiaoxuan Wang"
                    },
                    {
                        "name": "Yushu Du"
                    },
                    {
                        "name": "Zhihang Tan"
                    },
                    {
                        "name": "Wenbo Xu"
                    },
                    {
                        "name": "Hui Wang"
                    },
                    {
                        "name": "Jiapeng Guan"
                    },
                    {
                        "name": "Zhenyuan Wang"
                    },
                    {
                        "name": "Ran Wei"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Zhe Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Zhe Jiang"
                },
                "author": "Zhe Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01582v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01582v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01288v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01288v4",
                "updated": "2025-04-02T04:57:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    4,
                    57,
                    15,
                    2,
                    92,
                    0
                ],
                "published": "2024-11-02T15:45:54Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    45,
                    54,
                    5,
                    307,
                    0
                ],
                "title": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hexa-MoE: Efficient and Heterogeneous-aware Training for\n  Mixture-of-Experts"
                },
                "summary": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) has emerged as a practical approach to scale up\nparameters for the Transformer model to achieve better generalization while\nmaintaining a sub-linear increase in computation overhead. Current MoE models\nare mainly built with expert parallelism on distributed devices. However, it\nusually depends on homogeneous devices to deploy and suffers from heavy\ncommunication overhead and computation redundancy. In this paper, we explore\ndeveloping a \\texttt{H}eterogeneous-aware \\texttt{EX}pert \\texttt{A}llocation\nframework, \\textbf{\\texttt{HEXA-MoE}}, with significantly enhanced computing\nefficiency. It contains two components: ($1$) \\textit{Expert-Specific\nOperators}. We replace the typical general matrix multiplication or grouped\nmatrix multiplication interfaces with our operators, which allows the computing\nto be performed in an in-place manner with \\textbf{ZERO} redundancy. ($2$)\n\\textit{Adaptive Data- and Model-Centric Configurations} for different workload\nscales. Specifically, we introduce a pipeline-shared cache on each device to\ntackle the heavy memory consumption in the existing data-centric MoE library.\nComprehensive experiments on the Swin-MoE benchmark consistently reveal the\neffectiveness of our \\texttt{HEXA-MoE} framework, i.e., reducing $10\\%\\sim48\\%$\nmemory consumption and achieving $0.5\\sim4.3\\times$ speed up compared to\ncurrent state-of-the-art MoE libraries. Furthermore, we examine our\n\\texttt{HEXA-MoE} with heterogeneous devices for both data- and model-centric\nsettings. Promising results show that employing optimal parallel configuration\nwith \\texttt{HEXA-MoE} on heterogeneous devices can substantially minimize\noverall latency. Codes are available at https://github.com/UNITES-Lab/HEXA-MoE."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Jie Peng"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "17 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01288v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01288v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.11049v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.11049v5",
                "updated": "2025-04-02T01:58:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    58,
                    38,
                    2,
                    92,
                    0
                ],
                "published": "2024-08-20T17:57:31Z",
                "published_parsed": [
                    2024,
                    8,
                    20,
                    17,
                    57,
                    31,
                    1,
                    233,
                    0
                ],
                "title": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MagicDec: Breaking the Latency-Throughput Tradeoff for Long Context\n  Generation with Speculative Decoding"
                },
                "summary": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become more prevalent in long-context\napplications such as interactive chatbots, document analysis, and agent\nworkflows, but it is challenging to serve long-context requests with low\nlatency and high throughput. Speculative decoding (SD) is a widely used\ntechnique to reduce latency losslessly, but the conventional wisdom suggests\nthat its efficacy is limited to small batch sizes. In MagicDec, we show that\nsurprisingly SD can achieve speedup even for a high throughput inference regime\nfor moderate to long sequences. More interestingly, an intelligent drafting\nstrategy can achieve better speedup with increasing batch size based on our\nrigorous analysis. MagicDec first identifies the bottleneck shifts with\nincreasing batch size and sequence length, and uses these insights to deploy SD\nmore effectively for high throughput inference. We leverage draft model with\nsparse KV cache to address the KV bottleneck, which scales with both sequence\nlength and batch size. Additionally, we propose a theoretical model to select\nthe optimal drafting strategy for maximum speedup. Our work highlights the\nbroad applicability of speculative decoding in long-context serving, as it can\nenhance throughput and reduce latency without compromising accuracy. For\nmoderate to long sequences, we demonstrate up to 2.51x speedup for Llama3.1-8B\nwhen serving batch sizes ranging from 32 to 256 on various types of hardware\nand tasks."
                },
                "authors": [
                    {
                        "name": "Ranajoy Sadhukhan"
                    },
                    {
                        "name": "Jian Chen"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Vashisth Tiwari"
                    },
                    {
                        "name": "Ruihang Lai"
                    },
                    {
                        "name": "Jinyuan Shi"
                    },
                    {
                        "name": "Ian En-Hsu Yen"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Tianqi Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.11049v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.11049v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01291v1",
                "updated": "2025-04-02T01:49:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "published": "2025-04-02T01:49:58Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    1,
                    49,
                    58,
                    2,
                    92,
                    0
                ],
                "title": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy Bands and Breakdown Characteristics in Al2O3/UWBG AlGaN\n  Heterostructures"
                },
                "summary": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We report on energy bands and breakdown characteristics of Al2O3 dielectrics\non ultra-wide bandgap (UWBG) AlGaN heterostructures.\nMetal-dielectric-semiconductor structures are important to sustain high fields\nneeded for future high-performance UWBG transistors. Using systematic\nexperiments, we determined the fixed charge density (> 1013 cm-2), the\ndielectric/interface, and electric fields in the oxide of under flat-band\nconditions in the semiconductor. Low gate-to-drain leakage current of up to 5 x\n10-7 A/cm2 were obtained in the metal-oxide-semiconductor structures. In\nlateral metal-semiconductor-insulator test structures, breakdown voltage\nexceeding 1 kV was obtained with a channel sheet charge density of 1.27 x 1013\ncm-2. The effective peak electric field and average breakdown field were\nestimated to be > 4.27 MV/cm and 1.99 MV/cm, respectively. These findings\ndemonstrate the potential of Al2O2 integration for enhancing the breakdown\nperformance of UWBG AlGaN HEMTs."
                },
                "authors": [
                    {
                        "name": "Seungheon Shin"
                    },
                    {
                        "name": "Kyle Liddy"
                    },
                    {
                        "name": "Yinxuan Zhu"
                    },
                    {
                        "name": "Chandan Joishi"
                    },
                    {
                        "name": "Brianna A. Klein"
                    },
                    {
                        "name": "Andrew Armstrong"
                    },
                    {
                        "name": "Andrew A. Allerman"
                    },
                    {
                        "name": "Siddharth Rajan"
                    }
                ],
                "author_detail": {
                    "name": "Siddharth Rajan"
                },
                "author": "Siddharth Rajan",
                "arxiv_comment": "11 pages, 6 figures, and 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01157v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01157v1",
                "updated": "2025-04-01T19:48:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T19:48:17Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    19,
                    48,
                    17,
                    1,
                    91,
                    0
                ],
                "title": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Quacking: Deep Integration of Language Models and RAG into DuckDB"
                },
                "summary": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-intensive analytical applications retrieve context from both\nstructured tabular data and unstructured, text-free documents for effective\ndecision-making. Large language models (LLMs) have made it significantly easier\nto prototype such retrieval and reasoning data pipelines. However, implementing\nthese pipelines efficiently still demands significant effort and has several\nchallenges. This often involves orchestrating heterogeneous data systems,\nmanaging data movement, and handling low-level implementation details, e.g.,\nLLM context management.\n  To address these challenges, we introduce FlockMTL: an extension for DBMSs\nthat deeply integrates LLM capabilities and retrieval-augmented generation\n(RAG). FlockMTL includes model-driven scalar and aggregate functions, enabling\nchained predictions through tuple-level mappings and reductions. Drawing\ninspiration from the relational model, FlockMTL incorporates: (i) cost-based\noptimizations, which seamlessly apply techniques such as batching and caching;\nand (ii) resource independence, enabled through novel SQL DDL abstractions:\nPROMPT and MODEL, introduced as first-class schema objects alongside TABLE.\nFlockMTL streamlines the development of knowledge-intensive analytical\napplications, and its optimizations ease the implementation burden."
                },
                "authors": [
                    {
                        "name": "Anas Dorbani"
                    },
                    {
                        "name": "Sunny Yasser"
                    },
                    {
                        "name": "Jimmy Lin"
                    },
                    {
                        "name": "Amine Mhedhbi"
                    }
                ],
                "author_detail": {
                    "name": "Amine Mhedhbi"
                },
                "author": "Amine Mhedhbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01157v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01157v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01104v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01104v1",
                "updated": "2025-04-01T18:21:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:21:43Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    21,
                    43,
                    1,
                    91,
                    0
                ],
                "title": "Fundamentals of Caching Layered Data objects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fundamentals of Caching Layered Data objects"
                },
                "summary": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The effective management of large amounts of data processed or required by\ntoday's cloud or edge computing systems remains a fundamental challenge. This\npaper focuses on cache management for applications where data objects can be\nstored in layered representations. In such representations, each additional\ndata layer enhances the \"quality\" of the object's version but comes with an\nincremental cost of memory space. This layered approach proves beneficial in\nvarious scenarios, including the delivery of zoomable maps, video coding,\nfuture Virtual Reality gaming, and layered neural network models where\nadditional data layers improve inference accuracy. In systems where users or\ndevices demand different versions of a data object, layered representations\noffer flexibility for caching policies to achieve improved hit rates.\n  In this paper, we explore the performance of various traditionally studied\ncaching policies, such as Belady, LRU, and LFU, both with and without layering.\nTo this end, we develop an asymptotically accurate analytical model for Layered\nLRU (LLRU). We study how the performance of LLRU is impacted by factors such as\nthe number of layers, the popularity of different objects and layers, and\noverheads associated with storing layered representations. For instance, we\nshow that, for LLRU, more layers are not always beneficial and indeed\nperformance depends in subtle ways on the popularity and size profiles of\nlayers."
                },
                "authors": [
                    {
                        "name": "Agrim Bari"
                    },
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "George Kesidis"
                    }
                ],
                "author_detail": {
                    "name": "George Kesidis"
                },
                "author": "George Kesidis",
                "arxiv_comment": "An abridged version of this paper has been accepted at the 45th IEEE\n  International Conference on Distributed Computing Systems (ICDCS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01104v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01104v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01084v1",
                "updated": "2025-04-01T18:00:48Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T18:00:48Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    18,
                    0,
                    48,
                    1,
                    91,
                    0
                ],
                "title": "Surfactants Screen Slide Electrification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Surfactants Screen Slide Electrification"
                },
                "summary": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Water drops spontaneously accumulate charges when they move on hydrophobic\ndielectric surfaces by slide electrification. On the one hand, slide\nelectrification generates electricity with possible applications on tiny\ndevices. On the other hand, the potential of up to 1 KV generated by slide\nelectrification alters wetting and drop motion. Therefore, it is important to\nknow the factors that affect slide electrification. To find out how surfactants\naffect slide electrification, we measured drop charges of aqueous drops\ncontaining cationic CTAB, anionic SDS and neutral C8E3 sliding on different\nhydrophobic surfaces. The result is: addition of surfactant significantly\nreduces the spontaneous charging of moving water drops. Based on zeta potential\nmeasurements, confocal microscopy of deposited surface-active dyes and drop\nimpact studies, we propose that several factors contribute to this suppression\nof charge separation: (1) Surfactants tend to lower the contact angles, which\nreduces charge separation. (2) Surfactant adsorption at the solid-liquid\ninterface can reduce the density of primary ions, particularly for anionic\nsurfactants. (3) Anionic and neutral surfactants are mostly transferred to the\nliquid-air interface at the rear of the sliding drop, retaining primary ions\nwithin the drop. (4) Deposited cationic surfactant directly reduces the charge\nof the drop."
                },
                "authors": [
                    {
                        "name": "Xiaomei Li"
                    },
                    {
                        "name": "Zhongyuan Ni"
                    },
                    {
                        "name": "Xiaoteng Zhou"
                    },
                    {
                        "name": "Lisa S. Bauer"
                    },
                    {
                        "name": "Diego Diaz"
                    },
                    {
                        "name": "Gabriele Schfer"
                    },
                    {
                        "name": "Hans-Jrgen Butt"
                    }
                ],
                "author_detail": {
                    "name": "Hans-Jrgen Butt"
                },
                "author": "Hans-Jrgen Butt",
                "arxiv_comment": "13 pages, 4 figures, 50 references",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00999v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00999v1",
                "updated": "2025-04-01T17:39:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:39:19Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    39,
                    19,
                    1,
                    91,
                    0
                ],
                "title": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MergeVQ: A Unified Framework for Visual Generation and Representation\n  with Disentangled Token Merging and Quantization"
                },
                "summary": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked Image Modeling (MIM) with Vector Quantization (VQ) has achieved great\nsuccess in both self-supervised pre-training and image generation. However,\nmost existing methods struggle to address the trade-off in shared latent space\nfor generation quality vs. representation learning and efficiency. To push the\nlimits of this paradigm, we propose MergeVQ, which incorporates token merging\ntechniques into VQ-based generative models to bridge the gap between image\ngeneration and visual representation learning in a unified architecture. During\npre-training, MergeVQ decouples top-k semantics from latent space with the\ntoken merge module after self-attention blocks in the encoder for subsequent\nLook-up Free Quantization (LFQ) and global alignment and recovers their\nfine-grained details through cross-attention in the decoder for reconstruction.\nAs for the second-stage generation, we introduce MergeAR, which performs KV\nCache compression for efficient raster-order prediction. Extensive experiments\non ImageNet verify that MergeVQ as an AR generative model achieves competitive\nperformance in both visual representation learning and image generation tasks\nwhile maintaining favorable token efficiency and inference speed. The code and\nmodel will be available at https://apexgen-x.github.io/MergeVQ."
                },
                "authors": [
                    {
                        "name": "Siyuan Li"
                    },
                    {
                        "name": "Luyuan Zhang"
                    },
                    {
                        "name": "Zedong Wang"
                    },
                    {
                        "name": "Juanxi Tian"
                    },
                    {
                        "name": "Cheng Tan"
                    },
                    {
                        "name": "Zicheng Liu"
                    },
                    {
                        "name": "Chang Yu"
                    },
                    {
                        "name": "Qingsong Xie"
                    },
                    {
                        "name": "Haonan Lu"
                    },
                    {
                        "name": "Haoqian Wang"
                    },
                    {
                        "name": "Zhen Lei"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Lei"
                },
                "author": "Zhen Lei",
                "arxiv_comment": "CVPR2025 (in process for more analysis and extension)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00999v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00999v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00970v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00970v1",
                "updated": "2025-04-01T17:08:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T17:08:57Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    17,
                    8,
                    57,
                    1,
                    91,
                    0
                ],
                "title": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SentenceKV: Efficient LLM Inference via Sentence-Level Semantic KV\n  Caching"
                },
                "summary": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models face significant computational and memory challenges\nwhen processing long contexts. During inference, efficient management of the\nkey-value (KV) cache, which stores intermediate activations for autoregressive\ngeneration, is critical to reducing memory overhead and improving computational\nefficiency. Traditional token-level efficient KV caching methods overlook\nsemantic information, treating tokens independently without considering their\nsemantic relationships. Meanwhile, existing semantic-preserving KV cache\nmanagement approaches often suffer from substantial memory usage and high\ntime-to-first-token. To address these limitations, we propose SentenceKV, a\nnovel sentence-level semantic KV caching approach designed to enhance inference\nefficiency while preserving semantic coherence. During prefilling, SentenceKV\ngroups tokens based on sentence-level semantic similarity, compressing sentence\nrepresentations into concise semantic vectors stored directly on the GPU, while\nindividual KV pairs are offloaded to CPU. During decoding, SentenceKV generates\ntokens by selectively retrieving semantically relevant sentence-level KV\nentries, leveraging the semantic similarity between the prefilling-stage\nsemantic vectors and decoding-stage queries. This ensures efficient and\ncontextually accurate predictions, minimizing the loading of redundant or\nirrelevant data into GPU memory and significantly reducing memory overhead\nwhile maintaining stable inference latency, even for extremely long contexts.\nExtensive evaluations on benchmarks including PG-19, LongBench, and\nNeedle-In-A-Haystack demonstrate that SentenceKV significantly outperforms\nstate-of-the-art methods in both efficiency and memory usage, without\ncompromising model accuracy."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Ali Falahati"
                    },
                    {
                        "name": "David H. Yang"
                    },
                    {
                        "name": "Mohammad Mohammadi Amiri"
                    }
                ],
                "author_detail": {
                    "name": "Mohammad Mohammadi Amiri"
                },
                "author": "Mohammad Mohammadi Amiri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00970v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00970v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13275v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13275v2",
                "updated": "2025-04-01T14:21:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    14,
                    21,
                    15,
                    1,
                    91,
                    0
                ],
                "published": "2025-03-17T15:27:02Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    15,
                    27,
                    2,
                    0,
                    76,
                    0
                ],
                "title": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge-Aware Iterative Retrieval for Multi-Agent Systems"
                },
                "summary": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a novel large language model (LLM)-driven agent framework, which\niteratively refines queries and filters contextual evidence by leveraging\ndynamically evolving knowledge. A defining feature of the system is its\ndecoupling of external sources from an internal knowledge cache that is\nprogressively updated to guide both query generation and evidence selection.\nThis design mitigates bias-reinforcement loops and enables dynamic, trackable\nsearch exploration paths, thereby optimizing the trade-off between exploring\ndiverse information and maintaining accuracy through autonomous agent\ndecision-making. Our approach is evaluated on a broad range of open-domain\nquestion answering benchmarks, including multi-step tasks that mirror\nreal-world scenarios where integrating information from multiple sources is\ncritical, especially given the vulnerabilities of LLMs that lack explicit\nreasoning or planning capabilities. The results show that the proposed system\nnot only outperforms single-step baselines regardless of task difficulty but\nalso, compared to conventional iterative retrieval methods, demonstrates\npronounced advantages in complex tasks through precise evidence-based reasoning\nand enhanced efficiency. The proposed system supports both competitive and\ncollaborative sharing of updated context, enabling multi-agent extension. The\nbenefits of multi-agent configurations become especially prominent as task\ndifficulty increases. The number of convergence steps scales with task\ndifficulty, suggesting cost-effective scalability."
                },
                "authors": [
                    {
                        "name": "Seyoung Song"
                    }
                ],
                "author_detail": {
                    "name": "Seyoung Song"
                },
                "author": "Seyoung Song",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13275v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13275v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; I.2.7; I.2.11; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00726v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00726v1",
                "updated": "2025-04-01T12:34:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T12:34:58Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    12,
                    34,
                    58,
                    1,
                    91,
                    0
                ],
                "title": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMO: Edge Model Overlays to Scale Model Size in Federated Learning"
                },
                "summary": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) trains machine learning models on edge devices with\ndistributed data. However, the computational and memory limitations of these\ndevices restrict the training of large models using FL. Split Federated\nLearning (SFL) addresses this challenge by distributing the model across the\ndevice and server, but it introduces a tightly coupled data flow, leading to\ncomputational bottlenecks and high communication costs. We propose EMO as a\nsolution to enable the training of large models in FL while mitigating the\nchallenges of SFL. EMO introduces Edge Model Overlay(s) between the device and\nserver, enabling the creation of a larger ensemble model without modifying the\nFL workflow. The key innovation in EMO is Augmented Federated Learning (AFL),\nwhich builds an ensemble model by connecting the original (smaller) FL model\nwith model(s) trained in the overlay(s) to facilitate horizontal or vertical\nscaling. This is accomplished through three key modules: a hierarchical\nactivation replay cache to decouple AFL from FL, a convergence-aware\ncommunication controller to optimize communication overhead, and an ensemble\ninference module. Evaluations on a real-world prototype show that EMO improves\naccuracy by up to 17.77% compared to FL, and reduces communication costs by up\nto 7.17x and decreases training time by up to 6.9x compared to SFL."
                },
                "authors": [
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Weibo He"
                    },
                    {
                        "name": "Wanglei Feng"
                    },
                    {
                        "name": "Zhenyu Wen"
                    },
                    {
                        "name": "Bin Qian"
                    },
                    {
                        "name": "Blesson Varghese"
                    }
                ],
                "author_detail": {
                    "name": "Blesson Varghese"
                },
                "author": "Blesson Varghese",
                "arxiv_comment": "Poster accepted at IEEE ICDCS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00726v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00726v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00557v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00557v1",
                "updated": "2025-04-01T09:10:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T09:10:32Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    10,
                    32,
                    1,
                    91,
                    0
                ],
                "title": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLaMA-3.2-Vision by Trimming Cross-attended Visual Features"
                },
                "summary": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual token reduction lowers inference costs caused by extensive image\nfeatures in large vision-language models (LVLMs). Unlike relevant studies that\nprune tokens in self-attention-only LVLMs, our work uniquely addresses\ncross-attention-based models, which achieve superior performance. We identify\nthat the key-value (KV) cache size for image tokens in cross-attention layers\nsignificantly exceeds that of text tokens in self-attention layers, posing a\nmajor compute bottleneck. To mitigate this issue, we exploit the sparse nature\nin cross-attention maps to selectively prune redundant visual features. Our\nTrimmed Llama effectively reduces KV cache demands without requiring additional\ntraining. By benefiting from 50%-reduced visual features, our model can reduce\ninference latency and memory usage while achieving benchmark parity."
                },
                "authors": [
                    {
                        "name": "Jewon Lee"
                    },
                    {
                        "name": "Ki-Ung Song"
                    },
                    {
                        "name": "Seungmin Yang"
                    },
                    {
                        "name": "Donguk Lim"
                    },
                    {
                        "name": "Jaeyeon Kim"
                    },
                    {
                        "name": "Wooksu Shin"
                    },
                    {
                        "name": "Bo-Kyeong Kim"
                    },
                    {
                        "name": "Yong Jae Lee"
                    },
                    {
                        "name": "Tae-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Tae-Ho Kim"
                },
                "author": "Tae-Ho Kim",
                "arxiv_comment": "accepted at CVPR 2025 Workshop on ELVM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00557v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00557v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00474v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00474v1",
                "updated": "2025-04-01T07:04:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "published": "2025-04-01T07:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    7,
                    4,
                    30,
                    1,
                    91,
                    0
                ],
                "title": "High specific impulse electrospray propulsion with small capillary\n  emitters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High specific impulse electrospray propulsion with small capillary\n  emitters"
                },
                "summary": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study demonstrates the feasibility of using smaller capillary emitters\nto achieve higher specific impulse ($I_\\text{sp}$) in electrospray propulsion.\nFour ionic liquids were characterized using capillary emitters with tip\ndiameters from 15 to 50 $\\mu$m. Smaller diameter capillaries produced smaller\nand more stable Taylor cones. This stabilization enabled steady cone-jet\noperation at significantly lower flow rates compared to larger emitters. This\nwas unexpected because when the jet diameter is much smaller than far-field\ngeometric features, the minimum flow rate is thought to be solely determined by\nthe physical properties of the propellant. Using the smaller emitters and\nacceleration voltages of 10 kV, specific impulses up to 3000 s could be\nachieved with efficiencies above 50%, approximately doubling the $I_\\text{sp}$\nobserved with larger emitters. For one of the liquids and the smallest\nemitters, the beam consisted solely of ions at the lowest flow rates, similarly\nto studies using externally wetted and porous emitters. Another important\nfinding was that at sufficiently low flow rates, a significant fraction of the\npropellant fed to the emitter is not accelerated by the electrostatic field.\nThese propellant losses make the time-of-flight technique unreliable for\ndetermining the $I_\\text{sp}$."
                },
                "authors": [
                    {
                        "name": "Manel Caballero-Prez"
                    },
                    {
                        "name": "Marc Galobardes-Esteban"
                    },
                    {
                        "name": "Manuel Gamero-Castao"
                    }
                ],
                "author_detail": {
                    "name": "Manuel Gamero-Castao"
                },
                "author": "Manuel Gamero-Castao",
                "arxiv_comment": "29 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00474v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00474v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v1",
                "updated": "2025-03-31T17:37:32Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24007v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24007v1",
                "updated": "2025-03-31T12:32:23Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:32:23Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    32,
                    23,
                    0,
                    90,
                    0
                ],
                "title": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CITRAS: Covariate-Informed Transformer for Time Series Forecasting"
                },
                "summary": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariates play an indispensable role in practical time series forecasting,\noffering rich context from the past and sometimes extending into the future.\nHowever, their availability varies depending on the scenario, and situations\noften involve multiple target variables simultaneously. Moreover, the\ncross-variate dependencies between them are multi-granular, with some\ncovariates having a short-term impact on target variables and others showing\nlong-term correlations. This heterogeneity and the intricate dependencies\narising in covariate-informed forecasting present significant challenges to\nexisting deep models. To address these issues, we propose CITRAS, a patch-based\nTransformer that flexibly leverages multiple targets and covariates covering\nboth the past and the future forecasting horizon. While preserving the strong\nautoregressive capabilities of the canonical Transformer, CITRAS introduces two\nnovel mechanisms in patch-wise cross-variate attention: Key-Value (KV) Shift\nand Attention Score Smoothing. KV Shift seamlessly incorporates future known\ncovariates into the forecasting of target variables based on their concurrent\ndependencies. Additionally, Attention Score Smoothing transforms locally\naccurate patch-wise cross-variate dependencies into global variate-level\ndependencies by smoothing the past series of attention scores. Experimentally,\nCITRAS achieves state-of-the-art performance in both covariate-informed and\nmultivariate forecasting, demonstrating its versatile ability to leverage\ncross-variate dependency for improved forecasting accuracy."
                },
                "authors": [
                    {
                        "name": "Yosuke Yamaguchi"
                    },
                    {
                        "name": "Issei Suemitsu"
                    },
                    {
                        "name": "Wenpeng Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wenpeng Wei"
                },
                "author": "Wenpeng Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24007v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24007v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24000v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24000v1",
                "updated": "2025-03-31T12:23:31Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T12:23:31Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    12,
                    23,
                    31,
                    0,
                    90,
                    0
                ],
                "title": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Key-Value Cache Compression Techniques for Large Language\n  Model Serving"
                },
                "summary": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value cache (\\texttt{KV} \\texttt{cache}) compression has emerged as a\npromising technique to optimize Large Language Model (LLM) serving. It\nprimarily decreases the memory consumption of \\texttt{KV} \\texttt{cache} to\nreduce the computation cost. Despite the development of many compression\nalgorithms, their applications in production environments are still not\nprevalent. In this paper, we revisit mainstream \\texttt{KV} \\texttt{cache}\ncompression solutions from a practical perspective. Our contributions are\nthree-fold. First, we comprehensively review existing algorithmic designs and\nbenchmark studies for \\texttt{KV} \\texttt{cache} compression and identify\nmissing pieces in their performance measurement, which could hinder their\nadoption in practice. Second, we empirically evaluate representative\n\\texttt{KV} \\texttt{cache} compression methods to uncover two key issues that\naffect the computational efficiency: (1) while compressing \\texttt{KV}\n\\texttt{cache} can reduce memory consumption, current implementations (e.g.,\nFlashAttention, PagedAttention) do not optimize for production-level LLM\nserving, resulting in suboptimal throughput performance; (2) compressing\n\\texttt{KV} \\texttt{cache} may lead to longer outputs, resulting in increased\nend-to-end latency. We further investigate the accuracy performance of\nindividual samples rather than the overall performance, revealing the intrinsic\nlimitations in \\texttt{KV} \\texttt{cache} compression when handling specific\nLLM tasks. Third, we provide tools to shed light on future \\texttt{KV}\n\\texttt{cache} compression studies and facilitate their practical deployment in\nproduction. They are open-sourced in\n\\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}."
                },
                "authors": [
                    {
                        "name": "Wei Gao"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Peng Sun"
                    },
                    {
                        "name": "Tianwei Zhang"
                    },
                    {
                        "name": "Yonggang Wen"
                    }
                ],
                "author_detail": {
                    "name": "Yonggang Wen"
                },
                "author": "Yonggang Wen",
                "arxiv_comment": "21 pages, 18 figures, published to MLSys2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24000v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24000v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23988v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23988v1",
                "updated": "2025-03-31T11:58:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:58:37Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    58,
                    37,
                    0,
                    90,
                    0
                ],
                "title": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning Model Deployment in Multiple Cloud Providers: an\n  Exploratory Study Using Low Computing Power Environments"
                },
                "summary": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of Machine Learning models at cloud have grown by tech\ncompanies. Hardware requirements are higher when these models involve Deep\nLearning (DL) techniques and the cloud providers' costs may be a barrier. We\nexplore deploying DL models using for experiments the GECToR model, a DL\nsolution for Grammatical Error Correction, across three of the major cloud\nplatforms (AWS, Google Cloud, Azure). We evaluate real-time latency, hardware\nusage and cost at each cloud provider by 7 execution environments with 10\nexperiments reproduced. We found that while GPUs excel in performance, they had\nan average cost 300% higher than solutions without GPU. Our analysis also\nidentifies that processor cache size is crucial for cost-effective CPU\ndeployments, enabling over 50% of cost reduction compared to GPUs. This study\ndemonstrates the feasibility and affordability of cloud-based DL inference\nsolutions without GPUs, benefiting resource-constrained users like startups."
                },
                "authors": [
                    {
                        "name": "Elayne Lemos"
                    },
                    {
                        "name": "Rodrigo Oliveira"
                    },
                    {
                        "name": "Jairson Rodrigues"
                    },
                    {
                        "name": "Rosalvo F. Oliveira Neto"
                    }
                ],
                "author_detail": {
                    "name": "Rosalvo F. Oliveira Neto"
                },
                "author": "Rosalvo F. Oliveira Neto",
                "arxiv_comment": "15 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23988v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23988v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07, 68U01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.4; I.2.0; B.8.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v1",
                "updated": "2025-03-31T11:13:18Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18334v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18334v2",
                "updated": "2025-03-31T10:28:04Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    10,
                    28,
                    4,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-24T04:32:35Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    4,
                    32,
                    35,
                    0,
                    83,
                    0
                ],
                "title": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language\n  Models"
                },
                "summary": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) of visual language models has recently attracted\nsignificant attention as a solution to the performance degradation caused by\ndistribution shifts in downstream tasks. However, existing cache-based TTA\nmethods have certain limitations. They mainly rely on the accuracy of cached\nfeature labels, and the presence of noisy pseudo-labels can cause these\nfeatures to deviate from their true distribution. This makes cache retrieval\nmethods based on similarity matching highly sensitive to outliers or extreme\nsamples. Moreover, current methods lack effective mechanisms to model class\ndistributions, which limits their ability to fully exploit the potential of\ncached information. To address these challenges, we introduce a comprehensive\nand reliable caching mechanism and propose a novel zero-shot TTA method called\n\"Cache, Residual, Gaussian\" (CRG). This method not only employs learnable\nresidual parameters to better align positive and negative visual prototypes\nwith text prototypes, thereby optimizing the quality of cached features, but\nalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically model\nintra-class feature distributions, further mitigating the impact of noisy\nfeatures. Experimental results on 13 benchmarks demonstrate that CRG\noutperforms state-of-the-art TTA methods, showcasing exceptional robustness and\nadaptability."
                },
                "authors": [
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Tianming Sha"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICME 2025 and ICLR 2025 Workshop on Foundation Models in\n  the Wild",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18334v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18334v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23897v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23897v1",
                "updated": "2025-03-31T09:46:56Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-31T09:46:56Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    9,
                    46,
                    56,
                    0,
                    90,
                    0
                ],
                "title": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Text-Guided Image Editing with Visual Autoregressive Model"
                },
                "summary": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-guided image editing is an essential task that enables users to modify\nimages through natural language descriptions. Recent advances in diffusion\nmodels and rectified flows have significantly improved editing quality,\nprimarily relying on inversion techniques to extract structured noise from\ninput images. However, inaccuracies in inversion can propagate errors, leading\nto unintended modifications and compromising fidelity. Moreover, even with\nperfect inversion, the entanglement between textual prompts and image features\noften results in global changes when only local edits are intended. To address\nthese challenges, we propose a novel text-guided image editing framework based\non VAR (Visual AutoRegressive modeling), which eliminates the need for explicit\ninversion while ensuring precise and controlled modifications. Our method\nintroduces a caching mechanism that stores token indices and probability\ndistributions from the original image, capturing the relationship between the\nsource prompt and the image. Using this cache, we design an adaptive\nfine-grained masking strategy that dynamically identifies and constrains\nmodifications to relevant regions, preventing unintended changes. A token\nreassembling approach further refines the editing process, enhancing diversity,\nfidelity, and control. Our framework operates in a training-free manner and\nachieves high-fidelity editing with faster inference speeds, processing a 1K\nresolution image in as fast as 1.2 seconds. Extensive experiments demonstrate\nthat our method achieves performance comparable to, or even surpassing,\nexisting diffusion- and rectified flow-based approaches in both quantitative\nmetrics and visual quality. The code will be released."
                },
                "authors": [
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Lanqing Guo"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Jiaxing Huang"
                    },
                    {
                        "name": "Pichao Wang"
                    },
                    {
                        "name": "Bihan Wen"
                    },
                    {
                        "name": "Jian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Wang"
                },
                "author": "Jian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23897v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23897v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17808v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17808v4",
                "updated": "2025-03-31T03:28:44Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    3,
                    28,
                    44,
                    0,
                    90,
                    0
                ],
                "published": "2024-06-24T03:59:17Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    3,
                    59,
                    17,
                    0,
                    176,
                    0
                ],
                "title": "Training-Free Exponential Context Extension via Cascading KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-Free Exponential Context Extension via Cascading KV Cache"
                },
                "summary": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transformer's context window is vital for tasks such as few-shot learning\nand conditional generation as it preserves previous tokens for active memory.\nHowever, as the context lengths increase, the computational costs grow\nquadratically, hindering the deployment of large language models (LLMs) in\nreal-world, long sequence scenarios. Although some recent key-value caching (KV\nCache) methods offer linear inference complexity, they naively manage the\nstored context, prematurely evicting tokens and losing valuable information.\nMoreover, they lack an optimized prefill/prompt stage strategy, resulting in\nhigher latency than even quadratic attention for realistic context sizes. In\nresponse, we introduce a novel mechanism that leverages cascading sub-cache\nbuffers to selectively retain the most relevant tokens, enabling the model to\nmaintain longer context histories without increasing the cache size. Our\napproach outperforms linear caching baselines across key benchmarks, including\nstreaming perplexity, question answering, book summarization, and passkey\nretrieval, where it retains better retrieval accuracy at 1M tokens after four\ndoublings of the cache size of 65K. Additionally, our method reduces prefill\nstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.\nThese innovations not only enhance the computational efficiency of LLMs but\nalso pave the way for their effective deployment in resource-constrained\nenvironments, enabling large-scale, real-time applications with significantly\nreduced latency."
                },
                "authors": [
                    {
                        "name": "Jeffrey Willette"
                    },
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17808v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17808v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21817v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21817v2",
                "updated": "2025-03-31T02:19:29Z",
                "updated_parsed": [
                    2025,
                    3,
                    31,
                    2,
                    19,
                    29,
                    0,
                    90,
                    0
                ],
                "published": "2025-03-26T04:16:48Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    4,
                    16,
                    48,
                    2,
                    85,
                    0
                ],
                "title": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skip-Vision: Efficient and Scalable Acceleration of Vision-Language\n  Models via Adaptive Token Skipping"
                },
                "summary": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have driven significant advancements in Multimodal\nLarge Language Models (MLLMs), yet their computational costs surge drastically\nwhen scaling resolution, training data, and model parameters. A key bottleneck\nstems from the proliferation of visual tokens required for fine-grained image\nunderstanding. We propose Skip-Vision, a unified framework addressing both\ntraining and inference inefficiencies in vision-language models. On top of\nconventional token compression approaches, our method introduces two\ncomplementary acceleration strategies. For training acceleration, we observe\nthat Feed-Forward Network (FFN) computations on visual tokens induce marginal\nfeature updates. This motivates our Skip-FFN strategy, which bypasses FFN\nlayers for redundant visual tokens. For inference acceleration, we design a\nselective KV-cache removal mechanism that prunes the skipped key-value pairs\nduring decoding while preserving model performance. Experimental results\ndemonstrate that Skip-Vision reduces training time by up to 35\\%, inference\nFLOPs by 75\\%, and latency by 45\\%, while achieving comparable or superior\nperformance to existing methods. Our work provides a practical solution for\nscaling high-performance MLLMs with enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Weili Zeng"
                    },
                    {
                        "name": "Ziyuan Huang"
                    },
                    {
                        "name": "Kaixiang Ji"
                    },
                    {
                        "name": "Yichao Yan"
                    }
                ],
                "author_detail": {
                    "name": "Yichao Yan"
                },
                "author": "Yichao Yan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21817v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21817v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10270v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10270v2",
                "updated": "2025-03-30T11:14:17Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    14,
                    17,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-13T11:26:45Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    11,
                    26,
                    45,
                    3,
                    72,
                    0
                ],
                "title": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EEdit: Rethinking the Spatial and Temporal Redundancy for Efficient\n  Image Editing"
                },
                "summary": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inversion-based image editing is rapidly gaining momentum while suffering\nfrom significant computation overhead, hindering its application in real-time\ninteractive scenarios. In this paper, we rethink that the redundancy in\ninversion-based image editing exists in both the spatial and temporal\ndimensions, such as the unnecessary computation in unedited regions and the\nredundancy in the inversion progress. To tackle these challenges, we propose a\npractical framework, named EEdit, to achieve efficient image editing.\nSpecifically, we introduce three techniques to solve them one by one. For\nspatial redundancy, spatial locality caching is introduced to compute the\nedited region and its neighboring regions while skipping the unedited regions,\nand token indexing preprocessing is designed to further accelerate the caching.\nFor temporal redundancy, inversion step skipping is proposed to reuse the\nlatent for efficient editing. Our experiments demonstrate an average of 2.46\n$\\times$ acceleration without performance drop in a wide range of editing tasks\nincluding prompt-guided image editing, dragging and image composition. Our\ncodes are available at https://github.com/yuriYanZeXuan/EEdit"
                },
                "authors": [
                    {
                        "name": "Zexuan Yan"
                    },
                    {
                        "name": "Yue Ma"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Wenteng Chen"
                    },
                    {
                        "name": "Qifeng Chen"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages,fix figure mistake(inv/fwd skipping) in fig2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10270v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10270v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23397v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23397v1",
                "updated": "2025-03-30T11:09:06Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T11:09:06Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    11,
                    9,
                    6,
                    6,
                    89,
                    0
                ],
                "title": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FB$^+$-tree: A Memory-Optimized B$^+$-tree with Latch-Free Update"
                },
                "summary": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "B$^+$-trees are prevalent in traditional database systems due to their\nversatility and balanced structure. While binary search is typically utilized\nfor branch operations, it may lead to inefficient cache utilization in\nmain-memory scenarios. In contrast, trie-based index structures drive branch\noperations through prefix matching. While these structures generally produce\nfewer cache misses and are thus increasingly popular, they may underperform in\nrange scans because of frequent pointer chasing. This paper proposes a new\nhigh-performance B$^+$-tree variant called \\textbf{Feature B$^+$-tree\n(FB$^+$-tree)}. Similar to employing bit or byte for branch operation in tries,\nFB$^+$-tree progressively considers several bytes following the common prefix\non each level of its inner nodes\\textemdash referred to as features, which\nallows FB$^+$-tree to benefit from prefix skewness. FB$^+$-tree blurs the lines\nbetween B$^+$-trees and tries, while still retaining balance. In the best case,\nFB$^+$-tree almost becomes a trie, whereas in the worst case, it continues to\nfunction as a B$^+$-tree. Meanwhile, a crafted synchronization protocol that\ncombines the link technique and optimistic lock is designed to support\nefficient concurrent index access. Distinctively, FB$^+$-tree leverages subtle\natomic operations seamlessly coordinated with optimistic lock to facilitate\nlatch-free updates, which can be easily extended to other structures. Intensive\nexperiments on multiple workload-dataset combinations demonstrate that\nFB$^+$-tree shows comparable lookup performance to state-of-the-art trie-based\nindexes and outperforms popular B$^+$-trees by 2.3x$\\ \\sim\\ $3.7x under 96\nthreads. FB$^+$-tree also exhibits significant potential on other workloads,\nespecially update workloads under contention and scan workloads."
                },
                "authors": [
                    {
                        "name": "Yuan Chen"
                    },
                    {
                        "name": "Ao Li"
                    },
                    {
                        "name": "Wenhai Li"
                    },
                    {
                        "name": "Lingfeng Deng"
                    }
                ],
                "author_detail": {
                    "name": "Lingfeng Deng"
                },
                "author": "Lingfeng Deng",
                "arxiv_comment": "14 pages,17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23397v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23397v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23388v1",
                "updated": "2025-03-30T10:34:45Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T10:34:45Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    10,
                    34,
                    45,
                    6,
                    89,
                    0
                ],
                "title": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COSMIC: Clique-Oriented Semantic Multi-space Integration for Robust CLIP\n  Test-Time Adaptation"
                },
                "summary": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent vision-language models (VLMs) face significant challenges in test-time\nadaptation to novel domains. While cache-based methods show promise by\nleveraging historical information, they struggle with both caching unreliable\nfeature-label pairs and indiscriminately using single-class information during\nquerying, significantly compromising adaptation accuracy. To address these\nlimitations, we propose COSMIC (Clique-Oriented Semantic Multi-space\nIntegration for CLIP), a robust test-time adaptation framework that enhances\nadaptability through multi-granular, cross-modal semantic caching and\ngraph-based querying mechanisms. Our framework introduces two key innovations:\nDual Semantics Graph (DSG) and Clique Guided Hyper-class (CGH). The Dual\nSemantics Graph constructs complementary semantic spaces by incorporating\ntextual features, coarse-grained CLIP features, and fine-grained DINOv2\nfeatures to capture rich semantic relationships. Building upon these dual\ngraphs, the Clique Guided Hyper-class component leverages structured class\nrelationships to enhance prediction robustness through correlated class\nselection. Extensive experiments demonstrate COSMIC's superior performance\nacross multiple benchmarks, achieving significant improvements over\nstate-of-the-art methods: 15.81% gain on out-of-distribution tasks and 5.33% on\ncross-domain generation with CLIP RN-50. Code is available at\ngithub.com/hf618/COSMIC."
                },
                "authors": [
                    {
                        "name": "Fanding Huang"
                    },
                    {
                        "name": "Jingyan Jiang"
                    },
                    {
                        "name": "Qinting Jiang"
                    },
                    {
                        "name": "Hebei Li"
                    },
                    {
                        "name": "Faisal Nadeem Khan"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "arxiv_comment": "Accepted to CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16588v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16588v2",
                "updated": "2025-03-30T09:46:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    46,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-20T17:37:15Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    17,
                    37,
                    15,
                    3,
                    79,
                    0
                ],
                "title": "A Unified Framework for Quantitative Cache Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Quantitative Cache Analysis"
                },
                "summary": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work we unify two existing lines of work towards cache analysis for\nnon-LRU policies. To this end, we extend the notion of competitiveness to block\ncompetitiveness and systematically analyze the competitiveness and block\ncompetitiveness of FIFO and MRU relative to LRU for arbitrary associativities.\nWe show how competitiveness and block competitiveness can be exploited in\nstate-of-the-art WCET analysis based on the results of existing persistence\nanalyses for LRU. Unlike prior work, our approach is applicable to\nmicroarchitectures that exhibit timing anomalies. We experimentally evaluate\nthe precision and cost of our approach on benchmarks from TACLeBench. The\nexperiments demonstrate that quantitative cache analysis for FIFO and MRU comes\nclose to the precision of LRU."
                },
                "authors": [
                    {
                        "name": "Sophie Kahlen"
                    },
                    {
                        "name": "Jan Reineke"
                    }
                ],
                "author_detail": {
                    "name": "Jan Reineke"
                },
                "author": "Jan Reineke",
                "arxiv_comment": "Extended version of RTAS 2025 paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16588v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16588v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.3.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16897v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16897v2",
                "updated": "2025-03-30T09:19:53Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    9,
                    19,
                    53,
                    6,
                    89,
                    0
                ],
                "published": "2024-12-22T07:14:45Z",
                "published_parsed": [
                    2024,
                    12,
                    22,
                    7,
                    14,
                    45,
                    6,
                    357,
                    0
                ],
                "title": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVREC: A General Few-shot Defect Classification Model Using Multi-View\n  Region-Context"
                },
                "summary": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Few-shot defect multi-classification (FSDMC) is an emerging trend in quality\ncontrol within industrial manufacturing. However, current FSDMC research often\nlacks generalizability due to its focus on specific datasets. Additionally,\ndefect classification heavily relies on contextual information within images,\nand existing methods fall short of effectively extracting this information. To\naddress these challenges, we propose a general FSDMC framework called MVREC,\nwhich offers two primary advantages: (1) MVREC extracts general features for\ndefect instances by incorporating the pre-trained AlphaCLIP model. (2) It\nutilizes a region-context framework to enhance defect features by leveraging\nmask region input and multi-view context augmentation. Furthermore, Few-shot\nZip-Adapter(-F) classifiers within the model are introduced to cache the visual\nfeatures of the support set and perform few-shot classification. We also\nintroduce MVTec-FS, a new FSDMC benchmark based on MVTec AD, which includes\n1228 defect images with instance-level mask annotations and 46 defect types.\nExtensive experiments conducted on MVTec-FS and four additional datasets\ndemonstrate its effectiveness in general defect classification and its ability\nto incorporate contextual information to improve classification performance.\nCode: https://github.com/ShuaiLYU/MVREC"
                },
                "authors": [
                    {
                        "name": "Shuai Lyu"
                    },
                    {
                        "name": "Rongchen Zhang"
                    },
                    {
                        "name": "Zeqi Ma"
                    },
                    {
                        "name": "Fangjian Liao"
                    },
                    {
                        "name": "Dongmei Mo"
                    },
                    {
                        "name": "Waikeung Wong"
                    }
                ],
                "author_detail": {
                    "name": "Waikeung Wong"
                },
                "author": "Waikeung Wong",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16897v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16897v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12820v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12820v2",
                "updated": "2025-03-30T08:13:50Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    8,
                    13,
                    50,
                    6,
                    89,
                    0
                ],
                "published": "2024-07-01T13:05:42Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    13,
                    5,
                    42,
                    0,
                    183,
                    0
                ],
                "title": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PQCache: Product Quantization-based KVCache for Long Context LLM\n  Inference"
                },
                "summary": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the field of Large Language Models (LLMs) continues to evolve, the context\nlength in inference is steadily growing. Key-Value Cache (KVCache), the\nintermediate representations of tokens within LLM inference, has now become the\nprimary memory bottleneck due to limited GPU memory. Current methods\nselectively determine suitable keys and values for self-attention computation\nin LLMs to address the issue. However, they either fall short in maintaining\nmodel quality or result in high serving latency. Drawing inspiration from\nadvanced embedding retrieval techniques prevalent in the data management\ncommunity, we consider the storage and retrieval of KVCache as a typical\nembedding retrieval problem. We propose PQCache, which employs Product\nQuantization (PQ) to manage KVCache, maintaining model quality while ensuring\nlow serving latency. During the prefilling phase, we apply PQ to tokens' keys\nfor each LLM layer and head. During the autoregressive decoding phase, we use\nPQ codes and centroids to approximately identify important preceding tokens,\nthen fetch the corresponding key-value pairs for self-attention computation.\nThrough meticulous design of overlapping and caching, we minimize any\nadditional computation and communication overhead during both phases. Extensive\nexperiments demonstrate that PQCache achieves both effectiveness and\nefficiency, with 4.60% score improvement over existing methods on InfiniteBench\nand low system latency in both prefilling and decoding."
                },
                "authors": [
                    {
                        "name": "Hailin Zhang"
                    },
                    {
                        "name": "Xiaodong Ji"
                    },
                    {
                        "name": "Yilin Chen"
                    },
                    {
                        "name": "Fangcheng Fu"
                    },
                    {
                        "name": "Xupeng Miao"
                    },
                    {
                        "name": "Xiaonan Nie"
                    },
                    {
                        "name": "Weipeng Chen"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12820v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12820v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23294v1",
                "updated": "2025-03-30T03:20:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-30T03:20:34Z",
                "published_parsed": [
                    2025,
                    3,
                    30,
                    3,
                    20,
                    34,
                    6,
                    89,
                    0
                ],
                "title": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cocktail: Chunk-Adaptive Mixed-Precision Quantization for Long-Context\n  LLM Inference"
                },
                "summary": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have been able to handle longer and\nlonger contexts. However, a context that is too long may cause intolerant\ninference latency and GPU memory usage. Existing methods propose\nmixed-precision quantization to the key-value (KV) cache in LLMs based on token\ngranularity, which is time-consuming in the search process and hardware\ninefficient during computation. This paper introduces a novel approach called\nCocktail, which employs chunk-adaptive mixed-precision quantization to optimize\nthe KV cache. Cocktail consists of two modules: chunk-level quantization search\nand chunk-level KV cache computation. Chunk-level quantization search\ndetermines the optimal bitwidth configuration of the KV cache chunks quickly\nbased on the similarity scores between the corresponding context chunks and the\nquery, maintaining the model accuracy. Furthermore, chunk-level KV cache\ncomputation reorders the KV cache chunks before quantization, avoiding the\nhardware inefficiency caused by mixed-precision quantization in inference\ncomputation. Extensive experiments demonstrate that Cocktail outperforms\nstate-of-the-art KV cache quantization methods on various models and datasets."
                },
                "authors": [
                    {
                        "name": "Wei Tao"
                    },
                    {
                        "name": "Bin Zhang"
                    },
                    {
                        "name": "Xiaoyang Qu"
                    },
                    {
                        "name": "Jiguang Wan"
                    },
                    {
                        "name": "Jianzong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jianzong Wang"
                },
                "author": "Jianzong Wang",
                "arxiv_comment": "Accepted by the Design, Automation, and Test in Europe 2025 (DATE\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11816v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11816v2",
                "updated": "2025-03-30T02:45:00Z",
                "updated_parsed": [
                    2025,
                    3,
                    30,
                    2,
                    45,
                    0,
                    6,
                    89,
                    0
                ],
                "published": "2025-03-14T19:02:16Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    19,
                    2,
                    16,
                    4,
                    73,
                    0
                ],
                "title": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key, Value, Compress: A Systematic Exploration of KV Cache Compression\n  Techniques"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional capabilities in\ngenerating text, images, and video content. However, as context length grows,\nthe computational cost of attention increases quadratically with the number of\ntokens, presenting significant efficiency challenges. This paper presents an\nanalysis of various Key-Value (KV) cache compression strategies, offering a\ncomprehensive taxonomy that categorizes these methods by their underlying\nprinciples and implementation techniques. Furthermore, we evaluate their impact\non performance and inference latency, providing critical insights into their\neffectiveness. Our findings highlight the trade-offs involved in KV cache\ncompression and its influence on handling long-context scenarios, paving the\nway for more efficient LLM implementations."
                },
                "authors": [
                    {
                        "name": "Neusha Javidnia"
                    },
                    {
                        "name": "Bita Darvish Rouhani"
                    },
                    {
                        "name": "Farinaz Koushanfar"
                    }
                ],
                "author_detail": {
                    "name": "Farinaz Koushanfar"
                },
                "author": "Farinaz Koushanfar",
                "arxiv_comment": "Invited paper to IEEE Custom Integrated Circuits Conference (CICC)\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11816v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11816v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18278v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18278v2",
                "updated": "2025-03-29T23:00:27Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    23,
                    0,
                    27,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-24T01:47:26Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    1,
                    47,
                    26,
                    0,
                    83,
                    0
                ],
                "title": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TopV: Compatible Token Pruning with Inference Time Optimization for Fast\n  and Low-Memory Multimodal Vision Language Model"
                },
                "summary": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Models (VLMs) demand substantial computational resources\nduring inference, largely due to the extensive visual input tokens for\nrepresenting visual information. Previous studies have noted that visual tokens\ntend to receive less attention than text tokens, suggesting their lower\nimportance during inference and potential for pruning. However, their methods\nencounter several challenges: reliance on greedy heuristic criteria for token\nimportance and incompatibility with FlashAttention and KV cache. To address\nthese issues, we introduce \\textbf{TopV}, a compatible \\textbf{TO}ken\n\\textbf{P}runing with inference Time Optimization for fast and low-memory\n\\textbf{V}LM, achieving efficient pruning without additional training or\nfine-tuning. Instead of relying on attention scores, we formulate token pruning\nas an optimization problem, accurately identifying important visual tokens\nwhile remaining compatible with FlashAttention. Additionally, since we only\nperform this pruning once during the prefilling stage, it effectively reduces\nKV cache size. Our optimization framework incorporates a visual-aware cost\nfunction considering factors such as Feature Similarity, Relative Spatial\nDistance, and Absolute Central Distance, to measure the importance of each\nsource visual token, enabling effective pruning of low-importance tokens.\nExtensive experiments demonstrate that our method outperforms previous token\npruning methods, validating the effectiveness and efficiency of our approach."
                },
                "authors": [
                    {
                        "name": "Cheng Yang"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Jinqi Xiao"
                    },
                    {
                        "name": "Lingyi Huang"
                    },
                    {
                        "name": "Yu Gong"
                    },
                    {
                        "name": "Chendi Li"
                    },
                    {
                        "name": "Jinghua Yan"
                    },
                    {
                        "name": "Yu Bai"
                    },
                    {
                        "name": "Ponnuswamy Sadayappan"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Bo Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Bo Yuan"
                },
                "author": "Bo Yuan",
                "arxiv_comment": "Accepted by CVPR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18278v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18278v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11132v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11132v2",
                "updated": "2025-03-29T04:43:11Z",
                "updated_parsed": [
                    2025,
                    3,
                    29,
                    4,
                    43,
                    11,
                    5,
                    88,
                    0
                ],
                "published": "2025-03-14T06:49:37Z",
                "published_parsed": [
                    2025,
                    3,
                    14,
                    6,
                    49,
                    37,
                    4,
                    73,
                    0
                ],
                "title": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "X-EcoMLA: Upcycling Pre-Trained Attention into MLA for Efficient and\n  Extreme KV Compression"
                },
                "summary": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-head latent attention (MLA) is designed to optimize KV cache memory\nthrough low-rank key-value joint compression. Rather than caching keys and\nvalues separately, MLA stores their compressed latent representations, reducing\nmemory overhead while maintaining the performance. While MLA improves memory\nefficiency without compromising language model accuracy, its major limitation\nlies in its integration during the pre-training phase, requiring models to be\ntrained from scratch. This raises a key question: can we use MLA's benefits\nfully or partially in models that have already been pre-trained with different\nattention mechanisms? In this paper, we propose X-EcoMLA to deploy post\ntraining distillation to enable the upcycling of Transformer-based attention\ninto an efficient hybrid MLA variant through lightweight post-training\nadaptation, bypassing the need for extensive pre-training. We demonstrate that\nleveraging the dark knowledge of a well-trained model can enhance training\naccuracy and enable extreme KV cache compression in MLA without compromising\nmodel performance. The experimental results show that our proposed method can\neffectively compress the KV cache while preserving the performance on the\nbenchmarks; specifically, for Llama3.2-1B-Instruct baseline, a 6.4x compression\nachieves the same average score by using only 3.6B training tokens and 70 GPU\nhours on AMD MI300, whereas a 10.6x compression have less than 0.1\\% average\nscore drop with 7B training tokens and 140 GPU hours."
                },
                "authors": [
                    {
                        "name": "Guihong Li"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    },
                    {
                        "name": "Mingyu Yang"
                    },
                    {
                        "name": "Vikram Appia"
                    },
                    {
                        "name": "Emad Barsoum"
                    }
                ],
                "author_detail": {
                    "name": "Emad Barsoum"
                },
                "author": "Emad Barsoum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11132v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11132v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22796v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22796v1",
                "updated": "2025-03-28T18:00:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T18:00:12Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    18,
                    0,
                    12,
                    4,
                    87,
                    0
                ],
                "title": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiTFastAttnV2: Head-wise Attention Compression for Multi-Modality\n  Diffusion Transformers"
                },
                "summary": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation models, especially Multimodal Diffusion Transformers\n(MMDiT), have shown remarkable progress in generating high-quality images.\nHowever, these models often face significant computational bottlenecks,\nparticularly in attention mechanisms, which hinder their scalability and\nefficiency. In this paper, we introduce DiTFastAttnV2, a post-training\ncompression method designed to accelerate attention in MMDiT. Through an\nin-depth analysis of MMDiT's attention patterns, we identify key differences\nfrom prior DiT-based methods and propose head-wise arrow attention and caching\nmechanisms to dynamically adjust attention heads, effectively bridging this\ngap. We also design an Efficient Fused Kernel for further acceleration. By\nleveraging local metric methods and optimization techniques, our approach\nsignificantly reduces the search time for optimal compression schemes to just\nminutes while maintaining generation quality. Furthermore, with the customized\nkernel, DiTFastAttnV2 achieves a 68% reduction in attention FLOPs and 1.5x\nend-to-end speedup on 2K image generation without compromising visual fidelity."
                },
                "authors": [
                    {
                        "name": "Hanling Zhang"
                    },
                    {
                        "name": "Rundong Su"
                    },
                    {
                        "name": "Zhihang Yuan"
                    },
                    {
                        "name": "Pengtao Chen"
                    },
                    {
                        "name": "Mingzhu Shen Yibo Fan"
                    },
                    {
                        "name": "Shengen Yan"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22796v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22796v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v3",
                "updated": "2025-03-28T16:15:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    16,
                    15,
                    19,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Stabilized and Efficient Diffusion Transformers through\n  Long-Skip-Connections with Spectral Constraints"
                },
                "summary": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT) have emerged as a powerful architecture for\nimage and video generation, offering superior quality and scalability. However,\ntheir practical application suffers from inherent dynamic feature instability,\nleading to error amplification during cached inference. Through systematic\nanalysis, we identify the absence of long-range feature preservation mechanisms\nas the root cause of unstable feature propagation and perturbation sensitivity.\nTo this end, we propose Skip-DiT, a novel DiT variant enhanced with\nLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.\nTheoretical spectral norm and visualization analysis demonstrate how LSCs\nstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamic\nfeature enable an efficient statical caching mechanism that reuses deep\nfeatures across timesteps while updating shallow components. Extensive\nexperiments across image and video generation tasks demonstrate that Skip-DiT\nachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2\ntimes inference acceleration without quality loss and high fidelity to original\noutput, outperforming existing DiT caching methods across various quantitative\nmetrics. Our findings establish long-skip connections as critical architectural\ncomponents for training stable and efficient diffusion transformers."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Xiaoye Qu"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v3",
                "updated": "2025-03-28T14:11:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    14,
                    11,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22329v1",
                "updated": "2025-03-28T11:08:34Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T11:08:34Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    11,
                    8,
                    34,
                    4,
                    87,
                    0
                ],
                "title": "A Refined Analysis of Massive Activations in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Refined Analysis of Massive Activations in LLMs"
                },
                "summary": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated in part by their relevance for low-precision training and\nquantization, massive activations in large language models (LLMs) have recently\nemerged as a topic of interest. However, existing analyses are limited in\nscope, and generalizability across architectures is unclear. This paper helps\naddress some of these gaps by conducting an analysis of massive activations\nacross a broad range of LLMs, including both GLU-based and non-GLU-based\narchitectures. Our findings challenge several prior assumptions, most\nimportantly: (1) not all massive activations are detrimental, i.e. suppressing\nthem does not lead to an explosion of perplexity or a collapse in downstream\ntask performance; (2) proposed mitigation strategies such as Attention KV bias\nare model-specific and ineffective in certain cases. We consequently\ninvestigate novel hybrid mitigation strategies; in particular pairing Target\nVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)\nsuccessfully balances the mitigation of massive activations with preserved\ndownstream model performance in the scenarios we investigated. Our code is\navailable at: https://github.com/bluorion-com/refine_massive_activations."
                },
                "authors": [
                    {
                        "name": "Louis Owen"
                    },
                    {
                        "name": "Nilabhra Roy Chowdhury"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Fabian Gra"
                    }
                ],
                "author_detail": {
                    "name": "Fabian Gra"
                },
                "author": "Fabian Gra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22196v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22196v1",
                "updated": "2025-03-28T07:26:37Z",
                "updated_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "published": "2025-03-28T07:26:37Z",
                "published_parsed": [
                    2025,
                    3,
                    28,
                    7,
                    26,
                    37,
                    4,
                    87,
                    0
                ],
                "title": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge\n  Devices"
                },
                "summary": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based large language models (LLMs) encounter challenges in\nprocessing long sequences on edge devices due to the quadratic complexity of\nattention mechanisms and growing memory demands from Key-Value (KV) cache.\nExisting KV cache optimizations struggle with irreversible token eviction in\nlong-output tasks, while alternative sequence modeling architectures prove\ncostly to adopt within established Transformer infrastructure. We present\nEdgeInfinite, a memory-efficient solution for infinite contexts that integrates\ncompressed memory into Transformer-based LLMs through a trainable memory-gating\nmodule. This approach maintains full compatibility with standard Transformer\narchitectures, requiring fine-tuning only a small part of parameters, and\nenables selective activation of the memory-gating module for long and short\ncontext task routing. The experimental result shows that EdgeInfinite achieves\ncomparable performance to baseline Transformer-based LLM on long context\nbenchmarks while optimizing memory consumption and time to first token."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Renshou Wu"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoxin Chen"
                },
                "author": "Xiaoxin Chen",
                "arxiv_comment": "8 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22196v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22196v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.22017v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.22017v1",
                "updated": "2025-03-27T22:16:57Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T22:16:57Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    22,
                    16,
                    57,
                    3,
                    86,
                    0
                ],
                "title": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance Characterizations and Usage Guidelines of Samsung CXL Memory\n  Module Hybrid Prototype"
                },
                "summary": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing prevalence of data-intensive workloads, such as artificial\nintelligence (AI), machine learning (ML), high-performance computing (HPC),\nin-memory databases, and real-time analytics, has exposed limitations in\nconventional memory technologies like DRAM. While DRAM offers low latency and\nhigh throughput, it is constrained by high costs, scalability challenges, and\nvolatility, making it less viable for capacity-bound and persistent\napplications in modern datacenters.\n  Recently, Compute Express Link (CXL) has emerged as a promising alternative,\nenabling high-speed, cacheline-granular communication between CPUs and external\ndevices. By leveraging CXL technology, NAND flash can now be used as memory\nexpansion, offering three-fold benefits: byte-addressability, scalable\ncapacity, and persistence at a low cost. Samsung's CXL Memory Module Hybrid\n(CMM-H) is the first product to deliver these benefits through a hardware-only\nsolution, i.e., it does not incur any OS and IO overheads like conventional\nblock devices. In particular, CMM-H integrates a DRAM cache with NAND flash in\na single device to deliver near-DRAM latency. This paper presents the first\npublicly available study for comprehensive characterizations of an FPGA-based\nCMM-H prototype. Through this study, we address users' concerns about whether a\nwide variety of applications can successfully run on a memory device backed by\nNAND flash medium. Additionally, based on these characterizations, we provide\nkey insights into how to best take advantage of the CMM-H device."
                },
                "authors": [
                    {
                        "name": "Jianping Zeng"
                    },
                    {
                        "name": "Shuyi Pei"
                    },
                    {
                        "name": "Da Zhang"
                    },
                    {
                        "name": "Yuchen Zhou"
                    },
                    {
                        "name": "Amir Beygi"
                    },
                    {
                        "name": "Xuebin Yao"
                    },
                    {
                        "name": "Ramdas Kachare"
                    },
                    {
                        "name": "Tong Zhang"
                    },
                    {
                        "name": "Zongwang Li"
                    },
                    {
                        "name": "Marie Nguyen"
                    },
                    {
                        "name": "Rekha Pitchumani"
                    },
                    {
                        "name": "Yang Soek Ki"
                    },
                    {
                        "name": "Changhee Jung"
                    }
                ],
                "author_detail": {
                    "name": "Changhee Jung"
                },
                "author": "Changhee Jung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.22017v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.22017v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18869v2",
                "updated": "2025-03-27T17:48:14Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    48,
                    14,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-24T16:44:32Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    16,
                    44,
                    32,
                    0,
                    83,
                    0
                ],
                "title": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reimagining Memory Access for LLM Inference: Compression-Aware Memory\n  Controller Design"
                },
                "summary": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficiency of Large Language Model~(LLM) inference is often constrained\nby substantial memory bandwidth and capacity demands. Existing techniques, such\nas pruning, quantization, and mixture of experts/depth, reduce memory capacity\nand/or bandwidth consumption at the cost of slight degradation in inference\nquality. This paper introduces a design solution that further alleviates memory\nbottlenecks by enhancing the on-chip memory controller in AI accelerators to\nachieve two main objectives: (1) significantly reducing memory capacity and\nbandwidth usage through lossless block compression~(e.g., LZ4 and ZSTD) of\nmodel weights and key-value (KV) cache without compromising inference quality,\nand (2) enabling memory bandwidth and energy consumption to scale\nproportionally with context-dependent dynamic quantization. These goals are\naccomplished by equipping the on-chip memory controller with mechanisms to\nimprove fine-grained bit-level accessibility and compressibility of weights and\nKV cache through LLM-aware configuration of in-memory placement and\nrepresentation. Experimental results on publicly available LLMs demonstrate the\neffectiveness of this approach, showing memory footprint reductions of 25.2\\%\nfor model weights and 46.9\\% for KV cache. In addition, our hardware prototype\nat 4\\,GHz and 32 lanes (7\\,nm) achieves 8\\,TB/s throughput with a modest area\noverhead (under 3.8\\,mm\\(^2\\)), which underscores the viability of LLM-aware\nmemory control as a key to efficient large-scale inference."
                },
                "authors": [
                    {
                        "name": "Rui Xie"
                    },
                    {
                        "name": "Asad Ul Haq"
                    },
                    {
                        "name": "Linsen Ma"
                    },
                    {
                        "name": "Yunhua Fang"
                    },
                    {
                        "name": "Zirak Burzin Engineer"
                    },
                    {
                        "name": "Liu Liu"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Tong Zhang"
                },
                "author": "Tong Zhang",
                "arxiv_comment": "9 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v1",
                "updated": "2025-03-27T17:37:12Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_comment": "34 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.21725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v4",
                "updated": "2025-03-27T15:21:19Z",
                "updated_parsed": [
                    2025,
                    3,
                    27,
                    15,
                    21,
                    19,
                    3,
                    86,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.08727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08727v1",
                "updated": "2025-04-11T17:55:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    55,
                    45,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T17:55:45Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    55,
                    45,
                    4,
                    101,
                    0
                ],
                "title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images"
                },
                "summary": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles."
                },
                "authors": [
                    {
                        "name": "Boyang Deng"
                    },
                    {
                        "name": "Songyou Peng"
                    },
                    {
                        "name": "Kyle Genova"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    },
                    {
                        "name": "Noah Snavely"
                    },
                    {
                        "name": "Leonidas Guibas"
                    },
                    {
                        "name": "Thomas Funkhouser"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Funkhouser"
                },
                "author": "Thomas Funkhouser",
                "arxiv_comment": "Project page: https://boyangdeng.com/visual-chronicles; second and\n  third listed authors have equal contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08725v1",
                "updated": "2025-04-11T17:50:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    50,
                    8,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T17:50:08Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    50,
                    8,
                    4,
                    101,
                    0
                ],
                "title": "DocAgent: A Multi-Agent System for Automated Code Documentation\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocAgent: A Multi-Agent System for Automated Code Documentation\n  Generation"
                },
                "summary": "High-quality code documentation is crucial for software development\nespecially in the era of AI. However, generating it automatically using Large\nLanguage Models (LLMs) remains challenging, as existing approaches often\nproduce incomplete, unhelpful, or factually incorrect outputs. We introduce\nDocAgent, a novel multi-agent collaborative system using topological code\nprocessing for incremental context building. Specialized agents (Reader,\nSearcher, Writer, Verifier, Orchestrator) then collaboratively generate\ndocumentation. We also propose a multi-faceted evaluation framework assessing\nCompleteness, Helpfulness, and Truthfulness. Comprehensive experiments show\nDocAgent significantly outperforms baselines consistently. Our ablation study\nconfirms the vital role of the topological processing order. DocAgent offers a\nrobust approach for reliable code documentation generation in complex and\nproprietary repositories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality code documentation is crucial for software development\nespecially in the era of AI. However, generating it automatically using Large\nLanguage Models (LLMs) remains challenging, as existing approaches often\nproduce incomplete, unhelpful, or factually incorrect outputs. We introduce\nDocAgent, a novel multi-agent collaborative system using topological code\nprocessing for incremental context building. Specialized agents (Reader,\nSearcher, Writer, Verifier, Orchestrator) then collaboratively generate\ndocumentation. We also propose a multi-faceted evaluation framework assessing\nCompleteness, Helpfulness, and Truthfulness. Comprehensive experiments show\nDocAgent significantly outperforms baselines consistently. Our ablation study\nconfirms the vital role of the topological processing order. DocAgent offers a\nrobust approach for reliable code documentation generation in complex and\nproprietary repositories."
                },
                "authors": [
                    {
                        "name": "Dayu Yang"
                    },
                    {
                        "name": "Antoine Simoulin"
                    },
                    {
                        "name": "Xin Qian"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Yuwei Cao"
                    },
                    {
                        "name": "Zhaopu Teng"
                    },
                    {
                        "name": "Grey Yang"
                    }
                ],
                "author_detail": {
                    "name": "Grey Yang"
                },
                "author": "Grey Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08719v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08719v1",
                "updated": "2025-04-11T17:33:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    33,
                    32,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T17:33:32Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    33,
                    32,
                    4,
                    101,
                    0
                ],
                "title": "SWAN-GPT: An Efficient and Scalable Approach for Long-Context Language\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SWAN-GPT: An Efficient and Scalable Approach for Long-Context Language\n  Modeling"
                },
                "summary": "We present a decoder-only Transformer architecture that robustly generalizes\nto sequence lengths substantially longer than those seen during training. Our\nmodel, SWAN-GPT, interleaves layers without positional encodings (NoPE) and\nsliding-window attention layers equipped with rotary positional encodings\n(SWA-RoPE). Experiments demonstrate strong performance on sequence lengths\nsignificantly longer than the training length without the need for additional\nlong-context training. This robust length extrapolation is achieved through our\nnovel architecture, enhanced by a straightforward dynamic scaling of attention\nscores during inference. In addition, SWAN-GPT is more computationally\nefficient than standard GPT architectures, resulting in cheaper training and\nhigher throughput. Further, we demonstrate that existing pre-trained\ndecoder-only models can be efficiently converted to the SWAN architecture with\nminimal continued training, enabling longer contexts. Overall, our work\npresents an effective approach for scaling language models to longer contexts\nin a robust and efficient manner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a decoder-only Transformer architecture that robustly generalizes\nto sequence lengths substantially longer than those seen during training. Our\nmodel, SWAN-GPT, interleaves layers without positional encodings (NoPE) and\nsliding-window attention layers equipped with rotary positional encodings\n(SWA-RoPE). Experiments demonstrate strong performance on sequence lengths\nsignificantly longer than the training length without the need for additional\nlong-context training. This robust length extrapolation is achieved through our\nnovel architecture, enhanced by a straightforward dynamic scaling of attention\nscores during inference. In addition, SWAN-GPT is more computationally\nefficient than standard GPT architectures, resulting in cheaper training and\nhigher throughput. Further, we demonstrate that existing pre-trained\ndecoder-only models can be efficiently converted to the SWAN architecture with\nminimal continued training, enabling longer contexts. Overall, our work\npresents an effective approach for scaling language models to longer contexts\nin a robust and efficient manner."
                },
                "authors": [
                    {
                        "name": "Krishna C. Puvvada"
                    },
                    {
                        "name": "Faisal Ladhak"
                    },
                    {
                        "name": "Santiago Akle Serrano"
                    },
                    {
                        "name": "Cheng-Ping Hsieh"
                    },
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Somshubra Majumdar"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Samuel Kriman"
                    },
                    {
                        "name": "Simeng Sun"
                    },
                    {
                        "name": "Dima Rekesh"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08719v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08719v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08718v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08718v1",
                "updated": "2025-04-11T17:30:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    30,
                    46,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T17:30:46Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    30,
                    46,
                    4,
                    101,
                    0
                ],
                "title": "EMO-X: Efficient Multi-Person Pose and Shape Estimation in One-Stage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMO-X: Efficient Multi-Person Pose and Shape Estimation in One-Stage"
                },
                "summary": "Expressive Human Pose and Shape Estimation (EHPS) aims to jointly estimate\nhuman pose, hand gesture, and facial expression from monocular images. Existing\nmethods predominantly rely on Transformer-based architectures, which suffer\nfrom quadratic complexity in self-attention, leading to substantial\ncomputational overhead, especially in multi-person scenarios. Recently, Mamba\nhas emerged as a promising alternative to Transformers due to its efficient\nglobal modeling capability. However, it remains limited in capturing\nfine-grained local dependencies, which are essential for precise EHPS. To\naddress these issues, we propose EMO-X, the Efficient Multi-person One-stage\nmodel for multi-person EHPS. Specifically, we explore a Scan-based Global-Local\nDecoder (SGLD) that integrates global context with skeleton-aware local\nfeatures to iteratively enhance human tokens. Our EMO-X leverages the superior\nglobal modeling capability of Mamba and designs a local bidirectional scan\nmechanism for skeleton-aware local refinement. Comprehensive experiments\ndemonstrate that EMO-X strikes an excellent balance between efficiency and\naccuracy. Notably, it achieves a significant reduction in computational\ncomplexity, requiring 69.8% less inference time compared to state-of-the-art\n(SOTA) methods, while outperforming most of them in accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Expressive Human Pose and Shape Estimation (EHPS) aims to jointly estimate\nhuman pose, hand gesture, and facial expression from monocular images. Existing\nmethods predominantly rely on Transformer-based architectures, which suffer\nfrom quadratic complexity in self-attention, leading to substantial\ncomputational overhead, especially in multi-person scenarios. Recently, Mamba\nhas emerged as a promising alternative to Transformers due to its efficient\nglobal modeling capability. However, it remains limited in capturing\nfine-grained local dependencies, which are essential for precise EHPS. To\naddress these issues, we propose EMO-X, the Efficient Multi-person One-stage\nmodel for multi-person EHPS. Specifically, we explore a Scan-based Global-Local\nDecoder (SGLD) that integrates global context with skeleton-aware local\nfeatures to iteratively enhance human tokens. Our EMO-X leverages the superior\nglobal modeling capability of Mamba and designs a local bidirectional scan\nmechanism for skeleton-aware local refinement. Comprehensive experiments\ndemonstrate that EMO-X strikes an excellent balance between efficiency and\naccuracy. Notably, it achieves a significant reduction in computational\ncomplexity, requiring 69.8% less inference time compared to state-of-the-art\n(SOTA) methods, while outperforming most of them in accuracy."
                },
                "authors": [
                    {
                        "name": "Haohang Jian"
                    },
                    {
                        "name": "Jinlu Zhang"
                    },
                    {
                        "name": "Junyi Wu"
                    },
                    {
                        "name": "Zhigang Tu"
                    }
                ],
                "author_detail": {
                    "name": "Zhigang Tu"
                },
                "author": "Zhigang Tu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08718v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08718v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08716v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08716v1",
                "updated": "2025-04-11T17:29:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    29,
                    35,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T17:29:35Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    29,
                    35,
                    4,
                    101,
                    0
                ],
                "title": "ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on\n  Transformer Encoder Models Performance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on\n  Transformer Encoder Models Performance"
                },
                "summary": "Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce\narchitectural advancements aimed at improving efficiency and performance.\nAlthough the authors of ModernBERT report improved performance over DeBERTaV3\non several benchmarks, the lack of disclosed training data and the absence of\ncomparisons using a shared dataset make it difficult to determine whether these\ngains are due to architectural improvements or differences in training data. In\nthis work, we conduct a controlled study by pretraining ModernBERT on the same\ndataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of\nmodel design. Our results show that the previous model generation remains\nsuperior in sample efficiency and overall benchmark performance, with\nModernBERT's primary advantage being faster training and inference speed.\nHowever, the new proposed model still provides meaningful architectural\nimprovements compared to earlier models such as BERT and RoBERTa. Additionally,\nwe observe that high-quality pre-training data accelerates convergence but does\nnot significantly improve final performance, suggesting potential benchmark\nsaturation. These findings show the importance of disentangling pretraining\ndata from architectural innovations when evaluating transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce\narchitectural advancements aimed at improving efficiency and performance.\nAlthough the authors of ModernBERT report improved performance over DeBERTaV3\non several benchmarks, the lack of disclosed training data and the absence of\ncomparisons using a shared dataset make it difficult to determine whether these\ngains are due to architectural improvements or differences in training data. In\nthis work, we conduct a controlled study by pretraining ModernBERT on the same\ndataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of\nmodel design. Our results show that the previous model generation remains\nsuperior in sample efficiency and overall benchmark performance, with\nModernBERT's primary advantage being faster training and inference speed.\nHowever, the new proposed model still provides meaningful architectural\nimprovements compared to earlier models such as BERT and RoBERTa. Additionally,\nwe observe that high-quality pre-training data accelerates convergence but does\nnot significantly improve final performance, suggesting potential benchmark\nsaturation. These findings show the importance of disentangling pretraining\ndata from architectural innovations when evaluating transformer models."
                },
                "authors": [
                    {
                        "name": "Wissam Antoun"
                    },
                    {
                        "name": "Benot Sagot"
                    },
                    {
                        "name": "Djam Seddah"
                    }
                ],
                "author_detail": {
                    "name": "Djam Seddah"
                },
                "author": "Djam Seddah",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08716v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08716v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08714v1",
                "updated": "2025-04-11T17:24:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    24,
                    58,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T17:24:58Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    24,
                    58,
                    4,
                    101,
                    0
                ],
                "title": "Generating Fine Details of Entity Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Fine Details of Entity Interactions"
                },
                "summary": "Images not only depict objects but also encapsulate rich interactions between\nthem. However, generating faithful and high-fidelity images involving multiple\nentities interacting with each other, is a long-standing challenge. While\npre-trained text-to-image models are trained on large-scale datasets to follow\ndiverse text instructions, they struggle to generate accurate interactions,\nlikely due to the scarcity of training data for uncommon object interactions.\nThis paper introduces InterActing, an interaction-focused dataset with 1000\nfine-grained prompts covering three key scenarios: (1) functional and\naction-based interactions, (2) compositional spatial relationships, and (3)\nmulti-subject interactions. To address interaction generation challenges, we\npropose a decomposition-augmented refinement procedure. Our approach,\nDetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose\ninteractions into finer-grained concepts, uses a VLM to critique generated\nimages, and applies targeted interventions within the diffusion process in\nrefinement. Automatic and human evaluations show significantly improved image\nquality, demonstrating the potential of enhanced inference strategies. Our\ndataset and code are available at https://concepts-ai.com/p/detailscribe/ to\nfacilitate future exploration of interaction-rich image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Images not only depict objects but also encapsulate rich interactions between\nthem. However, generating faithful and high-fidelity images involving multiple\nentities interacting with each other, is a long-standing challenge. While\npre-trained text-to-image models are trained on large-scale datasets to follow\ndiverse text instructions, they struggle to generate accurate interactions,\nlikely due to the scarcity of training data for uncommon object interactions.\nThis paper introduces InterActing, an interaction-focused dataset with 1000\nfine-grained prompts covering three key scenarios: (1) functional and\naction-based interactions, (2) compositional spatial relationships, and (3)\nmulti-subject interactions. To address interaction generation challenges, we\npropose a decomposition-augmented refinement procedure. Our approach,\nDetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose\ninteractions into finer-grained concepts, uses a VLM to critique generated\nimages, and applies targeted interventions within the diffusion process in\nrefinement. Automatic and human evaluations show significantly improved image\nquality, demonstrating the potential of enhanced inference strategies. Our\ndataset and code are available at https://concepts-ai.com/p/detailscribe/ to\nfacilitate future exploration of interaction-rich image generation."
                },
                "authors": [
                    {
                        "name": "Xinyi Gu"
                    },
                    {
                        "name": "Jiayuan Mao"
                    }
                ],
                "author_detail": {
                    "name": "Jiayuan Mao"
                },
                "author": "Jiayuan Mao",
                "arxiv_comment": "Project Page: https://concepts-ai.com/p/detailscribe/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08700v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08700v1",
                "updated": "2025-04-11T17:07:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    7,
                    9,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T17:07:09Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    7,
                    9,
                    4,
                    101,
                    0
                ],
                "title": "Don't torque like that. Measuring compact object magnetic fields with\n  analytic torque models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't torque like that. Measuring compact object magnetic fields with\n  analytic torque models"
                },
                "summary": "Context. Changes of the rotational period observed in various magnetized\naccreting sources are generally attributed to the interaction between the\nin-falling plasma and the large-scale magnetic field of the accretor. A number\nof models have been proposed to link these changes to the mass accretion rate,\nbased on different assumptions on the relevant physical processes and system\nparameters. For X-ray binaries with neutron stars, with the help of precise\nmeasurements of the spin periods provided by current instrumentation, these\nmodels render a way to infer such parameters as the strength of the dipolar\nfield and a distance to the system. Often, the obtained magnetic field strength\nvalues contradict those from other methods used to obtain magnetic field\nestimates.\n  Aims. We want to compare the results of several of the proposed accretion\nmodels. To this end an example application of these models to data is\nperformed.\n  Methods. We reformulate the set of disk accretion torque models in a way that\ntheir parametrization are directly comparable. The application of the\nreformulated models is discussed and demonstrated using Fermi/GBM and Swift/BAT\nmonitoring data covering several X-ray outbursts of the accreting pulsar 4U\n0115+63.\n  Results. We find that most of the models under consideration are able to\ndescribe the observations to a high degree of accuracy and with little\nindication for one model being preferred over the others. Yet, derived\nparameters from those models show a large spread. Specifically the magnetic\nfield strength ranges over one order of magnitude for the different models.\nThis indicates that the results are heavily influenced by systematic\nuncertainties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context. Changes of the rotational period observed in various magnetized\naccreting sources are generally attributed to the interaction between the\nin-falling plasma and the large-scale magnetic field of the accretor. A number\nof models have been proposed to link these changes to the mass accretion rate,\nbased on different assumptions on the relevant physical processes and system\nparameters. For X-ray binaries with neutron stars, with the help of precise\nmeasurements of the spin periods provided by current instrumentation, these\nmodels render a way to infer such parameters as the strength of the dipolar\nfield and a distance to the system. Often, the obtained magnetic field strength\nvalues contradict those from other methods used to obtain magnetic field\nestimates.\n  Aims. We want to compare the results of several of the proposed accretion\nmodels. To this end an example application of these models to data is\nperformed.\n  Methods. We reformulate the set of disk accretion torque models in a way that\ntheir parametrization are directly comparable. The application of the\nreformulated models is discussed and demonstrated using Fermi/GBM and Swift/BAT\nmonitoring data covering several X-ray outbursts of the accreting pulsar 4U\n0115+63.\n  Results. We find that most of the models under consideration are able to\ndescribe the observations to a high degree of accuracy and with little\nindication for one model being preferred over the others. Yet, derived\nparameters from those models show a large spread. Specifically the magnetic\nfield strength ranges over one order of magnitude for the different models.\nThis indicates that the results are heavily influenced by systematic\nuncertainties."
                },
                "authors": [
                    {
                        "name": "J. J. R. Stierhof"
                    },
                    {
                        "name": "E. Sokolova-Lapa"
                    },
                    {
                        "name": "K. Berger"
                    },
                    {
                        "name": "G. Vasilopoulos"
                    },
                    {
                        "name": "P. Thalhammer"
                    },
                    {
                        "name": "N. Zalot"
                    },
                    {
                        "name": "R. Ballhausen"
                    },
                    {
                        "name": "I. El Mellah"
                    },
                    {
                        "name": "C. Malacaria"
                    },
                    {
                        "name": "R. E. Rothschild"
                    },
                    {
                        "name": "P. Kretschmar"
                    },
                    {
                        "name": "K. Pottschmidt"
                    },
                    {
                        "name": "J. Wilms"
                    }
                ],
                "author_detail": {
                    "name": "J. Wilms"
                },
                "arxiv_affiliation": "Karl Remeis-Sternwarte and Erlangen Centre for Astroparticle Physics, Friedrich-Alexander-Universitt Erlangen-Nrnberg",
                "author": "J. Wilms",
                "arxiv_comment": "Accepted for publication in A&A, 20 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08700v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08700v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08697v1",
                "updated": "2025-04-11T17:04:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    4,
                    51,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T17:04:51Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    4,
                    51,
                    4,
                    101,
                    0
                ],
                "title": "Large Language Models as Span Annotators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Span Annotators"
                },
                "summary": "For high-quality texts, single-score metrics seldom provide actionable\nfeedback. In contrast, span annotation - pointing out issues in the text by\nannotating their spans - can guide improvements and provide insights. Until\nrecently, span annotation was limited to human annotators or fine-tuned encoder\nmodels. In this study, we automate span annotation with large language models\n(LLMs). We compare expert or skilled crowdworker annotators with open and\nproprietary LLMs on three tasks: data-to-text generation evaluation, machine\ntranslation evaluation, and propaganda detection in human-written texts. In our\nexperiments, we show that LLMs as span annotators are straightforward to\nimplement and notably more cost-efficient than human annotators. The LLMs\nachieve moderate agreement with skilled human annotators, in some scenarios\ncomparable to the average agreement among the annotators themselves.\nQualitative analysis shows that reasoning models outperform their\ninstruction-tuned counterparts and provide more valid explanations for\nannotations. We release the dataset of more than 40k model and human\nannotations for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For high-quality texts, single-score metrics seldom provide actionable\nfeedback. In contrast, span annotation - pointing out issues in the text by\nannotating their spans - can guide improvements and provide insights. Until\nrecently, span annotation was limited to human annotators or fine-tuned encoder\nmodels. In this study, we automate span annotation with large language models\n(LLMs). We compare expert or skilled crowdworker annotators with open and\nproprietary LLMs on three tasks: data-to-text generation evaluation, machine\ntranslation evaluation, and propaganda detection in human-written texts. In our\nexperiments, we show that LLMs as span annotators are straightforward to\nimplement and notably more cost-efficient than human annotators. The LLMs\nachieve moderate agreement with skilled human annotators, in some scenarios\ncomparable to the average agreement among the annotators themselves.\nQualitative analysis shows that reasoning models outperform their\ninstruction-tuned counterparts and provide more valid explanations for\nannotations. We release the dataset of more than 40k model and human\nannotations for further research."
                },
                "authors": [
                    {
                        "name": "Zdenk Kasner"
                    },
                    {
                        "name": "Vilm Zouhar"
                    },
                    {
                        "name": "Patrcia Schmidtov"
                    },
                    {
                        "name": "Ivan Kart"
                    },
                    {
                        "name": "Kristna Onderkov"
                    },
                    {
                        "name": "Ondej Pltek"
                    },
                    {
                        "name": "Dimitra Gkatzia"
                    },
                    {
                        "name": "Saad Mahamood"
                    },
                    {
                        "name": "Ondej Duek"
                    },
                    {
                        "name": "Simone Balloccu"
                    }
                ],
                "author_detail": {
                    "name": "Simone Balloccu"
                },
                "author": "Simone Balloccu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08696v1",
                "updated": "2025-04-11T17:03:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    3,
                    58,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T17:03:58Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    3,
                    58,
                    4,
                    101,
                    0
                ],
                "title": "SeaView: Software Engineering Agent Visual Interface for Enhanced\n  Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaView: Software Engineering Agent Visual Interface for Enhanced\n  Workflow"
                },
                "summary": "Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE\nagents, have made tremendous progress (>60% on SWE-Bench Verified) on\nreal-world coding challenges including GitHub issue resolution. SWE agents use\na combination of reasoning, environment interaction and self-reflection to\nresolve issues thereby generating \"trajectories\". Analysis of SWE agent\ntrajectories is difficult, not only as they exceed LLM sequence length\n(sometimes, greater than 128k) but also because it involves a relatively\nprolonged interaction between an LLM and the environment managed by the agent.\nIn case of an agent error, it can be hard to decipher, locate and understand\nits scope. Similarly, it can be hard to track improvements or regression over\nmultiple runs or experiments. While a lot of research has gone into making\nthese SWE agents reach state-of-the-art, much less focus has been put into\ncreating tools to help analyze and visualize agent output. We propose a novel\ntool called SeaView: Software Engineering Agent Visual Interface for Enhanced\nWorkflow, with a vision to assist SWE-agent researchers to visualize and\ninspect their experiments. SeaView's novel mechanisms help compare experimental\nruns with varying hyper-parameters or LLMs, and quickly get an understanding of\nLLM or environment related problems. Based on our user study, experienced\nresearchers spend between 10 and 30 minutes to gather the information provided\nby SeaView, while researchers with little experience can spend between 30\nminutes to 1 hour to diagnose their experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE\nagents, have made tremendous progress (>60% on SWE-Bench Verified) on\nreal-world coding challenges including GitHub issue resolution. SWE agents use\na combination of reasoning, environment interaction and self-reflection to\nresolve issues thereby generating \"trajectories\". Analysis of SWE agent\ntrajectories is difficult, not only as they exceed LLM sequence length\n(sometimes, greater than 128k) but also because it involves a relatively\nprolonged interaction between an LLM and the environment managed by the agent.\nIn case of an agent error, it can be hard to decipher, locate and understand\nits scope. Similarly, it can be hard to track improvements or regression over\nmultiple runs or experiments. While a lot of research has gone into making\nthese SWE agents reach state-of-the-art, much less focus has been put into\ncreating tools to help analyze and visualize agent output. We propose a novel\ntool called SeaView: Software Engineering Agent Visual Interface for Enhanced\nWorkflow, with a vision to assist SWE-agent researchers to visualize and\ninspect their experiments. SeaView's novel mechanisms help compare experimental\nruns with varying hyper-parameters or LLMs, and quickly get an understanding of\nLLM or environment related problems. Based on our user study, experienced\nresearchers spend between 10 and 30 minutes to gather the information provided\nby SeaView, while researchers with little experience can spend between 30\nminutes to 1 hour to diagnose their experiment."
                },
                "authors": [
                    {
                        "name": "Timothy Bula"
                    },
                    {
                        "name": "Saurabh Pujar"
                    },
                    {
                        "name": "Luca Buratti"
                    },
                    {
                        "name": "Mihaela Bornea"
                    },
                    {
                        "name": "Avirup Sil"
                    }
                ],
                "author_detail": {
                    "name": "Avirup Sil"
                },
                "author": "Avirup Sil",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05058v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05058v3",
                "updated": "2025-04-11T17:03:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    3,
                    4,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-07T13:29:02Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    29,
                    2,
                    0,
                    97,
                    0
                ],
                "title": "Not All Data Are Unlearned Equally",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Data Are Unlearned Equally"
                },
                "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account."
                },
                "authors": [
                    {
                        "name": "Aravind Krishnan"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Marius Mosbach"
                    }
                ],
                "author_detail": {
                    "name": "Marius Mosbach"
                },
                "author": "Marius Mosbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05058v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05058v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08694v1",
                "updated": "2025-04-11T17:02:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    2,
                    40,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T17:02:40Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    2,
                    40,
                    4,
                    101,
                    0
                ],
                "title": "TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for\n  Spatiotemporal-Aware Travel Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for\n  Spatiotemporal-Aware Travel Planning"
                },
                "summary": "Large language models (LLMs) have shown promise in automating travel\nplanning, yet they often fall short in addressing nuanced spatiotemporal\nrationality. While existing benchmarks focus on basic plan validity, they\nneglect critical aspects such as route efficiency, POI appeal, and real-time\nadaptability. This paper introduces TP-RAG, the first benchmark tailored for\nretrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes\n2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784\nhigh-quality travel trajectory references sourced from online tourist\ndocuments, enabling dynamic and context-aware planning. Through extensive\nexperiments, we reveal that integrating reference trajectories significantly\nimproves spatial efficiency and POI rationality of the travel plan, while\nchallenges persist in universality and robustness due to conflicting references\nand noisy data. To address these issues, we propose EvoRAG, an evolutionary\nframework that potently synergizes diverse retrieved trajectories with LLMs'\nintrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving\nspatiotemporal compliance and reducing commonsense violation compared to\nground-up and retrieval-augmented baselines. Our work underscores the potential\nof hybridizing Web knowledge with LLM-driven optimization, paving the way for\nmore reliable and adaptive travel planning agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise in automating travel\nplanning, yet they often fall short in addressing nuanced spatiotemporal\nrationality. While existing benchmarks focus on basic plan validity, they\nneglect critical aspects such as route efficiency, POI appeal, and real-time\nadaptability. This paper introduces TP-RAG, the first benchmark tailored for\nretrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes\n2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784\nhigh-quality travel trajectory references sourced from online tourist\ndocuments, enabling dynamic and context-aware planning. Through extensive\nexperiments, we reveal that integrating reference trajectories significantly\nimproves spatial efficiency and POI rationality of the travel plan, while\nchallenges persist in universality and robustness due to conflicting references\nand noisy data. To address these issues, we propose EvoRAG, an evolutionary\nframework that potently synergizes diverse retrieved trajectories with LLMs'\nintrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving\nspatiotemporal compliance and reducing commonsense violation compared to\nground-up and retrieval-augmented baselines. Our work underscores the potential\nof hybridizing Web knowledge with LLM-driven optimization, paving the way for\nmore reliable and adaptive travel planning agents."
                },
                "authors": [
                    {
                        "name": "Hang Ni"
                    },
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Lixin Su"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Hui Xiong"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03767v2",
                "updated": "2025-04-11T16:59:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    59,
                    5,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-02T21:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    21,
                    46,
                    2,
                    2,
                    92,
                    0
                ],
                "title": "MCP Safety Audit: LLMs with the Model Context Protocol Allow Major\n  Security Exploits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCP Safety Audit: LLMs with the Model Context Protocol Allow Major\n  Security Exploits"
                },
                "summary": "To reduce development overhead and enable seamless integration between\npotential components comprising any given generative AI application, the Model\nContext Protocol (MCP) (Anthropic, 2024) has recently been released and\nsubsequently widely adopted. The MCP is an open protocol that standardizes API\ncalls to large language models (LLMs), data sources, and agentic tools. By\nconnecting multiple MCP servers, each defined with a set of tools, resources,\nand prompts, users are able to define automated workflows fully driven by LLMs.\nHowever, we show that the current MCP design carries a wide range of security\nrisks for end users. In particular, we demonstrate that industry-leading LLMs\nmay be coerced into using MCP tools to compromise an AI developer's system\nthrough various attacks, such as malicious code execution, remote access\ncontrol, and credential theft. To proactively mitigate these and related\nattacks, we introduce a safety auditing tool, MCPSafetyScanner, the first\nagentic tool to assess the security of an arbitrary MCP server. MCPScanner uses\nseveral agents to (a) automatically determine adversarial samples given an MCP\nserver's tools and resources; (b) search for related vulnerabilities and\nremediations based on those samples; and (c) generate a security report\ndetailing all findings. Our work highlights serious security issues with\ngeneral-purpose agentic workflows while also providing a proactive tool to\naudit MCP server safety and address detected vulnerabilities before deployment.\n  The described MCP server auditing tool, MCPSafetyScanner, is freely available\nat: https://github.com/johnhalloran321/mcpSafetyScanner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce development overhead and enable seamless integration between\npotential components comprising any given generative AI application, the Model\nContext Protocol (MCP) (Anthropic, 2024) has recently been released and\nsubsequently widely adopted. The MCP is an open protocol that standardizes API\ncalls to large language models (LLMs), data sources, and agentic tools. By\nconnecting multiple MCP servers, each defined with a set of tools, resources,\nand prompts, users are able to define automated workflows fully driven by LLMs.\nHowever, we show that the current MCP design carries a wide range of security\nrisks for end users. In particular, we demonstrate that industry-leading LLMs\nmay be coerced into using MCP tools to compromise an AI developer's system\nthrough various attacks, such as malicious code execution, remote access\ncontrol, and credential theft. To proactively mitigate these and related\nattacks, we introduce a safety auditing tool, MCPSafetyScanner, the first\nagentic tool to assess the security of an arbitrary MCP server. MCPScanner uses\nseveral agents to (a) automatically determine adversarial samples given an MCP\nserver's tools and resources; (b) search for related vulnerabilities and\nremediations based on those samples; and (c) generate a security report\ndetailing all findings. Our work highlights serious security issues with\ngeneral-purpose agentic workflows while also providing a proactive tool to\naudit MCP server safety and address detected vulnerabilities before deployment.\n  The described MCP server auditing tool, MCPSafetyScanner, is freely available\nat: https://github.com/johnhalloran321/mcpSafetyScanner"
                },
                "authors": [
                    {
                        "name": "Brandon Radosevich"
                    },
                    {
                        "name": "John Halloran"
                    }
                ],
                "author_detail": {
                    "name": "John Halloran"
                },
                "author": "John Halloran",
                "arxiv_comment": "27 pages, 21 figures, and 2 Tables. Cleans up the TeX source",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08690v1",
                "updated": "2025-04-11T16:57:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    57,
                    36,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T16:57:36Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    57,
                    36,
                    4,
                    101,
                    0
                ],
                "title": "Fast-Slow-Thinking: Complex Task Solving with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-Slow-Thinking: Complex Task Solving with Large Language Models"
                },
                "summary": "Nowadays, Large Language Models (LLMs) have been gradually employed to solve\ncomplex tasks. To face the challenge, task decomposition has become an\neffective way, which proposes to divide a complex task into multiple simpler\nsubtasks and then solve them separately so that the difficulty of the original\ntask can be reduced. However, the performance of existing task decomposition\nmethods can be suboptimal when the task contains overly complex logic and\nconstraints. In this situation, the solution generated by LLMs may deviate from\nthe original purpose of the task, or contain redundant or even erroneous\ncontent. Therefore, inspired by the fact that humans possess two thinking\nsystems including fast thinking and slow thinking, this paper introduces a new\ntask decomposition method termed ``Fast-Slow-Thinking'' (FST), which stimulates\nLLMs to solve tasks through the cooperation of Fast Thinking (FT) and Slow\nThinking (ST) steps. Here FT focuses more on the general and concise aspect of\nthe task, and ST focuses more on the details of the task. In FT, LLMs are\nprompted to remove the constraints of the original task, therefore simplifying\nit to a general and concise one. In ST, we recall the constraints removed in\nFT, so that LLMs can improve the answer generated in FT to meet the\nrequirements of the original task. Therefore, our FST method enables LLMs to\nconsider a complex problem via a human-like cognition process from coarse to\nfine, the effectiveness of which has been well demonstrated by the experiments\non three types of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, Large Language Models (LLMs) have been gradually employed to solve\ncomplex tasks. To face the challenge, task decomposition has become an\neffective way, which proposes to divide a complex task into multiple simpler\nsubtasks and then solve them separately so that the difficulty of the original\ntask can be reduced. However, the performance of existing task decomposition\nmethods can be suboptimal when the task contains overly complex logic and\nconstraints. In this situation, the solution generated by LLMs may deviate from\nthe original purpose of the task, or contain redundant or even erroneous\ncontent. Therefore, inspired by the fact that humans possess two thinking\nsystems including fast thinking and slow thinking, this paper introduces a new\ntask decomposition method termed ``Fast-Slow-Thinking'' (FST), which stimulates\nLLMs to solve tasks through the cooperation of Fast Thinking (FT) and Slow\nThinking (ST) steps. Here FT focuses more on the general and concise aspect of\nthe task, and ST focuses more on the details of the task. In FT, LLMs are\nprompted to remove the constraints of the original task, therefore simplifying\nit to a general and concise one. In ST, we recall the constraints removed in\nFT, so that LLMs can improve the answer generated in FT to meet the\nrequirements of the original task. Therefore, our FST method enables LLMs to\nconsider a complex problem via a human-like cognition process from coarse to\nfine, the effectiveness of which has been well demonstrated by the experiments\non three types of tasks."
                },
                "authors": [
                    {
                        "name": "Yiliu Sun"
                    },
                    {
                        "name": "Yanfang Zhang"
                    },
                    {
                        "name": "Zicheng Zhao"
                    },
                    {
                        "name": "Sheng Wan"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Chen Gong"
                    }
                ],
                "author_detail": {
                    "name": "Chen Gong"
                },
                "author": "Chen Gong",
                "arxiv_comment": "37 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08687v1",
                "updated": "2025-04-11T16:54:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    54,
                    12,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T16:54:12Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    54,
                    12,
                    4,
                    101,
                    0
                ],
                "title": "Voice Interaction With Conversational AI Could Facilitate Thoughtful\n  Reflection and Substantive Revision in Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voice Interaction With Conversational AI Could Facilitate Thoughtful\n  Reflection and Substantive Revision in Writing"
                },
                "summary": "Writing well requires not only expressing ideas but also refining them\nthrough revision, a process facilitated by reflection. Prior research suggests\nthat feedback delivered through dialogues, such as those in writing center\ntutoring sessions, can help writers reflect more thoughtfully on their work\ncompared to static feedback. Recent advancements in multi-modal large language\nmodels (LLMs) now offer new possibilities for supporting interactive and\nexpressive voice-based reflection in writing. In particular, we propose that\nLLM-generated static feedback can be repurposed as conversation starters,\nallowing writers to seek clarification, request examples, and ask follow-up\nquestions, thereby fostering deeper reflection on their writing. We argue that\nvoice-based interaction can naturally facilitate this conversational exchange,\nencouraging writers' engagement with higher-order concerns, facilitating\niterative refinement of their reflections, and reduce cognitive load compared\nto text-based interactions. To investigate these effects, we propose a\nformative study exploring how text vs. voice input influence writers'\nreflection and subsequent revisions. Findings from this study will inform the\ndesign of intelligent and interactive writing tools, offering insights into how\nvoice-based interactions with LLM-powered conversational agents can support\nreflection and revision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing well requires not only expressing ideas but also refining them\nthrough revision, a process facilitated by reflection. Prior research suggests\nthat feedback delivered through dialogues, such as those in writing center\ntutoring sessions, can help writers reflect more thoughtfully on their work\ncompared to static feedback. Recent advancements in multi-modal large language\nmodels (LLMs) now offer new possibilities for supporting interactive and\nexpressive voice-based reflection in writing. In particular, we propose that\nLLM-generated static feedback can be repurposed as conversation starters,\nallowing writers to seek clarification, request examples, and ask follow-up\nquestions, thereby fostering deeper reflection on their writing. We argue that\nvoice-based interaction can naturally facilitate this conversational exchange,\nencouraging writers' engagement with higher-order concerns, facilitating\niterative refinement of their reflections, and reduce cognitive load compared\nto text-based interactions. To investigate these effects, we propose a\nformative study exploring how text vs. voice input influence writers'\nreflection and subsequent revisions. Findings from this study will inform the\ndesign of intelligent and interactive writing tools, offering insights into how\nvoice-based interactions with LLM-powered conversational agents can support\nreflection and revision."
                },
                "authors": [
                    {
                        "name": "Jiho Kim"
                    },
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Xiang 'Anthony' Chen"
                    },
                    {
                        "name": "Kenneth C. Arnold"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth C. Arnold"
                },
                "author": "Kenneth C. Arnold",
                "arxiv_comment": "5 pages; Accepted to Fourth Workshop on Intelligent and Interactive\n  Writing Assistants (In2Writing 2025) at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05659v2",
                "updated": "2025-04-11T16:51:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    51,
                    59,
                    4,
                    101,
                    0
                ],
                "published": "2025-03-07T18:20:30Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    20,
                    30,
                    4,
                    66,
                    0
                ],
                "title": "A Survey of Large Language Model Empowered Agents for Recommendation and\n  Search: Towards Next-Generation Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Model Empowered Agents for Recommendation and\n  Search: Towards Next-Generation Information Retrieval"
                },
                "summary": "Information technology has profoundly altered the way humans interact with\ninformation. The vast amount of content created, shared, and disseminated\nonline has made it increasingly difficult to access relevant information. Over\nthe past two decades, recommender systems and search (collectively referred to\nas information retrieval systems) have evolved significantly to address these\nchallenges. Recent advances in large language models (LLMs) have demonstrated\ncapabilities that surpass human performance in various language-related tasks\nand exhibit general understanding, reasoning, and decision-making abilities.\nThis paper explores the transformative potential of LLM agents in enhancing\nrecommender and search systems. We discuss the motivations and roles of LLM\nagents, and establish a classification framework to elaborate on the existing\nresearch. We highlight the immense potential of LLM agents in addressing\ncurrent challenges in recommendation and search, providing insights into future\nresearch directions. This paper is the first to systematically review and\nclassify the research on LLM agents in these domains, offering a novel\nperspective on leveraging this advanced AI technology for information\nretrieval. To help understand the existing works, we list the existing papers\non LLM agent based recommendation and search at this link:\nhttps://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information technology has profoundly altered the way humans interact with\ninformation. The vast amount of content created, shared, and disseminated\nonline has made it increasingly difficult to access relevant information. Over\nthe past two decades, recommender systems and search (collectively referred to\nas information retrieval systems) have evolved significantly to address these\nchallenges. Recent advances in large language models (LLMs) have demonstrated\ncapabilities that surpass human performance in various language-related tasks\nand exhibit general understanding, reasoning, and decision-making abilities.\nThis paper explores the transformative potential of LLM agents in enhancing\nrecommender and search systems. We discuss the motivations and roles of LLM\nagents, and establish a classification framework to elaborate on the existing\nresearch. We highlight the immense potential of LLM agents in addressing\ncurrent challenges in recommendation and search, providing insights into future\nresearch directions. This paper is the first to systematically review and\nclassify the research on LLM agents in these domains, offering a novel\nperspective on leveraging this advanced AI technology for information\nretrieval. To help understand the existing works, we list the existing papers\non LLM agent based recommendation and search at this link:\nhttps://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shutong Qiao"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Tzu-Heng Lin"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.01736v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.01736v4",
                "updated": "2025-04-11T16:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    36,
                    20,
                    4,
                    101,
                    0
                ],
                "published": "2024-04-02T08:53:00Z",
                "published_parsed": [
                    2024,
                    4,
                    2,
                    8,
                    53,
                    0,
                    1,
                    93,
                    0
                ],
                "title": "Nonparametric efficient causal estimation of the intervention-specific\n  expected number of recurrent events with continuous-time targeted maximum\n  likelihood and highly adaptive lasso estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric efficient causal estimation of the intervention-specific\n  expected number of recurrent events with continuous-time targeted maximum\n  likelihood and highly adaptive lasso estimation"
                },
                "summary": "Longitudinal settings involving outcome, competing risks and censoring events\noccurring and recurring in continuous time are common in medical research, but\nare often analyzed with methods that do not allow for taking post-baseline\ninformation into account. In this work, we define statistical and causal target\nparameters via the g-computation formula by carrying out interventions directly\non the product integral representing the observed data distribution in a\ncontinuous-time counting process model framework. In recurrent events settings\nour target parameter identifies the expected number of recurrent events also in\nsettings where the censoring mechanism or post-baseline treatment decisions\ndepend on past information of post-baseline covariates such as the recurrent\nevent process. We propose a flexible estimation procedure based on targeted\nmaximum likelihood estimation coupled with highly adaptive lasso estimation to\nprovide a novel approach for double robust and nonparametric inference for the\nconsidered target parameter. We illustrate the methods in a simulation study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Longitudinal settings involving outcome, competing risks and censoring events\noccurring and recurring in continuous time are common in medical research, but\nare often analyzed with methods that do not allow for taking post-baseline\ninformation into account. In this work, we define statistical and causal target\nparameters via the g-computation formula by carrying out interventions directly\non the product integral representing the observed data distribution in a\ncontinuous-time counting process model framework. In recurrent events settings\nour target parameter identifies the expected number of recurrent events also in\nsettings where the censoring mechanism or post-baseline treatment decisions\ndepend on past information of post-baseline covariates such as the recurrent\nevent process. We propose a flexible estimation procedure based on targeted\nmaximum likelihood estimation coupled with highly adaptive lasso estimation to\nprovide a novel approach for double robust and nonparametric inference for the\nconsidered target parameter. We illustrate the methods in a simulation study."
                },
                "authors": [
                    {
                        "name": "Helene C. W. Rytgaard"
                    },
                    {
                        "name": "Mark J. van der Laan"
                    }
                ],
                "author_detail": {
                    "name": "Mark J. van der Laan"
                },
                "author": "Mark J. van der Laan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.01736v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.01736v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.03976v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.03976v2",
                "updated": "2025-04-11T16:33:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    33,
                    11,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-06T15:13:31Z",
                "published_parsed": [
                    2024,
                    11,
                    6,
                    15,
                    13,
                    31,
                    2,
                    311,
                    0
                ],
                "title": "HRDecoder: High-Resolution Decoder Network for Fundus Image Lesion\n  Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRDecoder: High-Resolution Decoder Network for Fundus Image Lesion\n  Segmentation"
                },
                "summary": "High resolution is crucial for precise segmentation in fundus images, yet\nhandling high-resolution inputs incurs considerable GPU memory costs, with\ndiminishing performance gains as overhead increases. To address this issue\nwhile tackling the challenge of segmenting tiny objects, recent studies have\nexplored local-global fusion methods. These methods preserve fine details using\nlocal regions and capture long-range context information from downscaled global\nimages. However, the necessity of multiple forward passes inevitably incurs\nsignificant computational overhead, adversely affecting inference speed. In\nthis paper, we propose HRDecoder, a simple High-Resolution Decoder network for\nfundus lesion segmentation. It integrates a high-resolution representation\nlearning module to capture fine-grained local features and a high-resolution\nfusion module to fuse multi-scale predictions. Our method effectively improves\nthe overall segmentation accuracy of fundus lesions while consuming reasonable\nmemory and computational overhead, and maintaining satisfying inference speed.\nExperimental results on the IDRiD and DDR datasets demonstrate the\neffectiveness of our method. Code is available at\nhttps://github.com/CVIU-CSU/HRDecoder.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High resolution is crucial for precise segmentation in fundus images, yet\nhandling high-resolution inputs incurs considerable GPU memory costs, with\ndiminishing performance gains as overhead increases. To address this issue\nwhile tackling the challenge of segmenting tiny objects, recent studies have\nexplored local-global fusion methods. These methods preserve fine details using\nlocal regions and capture long-range context information from downscaled global\nimages. However, the necessity of multiple forward passes inevitably incurs\nsignificant computational overhead, adversely affecting inference speed. In\nthis paper, we propose HRDecoder, a simple High-Resolution Decoder network for\nfundus lesion segmentation. It integrates a high-resolution representation\nlearning module to capture fine-grained local features and a high-resolution\nfusion module to fuse multi-scale predictions. Our method effectively improves\nthe overall segmentation accuracy of fundus lesions while consuming reasonable\nmemory and computational overhead, and maintaining satisfying inference speed.\nExperimental results on the IDRiD and DDR datasets demonstrate the\neffectiveness of our method. Code is available at\nhttps://github.com/CVIU-CSU/HRDecoder."
                },
                "authors": [
                    {
                        "name": "Ziyuan Ding"
                    },
                    {
                        "name": "Yixiong Liang"
                    },
                    {
                        "name": "Shichao Kan"
                    },
                    {
                        "name": "Qing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qing Liu"
                },
                "author": "Qing Liu",
                "arxiv_comment": "11 pages, 3 figures, accepted by MICCAI 2024, the revised version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.03976v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.03976v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08672v1",
                "updated": "2025-04-11T16:26:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    26,
                    23,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T16:26:23Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    26,
                    23,
                    4,
                    101,
                    0
                ],
                "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning"
                },
                "summary": "Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius."
                },
                "authors": [
                    {
                        "name": "Fangzhi Xu"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Chang Ma"
                    },
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Qiushi Sun"
                    },
                    {
                        "name": "Kanzhi Cheng"
                    },
                    {
                        "name": "Junxian He"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Zhiyong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Wu"
                },
                "author": "Zhiyong Wu",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08666v1",
                "updated": "2025-04-11T16:15:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    15,
                    27,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T16:15:27Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    15,
                    27,
                    4,
                    101,
                    0
                ],
                "title": "Variability-Driven User-Story Generation using LLM and Triadic Concept\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variability-Driven User-Story Generation using LLM and Triadic Concept\n  Analysis"
                },
                "summary": "A widely used Agile practice for requirements is to produce a set of user\nstories (also called ``agile product backlog''), which roughly includes a list\nof pairs (role, feature), where the role handles the feature for a certain\npurpose. In the context of Software Product Lines, the requirements for a\nfamily of similar systems is thus a family of user-story sets, one per system,\nleading to a 3-dimensional dataset composed of sets of triples (system, role,\nfeature). In this paper, we combine Triadic Concept Analysis (TCA) and Large\nLanguage Model (LLM) prompting to suggest the user-story set required to\ndevelop a new system relying on the variability logic of an existing system\nfamily. This process consists in 1) computing 3-dimensional variability\nexpressed as a set of TCA implications, 2) providing the designer with\nintelligible design options, 3) capturing the designer's selection of options,\n4) proposing a first user-story set corresponding to this selection, 5)\nconsolidating its validity according to the implications identified in step 1,\nwhile completing it if necessary, and 6) leveraging LLM to have a more\ncomprehensive website. This process is evaluated with a dataset comprising the\nuser-story sets of 67 similar-purpose websites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A widely used Agile practice for requirements is to produce a set of user\nstories (also called ``agile product backlog''), which roughly includes a list\nof pairs (role, feature), where the role handles the feature for a certain\npurpose. In the context of Software Product Lines, the requirements for a\nfamily of similar systems is thus a family of user-story sets, one per system,\nleading to a 3-dimensional dataset composed of sets of triples (system, role,\nfeature). In this paper, we combine Triadic Concept Analysis (TCA) and Large\nLanguage Model (LLM) prompting to suggest the user-story set required to\ndevelop a new system relying on the variability logic of an existing system\nfamily. This process consists in 1) computing 3-dimensional variability\nexpressed as a set of TCA implications, 2) providing the designer with\nintelligible design options, 3) capturing the designer's selection of options,\n4) proposing a first user-story set corresponding to this selection, 5)\nconsolidating its validity according to the implications identified in step 1,\nwhile completing it if necessary, and 6) leveraging LLM to have a more\ncomprehensive website. This process is evaluated with a dataset comprising the\nuser-story sets of 67 similar-purpose websites."
                },
                "authors": [
                    {
                        "name": "Alexandre Bazin"
                    },
                    {
                        "name": "Alain Gutierrez"
                    },
                    {
                        "name": "Marianne Huchard"
                    },
                    {
                        "name": "Pierre Martin"
                    },
                    {
                        "name": "Yulin"
                    },
                    {
                        "name": "Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhang"
                },
                "arxiv_affiliation": "Huaxi",
                "author": "Zhang",
                "arxiv_doi": "10.5220/0013360500003928",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5220/0013360500003928",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.08666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20th International Conference on Evaluation of Novel Approaches to\n  Software Engineering April 4-6, 2025, in Porto, Portugal",
                "arxiv_journal_ref": "Proceedings of ENASE 2025; SciTePress, pages 618-625 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08655v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08655v1",
                "updated": "2025-04-11T15:58:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    58,
                    46,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T15:58:46Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    58,
                    46,
                    4,
                    101,
                    0
                ],
                "title": "TinyCenterSpeed: Efficient Center-Based Object Detection for Autonomous\n  Racing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyCenterSpeed: Efficient Center-Based Object Detection for Autonomous\n  Racing"
                },
                "summary": "Perception within autonomous driving is nearly synonymous with Neural\nNetworks (NNs). Yet, the domain of autonomous racing is often characterized by\nscaled, computationally limited robots used for cost-effectiveness and safety.\nFor this reason, opponent detection and tracking systems typically resort to\ntraditional computer vision techniques due to computational constraints. This\npaper introduces TinyCenterSpeed, a streamlined adaptation of the seminal\nCenterPoint method, optimized for real-time performance on 1:10 scale\nautonomous racing platforms. This adaptation is viable even on OBCs powered\nsolely by Central Processing Units (CPUs), as it incorporates the use of an\nexternal Tensor Processing Unit (TPU). We demonstrate that, compared to\nAdaptive Breakpoint Detector (ABD), the current State-of-the-Art (SotA) in\nscaled autonomous racing, TinyCenterSpeed not only improves detection and\nvelocity estimation by up to 61.38% but also supports multi-opponent detection\nand estimation. It achieves real-time performance with an inference time of\njust 7.88 ms on the TPU, significantly reducing CPU utilization 8.3-fold.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perception within autonomous driving is nearly synonymous with Neural\nNetworks (NNs). Yet, the domain of autonomous racing is often characterized by\nscaled, computationally limited robots used for cost-effectiveness and safety.\nFor this reason, opponent detection and tracking systems typically resort to\ntraditional computer vision techniques due to computational constraints. This\npaper introduces TinyCenterSpeed, a streamlined adaptation of the seminal\nCenterPoint method, optimized for real-time performance on 1:10 scale\nautonomous racing platforms. This adaptation is viable even on OBCs powered\nsolely by Central Processing Units (CPUs), as it incorporates the use of an\nexternal Tensor Processing Unit (TPU). We demonstrate that, compared to\nAdaptive Breakpoint Detector (ABD), the current State-of-the-Art (SotA) in\nscaled autonomous racing, TinyCenterSpeed not only improves detection and\nvelocity estimation by up to 61.38% but also supports multi-opponent detection\nand estimation. It achieves real-time performance with an inference time of\njust 7.88 ms on the TPU, significantly reducing CPU utilization 8.3-fold."
                },
                "authors": [
                    {
                        "name": "Neil Reichlin"
                    },
                    {
                        "name": "Nicolas Baumann"
                    },
                    {
                        "name": "Edoardo Ghignone"
                    },
                    {
                        "name": "Michele Magno"
                    }
                ],
                "author_detail": {
                    "name": "Michele Magno"
                },
                "author": "Michele Magno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08655v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08655v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08654v1",
                "updated": "2025-04-11T15:58:31Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    58,
                    31,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T15:58:31Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    58,
                    31,
                    4,
                    101,
                    0
                ],
                "title": "The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Invisible EgoHand: 3D Hand Forecasting through EgoBody Pose\n  Estimation"
                },
                "summary": "Forecasting hand motion and pose from an egocentric perspective is essential\nfor understanding human intention. However, existing methods focus solely on\npredicting positions without considering articulation, and only when the hands\nare visible in the field of view. This limitation overlooks the fact that\napproximate hand positions can still be inferred even when they are outside the\ncamera's view. In this paper, we propose a method to forecast the 3D\ntrajectories and poses of both hands from an egocentric video, both in and out\nof the field of view. We propose a diffusion-based transformer architecture for\nEgocentric Hand Forecasting, EgoH4, which takes as input the observation\nsequence and camera poses, then predicts future 3D motion and poses for both\nhands of the camera wearer. We leverage full-body pose information, allowing\nother joints to provide constraints on hand motion. We denoise the hand and\nbody joints along with a visibility predictor for hand joints and a 3D-to-2D\nreprojection loss that minimizes the error when hands are in-view. We evaluate\nEgoH4 on the Ego-Exo4D dataset, combining subsets with body and hand\nannotations. We train on 156K sequences and evaluate on 34K sequences,\nrespectively. EgoH4 improves the performance by 3.4cm and 5.1cm over the\nbaseline in terms of ADE for hand trajectory forecasting and MPJPE for hand\npose forecasting. Project page: https://masashi-hatano.github.io/EgoH4/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting hand motion and pose from an egocentric perspective is essential\nfor understanding human intention. However, existing methods focus solely on\npredicting positions without considering articulation, and only when the hands\nare visible in the field of view. This limitation overlooks the fact that\napproximate hand positions can still be inferred even when they are outside the\ncamera's view. In this paper, we propose a method to forecast the 3D\ntrajectories and poses of both hands from an egocentric video, both in and out\nof the field of view. We propose a diffusion-based transformer architecture for\nEgocentric Hand Forecasting, EgoH4, which takes as input the observation\nsequence and camera poses, then predicts future 3D motion and poses for both\nhands of the camera wearer. We leverage full-body pose information, allowing\nother joints to provide constraints on hand motion. We denoise the hand and\nbody joints along with a visibility predictor for hand joints and a 3D-to-2D\nreprojection loss that minimizes the error when hands are in-view. We evaluate\nEgoH4 on the Ego-Exo4D dataset, combining subsets with body and hand\nannotations. We train on 156K sequences and evaluate on 34K sequences,\nrespectively. EgoH4 improves the performance by 3.4cm and 5.1cm over the\nbaseline in terms of ADE for hand trajectory forecasting and MPJPE for hand\npose forecasting. Project page: https://masashi-hatano.github.io/EgoH4/"
                },
                "authors": [
                    {
                        "name": "Masashi Hatano"
                    },
                    {
                        "name": "Zhifan Zhu"
                    },
                    {
                        "name": "Hideo Saito"
                    },
                    {
                        "name": "Dima Damen"
                    }
                ],
                "author_detail": {
                    "name": "Dima Damen"
                },
                "author": "Dima Damen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08645v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08645v1",
                "updated": "2025-04-11T15:45:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    45,
                    17,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T15:45:17Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    45,
                    17,
                    4,
                    101,
                    0
                ],
                "title": "Title block detection and information extraction for enhanced building\n  drawings search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Title block detection and information extraction for enhanced building\n  drawings search"
                },
                "summary": "The architecture, engineering, and construction (AEC) industry still heavily\nrelies on information stored in drawings for building construction,\nmaintenance, compliance and error checks. However, information extraction (IE)\nfrom building drawings is often time-consuming and costly, especially when\ndealing with historical buildings. Drawing search can be simplified by\nleveraging the information stored in the title block portion of the drawing,\nwhich can be seen as drawing metadata. However, title block IE can be complex\nespecially when dealing with historical drawings which do not follow existing\nstandards for uniformity. This work performs a comparison of existing methods\nfor this kind of IE task, and then proposes a novel title block detection and\nIE pipeline which outperforms existing methods, in particular when dealing with\ncomplex, noisy historical drawings. The pipeline is obtained by combining a\nlightweight Convolutional Neural Network and GPT-4o, the proposed inference\npipeline detects building engineering title blocks with high accuracy, and then\nextract structured drawing metadata from the title blocks, which can be used\nfor drawing search, filtering and grouping. The work demonstrates high accuracy\nand efficiency in IE for both vector (CAD) and hand-drawn (historical)\ndrawings. A user interface (UI) that leverages the extracted metadata for\ndrawing search is established and deployed on real projects, which demonstrates\nsignificant time savings. Additionally, an extensible domain-expert-annotated\ndataset for title block detection is developed, via an efficient AEC-friendly\nannotation workflow that lays the foundation for future work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The architecture, engineering, and construction (AEC) industry still heavily\nrelies on information stored in drawings for building construction,\nmaintenance, compliance and error checks. However, information extraction (IE)\nfrom building drawings is often time-consuming and costly, especially when\ndealing with historical buildings. Drawing search can be simplified by\nleveraging the information stored in the title block portion of the drawing,\nwhich can be seen as drawing metadata. However, title block IE can be complex\nespecially when dealing with historical drawings which do not follow existing\nstandards for uniformity. This work performs a comparison of existing methods\nfor this kind of IE task, and then proposes a novel title block detection and\nIE pipeline which outperforms existing methods, in particular when dealing with\ncomplex, noisy historical drawings. The pipeline is obtained by combining a\nlightweight Convolutional Neural Network and GPT-4o, the proposed inference\npipeline detects building engineering title blocks with high accuracy, and then\nextract structured drawing metadata from the title blocks, which can be used\nfor drawing search, filtering and grouping. The work demonstrates high accuracy\nand efficiency in IE for both vector (CAD) and hand-drawn (historical)\ndrawings. A user interface (UI) that leverages the extracted metadata for\ndrawing search is established and deployed on real projects, which demonstrates\nsignificant time savings. Additionally, an extensible domain-expert-annotated\ndataset for title block detection is developed, via an efficient AEC-friendly\nannotation workflow that lays the foundation for future work."
                },
                "authors": [
                    {
                        "name": "Alessio Lombardi"
                    },
                    {
                        "name": "Li Duan"
                    },
                    {
                        "name": "Ahmed Elnagar"
                    },
                    {
                        "name": "Ahmed Zaalouk"
                    },
                    {
                        "name": "Khalid Ismail"
                    },
                    {
                        "name": "Edlira Vakaj"
                    }
                ],
                "author_detail": {
                    "name": "Edlira Vakaj"
                },
                "arxiv_affiliation": "Birmingham City University",
                "author": "Edlira Vakaj",
                "arxiv_comment": "8 pages, 8 figures, 1 table. Accepted for publication in the 2025\n  European Conference on Computing in Construction (EC3,\n  https://ec-3.org/conference2025/)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08645v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08645v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08641v1",
                "updated": "2025-04-11T15:41:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    41,
                    43,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T15:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    41,
                    43,
                    4,
                    101,
                    0
                ],
                "title": "Training-free Guidance in Text-to-Video Generation via Multimodal\n  Planning and Structured Noise Initialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free Guidance in Text-to-Video Generation via Multimodal\n  Planning and Structured Noise Initialization"
                },
                "summary": "Recent advancements in text-to-video (T2V) diffusion models have\nsignificantly enhanced the visual quality of the generated videos. However,\neven recent T2V models find it challenging to follow text descriptions\naccurately, especially when the prompt requires accurate control of spatial\nlayouts or object trajectories. A recent line of research uses layout guidance\nfor T2V models that require fine-tuning or iterative manipulation of the\nattention map during inference time. This significantly increases the memory\nrequirement, making it difficult to adopt a large T2V model as a backbone. To\naddress this, we introduce Video-MSG, a training-free Guidance method for T2V\ngeneration based on Multimodal planning and Structured noise initialization.\nVideo-MSG consists of three steps, where in the first two steps, Video-MSG\ncreates Video Sketch, a fine-grained spatio-temporal plan for the final video,\nspecifying background, foreground, and object trajectories, in the form of\ndraft video frames. In the last step, Video-MSG guides a downstream T2V\ndiffusion model with Video Sketch through noise inversion and denoising.\nNotably, Video-MSG does not need fine-tuning or attention manipulation with\nadditional memory during inference time, making it easier to adopt large T2V\nmodels. Video-MSG demonstrates its effectiveness in enhancing text alignment\nwith multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V\ngeneration benchmarks (T2VCompBench and VBench). We provide comprehensive\nablation studies about noise inversion ratio, different background generators,\nbackground object detection, and foreground object segmentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in text-to-video (T2V) diffusion models have\nsignificantly enhanced the visual quality of the generated videos. However,\neven recent T2V models find it challenging to follow text descriptions\naccurately, especially when the prompt requires accurate control of spatial\nlayouts or object trajectories. A recent line of research uses layout guidance\nfor T2V models that require fine-tuning or iterative manipulation of the\nattention map during inference time. This significantly increases the memory\nrequirement, making it difficult to adopt a large T2V model as a backbone. To\naddress this, we introduce Video-MSG, a training-free Guidance method for T2V\ngeneration based on Multimodal planning and Structured noise initialization.\nVideo-MSG consists of three steps, where in the first two steps, Video-MSG\ncreates Video Sketch, a fine-grained spatio-temporal plan for the final video,\nspecifying background, foreground, and object trajectories, in the form of\ndraft video frames. In the last step, Video-MSG guides a downstream T2V\ndiffusion model with Video Sketch through noise inversion and denoising.\nNotably, Video-MSG does not need fine-tuning or attention manipulation with\nadditional memory during inference time, making it easier to adopt large T2V\nmodels. Video-MSG demonstrates its effectiveness in enhancing text alignment\nwith multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V\ngeneration benchmarks (T2VCompBench and VBench). We provide comprehensive\nablation studies about noise inversion ratio, different background generators,\nbackground object detection, and foreground object segmentation."
                },
                "authors": [
                    {
                        "name": "Jialu Li"
                    },
                    {
                        "name": "Shoubin Yu"
                    },
                    {
                        "name": "Han Lin"
                    },
                    {
                        "name": "Jaemin Cho"
                    },
                    {
                        "name": "Jaehong Yoon"
                    },
                    {
                        "name": "Mohit Bansal"
                    }
                ],
                "author_detail": {
                    "name": "Mohit Bansal"
                },
                "author": "Mohit Bansal",
                "arxiv_comment": "Website: https://video-msg.github.io; The first three authors\n  contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08640v1",
                "updated": "2025-04-11T15:41:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    41,
                    21,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T15:41:21Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    41,
                    21,
                    4,
                    101,
                    0
                ],
                "title": "Do LLMs trust AI regulation? Emerging behaviour of game-theoretic LLM\n  agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs trust AI regulation? Emerging behaviour of game-theoretic LLM\n  agents"
                },
                "summary": "There is general agreement that fostering trust and cooperation within the AI\ndevelopment ecosystem is essential to promote the adoption of trustworthy AI\nsystems. By embedding Large Language Model (LLM) agents within an evolutionary\ngame-theoretic framework, this paper investigates the complex interplay between\nAI developers, regulators and users, modelling their strategic choices under\ndifferent regulatory scenarios. Evolutionary game theory (EGT) is used to\nquantitatively model the dilemmas faced by each actor, and LLMs provide\nadditional degrees of complexity and nuances and enable repeated games and\nincorporation of personality traits. Our research identifies emerging\nbehaviours of strategic AI agents, which tend to adopt more \"pessimistic\" (not\ntrusting and defective) stances than pure game-theoretic agents. We observe\nthat, in case of full trust by users, incentives are effective to promote\neffective regulation; however, conditional trust may deteriorate the \"social\npact\". Establishing a virtuous feedback between users' trust and regulators'\nreputation thus appears to be key to nudge developers towards creating safe AI.\nHowever, the level at which this trust emerges may depend on the specific LLM\nused for testing. Our results thus provide guidance for AI regulation systems,\nand help predict the outcome of strategic LLM agents, should they be used to\naid regulation itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is general agreement that fostering trust and cooperation within the AI\ndevelopment ecosystem is essential to promote the adoption of trustworthy AI\nsystems. By embedding Large Language Model (LLM) agents within an evolutionary\ngame-theoretic framework, this paper investigates the complex interplay between\nAI developers, regulators and users, modelling their strategic choices under\ndifferent regulatory scenarios. Evolutionary game theory (EGT) is used to\nquantitatively model the dilemmas faced by each actor, and LLMs provide\nadditional degrees of complexity and nuances and enable repeated games and\nincorporation of personality traits. Our research identifies emerging\nbehaviours of strategic AI agents, which tend to adopt more \"pessimistic\" (not\ntrusting and defective) stances than pure game-theoretic agents. We observe\nthat, in case of full trust by users, incentives are effective to promote\neffective regulation; however, conditional trust may deteriorate the \"social\npact\". Establishing a virtuous feedback between users' trust and regulators'\nreputation thus appears to be key to nudge developers towards creating safe AI.\nHowever, the level at which this trust emerges may depend on the specific LLM\nused for testing. Our results thus provide guidance for AI regulation systems,\nand help predict the outcome of strategic LLM agents, should they be used to\naid regulation itself."
                },
                "authors": [
                    {
                        "name": "Alessio Buscemi"
                    },
                    {
                        "name": "Daniele Proverbio"
                    },
                    {
                        "name": "Paolo Bova"
                    },
                    {
                        "name": "Nataliya Balabanova"
                    },
                    {
                        "name": "Adeela Bashir"
                    },
                    {
                        "name": "Theodor Cimpeanu"
                    },
                    {
                        "name": "Henrique Correia da Fonseca"
                    },
                    {
                        "name": "Manh Hong Duong"
                    },
                    {
                        "name": "Elias Fernandez Domingos"
                    },
                    {
                        "name": "Antonio M. Fernandes"
                    },
                    {
                        "name": "Marcus Krellner"
                    },
                    {
                        "name": "Ndidi Bianca Ogbo"
                    },
                    {
                        "name": "Simon T. Powers"
                    },
                    {
                        "name": "Fernando P. Santos"
                    },
                    {
                        "name": "Zia Ush Shamszaman"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Alessandro Di Stefano"
                    },
                    {
                        "name": "The Anh Han"
                    }
                ],
                "author_detail": {
                    "name": "The Anh Han"
                },
                "author": "The Anh Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08637v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08637v1",
                "updated": "2025-04-11T15:39:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    39,
                    6,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T15:39:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    39,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "Simple low-dimensional computations explain variability in neuronal\n  activity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple low-dimensional computations explain variability in neuronal\n  activity"
                },
                "summary": "Our understanding of neural computation is founded on the assumption that\nneurons fire in response to a linear summation of inputs. Yet experiments\ndemonstrate that some neurons are capable of complex computations that require\ninteractions between inputs. Here we show, across multiple brain regions and\nspecies, that simple computations (without interactions between inputs) explain\nmost of the variability in neuronal activity. Neurons are quantitatively\ndescribed by models that capture the measured dependence on each input\nindividually, but assume nothing about combinations of inputs. These minimal\nmodels, which are equivalent to binary artificial neurons, predict complex\nhigher-order dependencies and recover known features of synaptic connectivity.\nThe inferred computations are low-dimensional, indicating a highly redundant\nneural code that is necessary for error correction. These results suggest that,\ndespite intricate biophysical details, most neurons perform simple computations\ntypically reserved for artificial models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Our understanding of neural computation is founded on the assumption that\nneurons fire in response to a linear summation of inputs. Yet experiments\ndemonstrate that some neurons are capable of complex computations that require\ninteractions between inputs. Here we show, across multiple brain regions and\nspecies, that simple computations (without interactions between inputs) explain\nmost of the variability in neuronal activity. Neurons are quantitatively\ndescribed by models that capture the measured dependence on each input\nindividually, but assume nothing about combinations of inputs. These minimal\nmodels, which are equivalent to binary artificial neurons, predict complex\nhigher-order dependencies and recover known features of synaptic connectivity.\nThe inferred computations are low-dimensional, indicating a highly redundant\nneural code that is necessary for error correction. These results suggest that,\ndespite intricate biophysical details, most neurons perform simple computations\ntypically reserved for artificial models."
                },
                "authors": [
                    {
                        "name": "Christopher W. Lynn"
                    }
                ],
                "author_detail": {
                    "name": "Christopher W. Lynn"
                },
                "author": "Christopher W. Lynn",
                "arxiv_comment": "34 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08637v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08637v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08635v1",
                "updated": "2025-04-11T15:37:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    37,
                    46,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T15:37:46Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    37,
                    46,
                    4,
                    101,
                    0
                ],
                "title": "Latent Diffusion Autoencoders: Toward Efficient and Meaningful\n  Unsupervised Representation Learning in Medical Imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latent Diffusion Autoencoders: Toward Efficient and Meaningful\n  Unsupervised Representation Learning in Medical Imaging"
                },
                "summary": "This study presents Latent Diffusion Autoencoder (LDAE), a novel\nencoder-decoder diffusion-based framework for efficient and meaningful\nunsupervised learning in medical imaging, focusing on Alzheimer disease (AD)\nusing brain MR from the ADNI database as a case study. Unlike conventional\ndiffusion autoencoders operating in image space, LDAE applies the diffusion\nprocess in a compressed latent representation, improving computational\nefficiency and making 3D medical imaging representation learning tractable. To\nvalidate the proposed approach, we explore two key hypotheses: (i) LDAE\neffectively captures meaningful semantic representations on 3D brain MR\nassociated with AD and ageing, and (ii) LDAE achieves high-quality image\ngeneration and reconstruction while being computationally efficient.\nExperimental results support both hypotheses: (i) linear-probe evaluations\ndemonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%)\nand age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic\nrepresentations enable attribute manipulation, yielding anatomically plausible\nmodifications; (iii) semantic interpolation experiments show strong\nreconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month\ngap. Even for longer gaps (24 months), the model maintains robust performance\n(SSIM > 0.93, MSE < 0.004), indicating an ability to capture temporal\nprogression trends; (iv) compared to conventional diffusion autoencoders, LDAE\nsignificantly increases inference throughput (20x faster) while also enhancing\nreconstruction quality. These findings position LDAE as a promising framework\nfor scalable medical imaging applications, with the potential to serve as a\nfoundation model for medical image analysis. Code available at\nhttps://github.com/GabrieleLozupone/LDAE",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents Latent Diffusion Autoencoder (LDAE), a novel\nencoder-decoder diffusion-based framework for efficient and meaningful\nunsupervised learning in medical imaging, focusing on Alzheimer disease (AD)\nusing brain MR from the ADNI database as a case study. Unlike conventional\ndiffusion autoencoders operating in image space, LDAE applies the diffusion\nprocess in a compressed latent representation, improving computational\nefficiency and making 3D medical imaging representation learning tractable. To\nvalidate the proposed approach, we explore two key hypotheses: (i) LDAE\neffectively captures meaningful semantic representations on 3D brain MR\nassociated with AD and ageing, and (ii) LDAE achieves high-quality image\ngeneration and reconstruction while being computationally efficient.\nExperimental results support both hypotheses: (i) linear-probe evaluations\ndemonstrate promising diagnostic performance for AD (ROC-AUC: 90%, ACC: 84%)\nand age prediction (MAE: 4.1 years, RMSE: 5.2 years); (ii) the learned semantic\nrepresentations enable attribute manipulation, yielding anatomically plausible\nmodifications; (iii) semantic interpolation experiments show strong\nreconstruction of missing scans, with SSIM of 0.969 (MSE: 0.0019) for a 6-month\ngap. Even for longer gaps (24 months), the model maintains robust performance\n(SSIM > 0.93, MSE < 0.004), indicating an ability to capture temporal\nprogression trends; (iv) compared to conventional diffusion autoencoders, LDAE\nsignificantly increases inference throughput (20x faster) while also enhancing\nreconstruction quality. These findings position LDAE as a promising framework\nfor scalable medical imaging applications, with the potential to serve as a\nfoundation model for medical image analysis. Code available at\nhttps://github.com/GabrieleLozupone/LDAE"
                },
                "authors": [
                    {
                        "name": "Gabriele Lozupone"
                    },
                    {
                        "name": "Alessandro Bria"
                    },
                    {
                        "name": "Francesco Fontanella"
                    },
                    {
                        "name": "Frederick J. A. Meijer"
                    },
                    {
                        "name": "Claudio De Stefano"
                    },
                    {
                        "name": "Henkjan Huisman"
                    }
                ],
                "author_detail": {
                    "name": "Henkjan Huisman"
                },
                "author": "Henkjan Huisman",
                "arxiv_comment": "15 pages, 9 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "41A05, 41A10, 65D05, 65D17,",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05439v2",
                "updated": "2025-04-11T15:33:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    33,
                    14,
                    4,
                    101,
                    0
                ],
                "published": "2025-03-07T14:10:10Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    10,
                    10,
                    4,
                    66,
                    0
                ],
                "title": "An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for\n  Robust Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for\n  Robust Reasoning"
                },
                "summary": "In this paper, we examine the use of Conformal Language Modelling (CLM)\nalongside Answer Set Programming (ASP) to enhance the performance of standard\nopen-weight LLMs on complex multi-step reasoning tasks. Using the StepGame\ndataset, which requires spatial reasoning, we apply CLM to generate sets of ASP\nprograms from an LLM, providing statistical guarantees on the correctness of\nthe outputs. Experimental results show that CLM significantly outperforms\nbaseline models that use standard sampling methods, achieving substantial\naccuracy improvements across different levels of reasoning complexity.\nAdditionally, the LLM-as-Judge metric enhances CLM's performance, especially in\nassessing structurally and logically correct ASP outputs. However, calibrating\nCLM with diverse calibration sets did not improve generalizability for tasks\nrequiring much longer reasoning steps, indicating limitations in handling more\ncomplex tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we examine the use of Conformal Language Modelling (CLM)\nalongside Answer Set Programming (ASP) to enhance the performance of standard\nopen-weight LLMs on complex multi-step reasoning tasks. Using the StepGame\ndataset, which requires spatial reasoning, we apply CLM to generate sets of ASP\nprograms from an LLM, providing statistical guarantees on the correctness of\nthe outputs. Experimental results show that CLM significantly outperforms\nbaseline models that use standard sampling methods, achieving substantial\naccuracy improvements across different levels of reasoning complexity.\nAdditionally, the LLM-as-Judge metric enhances CLM's performance, especially in\nassessing structurally and logically correct ASP outputs. However, calibrating\nCLM with diverse calibration sets did not improve generalizability for tasks\nrequiring much longer reasoning steps, indicating limitations in handling more\ncomplex tasks."
                },
                "authors": [
                    {
                        "name": "Navdeep Kaur"
                    },
                    {
                        "name": "Lachlan McPheat"
                    },
                    {
                        "name": "Alessandra Russo"
                    },
                    {
                        "name": "Anthony G Cohn"
                    },
                    {
                        "name": "Pranava Madhyastha"
                    }
                ],
                "author_detail": {
                    "name": "Pranava Madhyastha"
                },
                "author": "Pranava Madhyastha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08621v1",
                "updated": "2025-04-11T15:25:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    25,
                    50,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T15:25:50Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    25,
                    50,
                    4,
                    101,
                    0
                ],
                "title": "MooseAgent: A LLM Based Multi-agent Framework for Automating Moose\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MooseAgent: A LLM Based Multi-agent Framework for Automating Moose\n  Simulation"
                },
                "summary": "The Finite Element Method (FEM) is widely used in engineering and scientific\ncomputing, but its pre-processing, solver configuration, and post-processing\nstages are often time-consuming and require specialized knowledge. This paper\nproposes an automated solution framework, MooseAgent, for the multi-physics\nsimulation framework MOOSE, which combines large-scale pre-trained language\nmodels (LLMs) with a multi-agent system. The framework uses LLMs to understand\nuser-described simulation requirements in natural language and employs task\ndecomposition and multi-round iterative verification strategies to\nautomatically generate MOOSE input files. To improve accuracy and reduce model\nhallucinations, the system builds and utilizes a vector database containing\nannotated MOOSE input cards and function documentation. We conducted\nexperimental evaluations on several typical cases, including heat transfer,\nmechanics, phase field, and multi-physics coupling. The results show that\nMooseAgent can automate the MOOSE simulation process to a certain extent,\nespecially demonstrating a high success rate when dealing with relatively\nsimple single-physics problems. The main contribution of this research is the\nproposal of a multi-agent automated framework for MOOSE, which validates its\npotential in simplifying finite element simulation processes and lowering the\nuser barrier, providing new ideas for the development of intelligent finite\nelement simulation software. The code for the MooseAgent framework proposed in\nthis paper has been open-sourced and is available at\nhttps://github.com/taozhan18/MooseAgent",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Finite Element Method (FEM) is widely used in engineering and scientific\ncomputing, but its pre-processing, solver configuration, and post-processing\nstages are often time-consuming and require specialized knowledge. This paper\nproposes an automated solution framework, MooseAgent, for the multi-physics\nsimulation framework MOOSE, which combines large-scale pre-trained language\nmodels (LLMs) with a multi-agent system. The framework uses LLMs to understand\nuser-described simulation requirements in natural language and employs task\ndecomposition and multi-round iterative verification strategies to\nautomatically generate MOOSE input files. To improve accuracy and reduce model\nhallucinations, the system builds and utilizes a vector database containing\nannotated MOOSE input cards and function documentation. We conducted\nexperimental evaluations on several typical cases, including heat transfer,\nmechanics, phase field, and multi-physics coupling. The results show that\nMooseAgent can automate the MOOSE simulation process to a certain extent,\nespecially demonstrating a high success rate when dealing with relatively\nsimple single-physics problems. The main contribution of this research is the\nproposal of a multi-agent automated framework for MOOSE, which validates its\npotential in simplifying finite element simulation processes and lowering the\nuser barrier, providing new ideas for the development of intelligent finite\nelement simulation software. The code for the MooseAgent framework proposed in\nthis paper has been open-sourced and is available at\nhttps://github.com/taozhan18/MooseAgent"
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Zhenhai Liu"
                    },
                    {
                        "name": "Yong Xin"
                    },
                    {
                        "name": "Yongjun Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Yongjun Jiao"
                },
                "author": "Yongjun Jiao",
                "arxiv_comment": "7 pages, 2 Figs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08619v1",
                "updated": "2025-04-11T15:24:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    24,
                    23,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T15:24:23Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    24,
                    23,
                    4,
                    101,
                    0
                ],
                "title": "Analyzing 16,193 LLM Papers for Fun and Profits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing 16,193 LLM Papers for Fun and Profits"
                },
                "summary": "Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem."
                },
                "authors": [
                    {
                        "name": "Zhiqiu Xia"
                    },
                    {
                        "name": "Lang Zhu"
                    },
                    {
                        "name": "Bingzhe Li"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Qiannan Li"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08616v1",
                "updated": "2025-04-11T15:21:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    21,
                    12,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T15:21:12Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    21,
                    12,
                    4,
                    101,
                    0
                ],
                "title": "Preserving Privacy Without Compromising Accuracy: Machine Unlearning for\n  Handwritten Text Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preserving Privacy Without Compromising Accuracy: Machine Unlearning for\n  Handwritten Text Recognition"
                },
                "summary": "Handwritten Text Recognition (HTR) is essential for document analysis and\ndigitization. However, handwritten data often contains user-identifiable\ninformation, such as unique handwriting styles and personal lexicon choices,\nwhich can compromise privacy and erode trust in AI services. Legislation like\nthe ``right to be forgotten'' underscores the necessity for methods that can\nexpunge sensitive information from trained models. Machine unlearning addresses\nthis by selectively removing specific data from models without necessitating\ncomplete retraining. Yet, it frequently encounters a privacy-accuracy tradeoff,\nwhere safeguarding privacy leads to diminished model performance. In this\npaper, we introduce a novel two-stage unlearning strategy for a multi-head\ntransformer-based HTR model, integrating pruning and random labeling. Our\nproposed method utilizes a writer classification head both as an indicator and\na trigger for unlearning, while maintaining the efficacy of the recognition\nhead. To our knowledge, this represents the first comprehensive exploration of\nmachine unlearning within HTR tasks. We further employ Membership Inference\nAttacks (MIA) to evaluate the effectiveness of unlearning user-identifiable\ninformation. Extensive experiments demonstrate that our approach effectively\npreserves privacy while maintaining model accuracy, paving the way for new\nresearch directions in the document analysis community. Our code will be\npublicly available upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handwritten Text Recognition (HTR) is essential for document analysis and\ndigitization. However, handwritten data often contains user-identifiable\ninformation, such as unique handwriting styles and personal lexicon choices,\nwhich can compromise privacy and erode trust in AI services. Legislation like\nthe ``right to be forgotten'' underscores the necessity for methods that can\nexpunge sensitive information from trained models. Machine unlearning addresses\nthis by selectively removing specific data from models without necessitating\ncomplete retraining. Yet, it frequently encounters a privacy-accuracy tradeoff,\nwhere safeguarding privacy leads to diminished model performance. In this\npaper, we introduce a novel two-stage unlearning strategy for a multi-head\ntransformer-based HTR model, integrating pruning and random labeling. Our\nproposed method utilizes a writer classification head both as an indicator and\na trigger for unlearning, while maintaining the efficacy of the recognition\nhead. To our knowledge, this represents the first comprehensive exploration of\nmachine unlearning within HTR tasks. We further employ Membership Inference\nAttacks (MIA) to evaluate the effectiveness of unlearning user-identifiable\ninformation. Extensive experiments demonstrate that our approach effectively\npreserves privacy while maintaining model accuracy, paving the way for new\nresearch directions in the document analysis community. Our code will be\npublicly available upon acceptance."
                },
                "authors": [
                    {
                        "name": "Lei Kang"
                    },
                    {
                        "name": "Xuanshuo Fu"
                    },
                    {
                        "name": "Lluis Gomez"
                    },
                    {
                        "name": "Alicia Forns"
                    },
                    {
                        "name": "Ernest Valveny"
                    },
                    {
                        "name": "Dimosthenis Karatzas"
                    }
                ],
                "author_detail": {
                    "name": "Dimosthenis Karatzas"
                },
                "author": "Dimosthenis Karatzas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08604v1",
                "updated": "2025-04-11T15:12:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    12,
                    12,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T15:12:12Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    12,
                    12,
                    4,
                    101,
                    0
                ],
                "title": "Neural Fidelity Calibration for Informative Sim-to-Real Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Fidelity Calibration for Informative Sim-to-Real Adaptation"
                },
                "summary": "Deep reinforcement learning can seamlessly transfer agile locomotion and\nnavigation skills from the simulator to real world. However, bridging the\nsim-to-real gap with domain randomization or adversarial methods often demands\nexpert physics knowledge to ensure policy robustness. Even so, cutting-edge\nsimulators may fall short of capturing every real-world detail, and the\nreconstructed environment may introduce errors due to various perception\nuncertainties. To address these challenges, we propose Neural Fidelity\nCalibration (NFC), a novel framework that employs conditional score-based\ndiffusion models to calibrate simulator physical coefficients and residual\nfidelity domains online during robot execution. Specifically, the residual\nfidelity reflects the simulation model shift relative to the real-world\ndynamics and captures the uncertainty of the perceived environment, enabling us\nto sample realistic environments under the inferred distribution for policy\nfine-tuning. Our framework is informative and adaptive in three key ways: (a)\nwe fine-tune the pretrained policy only under anomalous scenarios, (b) we build\nsequential NFC online with the pretrained NFC's proposal prior, reducing the\ndiffusion model's training burden, and (c) when NFC uncertainty is high and may\ndegrade policy improvement, we leverage optimistic exploration to enable\nhallucinated policy optimization. Our framework achieves superior simulator\ncalibration precision compared to state-of-the-art methods across diverse\nrobots with high-dimensional parametric spaces. We study the critical\ncontribution of residual fidelity to policy improvement in simulation and\nreal-world experiments. Notably, our approach demonstrates robust robot\nnavigation under challenging real-world conditions, such as a broken wheel axle\non snowy surfaces.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep reinforcement learning can seamlessly transfer agile locomotion and\nnavigation skills from the simulator to real world. However, bridging the\nsim-to-real gap with domain randomization or adversarial methods often demands\nexpert physics knowledge to ensure policy robustness. Even so, cutting-edge\nsimulators may fall short of capturing every real-world detail, and the\nreconstructed environment may introduce errors due to various perception\nuncertainties. To address these challenges, we propose Neural Fidelity\nCalibration (NFC), a novel framework that employs conditional score-based\ndiffusion models to calibrate simulator physical coefficients and residual\nfidelity domains online during robot execution. Specifically, the residual\nfidelity reflects the simulation model shift relative to the real-world\ndynamics and captures the uncertainty of the perceived environment, enabling us\nto sample realistic environments under the inferred distribution for policy\nfine-tuning. Our framework is informative and adaptive in three key ways: (a)\nwe fine-tune the pretrained policy only under anomalous scenarios, (b) we build\nsequential NFC online with the pretrained NFC's proposal prior, reducing the\ndiffusion model's training burden, and (c) when NFC uncertainty is high and may\ndegrade policy improvement, we leverage optimistic exploration to enable\nhallucinated policy optimization. Our framework achieves superior simulator\ncalibration precision compared to state-of-the-art methods across diverse\nrobots with high-dimensional parametric spaces. We study the critical\ncontribution of residual fidelity to policy improvement in simulation and\nreal-world experiments. Notably, our approach demonstrates robust robot\nnavigation under challenging real-world conditions, such as a broken wheel axle\non snowy surfaces."
                },
                "authors": [
                    {
                        "name": "Youwei Yu"
                    },
                    {
                        "name": "Lantao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Lantao Liu"
                },
                "author": "Lantao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07574v2",
                "updated": "2025-04-11T15:06:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    6,
                    17,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:17:45Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    17,
                    45,
                    3,
                    100,
                    0
                ],
                "title": "Malware analysis assisted by AI with R2AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malware analysis assisted by AI with R2AI"
                },
                "summary": "This research studies the quality, speed and cost of malware analysis\nassisted by artificial intelligence. It focuses on Linux and IoT malware of\n2024-2025, and uses r2ai, the AI extension of Radare2's disassembler. Not all\nmalware and not all LLMs are equivalent but the study shows excellent results\nwith Claude 3.5 and 3.7 Sonnet. Despite a few errors, the quality of analysis\nis overall equal or better than without AI assistance. For good results, the AI\ncannot operate alone and must constantly be guided by an experienced analyst.\nThe gain of speed is largely visible with AI assistance, even when taking\naccount the time to understand AI's hallucinations, exaggerations and\nomissions. The cost is usually noticeably lower than the salary of a malware\nanalyst, but attention and guidance is needed to keep it under control in cases\nwhere the AI would naturally loop without showing progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research studies the quality, speed and cost of malware analysis\nassisted by artificial intelligence. It focuses on Linux and IoT malware of\n2024-2025, and uses r2ai, the AI extension of Radare2's disassembler. Not all\nmalware and not all LLMs are equivalent but the study shows excellent results\nwith Claude 3.5 and 3.7 Sonnet. Despite a few errors, the quality of analysis\nis overall equal or better than without AI assistance. For good results, the AI\ncannot operate alone and must constantly be guided by an experienced analyst.\nThe gain of speed is largely visible with AI assistance, even when taking\naccount the time to understand AI's hallucinations, exaggerations and\nomissions. The cost is usually noticeably lower than the salary of a malware\nanalyst, but attention and guidance is needed to keep it under control in cases\nwhere the AI would naturally loop without showing progress."
                },
                "authors": [
                    {
                        "name": "Axelle Apvrille"
                    },
                    {
                        "name": "Daniel Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Nakov"
                },
                "author": "Daniel Nakov",
                "arxiv_comment": "11 pages;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18992v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18992v3",
                "updated": "2025-04-11T15:04:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    4,
                    54,
                    4,
                    101,
                    0
                ],
                "published": "2024-04-29T18:00:00Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    18,
                    0,
                    0,
                    0,
                    120,
                    0
                ],
                "title": "Unifying Simulation and Inference with Normalizing Flows",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Simulation and Inference with Normalizing Flows"
                },
                "summary": "There have been many applications of deep neural networks to detector\ncalibrations and a growing number of studies that propose deep generative\nmodels as automated fast detector simulators. We show that these two tasks can\nbe unified by using maximum likelihood estimation (MLE) from conditional\ngenerative models for energy regression. Unlike direct regression techniques,\nthe MLE approach is prior-independent and non-Gaussian resolutions can be\ndetermined from the shape of the likelihood near the maximum. Using an\nATLAS-like calorimeter simulation, we demonstrate this concept in the context\nof calorimeter energy calibration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There have been many applications of deep neural networks to detector\ncalibrations and a growing number of studies that propose deep generative\nmodels as automated fast detector simulators. We show that these two tasks can\nbe unified by using maximum likelihood estimation (MLE) from conditional\ngenerative models for energy regression. Unlike direct regression techniques,\nthe MLE approach is prior-independent and non-Gaussian resolutions can be\ndetermined from the shape of the likelihood near the maximum. Using an\nATLAS-like calorimeter simulation, we demonstrate this concept in the context\nof calorimeter energy calibration."
                },
                "authors": [
                    {
                        "name": "Haoxing Du"
                    },
                    {
                        "name": "Claudius Krause"
                    },
                    {
                        "name": "Vinicius Mikuni"
                    },
                    {
                        "name": "Benjamin Nachman"
                    },
                    {
                        "name": "Ian Pang"
                    },
                    {
                        "name": "David Shih"
                    }
                ],
                "author_detail": {
                    "name": "David Shih"
                },
                "author": "David Shih",
                "arxiv_doi": "10.1103/PhysRevD.111.076004",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.076004",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.18992v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18992v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "13 pages, 7 figures; v3: matches published version",
                "arxiv_journal_ref": "Phys. Rev. D 111, 076004 (2025)",
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08600v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08600v1",
                "updated": "2025-04-11T15:01:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    1,
                    30,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T15:01:30Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    1,
                    30,
                    4,
                    101,
                    0
                ],
                "title": "SQL-R1: Training Natural Language to SQL Reasoning Model By\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQL-R1: Training Natural Language to SQL Reasoning Model By\n  Reinforcement Learning"
                },
                "summary": "Natural Language to SQL (NL2SQL) enables intuitive interactions with\ndatabases by transforming natural language queries into structured SQL\nstatements. Despite recent advancements in enhancing human-computer interaction\nwithin database applications, significant challenges persist, particularly\nregarding the inference performance in complex scenarios involving multi-table\njoins and nested queries. Current methodologies primarily utilize supervised\nfine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and\ninterpretability in new environments (e.g., finance and healthcare). In order\nto enhance the reasoning performance of the NL2SQL model in the above complex\nsituations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the\nreinforcement learning (RL) algorithms. We design a specialized RL-based reward\nfunction tailored for NL2SQL tasks and discussed the impact of cold start on\nthe effectiveness of intensive training. In addition, we achieve competitive\naccuracy using only a tiny amount of synthetic NL2SQL data for augmented\ntraining and further explore data engineering for RL. In existing experiments,\nSQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider\nand BIRD, respectively, only using the 7B base model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural Language to SQL (NL2SQL) enables intuitive interactions with\ndatabases by transforming natural language queries into structured SQL\nstatements. Despite recent advancements in enhancing human-computer interaction\nwithin database applications, significant challenges persist, particularly\nregarding the inference performance in complex scenarios involving multi-table\njoins and nested queries. Current methodologies primarily utilize supervised\nfine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and\ninterpretability in new environments (e.g., finance and healthcare). In order\nto enhance the reasoning performance of the NL2SQL model in the above complex\nsituations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the\nreinforcement learning (RL) algorithms. We design a specialized RL-based reward\nfunction tailored for NL2SQL tasks and discussed the impact of cold start on\nthe effectiveness of intensive training. In addition, we achieve competitive\naccuracy using only a tiny amount of synthetic NL2SQL data for augmented\ntraining and further explore data engineering for RL. In existing experiments,\nSQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider\nand BIRD, respectively, only using the 7B base model."
                },
                "authors": [
                    {
                        "name": "Peixian Ma"
                    },
                    {
                        "name": "Xialie Zhuang"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Ran Chen"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08600v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08600v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08596v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08596v1",
                "updated": "2025-04-11T14:55:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    14,
                    55,
                    15,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T14:55:15Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    14,
                    55,
                    15,
                    4,
                    101,
                    0
                ],
                "title": "MedHal: An Evaluation Dataset for Medical Hallucination Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedHal: An Evaluation Dataset for Medical Hallucination Detection"
                },
                "summary": "We present MedHal, a novel large-scale dataset specifically designed to\nevaluate if models can detect hallucinations in medical texts. Current\nhallucination detection methods face significant limitations when applied to\nspecialized domains like medicine, where they can have disastrous consequences.\nExisting medical datasets are either too small, containing only a few hundred\nsamples, or focus on a single task like Question Answering or Natural Language\nInference. MedHal addresses these gaps by: (1) incorporating diverse medical\ntext sources and tasks; (2) providing a substantial volume of annotated samples\nsuitable for training medical hallucination detection models; and (3) including\nexplanations for factual inconsistencies to guide model learning. We\ndemonstrate MedHal's utility by training and evaluating a baseline medical\nhallucination detection model, showing improvements over general-purpose\nhallucination detection approaches. This resource enables more efficient\nevaluation of medical text generation systems while reducing reliance on costly\nexpert review, potentially accelerating the development of medical AI research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MedHal, a novel large-scale dataset specifically designed to\nevaluate if models can detect hallucinations in medical texts. Current\nhallucination detection methods face significant limitations when applied to\nspecialized domains like medicine, where they can have disastrous consequences.\nExisting medical datasets are either too small, containing only a few hundred\nsamples, or focus on a single task like Question Answering or Natural Language\nInference. MedHal addresses these gaps by: (1) incorporating diverse medical\ntext sources and tasks; (2) providing a substantial volume of annotated samples\nsuitable for training medical hallucination detection models; and (3) including\nexplanations for factual inconsistencies to guide model learning. We\ndemonstrate MedHal's utility by training and evaluating a baseline medical\nhallucination detection model, showing improvements over general-purpose\nhallucination detection approaches. This resource enables more efficient\nevaluation of medical text generation systems while reducing reliance on costly\nexpert review, potentially accelerating the development of medical AI research."
                },
                "authors": [
                    {
                        "name": "Gaya Mehenni"
                    },
                    {
                        "name": "Amal Zouaq"
                    }
                ],
                "author_detail": {
                    "name": "Amal Zouaq"
                },
                "author": "Amal Zouaq",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08596v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08596v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07703v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07703v3",
                "updated": "2025-04-11T14:12:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    14,
                    12,
                    58,
                    4,
                    101,
                    0
                ],
                "published": "2024-09-12T02:08:00Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    8,
                    0,
                    3,
                    256,
                    0
                ],
                "title": "DSBench: How Far Are Data Science Agents from Becoming Data Science\n  Experts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSBench: How Far Are Data Science Agents from Becoming Data Science\n  Experts?"
                },
                "summary": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have\ndemonstrated impressive language/vision reasoning abilities, igniting the\nrecent trend of building agents for targeted applications such as shopping\nassistants or AI software engineers. Recently, many data science benchmarks\nhave been proposed to investigate their performance in the data science domain.\nHowever, existing data science benchmarks still fall short when compared to\nreal-world data science applications due to their simplified settings. To\nbridge this gap, we introduce DSBench, a comprehensive benchmark designed to\nevaluate data science agents with realistic tasks. This benchmark includes 466\ndata analysis tasks and 74 data modeling tasks, sourced from Eloquence and\nKaggle competitions. DSBench offers a realistic setting by encompassing long\ncontexts, multimodal task backgrounds, reasoning with large data files and\nmulti-table structures, and performing end-to-end data modeling tasks. Our\nevaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle\nwith most tasks, with the best agent solving only 34.12% of data analysis tasks\nand achieving a 34.74% Relative Performance Gap (RPG). These findings\nunderscore the need for further advancements in developing more practical,\nintelligent, and autonomous data science agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have\ndemonstrated impressive language/vision reasoning abilities, igniting the\nrecent trend of building agents for targeted applications such as shopping\nassistants or AI software engineers. Recently, many data science benchmarks\nhave been proposed to investigate their performance in the data science domain.\nHowever, existing data science benchmarks still fall short when compared to\nreal-world data science applications due to their simplified settings. To\nbridge this gap, we introduce DSBench, a comprehensive benchmark designed to\nevaluate data science agents with realistic tasks. This benchmark includes 466\ndata analysis tasks and 74 data modeling tasks, sourced from Eloquence and\nKaggle competitions. DSBench offers a realistic setting by encompassing long\ncontexts, multimodal task backgrounds, reasoning with large data files and\nmulti-table structures, and performing end-to-end data modeling tasks. Our\nevaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle\nwith most tasks, with the best agent solving only 34.12% of data analysis tasks\nand achieving a 34.74% Relative Performance Gap (RPG). These findings\nunderscore the need for further advancements in developing more practical,\nintelligent, and autonomous data science agents."
                },
                "authors": [
                    {
                        "name": "Liqiang Jing"
                    },
                    {
                        "name": "Zhehui Huang"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Wenlin Yao"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Kaixin Ma"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Xinya Du"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07703v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07703v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.05315v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.05315v3",
                "updated": "2025-04-11T13:44:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    13,
                    44,
                    18,
                    4,
                    101,
                    0
                ],
                "published": "2024-01-10T18:33:21Z",
                "published_parsed": [
                    2024,
                    1,
                    10,
                    18,
                    33,
                    21,
                    2,
                    10,
                    0
                ],
                "title": "Multi-resolution filters via linear projection for large spatio-temporal\n  datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-resolution filters via linear projection for large spatio-temporal\n  datasets"
                },
                "summary": "Advances in compact sensing devices mounted on satellites have facilitated\nthe collection of large spatio-temporal datasets with coordinates. Since such\ndatasets are often incomplete and noisy, it is useful to create the prediction\nsurface of a spatial field. To this end, we consider an online filtering\ninference by using the Kalman filter based on linear Gaussian state-space\nmodels. However, the Kalman filter is impractically time-consuming when the\nnumber of locations in spatio-temporal datasets is large. To address this\nproblem, we propose a multi-resolution filter via linear projection (MRF-lp), a\nfast computation method for online filtering inference. In the MRF-lp, by\ncarrying out a multi-resolution approximation via linear projection (MRA-lp),\nthe forecast covariance matrix can be approximated while capturing both the\nlarge- and small-scale spatial variations. As a result of this approximation,\nour proposed MRF-lp preserves a block-sparse structure of some matrices\nappearing in the MRF-lp through time, which leads to the scalability of this\nalgorithm. Additionally, we discuss extensions of the MRF-lp to a nonlinear and\nnon-Gaussian case. Simulation studies and real data analysis for total\nprecipitable water vapor demonstrate that our proposed approach performs well\ncompared with the related methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advances in compact sensing devices mounted on satellites have facilitated\nthe collection of large spatio-temporal datasets with coordinates. Since such\ndatasets are often incomplete and noisy, it is useful to create the prediction\nsurface of a spatial field. To this end, we consider an online filtering\ninference by using the Kalman filter based on linear Gaussian state-space\nmodels. However, the Kalman filter is impractically time-consuming when the\nnumber of locations in spatio-temporal datasets is large. To address this\nproblem, we propose a multi-resolution filter via linear projection (MRF-lp), a\nfast computation method for online filtering inference. In the MRF-lp, by\ncarrying out a multi-resolution approximation via linear projection (MRA-lp),\nthe forecast covariance matrix can be approximated while capturing both the\nlarge- and small-scale spatial variations. As a result of this approximation,\nour proposed MRF-lp preserves a block-sparse structure of some matrices\nappearing in the MRF-lp through time, which leads to the scalability of this\nalgorithm. Additionally, we discuss extensions of the MRF-lp to a nonlinear and\nnon-Gaussian case. Simulation studies and real data analysis for total\nprecipitable water vapor demonstrate that our proposed approach performs well\ncompared with the related methods."
                },
                "authors": [
                    {
                        "name": "Toshihiro Hirano"
                    },
                    {
                        "name": "Tsunehiro Ishihara"
                    }
                ],
                "author_detail": {
                    "name": "Tsunehiro Ishihara"
                },
                "author": "Tsunehiro Ishihara",
                "arxiv_comment": "44 pages, 10 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.05315v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.05315v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08526v1",
                "updated": "2025-04-11T13:38:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    13,
                    38,
                    56,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T13:38:56Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    13,
                    38,
                    56,
                    4,
                    101,
                    0
                ],
                "title": "Hallucination, reliability, and the role of generative AI in science",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hallucination, reliability, and the role of generative AI in science"
                },
                "summary": "Generative AI is increasingly used in scientific domains, from protein\nfolding to climate modeling. But these models produce distinctive errors known\nas hallucinations - outputs that are incorrect yet superficially plausible.\nWorse, some arguments suggest that hallucinations are an inevitable consequence\nof the mechanisms underlying generative inference. Fortunately, such arguments\nrely on a conception of hallucination defined solely with respect to internal\nproperties of the model, rather than in reference to the empirical target\nsystem. This conception fails to distinguish epistemically benign errors from\nthose that threaten scientific inference. I introduce the concept of corrosive\nhallucination to capture the epistemically troubling subclass:\nmisrepresentations that are substantively misleading and resistant to\nsystematic anticipation. I argue that although corrosive hallucinations do pose\na threat to scientific reliability, they are not inevitable. Scientific\nworkflows such as those surrounding AlphaFold and GenCast, both of which serve\nas case studies, can neutralize their effects by imposing theoretical\nconstraints during training, and by strategically screening for errors at\ninference time. When embedded in such workflows, generative AI can reliably\ncontribute to scientific knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI is increasingly used in scientific domains, from protein\nfolding to climate modeling. But these models produce distinctive errors known\nas hallucinations - outputs that are incorrect yet superficially plausible.\nWorse, some arguments suggest that hallucinations are an inevitable consequence\nof the mechanisms underlying generative inference. Fortunately, such arguments\nrely on a conception of hallucination defined solely with respect to internal\nproperties of the model, rather than in reference to the empirical target\nsystem. This conception fails to distinguish epistemically benign errors from\nthose that threaten scientific inference. I introduce the concept of corrosive\nhallucination to capture the epistemically troubling subclass:\nmisrepresentations that are substantively misleading and resistant to\nsystematic anticipation. I argue that although corrosive hallucinations do pose\na threat to scientific reliability, they are not inevitable. Scientific\nworkflows such as those surrounding AlphaFold and GenCast, both of which serve\nas case studies, can neutralize their effects by imposing theoretical\nconstraints during training, and by strategically screening for errors at\ninference time. When embedded in such workflows, generative AI can reliably\ncontribute to scientific knowledge."
                },
                "authors": [
                    {
                        "name": "Charles Rathkopf"
                    }
                ],
                "author_detail": {
                    "name": "Charles Rathkopf"
                },
                "author": "Charles Rathkopf",
                "arxiv_comment": "31 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08525v1",
                "updated": "2025-04-11T13:38:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    13,
                    38,
                    36,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T13:38:36Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    13,
                    38,
                    36,
                    4,
                    101,
                    0
                ],
                "title": "Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM\n  Agent Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM\n  Agent Tasks"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as autonomous agents for\nmulti-step tasks. However, most existing frameworks fail to maintain a\nstructured understanding of the task state, often relying on linear prompt\nconcatenation or shallow memory buffers. This leads to brittle performance,\nfrequent hallucinations, and poor long-range coherence. In this work, we\npropose the Task Memory Engine (TME), a lightweight and structured memory\nmodule that tracks task execution using a hierarchical Task Memory Tree (TMT).\nEach node in the tree corresponds to a task step, storing relevant input,\noutput, status, and sub-task relationships. We introduce a prompt synthesis\nmethod that dynamically generates LLM prompts based on the active node path,\nsignificantly improving execution consistency and contextual grounding. Through\ncase studies and comparative experiments on multi-step agent tasks, we\ndemonstrate that TME leads to better task completion accuracy and more\ninterpretable behavior with minimal implementation overhead. The full\nimplementation of TME is available at\nhttps://github.com/biubiutomato/TME-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as autonomous agents for\nmulti-step tasks. However, most existing frameworks fail to maintain a\nstructured understanding of the task state, often relying on linear prompt\nconcatenation or shallow memory buffers. This leads to brittle performance,\nfrequent hallucinations, and poor long-range coherence. In this work, we\npropose the Task Memory Engine (TME), a lightweight and structured memory\nmodule that tracks task execution using a hierarchical Task Memory Tree (TMT).\nEach node in the tree corresponds to a task step, storing relevant input,\noutput, status, and sub-task relationships. We introduce a prompt synthesis\nmethod that dynamically generates LLM prompts based on the active node path,\nsignificantly improving execution consistency and contextual grounding. Through\ncase studies and comparative experiments on multi-step agent tasks, we\ndemonstrate that TME leads to better task completion accuracy and more\ninterpretable behavior with minimal implementation overhead. The full\nimplementation of TME is available at\nhttps://github.com/biubiutomato/TME-Agent."
                },
                "authors": [
                    {
                        "name": "Ye Ye"
                    }
                ],
                "author_detail": {
                    "name": "Ye Ye"
                },
                "author": "Ye Ye",
                "arxiv_comment": "14 pages, 5 figures. Preprint prepared for future submission.\n  Includes implementation and token-efficiency analysis. Code at\n  https://github.com/biubiutomato/TME-Agent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.8; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08508v1",
                "updated": "2025-04-11T13:21:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    13,
                    21,
                    33,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T13:21:33Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    13,
                    21,
                    33,
                    4,
                    101,
                    0
                ],
                "title": "An Early Experience with Confidential Computing Architecture for\n  On-Device Model Protection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Early Experience with Confidential Computing Architecture for\n  On-Device Model Protection"
                },
                "summary": "Deploying machine learning (ML) models on user devices can improve privacy\n(by keeping data local) and reduce inference latency. Trusted Execution\nEnvironments (TEEs) are a practical solution for protecting proprietary models,\nyet existing TEE solutions have architectural constraints that hinder on-device\nmodel deployment. Arm Confidential Computing Architecture (CCA), a new Arm\nextension, addresses several of these limitations and shows promise as a secure\nplatform for on-device ML. In this paper, we evaluate the performance-privacy\ntrade-offs of deploying models within CCA, highlighting its potential to enable\nconfidential and efficient ML applications. Our evaluations show that CCA can\nachieve an overhead of, at most, 22% in running models of different sizes and\napplications, including image classification, voice recognition, and chat\nassistants. This performance overhead comes with privacy benefits; for example,\nour framework can successfully protect the model against membership inference\nattack by an 8.3% reduction in the adversary's success rate. To support further\nresearch and early adoption, we make our code and methodology publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying machine learning (ML) models on user devices can improve privacy\n(by keeping data local) and reduce inference latency. Trusted Execution\nEnvironments (TEEs) are a practical solution for protecting proprietary models,\nyet existing TEE solutions have architectural constraints that hinder on-device\nmodel deployment. Arm Confidential Computing Architecture (CCA), a new Arm\nextension, addresses several of these limitations and shows promise as a secure\nplatform for on-device ML. In this paper, we evaluate the performance-privacy\ntrade-offs of deploying models within CCA, highlighting its potential to enable\nconfidential and efficient ML applications. Our evaluations show that CCA can\nachieve an overhead of, at most, 22% in running models of different sizes and\napplications, including image classification, voice recognition, and chat\nassistants. This performance overhead comes with privacy benefits; for example,\nour framework can successfully protect the model against membership inference\nattack by an 8.3% reduction in the adversary's success rate. To support further\nresearch and early adoption, we make our code and methodology publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Sina Abdollahi"
                    },
                    {
                        "name": "Mohammad Maheri"
                    },
                    {
                        "name": "Sandra Siby"
                    },
                    {
                        "name": "Marios Kogias"
                    },
                    {
                        "name": "Hamed Haddadi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Haddadi"
                },
                "author": "Hamed Haddadi",
                "arxiv_comment": "Accepted to the 8th Workshop on System Software for Trusted Execution\n  (SysTEX 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06553v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06553v3",
                "updated": "2025-04-11T12:57:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    57,
                    13,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-09T03:22:52Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    22,
                    52,
                    2,
                    99,
                    0
                ],
                "title": "ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis"
                },
                "summary": "While recent work in scene reconstruction and understanding has made strides\nin grounding natural language to physical 3D environments, it is still\nchallenging to ground abstract, high-level instructions to a 3D scene.\nHigh-level instructions might not explicitly invoke semantic elements in the\nscene, and even the process of breaking a high-level task into a set of more\nconcrete subtasks, a process called hierarchical task analysis, is\nenvironment-dependent. In this work, we propose ASHiTA, the first framework\nthat generates a task hierarchy grounded to a 3D scene graph by breaking down\nhigh-level tasks into grounded subtasks. ASHiTA alternates LLM-assisted\nhierarchical task analysis, to generate the task breakdown, with task-driven 3D\nscene graph construction to generate a suitable representation of the\nenvironment. Our experiments show that ASHiTA performs significantly better\nthan LLM baselines in breaking down high-level tasks into environment-dependent\nsubtasks and is additionally able to achieve grounding performance comparable\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent work in scene reconstruction and understanding has made strides\nin grounding natural language to physical 3D environments, it is still\nchallenging to ground abstract, high-level instructions to a 3D scene.\nHigh-level instructions might not explicitly invoke semantic elements in the\nscene, and even the process of breaking a high-level task into a set of more\nconcrete subtasks, a process called hierarchical task analysis, is\nenvironment-dependent. In this work, we propose ASHiTA, the first framework\nthat generates a task hierarchy grounded to a 3D scene graph by breaking down\nhigh-level tasks into grounded subtasks. ASHiTA alternates LLM-assisted\nhierarchical task analysis, to generate the task breakdown, with task-driven 3D\nscene graph construction to generate a suitable representation of the\nenvironment. Our experiments show that ASHiTA performs significantly better\nthan LLM baselines in breaking down high-level tasks into environment-dependent\nsubtasks and is additionally able to achieve grounding performance comparable\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yun Chang"
                    },
                    {
                        "name": "Leonor Fermoselle"
                    },
                    {
                        "name": "Duy Ta"
                    },
                    {
                        "name": "Bernadette Bucher"
                    },
                    {
                        "name": "Luca Carlone"
                    },
                    {
                        "name": "Jiuguang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiuguang Wang"
                },
                "author": "Jiuguang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06553v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06553v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.15317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.15317v2",
                "updated": "2025-04-11T12:42:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    42,
                    25,
                    4,
                    101,
                    0
                ],
                "published": "2024-07-22T01:04:16Z",
                "published_parsed": [
                    2024,
                    7,
                    22,
                    1,
                    4,
                    16,
                    0,
                    204,
                    0
                ],
                "title": "Open-CD: A Comprehensive Toolbox for Change Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-CD: A Comprehensive Toolbox for Change Detection"
                },
                "summary": "We present Open-CD, a change detection toolbox that contains a rich set of\nchange detection methods as well as related components and modules. The toolbox\nstarted from a series of open source general vision task tools, including\nOpenMMLab Toolkits, PyTorch Image Models, etc. It gradually evolves into a\nunified platform that covers many popular change detection methods and\ncontemporary modules. It not only includes training and inference codes, but\nalso provides some useful scripts for data analysis. We believe this toolbox is\nby far the most complete change detection toolbox. In this report, we introduce\nthe various features, supported methods and applications of Open-CD. In\naddition, we also conduct a benchmarking study on different methods and\ncomponents. We wish that the toolbox and benchmark could serve the growing\nresearch community by providing a flexible toolkit to reimplement existing\nmethods and develop their own new change detectors. Code and models are\navailable at https://github.com/likyoo/open-cd. Pioneeringly, this report also\nincludes brief descriptions of the algorithms supported in Open-CD, mainly\ncontributed by their authors. We sincerely encourage researchers in this field\nto participate in this project and work together to create a more open\ncommunity. This toolkit and report will be kept updated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Open-CD, a change detection toolbox that contains a rich set of\nchange detection methods as well as related components and modules. The toolbox\nstarted from a series of open source general vision task tools, including\nOpenMMLab Toolkits, PyTorch Image Models, etc. It gradually evolves into a\nunified platform that covers many popular change detection methods and\ncontemporary modules. It not only includes training and inference codes, but\nalso provides some useful scripts for data analysis. We believe this toolbox is\nby far the most complete change detection toolbox. In this report, we introduce\nthe various features, supported methods and applications of Open-CD. In\naddition, we also conduct a benchmarking study on different methods and\ncomponents. We wish that the toolbox and benchmark could serve the growing\nresearch community by providing a flexible toolkit to reimplement existing\nmethods and develop their own new change detectors. Code and models are\navailable at https://github.com/likyoo/open-cd. Pioneeringly, this report also\nincludes brief descriptions of the algorithms supported in Open-CD, mainly\ncontributed by their authors. We sincerely encourage researchers in this field\nto participate in this project and work together to create a more open\ncommunity. This toolkit and report will be kept updated."
                },
                "authors": [
                    {
                        "name": "Kaiyu Li"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Andrea Codegoni"
                    },
                    {
                        "name": "Chengxi Han"
                    },
                    {
                        "name": "Yupeng Deng"
                    },
                    {
                        "name": "Keyan Chen"
                    },
                    {
                        "name": "Zhuo Zheng"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Ziyuan Liu"
                    },
                    {
                        "name": "Yuantao Gu"
                    },
                    {
                        "name": "Zhengxia Zou"
                    },
                    {
                        "name": "Zhenwei Shi"
                    },
                    {
                        "name": "Sheng Fang"
                    },
                    {
                        "name": "Deyu Meng"
                    },
                    {
                        "name": "Zhi Wang"
                    },
                    {
                        "name": "Xiangyong Cao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyong Cao"
                },
                "author": "Xiangyong Cao",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.15317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.15317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08490v1",
                "updated": "2025-04-11T12:42:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    42,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T12:42:01Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    42,
                    1,
                    4,
                    101,
                    0
                ],
                "title": "Adopting Large Language Models to Automated System Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adopting Large Language Models to Automated System Integration"
                },
                "summary": "Modern enterprise computing systems integrate numerous subsystems to resolve\na common task by yielding emergent behavior. A widespread approach is using\nservices implemented with Web technologies like REST or OpenAPI, which offer an\ninteraction mechanism and service documentation standard, respectively. Each\nservice represents a specific business functionality, allowing encapsulation\nand easier maintenance. Despite the reduced maintenance costs on an individual\nservice level, increased integration complexity arises. Consequently, automated\nservice composition approaches have arisen to mitigate this issue.\nNevertheless, these approaches have not achieved high acceptance in practice\ndue to their reliance on complex formal modeling. Within this Ph.D. thesis, we\nanalyze the application of Large Language Models (LLMs) to automatically\nintegrate the services based on a natural language input. The result is a\nreusable service composition, e.g., as program code. While not always\ngenerating entirely correct results, the result can still be helpful by\nproviding integration engineers with a close approximation of a suitable\nsolution, which requires little effort to become operational. Our research\ninvolves (i) introducing a software architecture for automated service\ncomposition using LLMs, (ii) analyzing Retrieval Augmented Generation (RAG) for\nservice discovery, (iii) proposing a novel natural language query-based\nbenchmark for service discovery, and (iv) extending the benchmark to complete\nservice composition scenarios. We have presented our software architecture as\nCompositio Prompto, the analysis of RAG for service discovery, and submitted a\nproposal for the service discovery benchmark. Open topics are primarily the\nextension of the service discovery benchmark to service composition scenarios\nand the improvements of the service composition generation, e.g., using\nfine-tuning or LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern enterprise computing systems integrate numerous subsystems to resolve\na common task by yielding emergent behavior. A widespread approach is using\nservices implemented with Web technologies like REST or OpenAPI, which offer an\ninteraction mechanism and service documentation standard, respectively. Each\nservice represents a specific business functionality, allowing encapsulation\nand easier maintenance. Despite the reduced maintenance costs on an individual\nservice level, increased integration complexity arises. Consequently, automated\nservice composition approaches have arisen to mitigate this issue.\nNevertheless, these approaches have not achieved high acceptance in practice\ndue to their reliance on complex formal modeling. Within this Ph.D. thesis, we\nanalyze the application of Large Language Models (LLMs) to automatically\nintegrate the services based on a natural language input. The result is a\nreusable service composition, e.g., as program code. While not always\ngenerating entirely correct results, the result can still be helpful by\nproviding integration engineers with a close approximation of a suitable\nsolution, which requires little effort to become operational. Our research\ninvolves (i) introducing a software architecture for automated service\ncomposition using LLMs, (ii) analyzing Retrieval Augmented Generation (RAG) for\nservice discovery, (iii) proposing a novel natural language query-based\nbenchmark for service discovery, and (iv) extending the benchmark to complete\nservice composition scenarios. We have presented our software architecture as\nCompositio Prompto, the analysis of RAG for service discovery, and submitted a\nproposal for the service discovery benchmark. Open topics are primarily the\nextension of the service discovery benchmark to service composition scenarios\nand the improvements of the service composition generation, e.g., using\nfine-tuning or LLM agents."
                },
                "authors": [
                    {
                        "name": "Robin D. Pesl"
                    }
                ],
                "author_detail": {
                    "name": "Robin D. Pesl"
                },
                "author": "Robin D. Pesl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v3",
                "updated": "2025-04-11T12:31:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    31,
                    7,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08486v1",
                "updated": "2025-04-11T12:24:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    24,
                    6,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T12:24:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    24,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "PlugSelect: Pruning Channels with Plug-and-Play Flexibility for\n  Electroencephalography-based Brain Computer Interface",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PlugSelect: Pruning Channels with Plug-and-Play Flexibility for\n  Electroencephalography-based Brain Computer Interface"
                },
                "summary": "Automatic minimization and optimization of the number of the electrodes is\nessential for the practical application of electroencephalography (EEG)-based\nbrain computer interface (BCI). Previous methods typically require additional\ntraining costs or rely on prior knowledge assumptions. This study proposed a\nnovel channel pruning model, plug-and-select (PlugSelect), applicable across a\nbroad range of BCI paradigms with no additional training cost and plug-and-play\nfunctionality. It integrates gradients along the input path to globally infer\nthe causal relationships between input channels and outputs, and ranks the\ncontribution sequences to identify the most highly attributed channels. The\nresults showed that for three BCI paradigms, i.e., auditory attention decoding\n(AAD), motor imagery (MI), affective computation (AC), PlugSelect could reduce\nthe number of channels by at least half while effectively maintaining decoding\nperformance and improving efficiency. The outcome benefits the design of\nwearable EEG-based devices, facilitating the practical application of BCI\ntechnology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic minimization and optimization of the number of the electrodes is\nessential for the practical application of electroencephalography (EEG)-based\nbrain computer interface (BCI). Previous methods typically require additional\ntraining costs or rely on prior knowledge assumptions. This study proposed a\nnovel channel pruning model, plug-and-select (PlugSelect), applicable across a\nbroad range of BCI paradigms with no additional training cost and plug-and-play\nfunctionality. It integrates gradients along the input path to globally infer\nthe causal relationships between input channels and outputs, and ranks the\ncontribution sequences to identify the most highly attributed channels. The\nresults showed that for three BCI paradigms, i.e., auditory attention decoding\n(AAD), motor imagery (MI), affective computation (AC), PlugSelect could reduce\nthe number of channels by at least half while effectively maintaining decoding\nperformance and improving efficiency. The outcome benefits the design of\nwearable EEG-based devices, facilitating the practical application of BCI\ntechnology."
                },
                "authors": [
                    {
                        "name": "Xue Yuan"
                    },
                    {
                        "name": "Keren Shi"
                    },
                    {
                        "name": "Ning Jiang"
                    },
                    {
                        "name": "Jiayuan He"
                    }
                ],
                "author_detail": {
                    "name": "Jiayuan He"
                },
                "author": "Jiayuan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.10425v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.10425v3",
                "updated": "2025-04-11T12:15:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    15,
                    38,
                    4,
                    101,
                    0
                ],
                "published": "2024-08-19T21:28:32Z",
                "published_parsed": [
                    2024,
                    8,
                    19,
                    21,
                    28,
                    32,
                    0,
                    232,
                    0
                ],
                "title": "Constraining the Generalized Tolman-Oppenheimer-Volkoff (GTOV) equation\n  with Bayesian analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraining the Generalized Tolman-Oppenheimer-Volkoff (GTOV) equation\n  with Bayesian analysis"
                },
                "summary": "In this work, we constrain the values of the parameters of the Generalized\nTolman-Oppenheimer-Volkoff (GTOV) equation through Bayesian inference. We use\nthe mass and radius data from the Neutron Star Interior Composition Explorer\n(NICER) for PSR J0740$+$6620 and PSR J0030$+$0451, as well as the mass, radius,\nand dimensionless tidal deformability from the gravitational wave (GW) events\nGW190814 and GW170817. We use two distinct parameterizations of the extended\nnon-linear Walecka model (eNLW) with and without hyperons. The GTOV employed\nfor the study contains additional free parameters with different physical\nmotivations. Two possible scenarios are considered in our analysis:\nconservative and speculative. In the first case, we take into account the most\nreliable neutron star (NS) data from NICER and the GW170817 event. In the\nsecond case, we consider the possibility that the compact object with a mass of\n$2.54 M_{\\odot}$ in the GW190814 event is an NS. Our findings show significant\nimprovements in the physical quantities analyzed, leading to better agreement\nwith the observational data compared to the results obtained using the TOV\nequation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we constrain the values of the parameters of the Generalized\nTolman-Oppenheimer-Volkoff (GTOV) equation through Bayesian inference. We use\nthe mass and radius data from the Neutron Star Interior Composition Explorer\n(NICER) for PSR J0740$+$6620 and PSR J0030$+$0451, as well as the mass, radius,\nand dimensionless tidal deformability from the gravitational wave (GW) events\nGW190814 and GW170817. We use two distinct parameterizations of the extended\nnon-linear Walecka model (eNLW) with and without hyperons. The GTOV employed\nfor the study contains additional free parameters with different physical\nmotivations. Two possible scenarios are considered in our analysis:\nconservative and speculative. In the first case, we take into account the most\nreliable neutron star (NS) data from NICER and the GW170817 event. In the\nsecond case, we consider the possibility that the compact object with a mass of\n$2.54 M_{\\odot}$ in the GW190814 event is an NS. Our findings show significant\nimprovements in the physical quantities analyzed, leading to better agreement\nwith the observational data compared to the results obtained using the TOV\nequation."
                },
                "authors": [
                    {
                        "name": "Franciele M. da Silva"
                    },
                    {
                        "name": "Fbio Kpp"
                    },
                    {
                        "name": "Marcelo D. Alloy"
                    },
                    {
                        "name": "Luis C. N. Santos"
                    },
                    {
                        "name": "Adamu Issifu"
                    },
                    {
                        "name": "Clsio E. Mota"
                    },
                    {
                        "name": "Dbora P. Menezes"
                    }
                ],
                "author_detail": {
                    "name": "Dbora P. Menezes"
                },
                "author": "Dbora P. Menezes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.10425v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.10425v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09111v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09111v7",
                "updated": "2025-04-11T12:09:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    9,
                    42,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-14T00:59:13Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    0,
                    59,
                    13,
                    3,
                    319,
                    0
                ],
                "title": "Reducing Reasoning Costs: The Path of Optimization for Chain of Thought\n  via Sparse Attention Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Reasoning Costs: The Path of Optimization for Chain of Thought\n  via Sparse Attention Mechanism"
                },
                "summary": "In order to address the chain of thought in the large language model\ninference cost surge, this research proposes to use a sparse attention\nmechanism that only focuses on a few relevant tokens. The researcher\nconstructed a new attention mechanism and used GiantRabbit trained with custom\nGPTs as an experimental tool. The experiment tested and compared the reasoning\ntime, correctness score and chain of thought length of this model and o1\nPreview in solving the linear algebra test questions of MIT OpenCourseWare. The\nresults show that GiantRabbit's reasoning time and chain of thought length are\nsignificantly lower than o1 Preview. It verifies the feasibility of sparse\nattention mechanism for optimizing chain of thought reasoning. Detailed\narchitectural details and experimental process have been uploaded to Github,\nthe link is:https://github.com/brucewang123456789/GeniusTrail.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to address the chain of thought in the large language model\ninference cost surge, this research proposes to use a sparse attention\nmechanism that only focuses on a few relevant tokens. The researcher\nconstructed a new attention mechanism and used GiantRabbit trained with custom\nGPTs as an experimental tool. The experiment tested and compared the reasoning\ntime, correctness score and chain of thought length of this model and o1\nPreview in solving the linear algebra test questions of MIT OpenCourseWare. The\nresults show that GiantRabbit's reasoning time and chain of thought length are\nsignificantly lower than o1 Preview. It verifies the feasibility of sparse\nattention mechanism for optimizing chain of thought reasoning. Detailed\narchitectural details and experimental process have been uploaded to Github,\nthe link is:https://github.com/brucewang123456789/GeniusTrail.git."
                },
                "authors": [
                    {
                        "name": "Libo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Libo Wang"
                },
                "author": "Libo Wang",
                "arxiv_comment": "The main text is 5 pages, totaling 9 pages; 4 figures, 1 table. It\n  have been submitted to NeurIPS 2024 Workshop MusIML and OpenReview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09111v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09111v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08452v1",
                "updated": "2025-04-11T11:28:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    11,
                    28,
                    0,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T11:28:00Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    11,
                    28,
                    0,
                    4,
                    101,
                    0
                ],
                "title": "Road Grip Uncertainty Estimation Through Surface State Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Road Grip Uncertainty Estimation Through Surface State Segmentation"
                },
                "summary": "Slippery road conditions pose significant challenges for autonomous driving.\nBeyond predicting road grip, it is crucial to estimate its uncertainty reliably\nto ensure safe vehicle control. In this work, we benchmark several uncertainty\nprediction methods to assess their effectiveness for grip uncertainty\nestimation. Additionally, we propose a novel approach that leverages road\nsurface state segmentation to predict grip uncertainty. Our method estimates a\npixel-wise grip probability distribution based on inferred road surface\nconditions. Experimental results indicate that the proposed approach enhances\nthe robustness of grip uncertainty prediction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Slippery road conditions pose significant challenges for autonomous driving.\nBeyond predicting road grip, it is crucial to estimate its uncertainty reliably\nto ensure safe vehicle control. In this work, we benchmark several uncertainty\nprediction methods to assess their effectiveness for grip uncertainty\nestimation. Additionally, we propose a novel approach that leverages road\nsurface state segmentation to predict grip uncertainty. Our method estimates a\npixel-wise grip probability distribution based on inferred road surface\nconditions. Experimental results indicate that the proposed approach enhances\nthe robustness of grip uncertainty prediction."
                },
                "authors": [
                    {
                        "name": "Jyri Maanp"
                    },
                    {
                        "name": "Julius Pesonen"
                    },
                    {
                        "name": "Iaroslav Melekhov"
                    },
                    {
                        "name": "Heikki Hyyti"
                    },
                    {
                        "name": "Juha Hyypp"
                    }
                ],
                "author_detail": {
                    "name": "Juha Hyypp"
                },
                "author": "Juha Hyypp",
                "arxiv_comment": "15 pages, 5 figures (supplementary material 2 pages, 1 figure).\n  Anonymized version submitted to Scandinavian Conference on Image Analysis\n  (SCIA) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08449v1",
                "updated": "2025-04-11T11:18:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    11,
                    18,
                    57,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T11:18:57Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    11,
                    18,
                    57,
                    4,
                    101,
                    0
                ],
                "title": "Ego4o: Egocentric Human Motion Capture and Understanding from\n  Multi-Modal Input",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ego4o: Egocentric Human Motion Capture and Understanding from\n  Multi-Modal Input"
                },
                "summary": "This work focuses on tracking and understanding human motion using consumer\nwearable devices, such as VR/AR headsets, smart glasses, cellphones, and\nsmartwatches. These devices provide diverse, multi-modal sensor inputs,\nincluding egocentric images, and 1-3 sparse IMU sensors in varied combinations.\nMotion descriptions can also accompany these signals. The diverse input\nmodalities and their intermittent availability pose challenges for consistent\nmotion capture and understanding. In this work, we present Ego4o (o for omni),\na new framework for simultaneous human motion capture and understanding from\nmulti-modal egocentric inputs. This method maintains performance with partial\ninputs while achieving better results when multiple modalities are combined.\nFirst, the IMU sensor inputs, the optional egocentric image, and text\ndescription of human motion are encoded into the latent space of a motion\nVQ-VAE. Next, the latent vectors are sent to the VQ-VAE decoder and optimized\nto track human motion. When motion descriptions are unavailable, the latent\nvectors can be input into a multi-modal LLM to generate human motion\ndescriptions, which can further enhance motion capture accuracy. Quantitative\nand qualitative evaluations demonstrate the effectiveness of our method in\npredicting accurate human motion and high-quality motion descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work focuses on tracking and understanding human motion using consumer\nwearable devices, such as VR/AR headsets, smart glasses, cellphones, and\nsmartwatches. These devices provide diverse, multi-modal sensor inputs,\nincluding egocentric images, and 1-3 sparse IMU sensors in varied combinations.\nMotion descriptions can also accompany these signals. The diverse input\nmodalities and their intermittent availability pose challenges for consistent\nmotion capture and understanding. In this work, we present Ego4o (o for omni),\na new framework for simultaneous human motion capture and understanding from\nmulti-modal egocentric inputs. This method maintains performance with partial\ninputs while achieving better results when multiple modalities are combined.\nFirst, the IMU sensor inputs, the optional egocentric image, and text\ndescription of human motion are encoded into the latent space of a motion\nVQ-VAE. Next, the latent vectors are sent to the VQ-VAE decoder and optimized\nto track human motion. When motion descriptions are unavailable, the latent\nvectors can be input into a multi-modal LLM to generate human motion\ndescriptions, which can further enhance motion capture accuracy. Quantitative\nand qualitative evaluations demonstrate the effectiveness of our method in\npredicting accurate human motion and high-quality motion descriptions."
                },
                "authors": [
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Rishabh Dabral"
                    },
                    {
                        "name": "Diogo Luvizon"
                    },
                    {
                        "name": "Zhe Cao"
                    },
                    {
                        "name": "Lingjie Liu"
                    },
                    {
                        "name": "Thabo Beeler"
                    },
                    {
                        "name": "Christian Theobalt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Theobalt"
                },
                "author": "Christian Theobalt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17013v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17013v2",
                "updated": "2025-04-11T11:11:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    11,
                    11,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-26T00:45:49Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    0,
                    45,
                    49,
                    1,
                    331,
                    0
                ],
                "title": "Conditional Extremes with Graphical Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Extremes with Graphical Models"
                },
                "summary": "Multivariate extreme value analysis quantifies the probability and magnitude\nof joint extreme events. River discharges from the upper Danube River basin\nprovide a challenging dataset for such analysis because the data, which is\nmeasured on a spatial network, exhibits both asymptotic dependence and\nasymptotic independence. To account for both features, we extend the\nconditional multivariate extreme value model (CMEVM) with a new approach for\nthe residual distribution. This allows sparse (graphical) dependence structures\nand fully parametric prediction. Our approach fills a current gap in\nstatistical methodology by extending graphical extremes models to\nasymptotically independent random variables. Further, the model can be used to\nlearn the graphical dependence structure when it is unknown a priori. To\nsupport inference in high dimensions, we propose a stepwise inference procedure\nthat is computationally efficient and loses no information or predictive power.\nWe show our method is flexible and accurately captures the extremal dependence\nfor the upper Danube River basin discharges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multivariate extreme value analysis quantifies the probability and magnitude\nof joint extreme events. River discharges from the upper Danube River basin\nprovide a challenging dataset for such analysis because the data, which is\nmeasured on a spatial network, exhibits both asymptotic dependence and\nasymptotic independence. To account for both features, we extend the\nconditional multivariate extreme value model (CMEVM) with a new approach for\nthe residual distribution. This allows sparse (graphical) dependence structures\nand fully parametric prediction. Our approach fills a current gap in\nstatistical methodology by extending graphical extremes models to\nasymptotically independent random variables. Further, the model can be used to\nlearn the graphical dependence structure when it is unknown a priori. To\nsupport inference in high dimensions, we propose a stepwise inference procedure\nthat is computationally efficient and loses no information or predictive power.\nWe show our method is flexible and accurately captures the extremal dependence\nfor the upper Danube River basin discharges."
                },
                "authors": [
                    {
                        "name": "Aiden Farrell"
                    },
                    {
                        "name": "Emma F. Eastoe"
                    },
                    {
                        "name": "Clement Lee"
                    }
                ],
                "author_detail": {
                    "name": "Clement Lee"
                },
                "author": "Clement Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17013v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17013v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08417v1",
                "updated": "2025-04-11T10:21:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    21,
                    58,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T10:21:58Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    21,
                    58,
                    4,
                    101,
                    0
                ],
                "title": "Belief States for Cooperative Multi-Agent Reinforcement Learning under\n  Partial Observability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Belief States for Cooperative Multi-Agent Reinforcement Learning under\n  Partial Observability"
                },
                "summary": "Reinforcement learning in partially observable environments is typically\nchallenging, as it requires agents to learn an estimate of the underlying\nsystem state. These challenges are exacerbated in multi-agent settings, where\nagents learn simultaneously and influence the underlying state as well as each\nothers' observations. We propose the use of learned beliefs on the underlying\nstate of the system to overcome these challenges and enable reinforcement\nlearning with fully decentralized training and execution. Our approach\nleverages state information to pre-train a probabilistic belief model in a\nself-supervised fashion. The resulting belief states, which capture both\ninferred state information as well as uncertainty over this information, are\nthen used in a state-based reinforcement learning algorithm to create an\nend-to-end model for cooperative multi-agent reinforcement learning under\npartial observability. By separating the belief and reinforcement learning\ntasks, we are able to significantly simplify the policy and value function\nlearning tasks and improve both the convergence speed and the final\nperformance. We evaluate our proposed method on diverse partially observable\nmulti-agent tasks designed to exhibit different variants of partial\nobservability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning in partially observable environments is typically\nchallenging, as it requires agents to learn an estimate of the underlying\nsystem state. These challenges are exacerbated in multi-agent settings, where\nagents learn simultaneously and influence the underlying state as well as each\nothers' observations. We propose the use of learned beliefs on the underlying\nstate of the system to overcome these challenges and enable reinforcement\nlearning with fully decentralized training and execution. Our approach\nleverages state information to pre-train a probabilistic belief model in a\nself-supervised fashion. The resulting belief states, which capture both\ninferred state information as well as uncertainty over this information, are\nthen used in a state-based reinforcement learning algorithm to create an\nend-to-end model for cooperative multi-agent reinforcement learning under\npartial observability. By separating the belief and reinforcement learning\ntasks, we are able to significantly simplify the policy and value function\nlearning tasks and improve both the convergence speed and the final\nperformance. We evaluate our proposed method on diverse partially observable\nmulti-agent tasks designed to exhibit different variants of partial\nobservability."
                },
                "authors": [
                    {
                        "name": "Paul J. Pritz"
                    },
                    {
                        "name": "Kin K. Leung"
                    }
                ],
                "author_detail": {
                    "name": "Kin K. Leung"
                },
                "author": "Kin K. Leung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08415v1",
                "updated": "2025-04-11T10:19:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    19,
                    49,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T10:19:49Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    19,
                    49,
                    4,
                    101,
                    0
                ],
                "title": "Constrained Machine Learning Through Hyperspherical Representation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constrained Machine Learning Through Hyperspherical Representation"
                },
                "summary": "The problem of ensuring constraints satisfaction on the output of machine\nlearning models is critical for many applications, especially in\nsafety-critical domains. Modern approaches rely on penalty-based methods at\ntraining time, which do not guarantee to avoid constraints violations; or\nconstraint-specific model architectures (e.g., for monotonocity); or on output\nprojection, which requires to solve an optimization problem that might be\ncomputationally demanding. We present the Hypersherical Constrained\nRepresentation, a novel method to enforce constraints in the output space for\nconvex and bounded feasibility regions (generalizable to star domains). Our\nmethod operates on a different representation system, where Euclidean\ncoordinates are converted into hyperspherical coordinates relative to the\nconstrained region, which can only inherently represent feasible points.\nExperiments on a synthetic and a real-world dataset show that our method has\npredictive performance comparable to the other approaches, can guarantee 100%\nconstraint satisfaction, and has a minimal computational cost at inference\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The problem of ensuring constraints satisfaction on the output of machine\nlearning models is critical for many applications, especially in\nsafety-critical domains. Modern approaches rely on penalty-based methods at\ntraining time, which do not guarantee to avoid constraints violations; or\nconstraint-specific model architectures (e.g., for monotonocity); or on output\nprojection, which requires to solve an optimization problem that might be\ncomputationally demanding. We present the Hypersherical Constrained\nRepresentation, a novel method to enforce constraints in the output space for\nconvex and bounded feasibility regions (generalizable to star domains). Our\nmethod operates on a different representation system, where Euclidean\ncoordinates are converted into hyperspherical coordinates relative to the\nconstrained region, which can only inherently represent feasible points.\nExperiments on a synthetic and a real-world dataset show that our method has\npredictive performance comparable to the other approaches, can guarantee 100%\nconstraint satisfaction, and has a minimal computational cost at inference\ntime."
                },
                "authors": [
                    {
                        "name": "Gaetano Signorelli"
                    },
                    {
                        "name": "Michele Lombardi"
                    }
                ],
                "author_detail": {
                    "name": "Michele Lombardi"
                },
                "author": "Michele Lombardi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07199v2",
                "updated": "2025-04-11T10:14:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    14,
                    39,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-09T18:26:46Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    18,
                    26,
                    46,
                    2,
                    99,
                    0
                ],
                "title": "SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject\n  Tagging for a National Technical Library's Open-Access Catalog",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject\n  Tagging for a National Technical Library's Open-Access Catalog"
                },
                "summary": "We present SemEval-2025 Task 5: LLMs4Subjects, a shared task on automated\nsubject tagging for scientific and technical records in English and German\nusing the GND taxonomy. Participants developed LLM-based systems to recommend\ntop-k subjects, evaluated through quantitative metrics (precision, recall,\nF1-score) and qualitative assessments by subject specialists. Results highlight\nthe effectiveness of LLM ensembles, synthetic data generation, and multilingual\nprocessing, offering insights into applying LLMs for digital library\nclassification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SemEval-2025 Task 5: LLMs4Subjects, a shared task on automated\nsubject tagging for scientific and technical records in English and German\nusing the GND taxonomy. Participants developed LLM-based systems to recommend\ntop-k subjects, evaluated through quantitative metrics (precision, recall,\nF1-score) and qualitative assessments by subject specialists. Results highlight\nthe effectiveness of LLM ensembles, synthetic data generation, and multilingual\nprocessing, offering insights into applying LLMs for digital library\nclassification."
                },
                "authors": [
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Sameer Sadruddin"
                    },
                    {
                        "name": "Holger Israel"
                    },
                    {
                        "name": "Mathias Begoin"
                    },
                    {
                        "name": "Diana Slawig"
                    }
                ],
                "author_detail": {
                    "name": "Diana Slawig"
                },
                "author": "Diana Slawig",
                "arxiv_comment": "10 pages, 4 figures, Accepted as SemEval 2025 Task 5 description\n  paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08400v1",
                "updated": "2025-04-11T10:04:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    4,
                    12,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T10:04:12Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    4,
                    12,
                    4,
                    101,
                    0
                ],
                "title": "A Reproducibility Study of Graph-Based Legal Case Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducibility Study of Graph-Based Legal Case Retrieval"
                },
                "summary": "Legal retrieval is a widely studied area in Information Retrieval (IR) and a\nkey task in this domain is retrieving relevant cases based on a given query\ncase, often done by applying language models as encoders to model case\nsimilarity. Recently, Tang et al. proposed CaseLink, a novel graph-based method\nfor legal case retrieval, which models both cases and legal charges as nodes in\na network, with edges representing relationships such as references and shared\nsemantics. This approach offers a new perspective on the task by capturing\nhigher-order relationships of cases going beyond the stand-alone level of\ndocuments. However, while this shift in approaching legal case retrieval is a\npromising direction in an understudied area of graph-based legal IR, challenges\nin reproducing novel results have recently been highlighted, with multiple\nstudies reporting difficulties in reproducing previous findings. Thus, in this\nwork we reproduce CaseLink, a graph-based legal case retrieval method, to\nsupport future research in this area of IR. In particular, we aim to assess its\nreliability and generalizability by (i) first reproducing the original study\nsetup and (ii) applying the approach to an additional dataset. We then build\nupon the original implementations by (iii) evaluating the approach's\nperformance when using a more sophisticated graph data representation and (iv)\nusing an open large language model (LLM) in the pipeline to address limitations\nthat are known to result from using closed models accessed via an API. Our\nfindings aim to improve the understanding of graph-based approaches in legal IR\nand contribute to improving reproducibility in the field. To achieve this, we\nshare all our implementations and experimental artifacts with the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal retrieval is a widely studied area in Information Retrieval (IR) and a\nkey task in this domain is retrieving relevant cases based on a given query\ncase, often done by applying language models as encoders to model case\nsimilarity. Recently, Tang et al. proposed CaseLink, a novel graph-based method\nfor legal case retrieval, which models both cases and legal charges as nodes in\na network, with edges representing relationships such as references and shared\nsemantics. This approach offers a new perspective on the task by capturing\nhigher-order relationships of cases going beyond the stand-alone level of\ndocuments. However, while this shift in approaching legal case retrieval is a\npromising direction in an understudied area of graph-based legal IR, challenges\nin reproducing novel results have recently been highlighted, with multiple\nstudies reporting difficulties in reproducing previous findings. Thus, in this\nwork we reproduce CaseLink, a graph-based legal case retrieval method, to\nsupport future research in this area of IR. In particular, we aim to assess its\nreliability and generalizability by (i) first reproducing the original study\nsetup and (ii) applying the approach to an additional dataset. We then build\nupon the original implementations by (iii) evaluating the approach's\nperformance when using a more sophisticated graph data representation and (iv)\nusing an open large language model (LLM) in the pipeline to address limitations\nthat are known to result from using closed models accessed via an API. Our\nfindings aim to improve the understanding of graph-based approaches in legal IR\nand contribute to improving reproducibility in the field. To achieve this, we\nshare all our implementations and experimental artifacts with the community."
                },
                "authors": [
                    {
                        "name": "Gregor Donabauer"
                    },
                    {
                        "name": "Udo Kruschwitz"
                    }
                ],
                "author_detail": {
                    "name": "Udo Kruschwitz"
                },
                "author": "Udo Kruschwitz",
                "arxiv_comment": "Preprint accepted at SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15865v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15865v2",
                "updated": "2025-04-11T10:03:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    3,
                    59,
                    4,
                    101,
                    0
                ],
                "published": "2025-01-27T08:42:40Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    8,
                    42,
                    40,
                    0,
                    27,
                    0
                ],
                "title": "Transfer of Knowledge through Reverse Annealing: A Preliminary Analysis\n  of the Benefits and What to Share",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transfer of Knowledge through Reverse Annealing: A Preliminary Analysis\n  of the Benefits and What to Share"
                },
                "summary": "Being immersed in the NISQ-era, current quantum annealers present limitations\nfor solving optimization problems efficiently. To mitigate these limitations,\nD-Wave Systems developed a mechanism called Reverse Annealing, a specific type\nof quantum annealing designed to perform local refinement of good states found\nelsewhere. Despite the research activity around Reverse Annealing, none has\ntheorized about the possible benefits related to the transfer of knowledge\nunder this paradigm. This work moves in that direction and is driven by\nexperimentation focused on answering two key research questions: i) is reverse\nannealing a paradigm that can benefit from knowledge transfer between similar\nproblems? and ii) can we infer the characteristics that an input solution\nshould meet to help increase the probability of success? To properly guide the\ntests in this paper, the well-known Knapsack Problem has been chosen for\nbenchmarking purposes, using a total of 34 instances composed of 14 and 16\nitems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Being immersed in the NISQ-era, current quantum annealers present limitations\nfor solving optimization problems efficiently. To mitigate these limitations,\nD-Wave Systems developed a mechanism called Reverse Annealing, a specific type\nof quantum annealing designed to perform local refinement of good states found\nelsewhere. Despite the research activity around Reverse Annealing, none has\ntheorized about the possible benefits related to the transfer of knowledge\nunder this paradigm. This work moves in that direction and is driven by\nexperimentation focused on answering two key research questions: i) is reverse\nannealing a paradigm that can benefit from knowledge transfer between similar\nproblems? and ii) can we infer the characteristics that an input solution\nshould meet to help increase the probability of success? To properly guide the\ntests in this paper, the well-known Knapsack Problem has been chosen for\nbenchmarking purposes, using a total of 34 instances composed of 14 and 16\nitems."
                },
                "authors": [
                    {
                        "name": "Eneko Osaba"
                    },
                    {
                        "name": "Esther Villar-Rodriguez"
                    }
                ],
                "author_detail": {
                    "name": "Esther Villar-Rodriguez"
                },
                "author": "Esther Villar-Rodriguez",
                "arxiv_comment": "13 pages, 2 figures and 2 tables. Paper submitted to Frontiers in\n  Physics journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15865v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15865v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08399v1",
                "updated": "2025-04-11T10:03:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    3,
                    55,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T10:03:55Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    3,
                    55,
                    4,
                    101,
                    0
                ],
                "title": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in\n  Large Language Models"
                },
                "summary": "There is a growing interest in assessing the personality traits of Large\nlanguage models (LLMs). However, traditional personality assessments based on\nself-report questionnaires may fail to capture their true behavioral nuances\ndue to inherent biases and meta-knowledge contamination. This paper introduces\na novel multi-observer framework for LLM personality assessment that draws\ninspiration from informant-report methods in psychology. Instead of relying\nsolely on self-assessments, our approach employs multiple observer agents\nconfigured with a specific relationship context (e.g., family, friend, or\nworkplace) to simulate interactive scenarios with a subject LLM. These\nobservers engage in dialogues and subsequently provide ratings across the Big\nFive personality dimensions. Our experiments reveal that LLMs possess\nsystematic biases in self-report personality ratings. Moreover, aggregating\nobserver ratings effectively reduces non-systematic biases and achieves optimal\nreliability with 5-7 observers. The findings highlight the significant impact\nof relationship context on personality perception and demonstrate that a\nmulti-observer paradigm yields a more robust and context-sensitive evaluation\nof LLM personality traits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing interest in assessing the personality traits of Large\nlanguage models (LLMs). However, traditional personality assessments based on\nself-report questionnaires may fail to capture their true behavioral nuances\ndue to inherent biases and meta-knowledge contamination. This paper introduces\na novel multi-observer framework for LLM personality assessment that draws\ninspiration from informant-report methods in psychology. Instead of relying\nsolely on self-assessments, our approach employs multiple observer agents\nconfigured with a specific relationship context (e.g., family, friend, or\nworkplace) to simulate interactive scenarios with a subject LLM. These\nobservers engage in dialogues and subsequently provide ratings across the Big\nFive personality dimensions. Our experiments reveal that LLMs possess\nsystematic biases in self-report personality ratings. Moreover, aggregating\nobserver ratings effectively reduces non-systematic biases and achieves optimal\nreliability with 5-7 observers. The findings highlight the significant impact\nof relationship context on personality perception and demonstrate that a\nmulti-observer paradigm yields a more robust and context-sensitive evaluation\nof LLM personality traits."
                },
                "authors": [
                    {
                        "name": "Yin Jou Huang"
                    },
                    {
                        "name": "Rafik Hadfi"
                    }
                ],
                "author_detail": {
                    "name": "Rafik Hadfi"
                },
                "author": "Rafik Hadfi",
                "arxiv_comment": "13 pages, 5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08398v1",
                "updated": "2025-04-11T10:03:06Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    3,
                    6,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T10:03:06Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    3,
                    6,
                    4,
                    101,
                    0
                ],
                "title": "MixDiT: Accelerating Image Diffusion Transformer Inference with\n  Mixed-Precision MX Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MixDiT: Accelerating Image Diffusion Transformer Inference with\n  Mixed-Precision MX Quantization"
                },
                "summary": "Diffusion Transformer (DiT) has driven significant progress in image\ngeneration tasks. However, DiT inferencing is notoriously compute-intensive and\nincurs long latency even on datacenter-scale GPUs, primarily due to its\niterative nature and heavy reliance on GEMM operations inherent to its\nencoder-based structure. To address the challenge, prior work has explored\nquantization, but achieving low-precision quantization for DiT inferencing with\nboth high accuracy and substantial speedup remains an open problem. To this\nend, this paper proposes MixDiT, an algorithm-hardware co-designed acceleration\nsolution that exploits mixed Microscaling (MX) formats to quantize DiT\nactivation values. MixDiT quantizes the DiT activation tensors by selectively\napplying higher precision to magnitude-based outliers, which produce\nmixed-precision GEMM operations. To achieve tangible speedup from the\nmixed-precision arithmetic, we design a MixDiT accelerator that enables\nprecision-flexible multiplications and efficient MX precision conversions. Our\nexperimental results show that MixDiT delivers a speedup of 2.10-5.32 times\nover RTX 3090, with no loss in FID.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) has driven significant progress in image\ngeneration tasks. However, DiT inferencing is notoriously compute-intensive and\nincurs long latency even on datacenter-scale GPUs, primarily due to its\niterative nature and heavy reliance on GEMM operations inherent to its\nencoder-based structure. To address the challenge, prior work has explored\nquantization, but achieving low-precision quantization for DiT inferencing with\nboth high accuracy and substantial speedup remains an open problem. To this\nend, this paper proposes MixDiT, an algorithm-hardware co-designed acceleration\nsolution that exploits mixed Microscaling (MX) formats to quantize DiT\nactivation values. MixDiT quantizes the DiT activation tensors by selectively\napplying higher precision to magnitude-based outliers, which produce\nmixed-precision GEMM operations. To achieve tangible speedup from the\nmixed-precision arithmetic, we design a MixDiT accelerator that enables\nprecision-flexible multiplications and efficient MX precision conversions. Our\nexperimental results show that MixDiT delivers a speedup of 2.10-5.32 times\nover RTX 3090, with no loss in FID."
                },
                "authors": [
                    {
                        "name": "Daeun Kim"
                    },
                    {
                        "name": "Jinwoo Hwang"
                    },
                    {
                        "name": "Changhun Oh"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10727v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10727v3",
                "updated": "2025-04-11T10:01:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    1,
                    13,
                    4,
                    101,
                    0
                ],
                "published": "2024-01-19T14:44:37Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    14,
                    44,
                    37,
                    4,
                    19,
                    0
                ],
                "title": "MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning"
                },
                "summary": "Recently, the astonishing performance of large language models (LLMs) in\nnatural language comprehension and generation tasks triggered lots of\nexploration of using them as central controllers to build agent systems.\nMultiple studies focus on bridging the LLMs to external tools to extend the\napplication scenarios. However, the current LLMs' ability to perceive tool use\nis limited to a single text query, which may result in ambiguity in\nunderstanding the users' real intentions. LLMs are expected to eliminate that\nby perceiving the information in the visual- or auditory-grounded instructions.\nTherefore, in this paper, we propose MLLM-Tool, a system incorporating\nopen-source LLMs and multi-modal encoders so that the learned LLMs can be\nconscious of multi-modal input instruction and then select the function-matched\ntool correctly. To facilitate the evaluation of the model's capability, we\ncollect a dataset featuring multi-modal input tools from HuggingFace. Another\nessential feature of our dataset is that it also contains multiple potential\nchoices for the same instruction due to the existence of identical functions\nand synonymous functions, which provides more potential solutions for the same\nquery. The experiments reveal that our MLLM-Tool is capable of recommending\nappropriate tools for multi-modal instructions. Codes and data are available at\nhttps://github.com/MLLM-Tool/MLLM-Tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the astonishing performance of large language models (LLMs) in\nnatural language comprehension and generation tasks triggered lots of\nexploration of using them as central controllers to build agent systems.\nMultiple studies focus on bridging the LLMs to external tools to extend the\napplication scenarios. However, the current LLMs' ability to perceive tool use\nis limited to a single text query, which may result in ambiguity in\nunderstanding the users' real intentions. LLMs are expected to eliminate that\nby perceiving the information in the visual- or auditory-grounded instructions.\nTherefore, in this paper, we propose MLLM-Tool, a system incorporating\nopen-source LLMs and multi-modal encoders so that the learned LLMs can be\nconscious of multi-modal input instruction and then select the function-matched\ntool correctly. To facilitate the evaluation of the model's capability, we\ncollect a dataset featuring multi-modal input tools from HuggingFace. Another\nessential feature of our dataset is that it also contains multiple potential\nchoices for the same instruction due to the existence of identical functions\nand synonymous functions, which provides more potential solutions for the same\nquery. The experiments reveal that our MLLM-Tool is capable of recommending\nappropriate tools for multi-modal instructions. Codes and data are available at\nhttps://github.com/MLLM-Tool/MLLM-Tool."
                },
                "authors": [
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Weixin Luo"
                    },
                    {
                        "name": "Sixun Dong"
                    },
                    {
                        "name": "Xiaohua Xuan"
                    },
                    {
                        "name": "Zhengxin Li"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Shenghua Gao"
                    }
                ],
                "author_detail": {
                    "name": "Shenghua Gao"
                },
                "author": "Shenghua Gao",
                "arxiv_comment": "WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10727v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10727v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07557v2",
                "updated": "2025-04-11T09:54:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    54,
                    57,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T08:38:39Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    8,
                    38,
                    39,
                    3,
                    100,
                    0
                ],
                "title": "Using LLMs for Analyzing AIS Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs for Analyzing AIS Data"
                },
                "summary": "Recent research in Large Language Models (LLMs), has had a profound impact\nacross various fields, including mobility data science. This paper explores the\nand experiment with different approaches to using LLMs for analyzing AIS data.\nWe propose a set of carefully designed queries to assess the reasoning\ncapabilities of LLMs in this kind of tasks. Further, we experiment with four\ndifferent methods: (1) using LLMs as a natural language interface to a spatial\ndatabase, (2) reasoning on raw data, (3) reasoning on compressed trajectories,\nand (4) reasoning on semantic trajectories. We investigate the strengths and\nweaknesses for the four methods, and discuss the findings. The goal is to\nprovide valuable insights for both researchers and practitioners on selecting\nthe most appropriate LLM-based method depending on their specific data analysis\nobjectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research in Large Language Models (LLMs), has had a profound impact\nacross various fields, including mobility data science. This paper explores the\nand experiment with different approaches to using LLMs for analyzing AIS data.\nWe propose a set of carefully designed queries to assess the reasoning\ncapabilities of LLMs in this kind of tasks. Further, we experiment with four\ndifferent methods: (1) using LLMs as a natural language interface to a spatial\ndatabase, (2) reasoning on raw data, (3) reasoning on compressed trajectories,\nand (4) reasoning on semantic trajectories. We investigate the strengths and\nweaknesses for the four methods, and discuss the findings. The goal is to\nprovide valuable insights for both researchers and practitioners on selecting\nthe most appropriate LLM-based method depending on their specific data analysis\nobjectives."
                },
                "authors": [
                    {
                        "name": "Gaspard Merten"
                    },
                    {
                        "name": "Gilles Dejaegere"
                    },
                    {
                        "name": "Mahmoud Sakr"
                    }
                ],
                "author_detail": {
                    "name": "Mahmoud Sakr"
                },
                "author": "Mahmoud Sakr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04893v2",
                "updated": "2025-04-11T09:50:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    50,
                    50,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-07T10:01:38Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    1,
                    38,
                    0,
                    97,
                    0
                ],
                "title": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal\n  Foundation Models"
                },
                "summary": "Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with\nthe code for evaluations at https://github.com/Bliss-e-V/SCAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with\nthe code for evaluations at https://github.com/Bliss-e-V/SCAM."
                },
                "authors": [
                    {
                        "name": "Justus Westerhoff"
                    },
                    {
                        "name": "Erblina Purelku"
                    },
                    {
                        "name": "Jakob Hackstein"
                    },
                    {
                        "name": "Leo Pinetzki"
                    },
                    {
                        "name": "Lorenz Hufe"
                    }
                ],
                "author_detail": {
                    "name": "Lorenz Hufe"
                },
                "author": "Lorenz Hufe",
                "arxiv_comment": "Submitted to CVPR 2025 Workshop EVAL-FoMo-2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01745v2",
                "updated": "2025-04-11T09:43:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    43,
                    11,
                    4,
                    101,
                    0
                ],
                "published": "2025-02-03T19:00:05Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    19,
                    0,
                    5,
                    0,
                    34,
                    0
                ],
                "title": "$Lux$: A generative, multi-output, latent-variable model for\n  astronomical data with noisy labels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$Lux$: A generative, multi-output, latent-variable model for\n  astronomical data with noisy labels"
                },
                "summary": "The large volume of spectroscopic data available now and from near-future\nsurveys will enable high-dimensional measurements of stellar parameters and\nproperties. Current methods for determining stellar labels from spectra use\nphysics-driven models, which are computationally expensive and have limitations\nin their accuracy due to simplifications. While machine learning methods\nprovide efficient paths toward emulating physics-based pipelines, they often do\nnot properly account for uncertainties and have complex model structure, both\nof which can lead to biases and inaccurate label inference. Here we present\n$Lux$: a data-driven framework for modeling stellar spectra and labels that\naddresses prior limitations. $Lux$ is a generative, multi-output, latent\nvariable model framework built on JAX for computational efficiency and\nflexibility. As a generative model, $Lux$ properly accounts for uncertainties\nand missing data in the input stellar labels and spectral data and can either\nbe used in probabilistic or discriminative settings. Here, we present several\nexamples of how $Lux$ can successfully emulate methods for precise stellar\nlabel determinations for stars ranging in stellar type and signal-to-noise from\nthe $APOGEE$ surveys. We also show how a simple $Lux$ model is successful at\nperforming label transfer between the $APOGEE$ and $GALAH$ surveys. $Lux$ is a\npowerful new framework for the analysis of large-scale spectroscopic survey\ndata. Its ability to handle uncertainties while maintaining high precision\nmakes it particularly valuable for stellar survey label inference and\ncross-survey analysis, and the flexible model structure allows for easy\nextension to other data types.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large volume of spectroscopic data available now and from near-future\nsurveys will enable high-dimensional measurements of stellar parameters and\nproperties. Current methods for determining stellar labels from spectra use\nphysics-driven models, which are computationally expensive and have limitations\nin their accuracy due to simplifications. While machine learning methods\nprovide efficient paths toward emulating physics-based pipelines, they often do\nnot properly account for uncertainties and have complex model structure, both\nof which can lead to biases and inaccurate label inference. Here we present\n$Lux$: a data-driven framework for modeling stellar spectra and labels that\naddresses prior limitations. $Lux$ is a generative, multi-output, latent\nvariable model framework built on JAX for computational efficiency and\nflexibility. As a generative model, $Lux$ properly accounts for uncertainties\nand missing data in the input stellar labels and spectral data and can either\nbe used in probabilistic or discriminative settings. Here, we present several\nexamples of how $Lux$ can successfully emulate methods for precise stellar\nlabel determinations for stars ranging in stellar type and signal-to-noise from\nthe $APOGEE$ surveys. We also show how a simple $Lux$ model is successful at\nperforming label transfer between the $APOGEE$ and $GALAH$ surveys. $Lux$ is a\npowerful new framework for the analysis of large-scale spectroscopic survey\ndata. Its ability to handle uncertainties while maintaining high precision\nmakes it particularly valuable for stellar survey label inference and\ncross-survey analysis, and the flexible model structure allows for easy\nextension to other data types."
                },
                "authors": [
                    {
                        "name": "Danny Horta"
                    },
                    {
                        "name": "Adrian M. Price-Whelan"
                    },
                    {
                        "name": "David W. Hogg"
                    },
                    {
                        "name": "Melissa K. Ness"
                    },
                    {
                        "name": "Andrew R. Casey"
                    }
                ],
                "author_detail": {
                    "name": "Andrew R. Casey"
                },
                "author": "Andrew R. Casey",
                "arxiv_comment": "30 pages, 19 figures, 1 Table. Accepted for publication in AJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08576v2",
                "updated": "2025-04-11T09:41:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    41,
                    45,
                    4,
                    101,
                    0
                ],
                "published": "2025-02-12T17:10:34Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    10,
                    34,
                    2,
                    43,
                    0
                ],
                "title": "Mapping the Landscape of Generative AI in Network Monitoring and\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping the Landscape of Generative AI in Network Monitoring and\n  Management"
                },
                "summary": "Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and\nDiffusion Models have recently gained widespread attention from both the\nresearch and the industrial communities. This survey explores their application\nin network monitoring and management, focusing on prominent use cases, as well\nas challenges and opportunities. We discuss how network traffic generation and\nclassification, network intrusion detection, networked system log analysis, and\nnetwork digital assistance can benefit from the use of GenAI models.\nAdditionally, we provide an overview of the available GenAI models, datasets\nfor large-scale training phases, and platforms for the development of such\nmodels. Finally, we discuss research directions that potentially mitigate the\nroadblocks to the adoption of GenAI for network monitoring and management. Our\ninvestigation aims to map the current landscape and pave the way for future\nresearch in leveraging GenAI for network monitoring and management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and\nDiffusion Models have recently gained widespread attention from both the\nresearch and the industrial communities. This survey explores their application\nin network monitoring and management, focusing on prominent use cases, as well\nas challenges and opportunities. We discuss how network traffic generation and\nclassification, network intrusion detection, networked system log analysis, and\nnetwork digital assistance can benefit from the use of GenAI models.\nAdditionally, we provide an overview of the available GenAI models, datasets\nfor large-scale training phases, and platforms for the development of such\nmodels. Finally, we discuss research directions that potentially mitigate the\nroadblocks to the adoption of GenAI for network monitoring and management. Our\ninvestigation aims to map the current landscape and pave the way for future\nresearch in leveraging GenAI for network monitoring and management."
                },
                "authors": [
                    {
                        "name": "Giampaolo Bovenzi"
                    },
                    {
                        "name": "Francesco Cerasuolo"
                    },
                    {
                        "name": "Domenico Ciuonzo"
                    },
                    {
                        "name": "Davide Di Monda"
                    },
                    {
                        "name": "Idio Guarino"
                    },
                    {
                        "name": "Antonio Montieri"
                    },
                    {
                        "name": "Valerio Persico"
                    },
                    {
                        "name": "Antonio Pescap"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Pescap"
                },
                "author": "Antonio Pescap",
                "arxiv_doi": "10.1109/TNSM.2025.3543022",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TNSM.2025.3543022",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.08576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "32 pages, 9 figure, 10 tables",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08388v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08388v1",
                "updated": "2025-04-11T09:41:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    41,
                    4,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:41:04Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    41,
                    4,
                    4,
                    101,
                    0
                ],
                "title": "MineWorld: a Real-Time and Open-Source Interactive World Model on\n  Minecraft",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MineWorld: a Real-Time and Open-Source Interactive World Model on\n  Minecraft"
                },
                "summary": "World modeling is a crucial task for enabling intelligent agents to\neffectively interact with humans and operate in dynamic environments. In this\nwork, we propose MineWorld, a real-time interactive world model on Minecraft,\nan open-ended sandbox game which has been utilized as a common testbed for\nworld modeling. MineWorld is driven by a visual-action autoregressive\nTransformer, which takes paired game scenes and corresponding actions as input,\nand generates consequent new scenes following the actions. Specifically, by\ntransforming visual game scenes and actions into discrete token ids with an\nimage tokenizer and an action tokenizer correspondingly, we consist the model\ninput with the concatenation of the two kinds of ids interleaved. The model is\nthen trained with next token prediction to learn rich representations of game\nstates as well as the conditions between states and actions simultaneously. In\ninference, we develop a novel parallel decoding algorithm that predicts the\nspatial redundant tokens in each frame at the same time, letting models in\ndifferent scales generate $4$ to $7$ frames per second and enabling real-time\ninteractions with game players. In evaluation, we propose new metrics to assess\nnot only visual quality but also the action following capacity when generating\nnew scenes, which is crucial for a world model. Our comprehensive evaluation\nshows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion\nbased world models significantly. The code and model have been released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "World modeling is a crucial task for enabling intelligent agents to\neffectively interact with humans and operate in dynamic environments. In this\nwork, we propose MineWorld, a real-time interactive world model on Minecraft,\nan open-ended sandbox game which has been utilized as a common testbed for\nworld modeling. MineWorld is driven by a visual-action autoregressive\nTransformer, which takes paired game scenes and corresponding actions as input,\nand generates consequent new scenes following the actions. Specifically, by\ntransforming visual game scenes and actions into discrete token ids with an\nimage tokenizer and an action tokenizer correspondingly, we consist the model\ninput with the concatenation of the two kinds of ids interleaved. The model is\nthen trained with next token prediction to learn rich representations of game\nstates as well as the conditions between states and actions simultaneously. In\ninference, we develop a novel parallel decoding algorithm that predicts the\nspatial redundant tokens in each frame at the same time, letting models in\ndifferent scales generate $4$ to $7$ frames per second and enabling real-time\ninteractions with game players. In evaluation, we propose new metrics to assess\nnot only visual quality but also the action following capacity when generating\nnew scenes, which is crucial for a world model. Our comprehensive evaluation\nshows the efficacy of MineWorld, outperforming SoTA open-sourced diffusion\nbased world models significantly. The code and model have been released."
                },
                "authors": [
                    {
                        "name": "Junliang Guo"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Tianyu He"
                    },
                    {
                        "name": "Haoyu Wu"
                    },
                    {
                        "name": "Yushu Jiang"
                    },
                    {
                        "name": "Tim Pearce"
                    },
                    {
                        "name": "Jiang Bian"
                    }
                ],
                "author_detail": {
                    "name": "Jiang Bian"
                },
                "author": "Jiang Bian",
                "arxiv_comment": "Technical report. Project page https://aka.ms/mineworld",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08388v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08388v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08387v1",
                "updated": "2025-04-11T09:39:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    39,
                    38,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:39:38Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    39,
                    38,
                    4,
                    101,
                    0
                ],
                "title": "Beetroots: spatially-regularized Bayesian inference of physical\n  parameter maps -- Application to Orion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beetroots: spatially-regularized Bayesian inference of physical\n  parameter maps -- Application to Orion"
                },
                "summary": "The current generation of millimeter receivers is able to produce cubes of\n800 000 pixels by 200 000 frequency channels to cover several square degrees\nover the 3 mm atmospheric window. Estimating the physical conditions of the\ninterstellar medium (ISM) with an astrophysical model on such datasets is\nchallenging. Common approaches tend to converge to local minima and typically\npoorly reconstruct regions with low signal-to-noise ratio (S/N). This\ninstrumental revolution thus calls for new scalable data analysis techniques.\nWe present Beetroots, a Python software that performs Bayesian reconstruction\nof maps of physical conditions from observation maps and an astrophysical\nmodel. It relies on an accurate statistical model, exploits spatial\nregularization to guide estimations, and uses state-of-the-art algorithms. It\nalso assesses the ability of the astrophysical model to explain the\nobservations, providing feedback to improve ISM models. We demonstrate the\npower of Beetroots with the Meudon PDR code on synthetic data, and then apply\nit to estimate physical condition maps in the full Orion molecular cloud 1\n(OMC-1) star forming region based on Herschel molecular line emission maps. The\napplication to the synthetic case shows that Beetroots can currently analyse\nmaps with up to ten thousand pixels, addressing large variations of S/N,\nescaping from local minima, and providing consistent uncertainty\nquantifications. On a laptop, the inference runtime ranges from a few minutes\nfor 100-pixel maps to 28 hours for 8100-pixel maps. The results on the OMC-1\nmaps are consistent with independent estimations from the literature, and\nimprove our understanding of the region. This work paves the way towards\nsystematic and rigorous analyses of observations produced by current and future\ninstruments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current generation of millimeter receivers is able to produce cubes of\n800 000 pixels by 200 000 frequency channels to cover several square degrees\nover the 3 mm atmospheric window. Estimating the physical conditions of the\ninterstellar medium (ISM) with an astrophysical model on such datasets is\nchallenging. Common approaches tend to converge to local minima and typically\npoorly reconstruct regions with low signal-to-noise ratio (S/N). This\ninstrumental revolution thus calls for new scalable data analysis techniques.\nWe present Beetroots, a Python software that performs Bayesian reconstruction\nof maps of physical conditions from observation maps and an astrophysical\nmodel. It relies on an accurate statistical model, exploits spatial\nregularization to guide estimations, and uses state-of-the-art algorithms. It\nalso assesses the ability of the astrophysical model to explain the\nobservations, providing feedback to improve ISM models. We demonstrate the\npower of Beetroots with the Meudon PDR code on synthetic data, and then apply\nit to estimate physical condition maps in the full Orion molecular cloud 1\n(OMC-1) star forming region based on Herschel molecular line emission maps. The\napplication to the synthetic case shows that Beetroots can currently analyse\nmaps with up to ten thousand pixels, addressing large variations of S/N,\nescaping from local minima, and providing consistent uncertainty\nquantifications. On a laptop, the inference runtime ranges from a few minutes\nfor 100-pixel maps to 28 hours for 8100-pixel maps. The results on the OMC-1\nmaps are consistent with independent estimations from the literature, and\nimprove our understanding of the region. This work paves the way towards\nsystematic and rigorous analyses of observations produced by current and future\ninstruments."
                },
                "authors": [
                    {
                        "name": "Pierre Palud"
                    },
                    {
                        "name": "Emeric Bron"
                    },
                    {
                        "name": "Pierre Chainais"
                    },
                    {
                        "name": "Franck Le Petit"
                    },
                    {
                        "name": "Pierre-Antoine Thouvenin"
                    },
                    {
                        "name": "Miriam G. Santa-Maria"
                    },
                    {
                        "name": "Javier R. Goicoechea"
                    },
                    {
                        "name": "David Languignon"
                    },
                    {
                        "name": "Maryvonne Gerin"
                    },
                    {
                        "name": "Jrme Pety"
                    },
                    {
                        "name": "Ivana Beli"
                    },
                    {
                        "name": "Simon Coud"
                    },
                    {
                        "name": "Lucas Einig"
                    },
                    {
                        "name": "Helena Mazurek"
                    },
                    {
                        "name": "Jan H. Orkisz"
                    },
                    {
                        "name": "Lontine Sgal"
                    },
                    {
                        "name": "Antoine Zakardjian"
                    },
                    {
                        "name": "Sbastien Bardeau"
                    },
                    {
                        "name": "Karine Demyk"
                    },
                    {
                        "name": "Victor de Souza Magalhes"
                    },
                    {
                        "name": "Pierre Gratier"
                    },
                    {
                        "name": "Viviana V. Guzmn"
                    },
                    {
                        "name": "Annie Hughes"
                    },
                    {
                        "name": "Franois Levrier"
                    },
                    {
                        "name": "Jacques Le Bourlot"
                    },
                    {
                        "name": "Dariusz C. Lis"
                    },
                    {
                        "name": "Harvey S. Liszt"
                    },
                    {
                        "name": "Nicolas Peretto"
                    },
                    {
                        "name": "Antoine Roueff"
                    },
                    {
                        "name": "Evelyne Roueff"
                    },
                    {
                        "name": "Albrecht Sievers"
                    }
                ],
                "author_detail": {
                    "name": "Albrecht Sievers"
                },
                "author": "Albrecht Sievers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v1",
                "updated": "2025-04-11T09:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17365v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17365v2",
                "updated": "2025-04-11T09:18:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    18,
                    53,
                    4,
                    101,
                    0
                ],
                "published": "2025-02-01T06:58:27Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    6,
                    58,
                    27,
                    5,
                    32,
                    0
                ],
                "title": "How Effective Is Constitutional AI in Small LLMs? A Study on DeepSeek-R1\n  and Its Peers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Effective Is Constitutional AI in Small LLMs? A Study on DeepSeek-R1\n  and Its Peers"
                },
                "summary": "Recent incidents highlight safety risks in Large Language Models (LLMs),\nmotivating research into alignment methods like Constitutional AI (CAI). This\npaper explores CAI's self-critique mechanism on small, uncensored 7-9B\nparameter models: DeepSeek-R1-8B, Gemma-2-9B, Llama 3.1-8B, and Qwen2.5-7B. We\nshow that while Llama-based models exhibited significant harm reduction through\nself-critique, other architectures demonstrated less improvement in harm\ndetection after abliteration. These results suggest CAI's effectiveness may\nvary depending on model architecture and reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent incidents highlight safety risks in Large Language Models (LLMs),\nmotivating research into alignment methods like Constitutional AI (CAI). This\npaper explores CAI's self-critique mechanism on small, uncensored 7-9B\nparameter models: DeepSeek-R1-8B, Gemma-2-9B, Llama 3.1-8B, and Qwen2.5-7B. We\nshow that while Llama-based models exhibited significant harm reduction through\nself-critique, other architectures demonstrated less improvement in harm\ndetection after abliteration. These results suggest CAI's effectiveness may\nvary depending on model architecture and reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Antonio-Gabriel Chacn Menke"
                    },
                    {
                        "name": "Phan Xuan Tan"
                    }
                ],
                "author_detail": {
                    "name": "Phan Xuan Tan"
                },
                "arxiv_affiliation": "Shibaura Institute of Technology",
                "author": "Phan Xuan Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17365v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17365v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08359v1",
                "updated": "2025-04-11T08:48:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    8,
                    48,
                    54,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T08:48:54Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    8,
                    48,
                    54,
                    4,
                    101,
                    0
                ],
                "title": "Kernel-Level Energy-Efficient Neural Architecture Search for Tabular\n  Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kernel-Level Energy-Efficient Neural Architecture Search for Tabular\n  Dataset"
                },
                "summary": "Many studies estimate energy consumption using proxy metrics like memory\nusage, FLOPs, and inference latency, with the assumption that reducing these\nmetrics will also lower energy consumption in neural networks. This paper,\nhowever, takes a different approach by introducing an energy-efficient Neural\nArchitecture Search (NAS) method that directly focuses on identifying\narchitectures that minimize energy consumption while maintaining acceptable\naccuracy. Unlike previous methods that primarily target vision and language\ntasks, the approach proposed here specifically addresses tabular datasets.\nRemarkably, the optimal architecture suggested by this method can reduce energy\nconsumption by up to 92% compared to architectures recommended by conventional\nNAS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many studies estimate energy consumption using proxy metrics like memory\nusage, FLOPs, and inference latency, with the assumption that reducing these\nmetrics will also lower energy consumption in neural networks. This paper,\nhowever, takes a different approach by introducing an energy-efficient Neural\nArchitecture Search (NAS) method that directly focuses on identifying\narchitectures that minimize energy consumption while maintaining acceptable\naccuracy. Unlike previous methods that primarily target vision and language\ntasks, the approach proposed here specifically addresses tabular datasets.\nRemarkably, the optimal architecture suggested by this method can reduce energy\nconsumption by up to 92% compared to architectures recommended by conventional\nNAS."
                },
                "authors": [
                    {
                        "name": "Hoang-Loc La"
                    },
                    {
                        "name": "Phuong Hoai Ha"
                    }
                ],
                "author_detail": {
                    "name": "Phuong Hoai Ha"
                },
                "author": "Phuong Hoai Ha",
                "arxiv_comment": "ACIIDS 2025 Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08355v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08355v1",
                "updated": "2025-04-11T08:42:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    8,
                    42,
                    29,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T08:42:29Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    8,
                    42,
                    29,
                    4,
                    101,
                    0
                ],
                "title": "Manifestation of critical effects in environmental parameter estimation\n  using a quantum sensor under dynamical control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Manifestation of critical effects in environmental parameter estimation\n  using a quantum sensor under dynamical control"
                },
                "summary": "Quantum probes offer a powerful platform for exploring environmental\ndynamics, particularly through their sensitivity to decoherence processes. In\nthis work, we investigate the emergence of critical behavior in the estimation\nof the environmental memory time $\\tau_c$, modeled as an Ornstein-Uhlenbeck\nprocess characterized by a Lorentzian spectral density. Using dynamically\ncontrolled qubit-based sensors -- realized experimentally via solid-state\nNuclear Magnetic Resonance (NMR) and supported by numerical simulations -- we\nimplement tailored filter functions to interrogate the environmental noise\nspectrum and extract $\\tau_c$ from its spectral width. Our results reveal a\nsharp transition in estimation performance between short-memory (SM) and\nlong-memory (LM) regimes, reflected in a non-monotonic estimation error that\nresembles a phase transition. This behavior is accompanied by an\navoided-crossing-like structure in the estimated parameter space, indicative of\ntwo competing solutions near the critical point. These features underscore the\ninterplay between control, decoherence, and inference in open quantum systems.\nBeyond their fundamental significance, these critical phenomena offer a\npractical diagnostic tool for identifying dynamical regimes and optimizing\nquantum sensing protocols. By exploiting this criticality, our findings pave\nthe way for adaptive control strategies aimed at enhancing precision in quantum\nparameter estimation -- particularly in complex or structured environments such\nas spin networks, diffusive media, and quantum materials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum probes offer a powerful platform for exploring environmental\ndynamics, particularly through their sensitivity to decoherence processes. In\nthis work, we investigate the emergence of critical behavior in the estimation\nof the environmental memory time $\\tau_c$, modeled as an Ornstein-Uhlenbeck\nprocess characterized by a Lorentzian spectral density. Using dynamically\ncontrolled qubit-based sensors -- realized experimentally via solid-state\nNuclear Magnetic Resonance (NMR) and supported by numerical simulations -- we\nimplement tailored filter functions to interrogate the environmental noise\nspectrum and extract $\\tau_c$ from its spectral width. Our results reveal a\nsharp transition in estimation performance between short-memory (SM) and\nlong-memory (LM) regimes, reflected in a non-monotonic estimation error that\nresembles a phase transition. This behavior is accompanied by an\navoided-crossing-like structure in the estimated parameter space, indicative of\ntwo competing solutions near the critical point. These features underscore the\ninterplay between control, decoherence, and inference in open quantum systems.\nBeyond their fundamental significance, these critical phenomena offer a\npractical diagnostic tool for identifying dynamical regimes and optimizing\nquantum sensing protocols. By exploiting this criticality, our findings pave\nthe way for adaptive control strategies aimed at enhancing precision in quantum\nparameter estimation -- particularly in complex or structured environments such\nas spin networks, diffusive media, and quantum materials."
                },
                "authors": [
                    {
                        "name": "M. Cristina Rodriguez"
                    },
                    {
                        "name": "Analia Zwick"
                    },
                    {
                        "name": "Gonzalo A. Alvarez"
                    }
                ],
                "author_detail": {
                    "name": "Gonzalo A. Alvarez"
                },
                "author": "Gonzalo A. Alvarez",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08355v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08355v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07583v2",
                "updated": "2025-04-11T08:22:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    8,
                    22,
                    54,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:24:54Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    24,
                    54,
                    3,
                    100,
                    0
                ],
                "title": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with\n  Question Answering"
                },
                "summary": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa"
                },
                "authors": [
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Emmanouil Zaranis"
                    },
                    {
                        "name": "Andr F. T. Martins"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08344v1",
                "updated": "2025-04-11T08:19:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    8,
                    19,
                    18,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T08:19:18Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    8,
                    19,
                    18,
                    4,
                    101,
                    0
                ],
                "title": "EasyGenNet: An Efficient Framework for Audio-Driven Gesture Video\n  Generation Based on Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EasyGenNet: An Efficient Framework for Audio-Driven Gesture Video\n  Generation Based on Diffusion Model"
                },
                "summary": "Audio-driven cospeech video generation typically involves two stages:\nspeech-to-gesture and gesture-to-video. While significant advances have been\nmade in speech-to-gesture generation, synthesizing natural expressions and\ngestures remains challenging in gesture-to-video systems. In order to improve\nthe generation effect, previous works adopted complex input and training\nstrategies and required a large amount of data sets for pre-training, which\nbrought inconvenience to practical applications. We propose a simple one-stage\ntraining method and a temporal inference method based on a diffusion model to\nsynthesize realistic and continuous gesture videos without the need for\nadditional training of temporal modules.The entire model makes use of existing\npre-trained weights, and only a few thousand frames of data are needed for each\ncharacter at a time to complete fine-tuning. Built upon the video generator, we\nintroduce a new audio-to-video pipeline to synthesize co-speech videos, using\n2D human skeleton as the intermediate motion representation. Our experiments\nshow that our method outperforms existing GAN-based and diffusion-based\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-driven cospeech video generation typically involves two stages:\nspeech-to-gesture and gesture-to-video. While significant advances have been\nmade in speech-to-gesture generation, synthesizing natural expressions and\ngestures remains challenging in gesture-to-video systems. In order to improve\nthe generation effect, previous works adopted complex input and training\nstrategies and required a large amount of data sets for pre-training, which\nbrought inconvenience to practical applications. We propose a simple one-stage\ntraining method and a temporal inference method based on a diffusion model to\nsynthesize realistic and continuous gesture videos without the need for\nadditional training of temporal modules.The entire model makes use of existing\npre-trained weights, and only a few thousand frames of data are needed for each\ncharacter at a time to complete fine-tuning. Built upon the video generator, we\nintroduce a new audio-to-video pipeline to synthesize co-speech videos, using\n2D human skeleton as the intermediate motion representation. Our experiments\nshow that our method outperforms existing GAN-based and diffusion-based\nmethods."
                },
                "authors": [
                    {
                        "name": "Renda Li"
                    },
                    {
                        "name": "Xiaohua Qi"
                    },
                    {
                        "name": "Qiang Ling"
                    },
                    {
                        "name": "Jun Yu"
                    },
                    {
                        "name": "Ziyi Chen"
                    },
                    {
                        "name": "Peng Chang"
                    },
                    {
                        "name": "Mei HanJing Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Mei HanJing Xiao"
                },
                "author": "Mei HanJing Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02623v2",
                "updated": "2025-04-11T08:14:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    8,
                    14,
                    19,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-03T14:21:33Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    21,
                    33,
                    3,
                    93,
                    0
                ],
                "title": "Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents\n  through Related and Dynamic Missions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents\n  through Related and Dynamic Missions"
                },
                "summary": "Large language models (LLMs) demonstrate strong potential as agents for tool\ninvocation due to their advanced comprehension and planning capabilities. Users\nincreasingly rely on LLM-based agents to solve complex missions through\niterative interactions. However, existing benchmarks predominantly access\nagents in single-mission scenarios, failing to capture real-world complexity.\nTo bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark,\neach test case comprises multiple interrelated missions. This design requires\nagents to dynamically adapt to evolving demands. Moreover, the proposed\nbenchmark explores all possible mission-switching patterns within a fixed\nmission number. Specifically, we propose a multi-agent data generation\nframework to construct the benchmark. We also propose a novel method to\nevaluate the accuracy and efficiency of agent decisions with dynamic decision\ntrees. Experiments on diverse open-source and closed-source LLMs reveal\ncritical factors influencing agent robustness and provide actionable insights\nto the tool invocation society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong potential as agents for tool\ninvocation due to their advanced comprehension and planning capabilities. Users\nincreasingly rely on LLM-based agents to solve complex missions through\niterative interactions. However, existing benchmarks predominantly access\nagents in single-mission scenarios, failing to capture real-world complexity.\nTo bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark,\neach test case comprises multiple interrelated missions. This design requires\nagents to dynamically adapt to evolving demands. Moreover, the proposed\nbenchmark explores all possible mission-switching patterns within a fixed\nmission number. Specifically, we propose a multi-agent data generation\nframework to construct the benchmark. We also propose a novel method to\nevaluate the accuracy and efficiency of agent decisions with dynamic decision\ntrees. Experiments on diverse open-source and closed-source LLMs reveal\ncritical factors influencing agent robustness and provide actionable insights\nto the tool invocation society."
                },
                "authors": [
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Haorui Wang"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Feng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhang"
                },
                "author": "Feng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08329v1",
                "updated": "2025-04-11T07:51:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    51,
                    58,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T07:51:58Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    51,
                    58,
                    4,
                    101,
                    0
                ],
                "title": "MedRep: Medical Concept Representation for General Electronic Health\n  Record Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedRep: Medical Concept Representation for General Electronic Health\n  Record Foundation Models"
                },
                "summary": "Electronic health record (EHR) foundation models have been an area ripe for\nexploration with their improved performance in various medical tasks. Despite\nthe rapid advances, there exists a fundamental limitation: Processing unseen\nmedical codes out of the vocabulary. This problem limits the generality of EHR\nfoundation models and the integration of models trained with different\nvocabularies. To deal with this problem, we propose MedRep for EHR foundation\nmodels based on the observational medical outcome partnership (OMOP) common\ndata model (CDM), providing the integrated medical concept representations and\nthe basic data augmentation strategy for patient trajectories. For concept\nrepresentation learning, we enrich the information of each concept with a\nminimal definition through large language model (LLM) prompts and enhance the\ntext-based representations through graph ontology of OMOP vocabulary.\nTrajectory augmentation randomly replaces selected concepts with other similar\nconcepts that have closely related representations to let the model practice\nwith the concepts out-of-vocabulary. Finally, we demonstrate that EHR\nfoundation models trained with MedRep better maintain the prediction\nperformance in external datasets. Our code implementation is publicly available\nat https://github.com/kicarussays/MedRep.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic health record (EHR) foundation models have been an area ripe for\nexploration with their improved performance in various medical tasks. Despite\nthe rapid advances, there exists a fundamental limitation: Processing unseen\nmedical codes out of the vocabulary. This problem limits the generality of EHR\nfoundation models and the integration of models trained with different\nvocabularies. To deal with this problem, we propose MedRep for EHR foundation\nmodels based on the observational medical outcome partnership (OMOP) common\ndata model (CDM), providing the integrated medical concept representations and\nthe basic data augmentation strategy for patient trajectories. For concept\nrepresentation learning, we enrich the information of each concept with a\nminimal definition through large language model (LLM) prompts and enhance the\ntext-based representations through graph ontology of OMOP vocabulary.\nTrajectory augmentation randomly replaces selected concepts with other similar\nconcepts that have closely related representations to let the model practice\nwith the concepts out-of-vocabulary. Finally, we demonstrate that EHR\nfoundation models trained with MedRep better maintain the prediction\nperformance in external datasets. Our code implementation is publicly available\nat https://github.com/kicarussays/MedRep."
                },
                "authors": [
                    {
                        "name": "Junmo Kim"
                    },
                    {
                        "name": "Namkyeong Lee"
                    },
                    {
                        "name": "Jiwon Kim"
                    },
                    {
                        "name": "Kwangsoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Kwangsoo Kim"
                },
                "author": "Kwangsoo Kim",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08324v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08324v1",
                "updated": "2025-04-11T07:48:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    48,
                    42,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T07:48:42Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    48,
                    42,
                    4,
                    101,
                    0
                ],
                "title": "An Introduction to Double/Debiased Machine Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Introduction to Double/Debiased Machine Learning"
                },
                "summary": "This paper provides a practical introduction to Double/Debiased Machine\nLearning (DML). DML provides a general approach to performing inference about a\ntarget parameter in the presence of nuisance parameters. The aim of DML is to\nreduce the impact of nuisance parameter estimation on estimators of the\nparameter of interest. We describe DML and its two essential components: Neyman\northogonality and cross-fitting. We highlight that DML reduces functional form\ndependence and accommodates the use of complex data types, such as text data.\nWe illustrate its application through three empirical examples that demonstrate\nDML's applicability in cross-sectional and panel settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides a practical introduction to Double/Debiased Machine\nLearning (DML). DML provides a general approach to performing inference about a\ntarget parameter in the presence of nuisance parameters. The aim of DML is to\nreduce the impact of nuisance parameter estimation on estimators of the\nparameter of interest. We describe DML and its two essential components: Neyman\northogonality and cross-fitting. We highlight that DML reduces functional form\ndependence and accommodates the use of complex data types, such as text data.\nWe illustrate its application through three empirical examples that demonstrate\nDML's applicability in cross-sectional and panel settings."
                },
                "authors": [
                    {
                        "name": "Achim Ahrens"
                    },
                    {
                        "name": "Victor Chernozhukov"
                    },
                    {
                        "name": "Christian Hansen"
                    },
                    {
                        "name": "Damian Kozbur"
                    },
                    {
                        "name": "Mark Schaffer"
                    },
                    {
                        "name": "Thomas Wiemann"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Wiemann"
                },
                "author": "Thomas Wiemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08324v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08324v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07866v2",
                "updated": "2025-04-11T07:47:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    47,
                    4,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T15:41:51Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    41,
                    51,
                    3,
                    100,
                    0
                ],
                "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\n  NPUs"
                },
                "summary": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\nparameters and dense Transformer modules trained on Ascend Neural Processing\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\nadvances in pushing the scale and capability of LLM in recent years, training\nsuch a large-scale model still involves significant optimization and system\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\nnormalization, which effectively eliminates loss spikes during the training\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\nhigh-quality tokens and further enhance its reasoning capabilities during\npost-training. To perform such large-scale training efficiently, we utilize\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\nmodel structure contains much more parameters. Our exploration demonstrates\nthat Ascend NPUs are capable of efficiently and effectively training dense\nmodels with more than 100 billion parameters. Our model and system will be\navailable for our commercial customers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\nparameters and dense Transformer modules trained on Ascend Neural Processing\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\nadvances in pushing the scale and capability of LLM in recent years, training\nsuch a large-scale model still involves significant optimization and system\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\nnormalization, which effectively eliminates loss spikes during the training\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\nhigh-quality tokens and further enhance its reasoning capabilities during\npost-training. To perform such large-scale training efficiently, we utilize\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\nmodel structure contains much more parameters. Our exploration demonstrates\nthat Ascend NPUs are capable of efficiently and effectively training dense\nmodels with more than 100 billion parameters. Our model and system will be\navailable for our commercial customers."
                },
                "authors": [
                    {
                        "name": "Yichun Yin"
                    },
                    {
                        "name": "Wenyong Huang"
                    },
                    {
                        "name": "Kaikai Song"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xueyu Wu"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Peng Guo"
                    },
                    {
                        "name": "Yaoyuan Wang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Can Chen"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Yin Li"
                    },
                    {
                        "name": "Fisher Yu"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yunhe Wang"
                    },
                    {
                        "name": "Baojun Wang"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Boxiao Liu"
                    },
                    {
                        "name": "Changzheng Zhang"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Fei Mi"
                    },
                    {
                        "name": "Hui Jin"
                    },
                    {
                        "name": "Jiansheng Wei"
                    },
                    {
                        "name": "Jiarui Qin"
                    },
                    {
                        "name": "Jinpeng Li"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Liqun Deng"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Minghui Xu"
                    },
                    {
                        "name": "Naifu Zhang"
                    },
                    {
                        "name": "Nianzu Zheng"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Rongju Ruan"
                    },
                    {
                        "name": "Shengjun Cheng"
                    },
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Xinyi Dai"
                    },
                    {
                        "name": "Yonghan Dong"
                    },
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Yujun Li"
                    },
                    {
                        "name": "Yunsheng Ni"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Zhenhe Zhang"
                    },
                    {
                        "name": "Zhicheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Liu"
                },
                "author": "Zhicheng Liu",
                "arxiv_comment": "fix conflicts of latex pacakges",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08312v1",
                "updated": "2025-04-11T07:29:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    29,
                    56,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T07:29:56Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    29,
                    56,
                    4,
                    101,
                    0
                ],
                "title": "SortBench: Benchmarking LLMs based on their ability to sort lists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SortBench: Benchmarking LLMs based on their ability to sort lists"
                },
                "summary": "Sorting is a tedious but simple task for human intelligence and can be solved\nfairly easily algorithmically. However, for Large Language Models (LLMs) this\ntask is surprisingly hard, as some properties of sorting are among known\nweaknesses of LLMs: being faithful to the input data, logical comparisons\nbetween values, and strictly differentiating between syntax (used for sorting)\nand semantics (typically learned by embeddings). Within this paper, we describe\nthe new SortBench benchmark for LLMs that comes with different difficulties and\nthat can be easily scaled in terms of difficulty. We apply this benchmark to\nseven state-of-the-art LLMs, including current test-time reasoning models. Our\nresults show that while the o3-mini model is very capable at sorting in\ngeneral, even this can be fooled if strings are defined to mix syntactical and\nsemantical aspects, e.g., by asking to sort numbers written-out as word.\nFurthermore, all models have problems with the faithfulness to the input of\nlong lists, i.e., they drop items and add new ones. Our results also show that\ntest-time reasoning has a tendency to overthink problems which leads to\nperformance degradation. Finally, models without test-time reasoning like\nGPT-4o are not much worse than reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sorting is a tedious but simple task for human intelligence and can be solved\nfairly easily algorithmically. However, for Large Language Models (LLMs) this\ntask is surprisingly hard, as some properties of sorting are among known\nweaknesses of LLMs: being faithful to the input data, logical comparisons\nbetween values, and strictly differentiating between syntax (used for sorting)\nand semantics (typically learned by embeddings). Within this paper, we describe\nthe new SortBench benchmark for LLMs that comes with different difficulties and\nthat can be easily scaled in terms of difficulty. We apply this benchmark to\nseven state-of-the-art LLMs, including current test-time reasoning models. Our\nresults show that while the o3-mini model is very capable at sorting in\ngeneral, even this can be fooled if strings are defined to mix syntactical and\nsemantical aspects, e.g., by asking to sort numbers written-out as word.\nFurthermore, all models have problems with the faithfulness to the input of\nlong lists, i.e., they drop items and add new ones. Our results also show that\ntest-time reasoning has a tendency to overthink problems which leads to\nperformance degradation. Finally, models without test-time reasoning like\nGPT-4o are not much worse than reasoning models."
                },
                "authors": [
                    {
                        "name": "Steffen Herbold"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Herbold"
                },
                "author": "Steffen Herbold",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14985v2",
                "updated": "2025-04-11T07:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2023-05-24T10:19:57Z",
                "published_parsed": [
                    2023,
                    5,
                    24,
                    10,
                    19,
                    57,
                    2,
                    144,
                    0
                ],
                "title": "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via\n  Large Language Models"
                },
                "summary": "The field of vision-and-language (VL) understanding has made unprecedented\nprogress with end-to-end large pre-trained VL models (VLMs). However, they\nstill fall short in zero-shot reasoning tasks that require multi-step\ninferencing. To achieve this goal, previous works resort to a\ndivide-and-conquer pipeline. In this paper, we argue that previous efforts have\nseveral inherent shortcomings: 1) They rely on domain-specific sub-question\ndecomposing models. 2) They force models to predict the final answer even if\nthe sub-questions or sub-answers provide insufficient information. We address\nthese limitations via IdealGPT, a framework that iteratively decomposes VL\nreasoning using large language models (LLMs). Specifically, IdealGPT utilizes\nan LLM to generate sub-questions, a VLM to provide corresponding sub-answers,\nand another LLM to reason to achieve the final answer. These three modules\nperform the divide-and-conquer procedure iteratively until the model is\nconfident about the final answer to the main question. We evaluate IdealGPT on\nmultiple challenging VL reasoning tasks under a zero-shot setting. In\nparticular, our IdealGPT outperforms the best existing GPT-4-like models by an\nabsolute 10% on VCR and 15% on SNLI-VE. Code is available at\nhttps://github.com/Hxyou/IdealGPT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of vision-and-language (VL) understanding has made unprecedented\nprogress with end-to-end large pre-trained VL models (VLMs). However, they\nstill fall short in zero-shot reasoning tasks that require multi-step\ninferencing. To achieve this goal, previous works resort to a\ndivide-and-conquer pipeline. In this paper, we argue that previous efforts have\nseveral inherent shortcomings: 1) They rely on domain-specific sub-question\ndecomposing models. 2) They force models to predict the final answer even if\nthe sub-questions or sub-answers provide insufficient information. We address\nthese limitations via IdealGPT, a framework that iteratively decomposes VL\nreasoning using large language models (LLMs). Specifically, IdealGPT utilizes\nan LLM to generate sub-questions, a VLM to provide corresponding sub-answers,\nand another LLM to reason to achieve the final answer. These three modules\nperform the divide-and-conquer procedure iteratively until the model is\nconfident about the final answer to the main question. We evaluate IdealGPT on\nmultiple challenging VL reasoning tasks under a zero-shot setting. In\nparticular, our IdealGPT outperforms the best existing GPT-4-like models by an\nabsolute 10% on VCR and 15% on SNLI-VE. Code is available at\nhttps://github.com/Hxyou/IdealGPT"
                },
                "authors": [
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Zhecan Wang"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Gengyu Wang"
                    },
                    {
                        "name": "Hammad A. Ayyubi"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Shih-Fu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shih-Fu Chang"
                },
                "author": "Shih-Fu Chang",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07881v2",
                "updated": "2025-04-11T07:26:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    26,
                    43,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T15:55:34Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    55,
                    34,
                    3,
                    100,
                    0
                ],
                "title": "An LLM-Driven Multi-Agent Debate System for Mendelian Diseases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-Driven Multi-Agent Debate System for Mendelian Diseases"
                },
                "summary": "Accurate diagnosis of Mendelian diseases is crucial for precision therapy and\nassistance in preimplantation genetic diagnosis. However, existing methods\noften fall short of clinical standards or depend on extensive datasets to build\npretrained machine learning models. To address this, we introduce an innovative\nLLM-Driven multi-agent debate system (MD2GPS) with natural language\nexplanations of the diagnostic results. It utilizes a language model to\ntransform results from data-driven and knowledge-driven agents into natural\nlanguage, then fostering a debate between these two specialized agents. This\nsystem has been tested on 1,185 samples across four independent datasets,\nenhancing the TOP1 accuracy from 42.9% to 66% on average. Additionally, in a\nchallenging cohort of 72 cases, MD2GPS identified potential pathogenic genes in\n12 patients, reducing the diagnostic time by 90%. The methods within each\nmodule of this multi-agent debate system are also replaceable, facilitating its\nadaptation for diagnosing and researching other complex diseases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate diagnosis of Mendelian diseases is crucial for precision therapy and\nassistance in preimplantation genetic diagnosis. However, existing methods\noften fall short of clinical standards or depend on extensive datasets to build\npretrained machine learning models. To address this, we introduce an innovative\nLLM-Driven multi-agent debate system (MD2GPS) with natural language\nexplanations of the diagnostic results. It utilizes a language model to\ntransform results from data-driven and knowledge-driven agents into natural\nlanguage, then fostering a debate between these two specialized agents. This\nsystem has been tested on 1,185 samples across four independent datasets,\nenhancing the TOP1 accuracy from 42.9% to 66% on average. Additionally, in a\nchallenging cohort of 72 cases, MD2GPS identified potential pathogenic genes in\n12 patients, reducing the diagnostic time by 90%. The methods within each\nmodule of this multi-agent debate system are also replaceable, facilitating its\nadaptation for diagnosing and researching other complex diseases."
                },
                "authors": [
                    {
                        "name": "Xinyang Zhou"
                    },
                    {
                        "name": "Yongyong Ren"
                    },
                    {
                        "name": "Qianqian Zhao"
                    },
                    {
                        "name": "Daoyi Huang"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Tingting Zhao"
                    },
                    {
                        "name": "Zhixing Zhu"
                    },
                    {
                        "name": "Wenyuan He"
                    },
                    {
                        "name": "Shuyuan Li"
                    },
                    {
                        "name": "Yan Xu"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Yongguo Yu"
                    },
                    {
                        "name": "Shengnan Wu"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Guangjun Yu"
                    },
                    {
                        "name": "Dake He"
                    },
                    {
                        "name": "Bo Ban"
                    },
                    {
                        "name": "Hui Lu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Lu"
                },
                "author": "Hui Lu",
                "arxiv_comment": "21 pages, 5 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02228v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02228v3",
                "updated": "2025-04-11T07:20:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    20,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2024-05-03T16:38:51Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    16,
                    38,
                    51,
                    4,
                    124,
                    0
                ],
                "title": "Attribution in Scientific Literature: New Benchmark and Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribution in Scientific Literature: New Benchmark and Methods"
                },
                "summary": "Large language models (LLMs) present a promising yet challenging frontier for\nautomated source citation in scientific communication. Previous approaches to\ncitation generation have been limited by citation ambiguity and LLM\novergeneralization. We introduce REASONS, a novel dataset with sentence-level\nannotations across 12 scientific domains from arXiv. Our evaluation framework\ncovers two key citation scenarios: indirect queries (matching sentences to\npaper titles) and direct queries (author attribution), both enhanced with\ncontextual metadata. We conduct extensive experiments with models such as\nGPT-O1, GPT-4O, GPT-3.5, DeepSeek, and other smaller models like Perplexity AI\n(7B). While top-tier LLMs achieve high performance in sentence attribution,\nthey struggle with high hallucination rates, a key metric for scientific\nreliability. Our metadata-augmented approach reduces hallucination rates across\nall tasks, offering a promising direction for improvement. Retrieval-augmented\ngeneration (RAG) with Mistral improves performance in indirect queries,\nreducing hallucination rates by 42% and maintaining competitive precision with\nlarger models. However, adversarial testing highlights challenges in linking\npaper titles to abstracts, revealing fundamental limitations in current LLMs.\nREASONS provides a challenging benchmark for developing reliable and\ntrustworthy LLMs in scientific applications",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) present a promising yet challenging frontier for\nautomated source citation in scientific communication. Previous approaches to\ncitation generation have been limited by citation ambiguity and LLM\novergeneralization. We introduce REASONS, a novel dataset with sentence-level\nannotations across 12 scientific domains from arXiv. Our evaluation framework\ncovers two key citation scenarios: indirect queries (matching sentences to\npaper titles) and direct queries (author attribution), both enhanced with\ncontextual metadata. We conduct extensive experiments with models such as\nGPT-O1, GPT-4O, GPT-3.5, DeepSeek, and other smaller models like Perplexity AI\n(7B). While top-tier LLMs achieve high performance in sentence attribution,\nthey struggle with high hallucination rates, a key metric for scientific\nreliability. Our metadata-augmented approach reduces hallucination rates across\nall tasks, offering a promising direction for improvement. Retrieval-augmented\ngeneration (RAG) with Mistral improves performance in indirect queries,\nreducing hallucination rates by 42% and maintaining competitive precision with\nlarger models. However, adversarial testing highlights challenges in linking\npaper titles to abstracts, revealing fundamental limitations in current LLMs.\nREASONS provides a challenging benchmark for developing reliable and\ntrustworthy LLMs in scientific applications"
                },
                "authors": [
                    {
                        "name": "Yash Saxena"
                    },
                    {
                        "name": "Deepa Tilwani"
                    },
                    {
                        "name": "Ali Mohammadi"
                    },
                    {
                        "name": "Edward Raff"
                    },
                    {
                        "name": "Amit Sheth"
                    },
                    {
                        "name": "Srinivasan Parthasarathy"
                    },
                    {
                        "name": "Manas Gaur"
                    }
                ],
                "author_detail": {
                    "name": "Manas Gaur"
                },
                "author": "Manas Gaur",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02228v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02228v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08306v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08306v1",
                "updated": "2025-04-11T07:15:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    15,
                    32,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T07:15:32Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    15,
                    32,
                    4,
                    101,
                    0
                ],
                "title": "STSeg-Complex Video Object Segmentation: The 1st Solution for 4th PVUW\n  MOSE Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STSeg-Complex Video Object Segmentation: The 1st Solution for 4th PVUW\n  MOSE Challenge"
                },
                "summary": "Segmentation of video objects in complex scenarios is highly challenging, and\nthe MOSE dataset has significantly contributed to the development of this\nfield. This technical report details the STSeg solution proposed by the\n\"imaplus\" team.By finetuning SAM2 and the unsupervised model TMO on the MOSE\ndataset, the STSeg solution demonstrates remarkable advantages in handling\ncomplex object motions and long-video sequences. In the inference phase, an\nAdaptive Pseudo-labels Guided Model Refinement Pipeline is adopted to\nintelligently select appropriate models for processing each video. Through\nfinetuning the models and employing the Adaptive Pseudo-labels Guided Model\nRefinement Pipeline in the inference phase, the STSeg solution achieved a J&F\nscore of 87.26% on the test set of the 2025 4th PVUW Challenge MOSE Track,\nsecuring the 1st place and advancing the technology for video object\nsegmentation in complex scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Segmentation of video objects in complex scenarios is highly challenging, and\nthe MOSE dataset has significantly contributed to the development of this\nfield. This technical report details the STSeg solution proposed by the\n\"imaplus\" team.By finetuning SAM2 and the unsupervised model TMO on the MOSE\ndataset, the STSeg solution demonstrates remarkable advantages in handling\ncomplex object motions and long-video sequences. In the inference phase, an\nAdaptive Pseudo-labels Guided Model Refinement Pipeline is adopted to\nintelligently select appropriate models for processing each video. Through\nfinetuning the models and employing the Adaptive Pseudo-labels Guided Model\nRefinement Pipeline in the inference phase, the STSeg solution achieved a J&F\nscore of 87.26% on the test set of the 2025 4th PVUW Challenge MOSE Track,\nsecuring the 1st place and advancing the technology for video object\nsegmentation in complex scenarios."
                },
                "authors": [
                    {
                        "name": "Kehuan Song"
                    },
                    {
                        "name": "Xinglin Xie"
                    },
                    {
                        "name": "Kexin Zhang"
                    },
                    {
                        "name": "Licheng Jiao"
                    },
                    {
                        "name": "Lingling Li"
                    },
                    {
                        "name": "Shuyuan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Shuyuan Yang"
                },
                "author": "Shuyuan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08306v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08306v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08301v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08301v1",
                "updated": "2025-04-11T07:05:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    5,
                    15,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T07:05:15Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    5,
                    15,
                    4,
                    101,
                    0
                ],
                "title": "Enhanced Marginal Sensitivity Model and Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Marginal Sensitivity Model and Bounds"
                },
                "summary": "Sensitivity analysis is important to assess the impact of unmeasured\nconfounding in causal inference from observational studies. The marginal\nsensitivity model (MSM) provides a useful approach in quantifying the influence\nof unmeasured confounders on treatment assignment and leading to tractable\nsharp bounds of common causal parameters. In this paper, to tighten MSM sharp\nbounds, we propose the enhanced MSM (eMSM) by incorporating another sensitivity\nconstraint that quantifies the influence of unmeasured confounders on outcomes.\nWe derive sharp population bounds of expected potential outcomes under eMSM,\nwhich are always narrower than the MSM sharp bounds in a simple and\ninterpretable way. We further discuss desirable specifications of sensitivity\nparameters related to the outcome sensitivity constraint, and obtain both\ndoubly robust point estimation and confidence intervals for the eMSM population\nbounds. The effectiveness of eMSM is also demonstrated numerically through two\nreal-data applications. Our development represents, for the first time, a\nsatisfactory extension of MSM to exploit both treatment and outcome sensitivity\nconstraints on unmeasured confounding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sensitivity analysis is important to assess the impact of unmeasured\nconfounding in causal inference from observational studies. The marginal\nsensitivity model (MSM) provides a useful approach in quantifying the influence\nof unmeasured confounders on treatment assignment and leading to tractable\nsharp bounds of common causal parameters. In this paper, to tighten MSM sharp\nbounds, we propose the enhanced MSM (eMSM) by incorporating another sensitivity\nconstraint that quantifies the influence of unmeasured confounders on outcomes.\nWe derive sharp population bounds of expected potential outcomes under eMSM,\nwhich are always narrower than the MSM sharp bounds in a simple and\ninterpretable way. We further discuss desirable specifications of sensitivity\nparameters related to the outcome sensitivity constraint, and obtain both\ndoubly robust point estimation and confidence intervals for the eMSM population\nbounds. The effectiveness of eMSM is also demonstrated numerically through two\nreal-data applications. Our development represents, for the first time, a\nsatisfactory extension of MSM to exploit both treatment and outcome sensitivity\nconstraints on unmeasured confounding."
                },
                "authors": [
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Wenfu Xu"
                    },
                    {
                        "name": "Zhiqiang Tan"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Tan"
                },
                "author": "Zhiqiang Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08301v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08301v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08300v1",
                "updated": "2025-04-11T07:04:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    4,
                    44,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T07:04:44Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    4,
                    44,
                    4,
                    101,
                    0
                ],
                "title": "Large language models could be rote learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models could be rote learners"
                },
                "summary": "Multiple-choice question (MCQ) benchmarks are widely used for evaluating\nLarge Language Models (LLMs), yet their reliability is undermined by benchmark\ncontamination. In this study, we reframe contamination as an inherent aspect of\nlearning and seek to disentangle genuine capability acquisition from\nsuperficial memorization in LLM evaluation. First, by analyzing model\nperformance under different memorization conditions, we uncover a\ncounterintuitive trend: LLMs perform worse on memorized MCQs than on\nnon-memorized ones, indicating the coexistence of two distinct learning\nphenomena, i.e., rote memorization and genuine capability learning. To\ndisentangle them, we propose TrinEval, a novel evaluation framework that\nreformulates MCQs into an alternative trinity format, reducing memorization\nwhile preserving knowledge assessment. Experiments validate TrinEval's\neffectiveness in reformulation, and its evaluation reveals that common LLMs may\nmemorize by rote 20.5% of knowledge points (in MMLU on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple-choice question (MCQ) benchmarks are widely used for evaluating\nLarge Language Models (LLMs), yet their reliability is undermined by benchmark\ncontamination. In this study, we reframe contamination as an inherent aspect of\nlearning and seek to disentangle genuine capability acquisition from\nsuperficial memorization in LLM evaluation. First, by analyzing model\nperformance under different memorization conditions, we uncover a\ncounterintuitive trend: LLMs perform worse on memorized MCQs than on\nnon-memorized ones, indicating the coexistence of two distinct learning\nphenomena, i.e., rote memorization and genuine capability learning. To\ndisentangle them, we propose TrinEval, a novel evaluation framework that\nreformulates MCQs into an alternative trinity format, reducing memorization\nwhile preserving knowledge assessment. Experiments validate TrinEval's\neffectiveness in reformulation, and its evaluation reveals that common LLMs may\nmemorize by rote 20.5% of knowledge points (in MMLU on average)."
                },
                "authors": [
                    {
                        "name": "Yuyang Xu"
                    },
                    {
                        "name": "Renjun Hu"
                    },
                    {
                        "name": "Haochao Ying"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Xing Shi"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08294v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08294v1",
                "updated": "2025-04-11T06:50:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    6,
                    50,
                    52,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T06:50:52Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    6,
                    50,
                    52,
                    4,
                    101,
                    0
                ],
                "title": "Causal attribution with confidence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal attribution with confidence"
                },
                "summary": "To answer questions of \"causes of effects\", the probability of necessity is\nintroduced for assessing whether or not an observed outcome was caused by an\nearlier treatment. However, the statistical inference for probability of\nnecessity is understudied due to several difficulties, which hinders its\napplication in practice. The evaluation of the probability of necessity\ninvolves the joint distribution of potential outcomes, and thus it is in\ngeneral not point identified and one can at best obtain lower and upper bounds\neven in randomized experiments, unless certain monotonicity assumptions on\npotential outcomes are made. Moreover, these bounds are non-smooth functionals\nof the observed data distribution and standard estimation and inference methods\ncannot be directly applied. In this paper, we investigate the statistical\ninference for the probability of necessity in general situations where it may\nnot be point identified. We introduce a mild margin condition to tackle the\nnon-smoothness, under which the bounds become pathwise differentiable. We\nestablish the semiparametric efficiency theory and propose novel asymptotically\nefficient estimators of the bounds, and further construct confidence intervals\nfor the probability of necessity based on the proposed bounds estimators. The\nresultant confidence intervals are less conservative than existing methods and\ncan effectively make use of the observed covariates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To answer questions of \"causes of effects\", the probability of necessity is\nintroduced for assessing whether or not an observed outcome was caused by an\nearlier treatment. However, the statistical inference for probability of\nnecessity is understudied due to several difficulties, which hinders its\napplication in practice. The evaluation of the probability of necessity\ninvolves the joint distribution of potential outcomes, and thus it is in\ngeneral not point identified and one can at best obtain lower and upper bounds\neven in randomized experiments, unless certain monotonicity assumptions on\npotential outcomes are made. Moreover, these bounds are non-smooth functionals\nof the observed data distribution and standard estimation and inference methods\ncannot be directly applied. In this paper, we investigate the statistical\ninference for the probability of necessity in general situations where it may\nnot be point identified. We introduce a mild margin condition to tackle the\nnon-smoothness, under which the bounds become pathwise differentiable. We\nestablish the semiparametric efficiency theory and propose novel asymptotically\nefficient estimators of the bounds, and further construct confidence intervals\nfor the probability of necessity based on the proposed bounds estimators. The\nresultant confidence intervals are less conservative than existing methods and\ncan effectively make use of the observed covariates."
                },
                "authors": [
                    {
                        "name": "Ping Zhang"
                    },
                    {
                        "name": "Ruoyu Wang"
                    },
                    {
                        "name": "Wang Miao"
                    }
                ],
                "author_detail": {
                    "name": "Wang Miao"
                },
                "author": "Wang Miao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08294v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08294v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07951v2",
                "updated": "2025-04-11T06:35:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    6,
                    35,
                    42,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T17:57:28Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    57,
                    28,
                    3,
                    100,
                    0
                ],
                "title": "Scaling Laws for Native Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Native Multimodal Models"
                },
                "summary": "Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance."
                },
                "authors": [
                    {
                        "name": "Mustafa Shukor"
                    },
                    {
                        "name": "Enrico Fini"
                    },
                    {
                        "name": "Victor Guilherme Turrisi da Costa"
                    },
                    {
                        "name": "Matthieu Cord"
                    },
                    {
                        "name": "Joshua Susskind"
                    },
                    {
                        "name": "Alaaeldin El-Nouby"
                    }
                ],
                "author_detail": {
                    "name": "Alaaeldin El-Nouby"
                },
                "author": "Alaaeldin El-Nouby",
                "arxiv_comment": "31 pages, 26 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08281v1",
                "updated": "2025-04-11T06:30:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    6,
                    30,
                    16,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T06:30:16Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    6,
                    30,
                    16,
                    4,
                    101,
                    0
                ],
                "title": "ELSA: A Style Aligned Dataset for Emotionally Intelligent Language\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELSA: A Style Aligned Dataset for Emotionally Intelligent Language\n  Generation"
                },
                "summary": "Advancements in emotion aware language processing increasingly shape vital\nNLP applications ranging from conversational AI and affective computing to\ncomputational psychology and creative content generation. Existing emotion\ndatasets either lack emotional granularity or fail to capture necessary\nstylistic diversity, limiting the advancement of effective emotion conditioned\ntext generation systems. Seeking to bridge this crucial gap between granularity\nand style diversity, this paper introduces a novel systematically constructed\ndataset named ELSA Emotion and Language Style Alignment Dataset leveraging fine\ngrained emotion taxonomies adapted from existing sources such as dair ai\nemotion dataset and GoEmotions taxonomy. This dataset comprises multiple\nemotionally nuanced variations of original sentences regenerated across\ndistinct contextual styles such as conversational, formal, poetic, and\nnarrative, using advanced Large Language Models LLMs. Rigorous computational\nevaluation using metrics such as perplexity, embedding variance, readability,\nlexical diversity, and semantic coherence measures validates the datasets\nemotional authenticity, linguistic fluency, and textual diversity.\nComprehensive metric analyses affirm its potential to support deeper\nexplorations into emotion conditioned style adaptive text generation. By\nenabling precision tuned emotionally nuanced language modeling, our dataset\ncreates fertile ground for research on fine grained emotional control, prompt\ndriven explanation, interpretability, and style adaptive expressive language\ngeneration with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in emotion aware language processing increasingly shape vital\nNLP applications ranging from conversational AI and affective computing to\ncomputational psychology and creative content generation. Existing emotion\ndatasets either lack emotional granularity or fail to capture necessary\nstylistic diversity, limiting the advancement of effective emotion conditioned\ntext generation systems. Seeking to bridge this crucial gap between granularity\nand style diversity, this paper introduces a novel systematically constructed\ndataset named ELSA Emotion and Language Style Alignment Dataset leveraging fine\ngrained emotion taxonomies adapted from existing sources such as dair ai\nemotion dataset and GoEmotions taxonomy. This dataset comprises multiple\nemotionally nuanced variations of original sentences regenerated across\ndistinct contextual styles such as conversational, formal, poetic, and\nnarrative, using advanced Large Language Models LLMs. Rigorous computational\nevaluation using metrics such as perplexity, embedding variance, readability,\nlexical diversity, and semantic coherence measures validates the datasets\nemotional authenticity, linguistic fluency, and textual diversity.\nComprehensive metric analyses affirm its potential to support deeper\nexplorations into emotion conditioned style adaptive text generation. By\nenabling precision tuned emotionally nuanced language modeling, our dataset\ncreates fertile ground for research on fine grained emotional control, prompt\ndriven explanation, interpretability, and style adaptive expressive language\ngeneration with LLMs."
                },
                "authors": [
                    {
                        "name": "Vishal Gandhi"
                    },
                    {
                        "name": "Sagar Gandhi"
                    }
                ],
                "author_detail": {
                    "name": "Sagar Gandhi"
                },
                "author": "Sagar Gandhi",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02281v2",
                "updated": "2025-04-11T06:05:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    6,
                    5,
                    40,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-03T05:08:04Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    5,
                    8,
                    4,
                    3,
                    93,
                    0
                ],
                "title": "Parallel Market Environments for FinRL Contests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Market Environments for FinRL Contests"
                },
                "summary": "Financial reinforcement learning has attracted lots of attention recently.\nFrom 2023 to 2025, we have organized three FinRL Contests featuring different\nfinancial tasks. Large language models have a strong capability to process\nfinancial documents. By integrating LLM-generated signals into the state,\ntrading agents can take smarter actions based on both structured market data\nand unstructured financial documents. In this paper, we summarize the parallel\nmarket environments for tasks used in FinRL Contests 2023-2025. To address the\nsampling bottleneck during training, we introduce GPU-optimized parallel market\nenvironments to address the sampling bottleneck. In particular, two new tasks\nincorporate LLM-generated signals and all tasks support massively parallel\nsimulation. Contestants have used these market environments to train robust and\npowerful trading agents for both stock and cryptocurrency trading tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial reinforcement learning has attracted lots of attention recently.\nFrom 2023 to 2025, we have organized three FinRL Contests featuring different\nfinancial tasks. Large language models have a strong capability to process\nfinancial documents. By integrating LLM-generated signals into the state,\ntrading agents can take smarter actions based on both structured market data\nand unstructured financial documents. In this paper, we summarize the parallel\nmarket environments for tasks used in FinRL Contests 2023-2025. To address the\nsampling bottleneck during training, we introduce GPU-optimized parallel market\nenvironments to address the sampling bottleneck. In particular, two new tasks\nincorporate LLM-generated signals and all tasks support massively parallel\nsimulation. Contestants have used these market environments to train robust and\npowerful trading agents for both stock and cryptocurrency trading tasks."
                },
                "authors": [
                    {
                        "name": "Keyi Wang"
                    },
                    {
                        "name": "Kairong Xiao"
                    },
                    {
                        "name": "Xiao-Yang Liu Yanglet"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Yang Liu Yanglet"
                },
                "author": "Xiao-Yang Liu Yanglet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15344v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15344v3",
                "updated": "2025-04-11T05:53:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    53,
                    17,
                    4,
                    101,
                    0
                ],
                "published": "2024-09-10T07:04:48Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    7,
                    4,
                    48,
                    1,
                    254,
                    0
                ],
                "title": "Video-Driven Graph Network-Based Simulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-Driven Graph Network-Based Simulators"
                },
                "summary": "Lifelike visualizations in design, cinematography, and gaming rely on precise\nphysics simulations, typically requiring extensive computational resources and\ndetailed physical input. This paper presents a method that can infer a system's\nphysical properties from a short video, eliminating the need for explicit\nparameter input, provided it is close to the training condition. The learned\nrepresentation is then used within a Graph Network-based Simulator to emulate\nthe trajectories of physical systems. We demonstrate that the video-derived\nencodings effectively capture the physical properties of the system and\nshowcase a linear dependence between some of the encodings and the system's\nmotion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lifelike visualizations in design, cinematography, and gaming rely on precise\nphysics simulations, typically requiring extensive computational resources and\ndetailed physical input. This paper presents a method that can infer a system's\nphysical properties from a short video, eliminating the need for explicit\nparameter input, provided it is close to the training condition. The learned\nrepresentation is then used within a Graph Network-based Simulator to emulate\nthe trajectories of physical systems. We demonstrate that the video-derived\nencodings effectively capture the physical properties of the system and\nshowcase a linear dependence between some of the encodings and the system's\nmotion."
                },
                "authors": [
                    {
                        "name": "Franciszek Szewczyk"
                    },
                    {
                        "name": "Gilles Louppe"
                    },
                    {
                        "name": "Matthia Sabatelli"
                    }
                ],
                "author_detail": {
                    "name": "Matthia Sabatelli"
                },
                "author": "Matthia Sabatelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15344v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15344v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09953v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09953v3",
                "updated": "2025-04-11T05:41:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    41,
                    19,
                    4,
                    101,
                    0
                ],
                "published": "2024-06-14T11:58:51Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    11,
                    58,
                    51,
                    4,
                    166,
                    0
                ],
                "title": "DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm\n  Cooperative Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm\n  Cooperative Planning"
                },
                "summary": "Dual-arm robots offer enhanced versatility and efficiency over single-arm\ncounterparts by enabling concurrent manipulation of multiple objects or\ncooperative execution of tasks using both arms. However, the coordination of\ndual-arm systems for long-horizon tasks continues to pose significant\nchallenges, stemming from the intricate temporal and spatial dependencies among\nsub-tasks, necessitating intelligent decisions regarding the allocation of\nactions between arms and their optimal execution order. Existing task planning\nmethods predominantly focus on single-arm robots or rely on predefined bimanual\noperations to use large language models (LLMs) generate task sequence with\nlinear temporal dependency, failing to fully leverage the capabilities of\ndual-arm systems. To address this limitation, we introduce DAG-Plan, a\nstructured task planning framework tailored for dual-arm robots. DAG-Plan\nharnesses LLMs to decompose intricate tasks into actionable sub-tasks\nrepresented as nodes within a directed acyclic graph (DAG). Critically,\nDAG-Plan dynamically assigns these sub-tasks to the appropriate arm based on\nreal-time environmental observations, enabling parallel and adaptive execution.\nWe evaluate DAG-Plan on the Dual-Arm Kitchen Benchmark, comprising 5 sequential\ntasks with 44 sub-tasks. Extensive experiments demonstrate the superiority of\nDAG-Plan over directly using LLM to generate linear task sequence, achieving\n52.8% higher efficiency compared to the single-arm task planning and 48% higher\nsuccess rate of the dual-arm task planning. Compared to iterative methods,\nDAG-Plan improving execution efficiency 84.1% due to its fewer query time. More\ndemos and information are available on https://sites.google.com/view/dag-plan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-arm robots offer enhanced versatility and efficiency over single-arm\ncounterparts by enabling concurrent manipulation of multiple objects or\ncooperative execution of tasks using both arms. However, the coordination of\ndual-arm systems for long-horizon tasks continues to pose significant\nchallenges, stemming from the intricate temporal and spatial dependencies among\nsub-tasks, necessitating intelligent decisions regarding the allocation of\nactions between arms and their optimal execution order. Existing task planning\nmethods predominantly focus on single-arm robots or rely on predefined bimanual\noperations to use large language models (LLMs) generate task sequence with\nlinear temporal dependency, failing to fully leverage the capabilities of\ndual-arm systems. To address this limitation, we introduce DAG-Plan, a\nstructured task planning framework tailored for dual-arm robots. DAG-Plan\nharnesses LLMs to decompose intricate tasks into actionable sub-tasks\nrepresented as nodes within a directed acyclic graph (DAG). Critically,\nDAG-Plan dynamically assigns these sub-tasks to the appropriate arm based on\nreal-time environmental observations, enabling parallel and adaptive execution.\nWe evaluate DAG-Plan on the Dual-Arm Kitchen Benchmark, comprising 5 sequential\ntasks with 44 sub-tasks. Extensive experiments demonstrate the superiority of\nDAG-Plan over directly using LLM to generate linear task sequence, achieving\n52.8% higher efficiency compared to the single-arm task planning and 48% higher\nsuccess rate of the dual-arm task planning. Compared to iterative methods,\nDAG-Plan improving execution efficiency 84.1% due to its fewer query time. More\ndemos and information are available on https://sites.google.com/view/dag-plan."
                },
                "authors": [
                    {
                        "name": "Zeyu Gao"
                    },
                    {
                        "name": "Yao Mu"
                    },
                    {
                        "name": "Jinye Qu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Shijia Peng"
                    },
                    {
                        "name": "Chengkai Hou"
                    },
                    {
                        "name": "Lingyue Guo"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Yanfeng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yanfeng Lu"
                },
                "author": "Yanfeng Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09953v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09953v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06943v2",
                "updated": "2025-04-11T05:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    34,
                    20,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-09T14:51:02Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    51,
                    2,
                    2,
                    99,
                    0
                ],
                "title": "Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations,\n  Architectural Components, and Cognitive Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations,\n  Architectural Components, and Cognitive Integration"
                },
                "summary": "Agents powered by Large Language Models (LLMs) have recently demonstrated\nimpressive capabilities in various tasks. Still, they face limitations in tasks\nrequiring specific, structured knowledge, flexibility, or accountable\ndecision-making. While agents are capable of perceiving their environments,\nforming inferences, planning, and executing actions towards goals, they often\nface issues such as hallucinations and lack of contextual memory across\ninteractions. This paper explores how Case-Based Reasoning (CBR), a strategy\nthat solves new problems by referencing past experiences, can be integrated\ninto LLM agent frameworks. This integration allows LLMs to leverage explicit\nknowledge, enhancing their effectiveness. We systematically review the\ntheoretical foundations of these enhanced agents, identify critical framework\ncomponents, and formulate a mathematical model for the CBR processes of case\nretrieval, adaptation, and learning. We also evaluate CBR-enhanced agents\nagainst other methods like Chain-of-Thought reasoning and standard\nRetrieval-Augmented Generation, analyzing their relative strengths. Moreover,\nwe explore how leveraging CBR's cognitive dimensions (including\nself-reflection, introspection, and curiosity) via goal-driven autonomy\nmechanisms can further enhance the LLM agent capabilities. Contributing to the\nongoing research on neuro-symbolic hybrid systems, this work posits CBR as a\nviable technique for enhancing the reasoning skills and cognitive aspects of\nautonomous LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents powered by Large Language Models (LLMs) have recently demonstrated\nimpressive capabilities in various tasks. Still, they face limitations in tasks\nrequiring specific, structured knowledge, flexibility, or accountable\ndecision-making. While agents are capable of perceiving their environments,\nforming inferences, planning, and executing actions towards goals, they often\nface issues such as hallucinations and lack of contextual memory across\ninteractions. This paper explores how Case-Based Reasoning (CBR), a strategy\nthat solves new problems by referencing past experiences, can be integrated\ninto LLM agent frameworks. This integration allows LLMs to leverage explicit\nknowledge, enhancing their effectiveness. We systematically review the\ntheoretical foundations of these enhanced agents, identify critical framework\ncomponents, and formulate a mathematical model for the CBR processes of case\nretrieval, adaptation, and learning. We also evaluate CBR-enhanced agents\nagainst other methods like Chain-of-Thought reasoning and standard\nRetrieval-Augmented Generation, analyzing their relative strengths. Moreover,\nwe explore how leveraging CBR's cognitive dimensions (including\nself-reflection, introspection, and curiosity) via goal-driven autonomy\nmechanisms can further enhance the LLM agent capabilities. Contributing to the\nongoing research on neuro-symbolic hybrid systems, this work posits CBR as a\nviable technique for enhancing the reasoning skills and cognitive aspects of\nautonomous LLM agents."
                },
                "authors": [
                    {
                        "name": "Kostas Hatalis"
                    },
                    {
                        "name": "Despina Christou"
                    },
                    {
                        "name": "Vyshnavi Kondapalli"
                    }
                ],
                "author_detail": {
                    "name": "Vyshnavi Kondapalli"
                },
                "author": "Vyshnavi Kondapalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08263v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08263v1",
                "updated": "2025-04-11T05:30:32Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    30,
                    32,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T05:30:32Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    30,
                    32,
                    4,
                    101,
                    0
                ],
                "title": "A roadmap for systematic identification and analysis of multiple biases\n  in causal inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A roadmap for systematic identification and analysis of multiple biases\n  in causal inference"
                },
                "summary": "Observational studies examining causal effects rely on unverifiable causal\nassumptions, the violation of which can induce multiple biases. Quantitative\nbias analysis (QBA) methods examine the sensitivity of findings to such\nviolations, generally by producing bias-adjusted estimates under alternative\nassumptions. Common strategies for QBA address either a single source of bias\nor multiple sources one at a time, thus not informing the overall impact of the\npotential biases. We propose a systematic approach (roadmap) for identifying\nand analysing multiple biases together. Briefly, this consists of (i)\narticulating the assumptions underlying the primary analysis through\nspecification and emulation of the \"ideal trial\" that defines the causal\nestimand of interest and depicting these assumptions using casual diagrams;\n(ii) depicting alternative assumptions under which biases arise using causal\ndiagrams; (iii) obtaining a single estimate simultaneously adjusted for all\nbiases under the alternative assumptions. We illustrate the roadmap in an\ninvestigation of the effect of breastfeeding on risk of childhood asthma. We\nfurther use simulations to evaluate a recent simultaneous adjustment approach\nand illustrate the need for simultaneous rather than one-at-a-time adjustment\nto examine the overall impact of biases. The proposed roadmap should facilitate\nthe conduct of high-quality multiple bias analyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Observational studies examining causal effects rely on unverifiable causal\nassumptions, the violation of which can induce multiple biases. Quantitative\nbias analysis (QBA) methods examine the sensitivity of findings to such\nviolations, generally by producing bias-adjusted estimates under alternative\nassumptions. Common strategies for QBA address either a single source of bias\nor multiple sources one at a time, thus not informing the overall impact of the\npotential biases. We propose a systematic approach (roadmap) for identifying\nand analysing multiple biases together. Briefly, this consists of (i)\narticulating the assumptions underlying the primary analysis through\nspecification and emulation of the \"ideal trial\" that defines the causal\nestimand of interest and depicting these assumptions using casual diagrams;\n(ii) depicting alternative assumptions under which biases arise using causal\ndiagrams; (iii) obtaining a single estimate simultaneously adjusted for all\nbiases under the alternative assumptions. We illustrate the roadmap in an\ninvestigation of the effect of breastfeeding on risk of childhood asthma. We\nfurther use simulations to evaluate a recent simultaneous adjustment approach\nand illustrate the need for simultaneous rather than one-at-a-time adjustment\nto examine the overall impact of biases. The proposed roadmap should facilitate\nthe conduct of high-quality multiple bias analyses."
                },
                "authors": [
                    {
                        "name": "Rushani Wijesuriya"
                    },
                    {
                        "name": "Rachael A. Hughes"
                    },
                    {
                        "name": "John B. Carlin"
                    },
                    {
                        "name": "Rachel L. Peters"
                    },
                    {
                        "name": "Jennifer J. Koplin"
                    },
                    {
                        "name": "Margarita Moreno-Betancur"
                    }
                ],
                "author_detail": {
                    "name": "Margarita Moreno-Betancur"
                },
                "author": "Margarita Moreno-Betancur",
                "arxiv_comment": "11 Pages, 4 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08263v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08263v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.OT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08864v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08864v3",
                "updated": "2025-04-11T05:27:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    27,
                    8,
                    4,
                    101,
                    0
                ],
                "published": "2024-12-12T01:52:25Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    1,
                    52,
                    25,
                    3,
                    347,
                    0
                ],
                "title": "A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning\n  Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning\n  Instructions"
                },
                "summary": "Synthesizing high-quality reasoning data for continual training has been\nproven to be effective in enhancing the performance of Large Language Models\n(LLMs). However, previous synthetic approaches struggle to easily scale up data\nand incur high costs in the pursuit of high quality. In this paper, we propose\nthe Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable\nframework for high-quality reasoning data synthesis. Inspired by knowledge\ngraphs, we extracted knowledge points from seed data and constructed a\nknowledge point relationships graph to explore their interconnections. By\nexploring the implicit relationships among knowledge, our method achieves\n$\\times$255 data expansion. Furthermore, GSDP led by open-source models,\nachieves synthesis quality comparable to GPT-4-0613 while maintaining\n$\\times$100 lower costs. To tackle the most challenging mathematical reasoning\ntask, we present the GSDP-MATH dataset comprising over 1.91 million pairs of\nmath problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on\nMistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating\nthe effectiveness of our method. The dataset and models will be released at\nhttps://github.com/Jayce1kk/GSDP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing high-quality reasoning data for continual training has been\nproven to be effective in enhancing the performance of Large Language Models\n(LLMs). However, previous synthetic approaches struggle to easily scale up data\nand incur high costs in the pursuit of high quality. In this paper, we propose\nthe Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable\nframework for high-quality reasoning data synthesis. Inspired by knowledge\ngraphs, we extracted knowledge points from seed data and constructed a\nknowledge point relationships graph to explore their interconnections. By\nexploring the implicit relationships among knowledge, our method achieves\n$\\times$255 data expansion. Furthermore, GSDP led by open-source models,\nachieves synthesis quality comparable to GPT-4-0613 while maintaining\n$\\times$100 lower costs. To tackle the most challenging mathematical reasoning\ntask, we present the GSDP-MATH dataset comprising over 1.91 million pairs of\nmath problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on\nMistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating\nthe effectiveness of our method. The dataset and models will be released at\nhttps://github.com/Jayce1kk/GSDP."
                },
                "authors": [
                    {
                        "name": "Jiankang Wang"
                    },
                    {
                        "name": "Jianjun Xu"
                    },
                    {
                        "name": "Xiaorui Wang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Mengting Xing"
                    },
                    {
                        "name": "Shancheng Fang"
                    },
                    {
                        "name": "Zhineng Chen"
                    },
                    {
                        "name": "Hongtao Xie"
                    },
                    {
                        "name": "Yongdong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongdong Zhang"
                },
                "author": "Yongdong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08864v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08864v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08260v1",
                "updated": "2025-04-11T05:11:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    11,
                    40,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T05:11:40Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    11,
                    40,
                    4,
                    101,
                    0
                ],
                "title": "Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in\n  Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in\n  Healthcare"
                },
                "summary": "Generative agents have been increasingly used to simulate human behaviour in\nsilico, driven by large language models (LLMs). These simulacra serve as\nsandboxes for studying human behaviour without compromising privacy or safety.\nHowever, it remains unclear whether such agents can truly represent real\nindividuals. This work compares survey data from the Understanding America\nStudy (UAS) on healthcare decision-making with simulated responses from\ngenerative agents. Using demographic-based prompt engineering, we create\ndigital twins of survey respondents and analyse how well different LLMs\nreproduce real-world behaviours. Our findings show that some LLMs fail to\nreflect realistic decision-making, such as predicting universal vaccine\nacceptance. However, Llama 3 captures variations across race and Income more\naccurately but also introduces biases not present in the UAS data. This study\nhighlights the potential of generative agents for behavioural research while\nunderscoring the risks of bias from both LLMs and prompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative agents have been increasingly used to simulate human behaviour in\nsilico, driven by large language models (LLMs). These simulacra serve as\nsandboxes for studying human behaviour without compromising privacy or safety.\nHowever, it remains unclear whether such agents can truly represent real\nindividuals. This work compares survey data from the Understanding America\nStudy (UAS) on healthcare decision-making with simulated responses from\ngenerative agents. Using demographic-based prompt engineering, we create\ndigital twins of survey respondents and analyse how well different LLMs\nreproduce real-world behaviours. Our findings show that some LLMs fail to\nreflect realistic decision-making, such as predicting universal vaccine\nacceptance. However, Llama 3 captures variations across race and Income more\naccurately but also introduces biases not present in the UAS data. This study\nhighlights the potential of generative agents for behavioural research while\nunderscoring the risks of bias from both LLMs and prompting strategies."
                },
                "authors": [
                    {
                        "name": "Yonchanok Khaokaew"
                    },
                    {
                        "name": "Flora D. Salim"
                    },
                    {
                        "name": "Andreas Zfle"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Taylor Anderson"
                    },
                    {
                        "name": "Matthew Scotch"
                    },
                    {
                        "name": "David J Heslop"
                    }
                ],
                "author_detail": {
                    "name": "David J Heslop"
                },
                "author": "David J Heslop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20052v2",
                "updated": "2025-04-11T05:07:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    7,
                    41,
                    4,
                    101,
                    0
                ],
                "published": "2024-09-30T07:57:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    57,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "Mitigating Propensity Bias of Large Language Models for Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Propensity Bias of Large Language Models for Recommender\n  Systems"
                },
                "summary": "The rapid development of Large Language Models (LLMs) creates new\nopportunities for recommender systems, especially by exploiting the side\ninformation (e.g., descriptions and analyses of items) generated by these\nmodels. However, aligning this side information with collaborative information\nfrom historical interactions poses significant challenges. The inherent biases\nwithin LLMs can skew recommendations, resulting in distorted and potentially\nunfair user experiences. On the other hand, propensity bias causes side\ninformation to be aligned in such a way that it often tends to represent all\ninputs in a low-dimensional subspace, leading to a phenomenon known as\ndimensional collapse, which severely restricts the recommender system's ability\nto capture user preferences and behaviours. To address these issues, we\nintroduce a novel framework named Counterfactual LLM Recommendation (CLLMR).\nSpecifically, we propose a spectrum-based side information encoder that\nimplicitly embeds structural information from historical interactions into the\nside information representation, thereby circumventing the risk of dimension\ncollapse. Furthermore, our CLLMR approach explores the causal relationships\ninherent in LLM-based recommender systems. By leveraging counterfactual\ninference, we counteract the biases introduced by LLMs. Extensive experiments\ndemonstrate that our CLLMR approach consistently enhances the performance of\nvarious recommender models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) creates new\nopportunities for recommender systems, especially by exploiting the side\ninformation (e.g., descriptions and analyses of items) generated by these\nmodels. However, aligning this side information with collaborative information\nfrom historical interactions poses significant challenges. The inherent biases\nwithin LLMs can skew recommendations, resulting in distorted and potentially\nunfair user experiences. On the other hand, propensity bias causes side\ninformation to be aligned in such a way that it often tends to represent all\ninputs in a low-dimensional subspace, leading to a phenomenon known as\ndimensional collapse, which severely restricts the recommender system's ability\nto capture user preferences and behaviours. To address these issues, we\nintroduce a novel framework named Counterfactual LLM Recommendation (CLLMR).\nSpecifically, we propose a spectrum-based side information encoder that\nimplicitly embeds structural information from historical interactions into the\nside information representation, thereby circumventing the risk of dimension\ncollapse. Furthermore, our CLLMR approach explores the causal relationships\ninherent in LLM-based recommender systems. By leveraging counterfactual\ninference, we counteract the biases introduced by LLMs. Extensive experiments\ndemonstrate that our CLLMR approach consistently enhances the performance of\nvarious recommender models."
                },
                "authors": [
                    {
                        "name": "Guixian Zhang"
                    },
                    {
                        "name": "Guan Yuan"
                    },
                    {
                        "name": "Debo Cheng"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Jiuyong Li"
                    },
                    {
                        "name": "Shichao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shichao Zhang"
                },
                "author": "Shichao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08257v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08257v1",
                "updated": "2025-04-11T05:02:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    2,
                    27,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T05:02:27Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    2,
                    27,
                    4,
                    101,
                    0
                ],
                "title": "Bayesian Reasoning Enabled by Spin-Orbit Torque Magnetic Tunnel\n  Junctions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Reasoning Enabled by Spin-Orbit Torque Magnetic Tunnel\n  Junctions"
                },
                "summary": "Bayesian networks play an increasingly important role in data mining,\ninference, and reasoning with the rapid development of artificial intelligence.\nIn this paper, we present proof-of-concept experiments demonstrating the use of\nspin-orbit torque magnetic tunnel junctions (SOT-MTJs) in Bayesian network\nreasoning. Not only can the target probability distribution function (PDF) of a\nBayesian network be precisely formulated by a conditional probability table as\nusual but also quantitatively parameterized by a probabilistic forward\npropagating neuron network. Moreover, the parameters of the network can also\napproach the optimum through a simple point-by point training algorithm, by\nleveraging which we do not need to memorize all historical data nor\nstatistically summarize conditional probabilities behind them, significantly\nimproving storage efficiency and economizing data pretreatment. Furthermore, we\ndeveloped a simple medical diagnostic system using the SOT-MTJ as a random\nnumber generator and sampler, showcasing the application of SOT-MTJ-based\nBayesian reasoning. This SOT-MTJ-based Bayesian reasoning shows great promise\nin the field of artificial probabilistic neural network, broadening the scope\nof spintronic device applications and providing an efficient and low-storage\nsolution for complex reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian networks play an increasingly important role in data mining,\ninference, and reasoning with the rapid development of artificial intelligence.\nIn this paper, we present proof-of-concept experiments demonstrating the use of\nspin-orbit torque magnetic tunnel junctions (SOT-MTJs) in Bayesian network\nreasoning. Not only can the target probability distribution function (PDF) of a\nBayesian network be precisely formulated by a conditional probability table as\nusual but also quantitatively parameterized by a probabilistic forward\npropagating neuron network. Moreover, the parameters of the network can also\napproach the optimum through a simple point-by point training algorithm, by\nleveraging which we do not need to memorize all historical data nor\nstatistically summarize conditional probabilities behind them, significantly\nimproving storage efficiency and economizing data pretreatment. Furthermore, we\ndeveloped a simple medical diagnostic system using the SOT-MTJ as a random\nnumber generator and sampler, showcasing the application of SOT-MTJ-based\nBayesian reasoning. This SOT-MTJ-based Bayesian reasoning shows great promise\nin the field of artificial probabilistic neural network, broadening the scope\nof spintronic device applications and providing an efficient and low-storage\nsolution for complex reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Yingqian Xu"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Caihua Wan"
                    },
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Bin He"
                    },
                    {
                        "name": "Shiqiang Liu"
                    },
                    {
                        "name": "Jihao Xia"
                    },
                    {
                        "name": "Dehao Kong"
                    },
                    {
                        "name": "Shilong Xiong"
                    },
                    {
                        "name": "Guoqiang Yu"
                    },
                    {
                        "name": "Xiufeng Han"
                    }
                ],
                "author_detail": {
                    "name": "Xiufeng Han"
                },
                "author": "Xiufeng Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08257v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08257v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08256v1",
                "updated": "2025-04-11T04:55:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    4,
                    55,
                    50,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T04:55:50Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    4,
                    55,
                    50,
                    4,
                    101,
                    0
                ],
                "title": "RAG-VR: Leveraging Retrieval-Augmented Generation for 3D Question\n  Answering in VR Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-VR: Leveraging Retrieval-Augmented Generation for 3D Question\n  Answering in VR Environments"
                },
                "summary": "Recent advances in large language models (LLMs) provide new opportunities for\ncontext understanding in virtual reality (VR). However, VR contexts are often\nhighly localized and personalized, limiting the effectiveness of\ngeneral-purpose LLMs. To address this challenge, we present RAG-VR, the first\n3D question-answering system for VR that incorporates retrieval-augmented\ngeneration (RAG), which augments an LLM with external knowledge retrieved from\na localized knowledge database to improve the answer quality. RAG-VR includes a\npipeline for extracting comprehensive knowledge about virtual environments and\nuser conditions for accurate answer generation. To ensure efficient retrieval,\nRAG-VR offloads the retrieval process to a nearby edge server and uses only\nessential information during retrieval. Moreover, we train the retriever to\neffectively distinguish among relevant, irrelevant, and hard-to-differentiate\ninformation in relation to questions. RAG-VR improves answer accuracy by\n17.9%-41.8% and reduces end-to-end latency by 34.5%-47.3% compared with two\nbaseline systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) provide new opportunities for\ncontext understanding in virtual reality (VR). However, VR contexts are often\nhighly localized and personalized, limiting the effectiveness of\ngeneral-purpose LLMs. To address this challenge, we present RAG-VR, the first\n3D question-answering system for VR that incorporates retrieval-augmented\ngeneration (RAG), which augments an LLM with external knowledge retrieved from\na localized knowledge database to improve the answer quality. RAG-VR includes a\npipeline for extracting comprehensive knowledge about virtual environments and\nuser conditions for accurate answer generation. To ensure efficient retrieval,\nRAG-VR offloads the retrieval process to a nearby edge server and uses only\nessential information during retrieval. Moreover, we train the retriever to\neffectively distinguish among relevant, irrelevant, and hard-to-differentiate\ninformation in relation to questions. RAG-VR improves answer accuracy by\n17.9%-41.8% and reduces end-to-end latency by 34.5%-47.3% compared with two\nbaseline systems."
                },
                "authors": [
                    {
                        "name": "Shiyi Ding"
                    },
                    {
                        "name": "Ying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Chen"
                },
                "author": "Ying Chen",
                "arxiv_comment": "Proceedings of the 2025 IEEE Conference on Virtual Reality and 3D\n  User Interfaces (VR), March 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08254v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08254v1",
                "updated": "2025-04-11T04:35:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    4,
                    35,
                    24,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T04:35:24Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    4,
                    35,
                    24,
                    4,
                    101,
                    0
                ],
                "title": "Understanding the Impact of Data Domain Extraction on Synthetic Data\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the Impact of Data Domain Extraction on Synthetic Data\n  Privacy"
                },
                "summary": "Privacy attacks, particularly membership inference attacks (MIAs), are widely\nused to assess the privacy of generative models for tabular synthetic data,\nincluding those with Differential Privacy (DP) guarantees. These attacks often\nexploit outliers, which are especially vulnerable due to their position at the\nboundaries of the data domain (e.g., at the minimum and maximum values).\nHowever, the role of data domain extraction in generative models and its impact\non privacy attacks have been overlooked. In this paper, we examine three\nstrategies for defining the data domain: assuming it is externally provided\n(ideally from public data), extracting it directly from the input data, and\nextracting it with DP mechanisms. While common in popular implementations and\nlibraries, we show that the second approach breaks end-to-end DP guarantees and\nleaves models vulnerable. While using a provided domain (if representative) is\npreferable, extracting it with DP can also defend against popular MIAs, even at\nhigh privacy budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Privacy attacks, particularly membership inference attacks (MIAs), are widely\nused to assess the privacy of generative models for tabular synthetic data,\nincluding those with Differential Privacy (DP) guarantees. These attacks often\nexploit outliers, which are especially vulnerable due to their position at the\nboundaries of the data domain (e.g., at the minimum and maximum values).\nHowever, the role of data domain extraction in generative models and its impact\non privacy attacks have been overlooked. In this paper, we examine three\nstrategies for defining the data domain: assuming it is externally provided\n(ideally from public data), extracting it directly from the input data, and\nextracting it with DP mechanisms. While common in popular implementations and\nlibraries, we show that the second approach breaks end-to-end DP guarantees and\nleaves models vulnerable. While using a provided domain (if representative) is\npreferable, extracting it with DP can also defend against popular MIAs, even at\nhigh privacy budgets."
                },
                "authors": [
                    {
                        "name": "Georgi Ganev"
                    },
                    {
                        "name": "Meenatchi Sundaram Muthu Selva Annamalai"
                    },
                    {
                        "name": "Sofiane Mahiou"
                    },
                    {
                        "name": "Emiliano De Cristofaro"
                    }
                ],
                "author_detail": {
                    "name": "Emiliano De Cristofaro"
                },
                "author": "Emiliano De Cristofaro",
                "arxiv_comment": "Accepted to the Synthetic Data x Data Access Problem workshop\n  (SynthData), part of ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08254v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08254v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11858v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11858v2",
                "updated": "2025-04-11T04:26:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    4,
                    26,
                    42,
                    4,
                    101,
                    0
                ],
                "published": "2025-01-21T03:22:10Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    22,
                    10,
                    1,
                    21,
                    0
                ],
                "title": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have shown significant advancements,\nproviding a promising future for embodied agents. Existing benchmarks for\nevaluating MLLMs primarily utilize static images or videos, limiting\nassessments to non-interactive scenarios. Meanwhile, existing embodied AI\nbenchmarks are task-specific and not diverse enough, which do not adequately\nevaluate the embodied capabilities of MLLMs. To address this, we propose\nEmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs\nwith embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied\n3D scenes, each of which is rigorously selected and annotated. It covers a\nbroad spectrum of existing embodied AI tasks with significantly enhanced\ndiversity, all within a unified simulation and evaluation framework tailored\nfor MLLMs. The tasks are organized into five categories: navigation, object\ninteraction, social interaction, attribute question answering, and spatial\nquestion answering to assess different capabilities of the agents. We evaluated\nthe state-of-the-art MLLMs on EmbodiedEval and found that they have a\nsignificant shortfall compared to human level on embodied tasks. Our analysis\ndemonstrates the limitations of existing MLLMs in embodied capabilities,\nproviding insights for their future development. We open-source all evaluation\ndata and simulation framework at https://github.com/thunlp/EmbodiedEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have shown significant advancements,\nproviding a promising future for embodied agents. Existing benchmarks for\nevaluating MLLMs primarily utilize static images or videos, limiting\nassessments to non-interactive scenarios. Meanwhile, existing embodied AI\nbenchmarks are task-specific and not diverse enough, which do not adequately\nevaluate the embodied capabilities of MLLMs. To address this, we propose\nEmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs\nwith embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied\n3D scenes, each of which is rigorously selected and annotated. It covers a\nbroad spectrum of existing embodied AI tasks with significantly enhanced\ndiversity, all within a unified simulation and evaluation framework tailored\nfor MLLMs. The tasks are organized into five categories: navigation, object\ninteraction, social interaction, attribute question answering, and spatial\nquestion answering to assess different capabilities of the agents. We evaluated\nthe state-of-the-art MLLMs on EmbodiedEval and found that they have a\nsignificant shortfall compared to human level on embodied tasks. Our analysis\ndemonstrates the limitations of existing MLLMs in embodied capabilities,\nproviding insights for their future development. We open-source all evaluation\ndata and simulation framework at https://github.com/thunlp/EmbodiedEval."
                },
                "authors": [
                    {
                        "name": "Zhili Cheng"
                    },
                    {
                        "name": "Yuge Tu"
                    },
                    {
                        "name": "Ran Li"
                    },
                    {
                        "name": "Shiqi Dai"
                    },
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Jiahao Li"
                    },
                    {
                        "name": "Yang Shi"
                    },
                    {
                        "name": "Tianyu Yu"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Lei Shi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11858v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11858v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15655v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15655v3",
                "updated": "2025-04-11T04:17:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    4,
                    17,
                    44,
                    4,
                    101,
                    0
                ],
                "published": "2024-12-20T08:13:05Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    13,
                    5,
                    4,
                    355,
                    0
                ],
                "title": "MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical\n  Speech-to-Formula",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical\n  Speech-to-Formula"
                },
                "summary": "In various academic and professional settings, such as mathematics lectures\nor research presentations, it is often necessary to convey mathematical\nexpressions orally. However, reading mathematical expressions aloud without\naccompanying visuals can significantly hinder comprehension, especially for\nthose who are hearing-impaired or rely on subtitles due to language barriers.\nFor instance, when a presenter reads Euler's Formula, current Automatic Speech\nRecognition (ASR) models often produce a verbose and error-prone textual\ndescription (e.g., e to the power of i x equals cosine of x plus i\n$\\textit{side}$ of x), instead of the concise $\\LaTeX{}$ format (i.e., $ e^{ix}\n= \\cos(x) + i\\sin(x) $), which hampers clear understanding and communication.\nTo address this issue, we introduce MathSpeech, a novel pipeline that\nintegrates ASR models with small Language Models (sLMs) to correct errors in\nmathematical expressions and accurately convert spoken expressions into\nstructured $\\LaTeX{}$ representations. Evaluated on a new dataset derived from\nlecture recordings, MathSpeech demonstrates $\\LaTeX{}$ generation capabilities\ncomparable to leading commercial Large Language Models (LLMs), while leveraging\nfine-tuned small language models of only 120M parameters. Specifically, in\nterms of CER, BLEU, and ROUGE scores for $\\LaTeX{}$ translation, MathSpeech\ndemonstrated significantly superior capabilities compared to GPT-4o. We\nobserved a decrease in CER from 0.390 to 0.298, and higher ROUGE/BLEU scores\ncompared to GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In various academic and professional settings, such as mathematics lectures\nor research presentations, it is often necessary to convey mathematical\nexpressions orally. However, reading mathematical expressions aloud without\naccompanying visuals can significantly hinder comprehension, especially for\nthose who are hearing-impaired or rely on subtitles due to language barriers.\nFor instance, when a presenter reads Euler's Formula, current Automatic Speech\nRecognition (ASR) models often produce a verbose and error-prone textual\ndescription (e.g., e to the power of i x equals cosine of x plus i\n$\\textit{side}$ of x), instead of the concise $\\LaTeX{}$ format (i.e., $ e^{ix}\n= \\cos(x) + i\\sin(x) $), which hampers clear understanding and communication.\nTo address this issue, we introduce MathSpeech, a novel pipeline that\nintegrates ASR models with small Language Models (sLMs) to correct errors in\nmathematical expressions and accurately convert spoken expressions into\nstructured $\\LaTeX{}$ representations. Evaluated on a new dataset derived from\nlecture recordings, MathSpeech demonstrates $\\LaTeX{}$ generation capabilities\ncomparable to leading commercial Large Language Models (LLMs), while leveraging\nfine-tuned small language models of only 120M parameters. Specifically, in\nterms of CER, BLEU, and ROUGE scores for $\\LaTeX{}$ translation, MathSpeech\ndemonstrated significantly superior capabilities compared to GPT-4o. We\nobserved a decrease in CER from 0.390 to 0.298, and higher ROUGE/BLEU scores\ncompared to GPT-4o."
                },
                "authors": [
                    {
                        "name": "Sieun Hyeon"
                    },
                    {
                        "name": "Kyudan Jung"
                    },
                    {
                        "name": "Jaehee Won"
                    },
                    {
                        "name": "Nam-Joon Kim"
                    },
                    {
                        "name": "Hyun Gon Ryu"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Jaeyoung Do"
                    }
                ],
                "author_detail": {
                    "name": "Jaeyoung Do"
                },
                "author": "Jaeyoung Do",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15655v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15655v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08543v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08543v2",
                "updated": "2025-04-11T04:17:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    4,
                    17,
                    20,
                    4,
                    101,
                    0
                ],
                "published": "2024-10-11T05:45:22Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    5,
                    45,
                    22,
                    4,
                    285,
                    0
                ],
                "title": "End-to-end design of multicolor scintillators for enhanced energy\n  resolution in X-ray imaging",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end design of multicolor scintillators for enhanced energy\n  resolution in X-ray imaging"
                },
                "summary": "Scintillators have been widely used in X-ray imaging due to their ability to\nconvert high-energy radiation into visible light, making them essential for\napplications such as medical imaging and high-energy physics. Recent advances\nin the artificial structuring of scintillators offer new opportunities for\nimproving the energy resolution of scintillator-based X-ray detectors. Here, we\npresent a three-bin energy-resolved X-ray imaging framework based on a\nthree-layer multicolor scintillator used in conjunction with a physics-aware\nimage postprocessing algorithm. The multicolor scintillator is able to preserve\nX-ray energy information through the combination of emission wavelength\nmultiplexing and energy-dependent isolation of X-ray absorption in specific\nlayers. The dominant emission color and the radius of the spot measured by the\ndetector are used to infer the incident X-ray energy based on prior knowledge\nof the energy-dependent absorption profiles of the scintillator stack. Through\nab initio Monte Carlo simulations, we show that our approach can achieve an\nenergy reconstruction accuracy of 49.7%, which is only 2% below the maximum\naccuracy achievable with realistic scintillators. We apply our framework to\nmedical phantom imaging simulations where we demonstrate that it can\neffectively differentiate iodine and gadolinium-based contrast agents from\nbone, muscle, and soft tissue.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scintillators have been widely used in X-ray imaging due to their ability to\nconvert high-energy radiation into visible light, making them essential for\napplications such as medical imaging and high-energy physics. Recent advances\nin the artificial structuring of scintillators offer new opportunities for\nimproving the energy resolution of scintillator-based X-ray detectors. Here, we\npresent a three-bin energy-resolved X-ray imaging framework based on a\nthree-layer multicolor scintillator used in conjunction with a physics-aware\nimage postprocessing algorithm. The multicolor scintillator is able to preserve\nX-ray energy information through the combination of emission wavelength\nmultiplexing and energy-dependent isolation of X-ray absorption in specific\nlayers. The dominant emission color and the radius of the spot measured by the\ndetector are used to infer the incident X-ray energy based on prior knowledge\nof the energy-dependent absorption profiles of the scintillator stack. Through\nab initio Monte Carlo simulations, we show that our approach can achieve an\nenergy reconstruction accuracy of 49.7%, which is only 2% below the maximum\naccuracy achievable with realistic scintillators. We apply our framework to\nmedical phantom imaging simulations where we demonstrate that it can\neffectively differentiate iodine and gadolinium-based contrast agents from\nbone, muscle, and soft tissue."
                },
                "authors": [
                    {
                        "name": "Seokhwan Min"
                    },
                    {
                        "name": "Seou Choi"
                    },
                    {
                        "name": "Simo Pajovic"
                    },
                    {
                        "name": "Sachin Vaidya"
                    },
                    {
                        "name": "Nicholas Rivera"
                    },
                    {
                        "name": "Shanhui Fan"
                    },
                    {
                        "name": "Marin Soljai"
                    },
                    {
                        "name": "Charles Roques-Carmes"
                    }
                ],
                "author_detail": {
                    "name": "Charles Roques-Carmes"
                },
                "author": "Charles Roques-Carmes",
                "arxiv_doi": "10.1038/s41377-025-01836-8",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1038/s41377-025-01836-8",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.08543v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08543v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Light Sci. Appl. 14, 158 (2025)",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10550v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10550v3",
                "updated": "2025-04-11T04:02:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    4,
                    2,
                    41,
                    4,
                    101,
                    0
                ],
                "published": "2024-04-16T13:19:46Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    13,
                    19,
                    46,
                    1,
                    107,
                    0
                ],
                "title": "Analytical Approximation of the ELBO Gradient in the Context of the\n  Clutter Problem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analytical Approximation of the ELBO Gradient in the Context of the\n  Clutter Problem"
                },
                "summary": "We propose an analytical solution for approximating the gradient of the\nEvidence Lower Bound (ELBO) in variational inference problems where the\nstatistical model is a Bayesian network consisting of observations drawn from a\nmixture of a Gaussian distribution embedded in unrelated clutter, known as the\nclutter problem. The method employs the reparameterization trick to move the\ngradient operator inside the expectation and relies on the assumption that,\nbecause the likelihood factorizes over the observed data, the variational\ndistribution is generally more compactly supported than the Gaussian\ndistribution in the likelihood factors. This allows efficient local\napproximation of the individual likelihood factors, which leads to an\nanalytical solution for the integral defining the gradient expectation. We\nintegrate the proposed gradient approximation as the expectation step in an EM\n(Expectation Maximization) algorithm for maximizing ELBO and test against\nclassical deterministic approaches in Bayesian inference, such as the Laplace\napproximation, Expectation Propagation and Mean-Field Variational Inference.\nThe proposed method demonstrates good accuracy and rate of convergence together\nwith linear computational complexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an analytical solution for approximating the gradient of the\nEvidence Lower Bound (ELBO) in variational inference problems where the\nstatistical model is a Bayesian network consisting of observations drawn from a\nmixture of a Gaussian distribution embedded in unrelated clutter, known as the\nclutter problem. The method employs the reparameterization trick to move the\ngradient operator inside the expectation and relies on the assumption that,\nbecause the likelihood factorizes over the observed data, the variational\ndistribution is generally more compactly supported than the Gaussian\ndistribution in the likelihood factors. This allows efficient local\napproximation of the individual likelihood factors, which leads to an\nanalytical solution for the integral defining the gradient expectation. We\nintegrate the proposed gradient approximation as the expectation step in an EM\n(Expectation Maximization) algorithm for maximizing ELBO and test against\nclassical deterministic approaches in Bayesian inference, such as the Laplace\napproximation, Expectation Propagation and Mean-Field Variational Inference.\nThe proposed method demonstrates good accuracy and rate of convergence together\nwith linear computational complexity."
                },
                "authors": [
                    {
                        "name": "Roumen Nikolaev Popov"
                    }
                ],
                "author_detail": {
                    "name": "Roumen Nikolaev Popov"
                },
                "author": "Roumen Nikolaev Popov",
                "arxiv_comment": "19 pages, 5 figures, supporting code available at\n  https://github.com/rpopov42/elbo_gaa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10550v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10550v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08242v1",
                "updated": "2025-04-11T03:58:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    3,
                    58,
                    59,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T03:58:59Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    3,
                    58,
                    59,
                    4,
                    101,
                    0
                ],
                "title": "Jupiter: Fast and Resource-Efficient Collaborative Inference of\n  Generative LLMs on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jupiter: Fast and Resource-Efficient Collaborative Inference of\n  Generative LLMs on Edge Devices"
                },
                "summary": "Generative large language models (LLMs) have garnered significant attention\ndue to their exceptional capabilities in various AI tasks. Traditionally\ndeployed in cloud datacenters, LLMs are now increasingly moving towards more\naccessible edge platforms to protect sensitive user data and ensure privacy\npreservation. The limited computational resources of individual edge devices,\nhowever, can result in excessively prolonged inference latency and overwhelmed\nmemory usage. While existing research has explored collaborative edge computing\nto break the resource wall of individual devices, these solutions yet suffer\nfrom massive communication overhead and under-utilization of edge resources.\nFurthermore, they focus exclusively on optimizing the prefill phase, neglecting\nthe crucial autoregressive decoding phase for generative LLMs. To address that,\nwe propose Jupiter, a fast, scalable, and resource-efficient collaborative edge\nAI system for generative LLM inference. Jupiter introduces a flexible pipelined\narchitecture as a principle and differentiates its system design according to\nthe differentiated characteristics of the prefill and decoding phases. For\nprefill phase, Jupiter submits a novel intra-sequence pipeline parallelism and\ndevelops a meticulous parallelism planning strategy to maximize resource\nefficiency; For decoding, Jupiter devises an effective outline-based pipeline\nparallel decoding mechanism combined with speculative decoding, which further\nmagnifies inference acceleration. Extensive evaluation based on realistic\nimplementation demonstrates that Jupiter remarkably outperforms\nstate-of-the-art approaches under various edge environment setups, achieving up\nto 26.1x end-to-end latency reduction while rendering on-par generation\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative large language models (LLMs) have garnered significant attention\ndue to their exceptional capabilities in various AI tasks. Traditionally\ndeployed in cloud datacenters, LLMs are now increasingly moving towards more\naccessible edge platforms to protect sensitive user data and ensure privacy\npreservation. The limited computational resources of individual edge devices,\nhowever, can result in excessively prolonged inference latency and overwhelmed\nmemory usage. While existing research has explored collaborative edge computing\nto break the resource wall of individual devices, these solutions yet suffer\nfrom massive communication overhead and under-utilization of edge resources.\nFurthermore, they focus exclusively on optimizing the prefill phase, neglecting\nthe crucial autoregressive decoding phase for generative LLMs. To address that,\nwe propose Jupiter, a fast, scalable, and resource-efficient collaborative edge\nAI system for generative LLM inference. Jupiter introduces a flexible pipelined\narchitecture as a principle and differentiates its system design according to\nthe differentiated characteristics of the prefill and decoding phases. For\nprefill phase, Jupiter submits a novel intra-sequence pipeline parallelism and\ndevelops a meticulous parallelism planning strategy to maximize resource\nefficiency; For decoding, Jupiter devises an effective outline-based pipeline\nparallel decoding mechanism combined with speculative decoding, which further\nmagnifies inference acceleration. Extensive evaluation based on realistic\nimplementation demonstrates that Jupiter remarkably outperforms\nstate-of-the-art approaches under various edge environment setups, achieving up\nto 26.1x end-to-end latency reduction while rendering on-par generation\nquality."
                },
                "authors": [
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Jian Tang"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by IEEE International Conference on Computer Communications\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2504.08727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08727v1",
                "updated": "2025-04-11T17:55:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    55,
                    45,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T17:55:45Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    55,
                    45,
                    4,
                    101,
                    0
                ],
                "title": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections\n  of Images"
                },
                "summary": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a system using Multimodal LLMs (MLLMs) to analyze a large database\nwith tens of millions of images captured at different times, with the aim of\ndiscovering patterns in temporal changes. Specifically, we aim to capture\nfrequent co-occurring changes (\"trends\") across a city over a certain period.\nUnlike previous visual analyses, our analysis answers open-ended queries (e.g.,\n\"what are the frequent types of changes in the city?\") without any\npredetermined target subjects or training labels. These properties cast prior\nlearning-based or unsupervised visual analysis tools unsuitable. We identify\nMLLMs as a novel tool for their open-ended semantic understanding capabilities.\nYet, our datasets are four orders of magnitude too large for an MLLM to ingest\nas context. So we introduce a bottom-up procedure that decomposes the massive\nvisual analysis problem into more tractable sub-problems. We carefully design\nMLLM-based solutions to each sub-problem. During experiments and ablation\nstudies with our system, we find it significantly outperforms baselines and is\nable to discover interesting trends from images captured in large cities (e.g.,\n\"addition of outdoor dining,\", \"overpass was painted blue,\" etc.). See more\nresults and interactive demos at https://boyangdeng.com/visual-chronicles."
                },
                "authors": [
                    {
                        "name": "Boyang Deng"
                    },
                    {
                        "name": "Songyou Peng"
                    },
                    {
                        "name": "Kyle Genova"
                    },
                    {
                        "name": "Gordon Wetzstein"
                    },
                    {
                        "name": "Noah Snavely"
                    },
                    {
                        "name": "Leonidas Guibas"
                    },
                    {
                        "name": "Thomas Funkhouser"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Funkhouser"
                },
                "author": "Thomas Funkhouser",
                "arxiv_comment": "Project page: https://boyangdeng.com/visual-chronicles; second and\n  third listed authors have equal contributions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08725v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08725v1",
                "updated": "2025-04-11T17:50:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    50,
                    8,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T17:50:08Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    50,
                    8,
                    4,
                    101,
                    0
                ],
                "title": "DocAgent: A Multi-Agent System for Automated Code Documentation\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DocAgent: A Multi-Agent System for Automated Code Documentation\n  Generation"
                },
                "summary": "High-quality code documentation is crucial for software development\nespecially in the era of AI. However, generating it automatically using Large\nLanguage Models (LLMs) remains challenging, as existing approaches often\nproduce incomplete, unhelpful, or factually incorrect outputs. We introduce\nDocAgent, a novel multi-agent collaborative system using topological code\nprocessing for incremental context building. Specialized agents (Reader,\nSearcher, Writer, Verifier, Orchestrator) then collaboratively generate\ndocumentation. We also propose a multi-faceted evaluation framework assessing\nCompleteness, Helpfulness, and Truthfulness. Comprehensive experiments show\nDocAgent significantly outperforms baselines consistently. Our ablation study\nconfirms the vital role of the topological processing order. DocAgent offers a\nrobust approach for reliable code documentation generation in complex and\nproprietary repositories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-quality code documentation is crucial for software development\nespecially in the era of AI. However, generating it automatically using Large\nLanguage Models (LLMs) remains challenging, as existing approaches often\nproduce incomplete, unhelpful, or factually incorrect outputs. We introduce\nDocAgent, a novel multi-agent collaborative system using topological code\nprocessing for incremental context building. Specialized agents (Reader,\nSearcher, Writer, Verifier, Orchestrator) then collaboratively generate\ndocumentation. We also propose a multi-faceted evaluation framework assessing\nCompleteness, Helpfulness, and Truthfulness. Comprehensive experiments show\nDocAgent significantly outperforms baselines consistently. Our ablation study\nconfirms the vital role of the topological processing order. DocAgent offers a\nrobust approach for reliable code documentation generation in complex and\nproprietary repositories."
                },
                "authors": [
                    {
                        "name": "Dayu Yang"
                    },
                    {
                        "name": "Antoine Simoulin"
                    },
                    {
                        "name": "Xin Qian"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Yuwei Cao"
                    },
                    {
                        "name": "Zhaopu Teng"
                    },
                    {
                        "name": "Grey Yang"
                    }
                ],
                "author_detail": {
                    "name": "Grey Yang"
                },
                "author": "Grey Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08725v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08725v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14348v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14348v2",
                "updated": "2025-04-11T17:25:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    25,
                    3,
                    4,
                    101,
                    0
                ],
                "published": "2025-01-24T09:18:48Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    18,
                    48,
                    4,
                    24,
                    0
                ],
                "title": "Advancing data-driven broadband seismic wavefield simulation with\n  multi-conditional diffusion model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing data-driven broadband seismic wavefield simulation with\n  multi-conditional diffusion model"
                },
                "summary": "Sparse distributions of seismic sensors and sources pose challenges for\nsubsurface imaging, source characterization, and ground motion modeling. While\nlarge-N arrays have shown the potential of dense observational data, their\ndeployment over extensive areas is constrained by economic and logistical\nlimitations. Numerical simulations offer an alternative, but modeling realistic\nwavefields remains computationally expensive. To address these challenges, we\ndevelop a multi-conditional diffusion transformer for generating seismic\nwavefields without requiring prior geological knowledge. Our method produces\nhigh-resolution wavefields that accurately capture both amplitude and phase\ninformation across diverse source and station configurations. The model first\ngenerates amplitude spectra conditioned on input attributes and subsequently\nrefines wavefields through iterative phase optimization. We validate our\napproach using data from the Geysers geothermal field, demonstrating the\ngeneration of wavefields with spatial continuity and fidelity in both spectral\namplitude and phase. These synthesized wavefields hold promise for advancing\nstructural imaging and source characterization in seismology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse distributions of seismic sensors and sources pose challenges for\nsubsurface imaging, source characterization, and ground motion modeling. While\nlarge-N arrays have shown the potential of dense observational data, their\ndeployment over extensive areas is constrained by economic and logistical\nlimitations. Numerical simulations offer an alternative, but modeling realistic\nwavefields remains computationally expensive. To address these challenges, we\ndevelop a multi-conditional diffusion transformer for generating seismic\nwavefields without requiring prior geological knowledge. Our method produces\nhigh-resolution wavefields that accurately capture both amplitude and phase\ninformation across diverse source and station configurations. The model first\ngenerates amplitude spectra conditioned on input attributes and subsequently\nrefines wavefields through iterative phase optimization. We validate our\napproach using data from the Geysers geothermal field, demonstrating the\ngeneration of wavefields with spatial continuity and fidelity in both spectral\namplitude and phase. These synthesized wavefields hold promise for advancing\nstructural imaging and source characterization in seismology."
                },
                "authors": [
                    {
                        "name": "Zhengfa Bi"
                    },
                    {
                        "name": "Nori Nakata"
                    },
                    {
                        "name": "Rie Nakata"
                    },
                    {
                        "name": "Pu Ren"
                    },
                    {
                        "name": "Xinming Wu"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    }
                ],
                "author_detail": {
                    "name": "Michael W. Mahoney"
                },
                "author": "Michael W. Mahoney",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14348v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14348v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.geo-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.geo-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08714v1",
                "updated": "2025-04-11T17:24:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    24,
                    58,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T17:24:58Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    24,
                    58,
                    4,
                    101,
                    0
                ],
                "title": "Generating Fine Details of Entity Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Fine Details of Entity Interactions"
                },
                "summary": "Images not only depict objects but also encapsulate rich interactions between\nthem. However, generating faithful and high-fidelity images involving multiple\nentities interacting with each other, is a long-standing challenge. While\npre-trained text-to-image models are trained on large-scale datasets to follow\ndiverse text instructions, they struggle to generate accurate interactions,\nlikely due to the scarcity of training data for uncommon object interactions.\nThis paper introduces InterActing, an interaction-focused dataset with 1000\nfine-grained prompts covering three key scenarios: (1) functional and\naction-based interactions, (2) compositional spatial relationships, and (3)\nmulti-subject interactions. To address interaction generation challenges, we\npropose a decomposition-augmented refinement procedure. Our approach,\nDetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose\ninteractions into finer-grained concepts, uses a VLM to critique generated\nimages, and applies targeted interventions within the diffusion process in\nrefinement. Automatic and human evaluations show significantly improved image\nquality, demonstrating the potential of enhanced inference strategies. Our\ndataset and code are available at https://concepts-ai.com/p/detailscribe/ to\nfacilitate future exploration of interaction-rich image generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Images not only depict objects but also encapsulate rich interactions between\nthem. However, generating faithful and high-fidelity images involving multiple\nentities interacting with each other, is a long-standing challenge. While\npre-trained text-to-image models are trained on large-scale datasets to follow\ndiverse text instructions, they struggle to generate accurate interactions,\nlikely due to the scarcity of training data for uncommon object interactions.\nThis paper introduces InterActing, an interaction-focused dataset with 1000\nfine-grained prompts covering three key scenarios: (1) functional and\naction-based interactions, (2) compositional spatial relationships, and (3)\nmulti-subject interactions. To address interaction generation challenges, we\npropose a decomposition-augmented refinement procedure. Our approach,\nDetailScribe, built on Stable Diffusion 3.5, leverages LLMs to decompose\ninteractions into finer-grained concepts, uses a VLM to critique generated\nimages, and applies targeted interventions within the diffusion process in\nrefinement. Automatic and human evaluations show significantly improved image\nquality, demonstrating the potential of enhanced inference strategies. Our\ndataset and code are available at https://concepts-ai.com/p/detailscribe/ to\nfacilitate future exploration of interaction-rich image generation."
                },
                "authors": [
                    {
                        "name": "Xinyi Gu"
                    },
                    {
                        "name": "Jiayuan Mao"
                    }
                ],
                "author_detail": {
                    "name": "Jiayuan Mao"
                },
                "author": "Jiayuan Mao",
                "arxiv_comment": "Project Page: https://concepts-ai.com/p/detailscribe/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08697v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08697v1",
                "updated": "2025-04-11T17:04:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    4,
                    51,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T17:04:51Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    4,
                    51,
                    4,
                    101,
                    0
                ],
                "title": "Large Language Models as Span Annotators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Span Annotators"
                },
                "summary": "For high-quality texts, single-score metrics seldom provide actionable\nfeedback. In contrast, span annotation - pointing out issues in the text by\nannotating their spans - can guide improvements and provide insights. Until\nrecently, span annotation was limited to human annotators or fine-tuned encoder\nmodels. In this study, we automate span annotation with large language models\n(LLMs). We compare expert or skilled crowdworker annotators with open and\nproprietary LLMs on three tasks: data-to-text generation evaluation, machine\ntranslation evaluation, and propaganda detection in human-written texts. In our\nexperiments, we show that LLMs as span annotators are straightforward to\nimplement and notably more cost-efficient than human annotators. The LLMs\nachieve moderate agreement with skilled human annotators, in some scenarios\ncomparable to the average agreement among the annotators themselves.\nQualitative analysis shows that reasoning models outperform their\ninstruction-tuned counterparts and provide more valid explanations for\nannotations. We release the dataset of more than 40k model and human\nannotations for further research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For high-quality texts, single-score metrics seldom provide actionable\nfeedback. In contrast, span annotation - pointing out issues in the text by\nannotating their spans - can guide improvements and provide insights. Until\nrecently, span annotation was limited to human annotators or fine-tuned encoder\nmodels. In this study, we automate span annotation with large language models\n(LLMs). We compare expert or skilled crowdworker annotators with open and\nproprietary LLMs on three tasks: data-to-text generation evaluation, machine\ntranslation evaluation, and propaganda detection in human-written texts. In our\nexperiments, we show that LLMs as span annotators are straightforward to\nimplement and notably more cost-efficient than human annotators. The LLMs\nachieve moderate agreement with skilled human annotators, in some scenarios\ncomparable to the average agreement among the annotators themselves.\nQualitative analysis shows that reasoning models outperform their\ninstruction-tuned counterparts and provide more valid explanations for\nannotations. We release the dataset of more than 40k model and human\nannotations for further research."
                },
                "authors": [
                    {
                        "name": "Zdenk Kasner"
                    },
                    {
                        "name": "Vilm Zouhar"
                    },
                    {
                        "name": "Patrcia Schmidtov"
                    },
                    {
                        "name": "Ivan Kart"
                    },
                    {
                        "name": "Kristna Onderkov"
                    },
                    {
                        "name": "Ondej Pltek"
                    },
                    {
                        "name": "Dimitra Gkatzia"
                    },
                    {
                        "name": "Saad Mahamood"
                    },
                    {
                        "name": "Ondej Duek"
                    },
                    {
                        "name": "Simone Balloccu"
                    }
                ],
                "author_detail": {
                    "name": "Simone Balloccu"
                },
                "author": "Simone Balloccu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08697v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08697v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08696v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08696v1",
                "updated": "2025-04-11T17:03:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    3,
                    58,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T17:03:58Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    3,
                    58,
                    4,
                    101,
                    0
                ],
                "title": "SeaView: Software Engineering Agent Visual Interface for Enhanced\n  Workflow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeaView: Software Engineering Agent Visual Interface for Enhanced\n  Workflow"
                },
                "summary": "Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE\nagents, have made tremendous progress (>60% on SWE-Bench Verified) on\nreal-world coding challenges including GitHub issue resolution. SWE agents use\na combination of reasoning, environment interaction and self-reflection to\nresolve issues thereby generating \"trajectories\". Analysis of SWE agent\ntrajectories is difficult, not only as they exceed LLM sequence length\n(sometimes, greater than 128k) but also because it involves a relatively\nprolonged interaction between an LLM and the environment managed by the agent.\nIn case of an agent error, it can be hard to decipher, locate and understand\nits scope. Similarly, it can be hard to track improvements or regression over\nmultiple runs or experiments. While a lot of research has gone into making\nthese SWE agents reach state-of-the-art, much less focus has been put into\ncreating tools to help analyze and visualize agent output. We propose a novel\ntool called SeaView: Software Engineering Agent Visual Interface for Enhanced\nWorkflow, with a vision to assist SWE-agent researchers to visualize and\ninspect their experiments. SeaView's novel mechanisms help compare experimental\nruns with varying hyper-parameters or LLMs, and quickly get an understanding of\nLLM or environment related problems. Based on our user study, experienced\nresearchers spend between 10 and 30 minutes to gather the information provided\nby SeaView, while researchers with little experience can spend between 30\nminutes to 1 hour to diagnose their experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-regressive LLM-based software engineering (SWE) agents, henceforth SWE\nagents, have made tremendous progress (>60% on SWE-Bench Verified) on\nreal-world coding challenges including GitHub issue resolution. SWE agents use\na combination of reasoning, environment interaction and self-reflection to\nresolve issues thereby generating \"trajectories\". Analysis of SWE agent\ntrajectories is difficult, not only as they exceed LLM sequence length\n(sometimes, greater than 128k) but also because it involves a relatively\nprolonged interaction between an LLM and the environment managed by the agent.\nIn case of an agent error, it can be hard to decipher, locate and understand\nits scope. Similarly, it can be hard to track improvements or regression over\nmultiple runs or experiments. While a lot of research has gone into making\nthese SWE agents reach state-of-the-art, much less focus has been put into\ncreating tools to help analyze and visualize agent output. We propose a novel\ntool called SeaView: Software Engineering Agent Visual Interface for Enhanced\nWorkflow, with a vision to assist SWE-agent researchers to visualize and\ninspect their experiments. SeaView's novel mechanisms help compare experimental\nruns with varying hyper-parameters or LLMs, and quickly get an understanding of\nLLM or environment related problems. Based on our user study, experienced\nresearchers spend between 10 and 30 minutes to gather the information provided\nby SeaView, while researchers with little experience can spend between 30\nminutes to 1 hour to diagnose their experiment."
                },
                "authors": [
                    {
                        "name": "Timothy Bula"
                    },
                    {
                        "name": "Saurabh Pujar"
                    },
                    {
                        "name": "Luca Buratti"
                    },
                    {
                        "name": "Mihaela Bornea"
                    },
                    {
                        "name": "Avirup Sil"
                    }
                ],
                "author_detail": {
                    "name": "Avirup Sil"
                },
                "author": "Avirup Sil",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08696v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08696v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05058v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05058v3",
                "updated": "2025-04-11T17:03:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    3,
                    4,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-07T13:29:02Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    29,
                    2,
                    0,
                    97,
                    0
                ],
                "title": "Not All Data Are Unlearned Equally",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Data Are Unlearned Equally"
                },
                "summary": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning is concerned with the task of removing knowledge learned\nfrom particular data points from a trained model. In the context of large\nlanguage models (LLMs), unlearning has recently received increased attention,\nparticularly for removing knowledge about named entities from models for\nprivacy purposes. While various approaches have been proposed to address the\nunlearning problem, most existing approaches treat all data points to be\nunlearned equally, i.e., unlearning that Montreal is a city in Canada is\ntreated exactly the same as unlearning the phone number of the first author of\nthis paper. In this work, we show that this all data is equal assumption does\nnot hold for LLM unlearning. We study how the success of unlearning depends on\nthe frequency of the knowledge we want to unlearn in the pre-training data of a\nmodel and find that frequency strongly affects unlearning, i.e., more frequent\nknowledge is harder to unlearn. Additionally, we uncover a misalignment between\nprobability and generation-based evaluations of unlearning and show that this\nproblem worsens as models become larger. Overall, our experiments highlight the\nneed for better evaluation practices and novel methods for LLM unlearning that\ntake the training data of models into account."
                },
                "authors": [
                    {
                        "name": "Aravind Krishnan"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Marius Mosbach"
                    }
                ],
                "author_detail": {
                    "name": "Marius Mosbach"
                },
                "author": "Marius Mosbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05058v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05058v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08694v1",
                "updated": "2025-04-11T17:02:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    2,
                    40,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T17:02:40Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    17,
                    2,
                    40,
                    4,
                    101,
                    0
                ],
                "title": "TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for\n  Spatiotemporal-Aware Travel Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for\n  Spatiotemporal-Aware Travel Planning"
                },
                "summary": "Large language models (LLMs) have shown promise in automating travel\nplanning, yet they often fall short in addressing nuanced spatiotemporal\nrationality. While existing benchmarks focus on basic plan validity, they\nneglect critical aspects such as route efficiency, POI appeal, and real-time\nadaptability. This paper introduces TP-RAG, the first benchmark tailored for\nretrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes\n2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784\nhigh-quality travel trajectory references sourced from online tourist\ndocuments, enabling dynamic and context-aware planning. Through extensive\nexperiments, we reveal that integrating reference trajectories significantly\nimproves spatial efficiency and POI rationality of the travel plan, while\nchallenges persist in universality and robustness due to conflicting references\nand noisy data. To address these issues, we propose EvoRAG, an evolutionary\nframework that potently synergizes diverse retrieved trajectories with LLMs'\nintrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving\nspatiotemporal compliance and reducing commonsense violation compared to\nground-up and retrieval-augmented baselines. Our work underscores the potential\nof hybridizing Web knowledge with LLM-driven optimization, paving the way for\nmore reliable and adaptive travel planning agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown promise in automating travel\nplanning, yet they often fall short in addressing nuanced spatiotemporal\nrationality. While existing benchmarks focus on basic plan validity, they\nneglect critical aspects such as route efficiency, POI appeal, and real-time\nadaptability. This paper introduces TP-RAG, the first benchmark tailored for\nretrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes\n2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784\nhigh-quality travel trajectory references sourced from online tourist\ndocuments, enabling dynamic and context-aware planning. Through extensive\nexperiments, we reveal that integrating reference trajectories significantly\nimproves spatial efficiency and POI rationality of the travel plan, while\nchallenges persist in universality and robustness due to conflicting references\nand noisy data. To address these issues, we propose EvoRAG, an evolutionary\nframework that potently synergizes diverse retrieved trajectories with LLMs'\nintrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving\nspatiotemporal compliance and reducing commonsense violation compared to\nground-up and retrieval-augmented baselines. Our work underscores the potential\nof hybridizing Web knowledge with LLM-driven optimization, paving the way for\nmore reliable and adaptive travel planning agents."
                },
                "authors": [
                    {
                        "name": "Hang Ni"
                    },
                    {
                        "name": "Fan Liu"
                    },
                    {
                        "name": "Xinyu Ma"
                    },
                    {
                        "name": "Lixin Su"
                    },
                    {
                        "name": "Shuaiqiang Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Hui Xiong"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03767v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03767v2",
                "updated": "2025-04-11T16:59:05Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    59,
                    5,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-02T21:46:02Z",
                "published_parsed": [
                    2025,
                    4,
                    2,
                    21,
                    46,
                    2,
                    2,
                    92,
                    0
                ],
                "title": "MCP Safety Audit: LLMs with the Model Context Protocol Allow Major\n  Security Exploits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCP Safety Audit: LLMs with the Model Context Protocol Allow Major\n  Security Exploits"
                },
                "summary": "To reduce development overhead and enable seamless integration between\npotential components comprising any given generative AI application, the Model\nContext Protocol (MCP) (Anthropic, 2024) has recently been released and\nsubsequently widely adopted. The MCP is an open protocol that standardizes API\ncalls to large language models (LLMs), data sources, and agentic tools. By\nconnecting multiple MCP servers, each defined with a set of tools, resources,\nand prompts, users are able to define automated workflows fully driven by LLMs.\nHowever, we show that the current MCP design carries a wide range of security\nrisks for end users. In particular, we demonstrate that industry-leading LLMs\nmay be coerced into using MCP tools to compromise an AI developer's system\nthrough various attacks, such as malicious code execution, remote access\ncontrol, and credential theft. To proactively mitigate these and related\nattacks, we introduce a safety auditing tool, MCPSafetyScanner, the first\nagentic tool to assess the security of an arbitrary MCP server. MCPScanner uses\nseveral agents to (a) automatically determine adversarial samples given an MCP\nserver's tools and resources; (b) search for related vulnerabilities and\nremediations based on those samples; and (c) generate a security report\ndetailing all findings. Our work highlights serious security issues with\ngeneral-purpose agentic workflows while also providing a proactive tool to\naudit MCP server safety and address detected vulnerabilities before deployment.\n  The described MCP server auditing tool, MCPSafetyScanner, is freely available\nat: https://github.com/johnhalloran321/mcpSafetyScanner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce development overhead and enable seamless integration between\npotential components comprising any given generative AI application, the Model\nContext Protocol (MCP) (Anthropic, 2024) has recently been released and\nsubsequently widely adopted. The MCP is an open protocol that standardizes API\ncalls to large language models (LLMs), data sources, and agentic tools. By\nconnecting multiple MCP servers, each defined with a set of tools, resources,\nand prompts, users are able to define automated workflows fully driven by LLMs.\nHowever, we show that the current MCP design carries a wide range of security\nrisks for end users. In particular, we demonstrate that industry-leading LLMs\nmay be coerced into using MCP tools to compromise an AI developer's system\nthrough various attacks, such as malicious code execution, remote access\ncontrol, and credential theft. To proactively mitigate these and related\nattacks, we introduce a safety auditing tool, MCPSafetyScanner, the first\nagentic tool to assess the security of an arbitrary MCP server. MCPScanner uses\nseveral agents to (a) automatically determine adversarial samples given an MCP\nserver's tools and resources; (b) search for related vulnerabilities and\nremediations based on those samples; and (c) generate a security report\ndetailing all findings. Our work highlights serious security issues with\ngeneral-purpose agentic workflows while also providing a proactive tool to\naudit MCP server safety and address detected vulnerabilities before deployment.\n  The described MCP server auditing tool, MCPSafetyScanner, is freely available\nat: https://github.com/johnhalloran321/mcpSafetyScanner"
                },
                "authors": [
                    {
                        "name": "Brandon Radosevich"
                    },
                    {
                        "name": "John Halloran"
                    }
                ],
                "author_detail": {
                    "name": "John Halloran"
                },
                "author": "John Halloran",
                "arxiv_comment": "27 pages, 21 figures, and 2 Tables. Cleans up the TeX source",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03767v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03767v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08690v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08690v1",
                "updated": "2025-04-11T16:57:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    57,
                    36,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T16:57:36Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    57,
                    36,
                    4,
                    101,
                    0
                ],
                "title": "Fast-Slow-Thinking: Complex Task Solving with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast-Slow-Thinking: Complex Task Solving with Large Language Models"
                },
                "summary": "Nowadays, Large Language Models (LLMs) have been gradually employed to solve\ncomplex tasks. To face the challenge, task decomposition has become an\neffective way, which proposes to divide a complex task into multiple simpler\nsubtasks and then solve them separately so that the difficulty of the original\ntask can be reduced. However, the performance of existing task decomposition\nmethods can be suboptimal when the task contains overly complex logic and\nconstraints. In this situation, the solution generated by LLMs may deviate from\nthe original purpose of the task, or contain redundant or even erroneous\ncontent. Therefore, inspired by the fact that humans possess two thinking\nsystems including fast thinking and slow thinking, this paper introduces a new\ntask decomposition method termed ``Fast-Slow-Thinking'' (FST), which stimulates\nLLMs to solve tasks through the cooperation of Fast Thinking (FT) and Slow\nThinking (ST) steps. Here FT focuses more on the general and concise aspect of\nthe task, and ST focuses more on the details of the task. In FT, LLMs are\nprompted to remove the constraints of the original task, therefore simplifying\nit to a general and concise one. In ST, we recall the constraints removed in\nFT, so that LLMs can improve the answer generated in FT to meet the\nrequirements of the original task. Therefore, our FST method enables LLMs to\nconsider a complex problem via a human-like cognition process from coarse to\nfine, the effectiveness of which has been well demonstrated by the experiments\non three types of tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, Large Language Models (LLMs) have been gradually employed to solve\ncomplex tasks. To face the challenge, task decomposition has become an\neffective way, which proposes to divide a complex task into multiple simpler\nsubtasks and then solve them separately so that the difficulty of the original\ntask can be reduced. However, the performance of existing task decomposition\nmethods can be suboptimal when the task contains overly complex logic and\nconstraints. In this situation, the solution generated by LLMs may deviate from\nthe original purpose of the task, or contain redundant or even erroneous\ncontent. Therefore, inspired by the fact that humans possess two thinking\nsystems including fast thinking and slow thinking, this paper introduces a new\ntask decomposition method termed ``Fast-Slow-Thinking'' (FST), which stimulates\nLLMs to solve tasks through the cooperation of Fast Thinking (FT) and Slow\nThinking (ST) steps. Here FT focuses more on the general and concise aspect of\nthe task, and ST focuses more on the details of the task. In FT, LLMs are\nprompted to remove the constraints of the original task, therefore simplifying\nit to a general and concise one. In ST, we recall the constraints removed in\nFT, so that LLMs can improve the answer generated in FT to meet the\nrequirements of the original task. Therefore, our FST method enables LLMs to\nconsider a complex problem via a human-like cognition process from coarse to\nfine, the effectiveness of which has been well demonstrated by the experiments\non three types of tasks."
                },
                "authors": [
                    {
                        "name": "Yiliu Sun"
                    },
                    {
                        "name": "Yanfang Zhang"
                    },
                    {
                        "name": "Zicheng Zhao"
                    },
                    {
                        "name": "Sheng Wan"
                    },
                    {
                        "name": "Dacheng Tao"
                    },
                    {
                        "name": "Chen Gong"
                    }
                ],
                "author_detail": {
                    "name": "Chen Gong"
                },
                "author": "Chen Gong",
                "arxiv_comment": "37 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08690v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08690v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08687v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08687v1",
                "updated": "2025-04-11T16:54:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    54,
                    12,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T16:54:12Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    54,
                    12,
                    4,
                    101,
                    0
                ],
                "title": "Voice Interaction With Conversational AI Could Facilitate Thoughtful\n  Reflection and Substantive Revision in Writing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Voice Interaction With Conversational AI Could Facilitate Thoughtful\n  Reflection and Substantive Revision in Writing"
                },
                "summary": "Writing well requires not only expressing ideas but also refining them\nthrough revision, a process facilitated by reflection. Prior research suggests\nthat feedback delivered through dialogues, such as those in writing center\ntutoring sessions, can help writers reflect more thoughtfully on their work\ncompared to static feedback. Recent advancements in multi-modal large language\nmodels (LLMs) now offer new possibilities for supporting interactive and\nexpressive voice-based reflection in writing. In particular, we propose that\nLLM-generated static feedback can be repurposed as conversation starters,\nallowing writers to seek clarification, request examples, and ask follow-up\nquestions, thereby fostering deeper reflection on their writing. We argue that\nvoice-based interaction can naturally facilitate this conversational exchange,\nencouraging writers' engagement with higher-order concerns, facilitating\niterative refinement of their reflections, and reduce cognitive load compared\nto text-based interactions. To investigate these effects, we propose a\nformative study exploring how text vs. voice input influence writers'\nreflection and subsequent revisions. Findings from this study will inform the\ndesign of intelligent and interactive writing tools, offering insights into how\nvoice-based interactions with LLM-powered conversational agents can support\nreflection and revision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Writing well requires not only expressing ideas but also refining them\nthrough revision, a process facilitated by reflection. Prior research suggests\nthat feedback delivered through dialogues, such as those in writing center\ntutoring sessions, can help writers reflect more thoughtfully on their work\ncompared to static feedback. Recent advancements in multi-modal large language\nmodels (LLMs) now offer new possibilities for supporting interactive and\nexpressive voice-based reflection in writing. In particular, we propose that\nLLM-generated static feedback can be repurposed as conversation starters,\nallowing writers to seek clarification, request examples, and ask follow-up\nquestions, thereby fostering deeper reflection on their writing. We argue that\nvoice-based interaction can naturally facilitate this conversational exchange,\nencouraging writers' engagement with higher-order concerns, facilitating\niterative refinement of their reflections, and reduce cognitive load compared\nto text-based interactions. To investigate these effects, we propose a\nformative study exploring how text vs. voice input influence writers'\nreflection and subsequent revisions. Findings from this study will inform the\ndesign of intelligent and interactive writing tools, offering insights into how\nvoice-based interactions with LLM-powered conversational agents can support\nreflection and revision."
                },
                "authors": [
                    {
                        "name": "Jiho Kim"
                    },
                    {
                        "name": "Philippe Laban"
                    },
                    {
                        "name": "Xiang 'Anthony' Chen"
                    },
                    {
                        "name": "Kenneth C. Arnold"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth C. Arnold"
                },
                "author": "Kenneth C. Arnold",
                "arxiv_comment": "5 pages; Accepted to Fourth Workshop on Intelligent and Interactive\n  Writing Assistants (In2Writing 2025) at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08687v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08687v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05659v2",
                "updated": "2025-04-11T16:51:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    51,
                    59,
                    4,
                    101,
                    0
                ],
                "published": "2025-03-07T18:20:30Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    18,
                    20,
                    30,
                    4,
                    66,
                    0
                ],
                "title": "A Survey of Large Language Model Empowered Agents for Recommendation and\n  Search: Towards Next-Generation Information Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey of Large Language Model Empowered Agents for Recommendation and\n  Search: Towards Next-Generation Information Retrieval"
                },
                "summary": "Information technology has profoundly altered the way humans interact with\ninformation. The vast amount of content created, shared, and disseminated\nonline has made it increasingly difficult to access relevant information. Over\nthe past two decades, recommender systems and search (collectively referred to\nas information retrieval systems) have evolved significantly to address these\nchallenges. Recent advances in large language models (LLMs) have demonstrated\ncapabilities that surpass human performance in various language-related tasks\nand exhibit general understanding, reasoning, and decision-making abilities.\nThis paper explores the transformative potential of LLM agents in enhancing\nrecommender and search systems. We discuss the motivations and roles of LLM\nagents, and establish a classification framework to elaborate on the existing\nresearch. We highlight the immense potential of LLM agents in addressing\ncurrent challenges in recommendation and search, providing insights into future\nresearch directions. This paper is the first to systematically review and\nclassify the research on LLM agents in these domains, offering a novel\nperspective on leveraging this advanced AI technology for information\nretrieval. To help understand the existing works, we list the existing papers\non LLM agent based recommendation and search at this link:\nhttps://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information technology has profoundly altered the way humans interact with\ninformation. The vast amount of content created, shared, and disseminated\nonline has made it increasingly difficult to access relevant information. Over\nthe past two decades, recommender systems and search (collectively referred to\nas information retrieval systems) have evolved significantly to address these\nchallenges. Recent advances in large language models (LLMs) have demonstrated\ncapabilities that surpass human performance in various language-related tasks\nand exhibit general understanding, reasoning, and decision-making abilities.\nThis paper explores the transformative potential of LLM agents in enhancing\nrecommender and search systems. We discuss the motivations and roles of LLM\nagents, and establish a classification framework to elaborate on the existing\nresearch. We highlight the immense potential of LLM agents in addressing\ncurrent challenges in recommendation and search, providing insights into future\nresearch directions. This paper is the first to systematically review and\nclassify the research on LLM agents in these domains, offering a novel\nperspective on leveraging this advanced AI technology for information\nretrieval. To help understand the existing works, we list the existing papers\non LLM agent based recommendation and search at this link:\nhttps://github.com/tsinghua-fib-lab/LLM-Agent-for-Recommendation-and-Search."
                },
                "authors": [
                    {
                        "name": "Yu Zhang"
                    },
                    {
                        "name": "Shutong Qiao"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Tzu-Heng Lin"
                    },
                    {
                        "name": "Chen Gao"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08672v1",
                "updated": "2025-04-11T16:26:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    26,
                    23,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T16:26:23Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    26,
                    23,
                    4,
                    101,
                    0
                ],
                "title": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genius: A Generalizable and Purely Unsupervised Self-Training Framework\n  For Advanced Reasoning"
                },
                "summary": "Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing LLM reasoning skills has captivated wide interest. However, current\npost-training techniques rely heavily on supervisory signals, such as outcome\nsupervision or auxiliary reward models, which face the problem of scalability\nand high annotation costs. This motivates us to enhance LLM reasoning without\nthe need for external supervision. We introduce a generalizable and purely\nunsupervised self-training framework, named Genius. Without external auxiliary,\nGenius requires to seek the optimal response sequence in a stepwise manner and\noptimize the LLM. To explore the potential steps and exploit the optimal ones,\nGenius introduces a stepwise foresight re-sampling strategy to sample and\nestimate the step value by simulating future outcomes. Further, we recognize\nthat the unsupervised setting inevitably induces the intrinsic noise and\nuncertainty. To provide a robust optimization, we propose an\nadvantage-calibrated optimization (ACO) loss function to mitigate estimation\ninconsistencies. Combining these techniques together, Genius provides an\nadvanced initial step towards self-improve LLM reasoning with general queries\nand without supervision, revolutionizing reasoning scaling laws given the vast\navailability of general queries. The code will be released at\nhttps://github.com/xufangzhi/Genius."
                },
                "authors": [
                    {
                        "name": "Fangzhi Xu"
                    },
                    {
                        "name": "Hang Yan"
                    },
                    {
                        "name": "Chang Ma"
                    },
                    {
                        "name": "Haiteng Zhao"
                    },
                    {
                        "name": "Qiushi Sun"
                    },
                    {
                        "name": "Kanzhi Cheng"
                    },
                    {
                        "name": "Junxian He"
                    },
                    {
                        "name": "Jun Liu"
                    },
                    {
                        "name": "Zhiyong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Wu"
                },
                "author": "Zhiyong Wu",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08666v1",
                "updated": "2025-04-11T16:15:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    15,
                    27,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T16:15:27Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    15,
                    27,
                    4,
                    101,
                    0
                ],
                "title": "Variability-Driven User-Story Generation using LLM and Triadic Concept\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variability-Driven User-Story Generation using LLM and Triadic Concept\n  Analysis"
                },
                "summary": "A widely used Agile practice for requirements is to produce a set of user\nstories (also called ``agile product backlog''), which roughly includes a list\nof pairs (role, feature), where the role handles the feature for a certain\npurpose. In the context of Software Product Lines, the requirements for a\nfamily of similar systems is thus a family of user-story sets, one per system,\nleading to a 3-dimensional dataset composed of sets of triples (system, role,\nfeature). In this paper, we combine Triadic Concept Analysis (TCA) and Large\nLanguage Model (LLM) prompting to suggest the user-story set required to\ndevelop a new system relying on the variability logic of an existing system\nfamily. This process consists in 1) computing 3-dimensional variability\nexpressed as a set of TCA implications, 2) providing the designer with\nintelligible design options, 3) capturing the designer's selection of options,\n4) proposing a first user-story set corresponding to this selection, 5)\nconsolidating its validity according to the implications identified in step 1,\nwhile completing it if necessary, and 6) leveraging LLM to have a more\ncomprehensive website. This process is evaluated with a dataset comprising the\nuser-story sets of 67 similar-purpose websites.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A widely used Agile practice for requirements is to produce a set of user\nstories (also called ``agile product backlog''), which roughly includes a list\nof pairs (role, feature), where the role handles the feature for a certain\npurpose. In the context of Software Product Lines, the requirements for a\nfamily of similar systems is thus a family of user-story sets, one per system,\nleading to a 3-dimensional dataset composed of sets of triples (system, role,\nfeature). In this paper, we combine Triadic Concept Analysis (TCA) and Large\nLanguage Model (LLM) prompting to suggest the user-story set required to\ndevelop a new system relying on the variability logic of an existing system\nfamily. This process consists in 1) computing 3-dimensional variability\nexpressed as a set of TCA implications, 2) providing the designer with\nintelligible design options, 3) capturing the designer's selection of options,\n4) proposing a first user-story set corresponding to this selection, 5)\nconsolidating its validity according to the implications identified in step 1,\nwhile completing it if necessary, and 6) leveraging LLM to have a more\ncomprehensive website. This process is evaluated with a dataset comprising the\nuser-story sets of 67 similar-purpose websites."
                },
                "authors": [
                    {
                        "name": "Alexandre Bazin"
                    },
                    {
                        "name": "Alain Gutierrez"
                    },
                    {
                        "name": "Marianne Huchard"
                    },
                    {
                        "name": "Pierre Martin"
                    },
                    {
                        "name": "Yulin"
                    },
                    {
                        "name": "Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhang"
                },
                "arxiv_affiliation": "Huaxi",
                "author": "Zhang",
                "arxiv_doi": "10.5220/0013360500003928",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.5220/0013360500003928",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.08666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "20th International Conference on Evaluation of Novel Approaches to\n  Software Engineering April 4-6, 2025, in Porto, Portugal",
                "arxiv_journal_ref": "Proceedings of ENASE 2025; SciTePress, pages 618-625 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08661v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08661v1",
                "updated": "2025-04-11T16:10:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    10,
                    58,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T16:10:58Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    16,
                    10,
                    58,
                    4,
                    101,
                    0
                ],
                "title": "Safe Flow Matching: Robot Motion Planning with Control Barrier Functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safe Flow Matching: Robot Motion Planning with Control Barrier Functions"
                },
                "summary": "Recent advances in generative modeling have led to promising results in robot\nmotion planning, particularly through diffusion and flow-based models that\ncapture complex, multimodal trajectory distributions. However, these methods\nare typically trained offline and remain limited when faced with unseen\nenvironments or dynamic constraints, often lacking explicit mechanisms to\nensure safety during deployment. In this work, we propose, Safe Flow Matching\n(SafeFM), a motion planning approach for trajectory generation that integrates\nflow matching with safety guarantees. By incorporating the proposed flow\nmatching barrier functions, SafeFM ensures that generated trajectories remain\nwithin safe regions throughout the planning horizon, even in the presence of\npreviously unseen obstacles or state-action constraints. Unlike diffusion-based\napproaches, our method allows for direct, efficient sampling of\nconstraint-satisfying trajectories, making it well-suited for real-time motion\nplanning. We evaluate SafeFM on a diverse set of tasks, including planar robot\nnavigation and 7-DoF manipulation, demonstrating superior safety,\ngeneralization, and planning performance compared to state-of-the-art\ngenerative planners. Comprehensive resources are available on the project\nwebsite: https://safeflowmatching.github.io/SafeFM/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in generative modeling have led to promising results in robot\nmotion planning, particularly through diffusion and flow-based models that\ncapture complex, multimodal trajectory distributions. However, these methods\nare typically trained offline and remain limited when faced with unseen\nenvironments or dynamic constraints, often lacking explicit mechanisms to\nensure safety during deployment. In this work, we propose, Safe Flow Matching\n(SafeFM), a motion planning approach for trajectory generation that integrates\nflow matching with safety guarantees. By incorporating the proposed flow\nmatching barrier functions, SafeFM ensures that generated trajectories remain\nwithin safe regions throughout the planning horizon, even in the presence of\npreviously unseen obstacles or state-action constraints. Unlike diffusion-based\napproaches, our method allows for direct, efficient sampling of\nconstraint-satisfying trajectories, making it well-suited for real-time motion\nplanning. We evaluate SafeFM on a diverse set of tasks, including planar robot\nnavigation and 7-DoF manipulation, demonstrating superior safety,\ngeneralization, and planning performance compared to state-of-the-art\ngenerative planners. Comprehensive resources are available on the project\nwebsite: https://safeflowmatching.github.io/SafeFM/"
                },
                "authors": [
                    {
                        "name": "Xiaobing Dai"
                    },
                    {
                        "name": "Dian Yu"
                    },
                    {
                        "name": "Shanshan Zhang"
                    },
                    {
                        "name": "Zewen Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zewen Yang"
                },
                "author": "Zewen Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08661v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08661v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05636v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05636v2",
                "updated": "2025-04-11T15:53:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    53,
                    20,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-08T03:29:40Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    3,
                    29,
                    40,
                    1,
                    98,
                    0
                ],
                "title": "A Multi-Modal AI System for Screening Mammography: Integrating 2D and 3D\n  Imaging to Improve Breast Cancer Detection in a Prospective Clinical Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Modal AI System for Screening Mammography: Integrating 2D and 3D\n  Imaging to Improve Breast Cancer Detection in a Prospective Clinical Study"
                },
                "summary": "Although digital breast tomosynthesis (DBT) improves diagnostic performance\nover full-field digital mammography (FFDM), false-positive recalls remain a\nconcern in breast cancer screening. We developed a multi-modal artificial\nintelligence system integrating FFDM, synthetic mammography, and DBT to provide\nbreast-level predictions and bounding-box localizations of suspicious findings.\nOur AI system, trained on approximately 500,000 mammography exams, achieved\n0.945 AUROC on an internal test set. It demonstrated capacity to reduce recalls\nby 31.7% and radiologist workload by 43.8% while maintaining 100% sensitivity,\nunderscoring its potential to improve clinical workflows. External validation\nconfirmed strong generalizability, reducing the gap to a perfect AUROC by\n35.31%-69.14% relative to strong baselines. In prospective deployment across 18\nsites, the system reduced recall rates for low-risk cases. An improved version,\ntrained on over 750,000 exams with additional labels, further reduced the gap\nby 18.86%-56.62% across large external datasets. Overall, these results\nunderscore the importance of utilizing all available imaging modalities,\ndemonstrate the potential for clinical impact, and indicate feasibility of\nfurther reduction of the test error with increased training set when using\nlarge-capacity neural networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although digital breast tomosynthesis (DBT) improves diagnostic performance\nover full-field digital mammography (FFDM), false-positive recalls remain a\nconcern in breast cancer screening. We developed a multi-modal artificial\nintelligence system integrating FFDM, synthetic mammography, and DBT to provide\nbreast-level predictions and bounding-box localizations of suspicious findings.\nOur AI system, trained on approximately 500,000 mammography exams, achieved\n0.945 AUROC on an internal test set. It demonstrated capacity to reduce recalls\nby 31.7% and radiologist workload by 43.8% while maintaining 100% sensitivity,\nunderscoring its potential to improve clinical workflows. External validation\nconfirmed strong generalizability, reducing the gap to a perfect AUROC by\n35.31%-69.14% relative to strong baselines. In prospective deployment across 18\nsites, the system reduced recall rates for low-risk cases. An improved version,\ntrained on over 750,000 exams with additional labels, further reduced the gap\nby 18.86%-56.62% across large external datasets. Overall, these results\nunderscore the importance of utilizing all available imaging modalities,\ndemonstrate the potential for clinical impact, and indicate feasibility of\nfurther reduction of the test error with increased training set when using\nlarge-capacity neural networks."
                },
                "authors": [
                    {
                        "name": "Jungkyu Park"
                    },
                    {
                        "name": "Jan Witowski"
                    },
                    {
                        "name": "Yanqi Xu"
                    },
                    {
                        "name": "Hari Trivedi"
                    },
                    {
                        "name": "Judy Gichoya"
                    },
                    {
                        "name": "Beatrice Brown-Mulry"
                    },
                    {
                        "name": "Malte Westerhoff"
                    },
                    {
                        "name": "Linda Moy"
                    },
                    {
                        "name": "Laura Heacock"
                    },
                    {
                        "name": "Alana Lewin"
                    },
                    {
                        "name": "Krzysztof J. Geras"
                    }
                ],
                "author_detail": {
                    "name": "Krzysztof J. Geras"
                },
                "author": "Krzysztof J. Geras",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05636v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05636v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08640v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08640v1",
                "updated": "2025-04-11T15:41:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    41,
                    21,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T15:41:21Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    41,
                    21,
                    4,
                    101,
                    0
                ],
                "title": "Do LLMs trust AI regulation? Emerging behaviour of game-theoretic LLM\n  agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs trust AI regulation? Emerging behaviour of game-theoretic LLM\n  agents"
                },
                "summary": "There is general agreement that fostering trust and cooperation within the AI\ndevelopment ecosystem is essential to promote the adoption of trustworthy AI\nsystems. By embedding Large Language Model (LLM) agents within an evolutionary\ngame-theoretic framework, this paper investigates the complex interplay between\nAI developers, regulators and users, modelling their strategic choices under\ndifferent regulatory scenarios. Evolutionary game theory (EGT) is used to\nquantitatively model the dilemmas faced by each actor, and LLMs provide\nadditional degrees of complexity and nuances and enable repeated games and\nincorporation of personality traits. Our research identifies emerging\nbehaviours of strategic AI agents, which tend to adopt more \"pessimistic\" (not\ntrusting and defective) stances than pure game-theoretic agents. We observe\nthat, in case of full trust by users, incentives are effective to promote\neffective regulation; however, conditional trust may deteriorate the \"social\npact\". Establishing a virtuous feedback between users' trust and regulators'\nreputation thus appears to be key to nudge developers towards creating safe AI.\nHowever, the level at which this trust emerges may depend on the specific LLM\nused for testing. Our results thus provide guidance for AI regulation systems,\nand help predict the outcome of strategic LLM agents, should they be used to\naid regulation itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is general agreement that fostering trust and cooperation within the AI\ndevelopment ecosystem is essential to promote the adoption of trustworthy AI\nsystems. By embedding Large Language Model (LLM) agents within an evolutionary\ngame-theoretic framework, this paper investigates the complex interplay between\nAI developers, regulators and users, modelling their strategic choices under\ndifferent regulatory scenarios. Evolutionary game theory (EGT) is used to\nquantitatively model the dilemmas faced by each actor, and LLMs provide\nadditional degrees of complexity and nuances and enable repeated games and\nincorporation of personality traits. Our research identifies emerging\nbehaviours of strategic AI agents, which tend to adopt more \"pessimistic\" (not\ntrusting and defective) stances than pure game-theoretic agents. We observe\nthat, in case of full trust by users, incentives are effective to promote\neffective regulation; however, conditional trust may deteriorate the \"social\npact\". Establishing a virtuous feedback between users' trust and regulators'\nreputation thus appears to be key to nudge developers towards creating safe AI.\nHowever, the level at which this trust emerges may depend on the specific LLM\nused for testing. Our results thus provide guidance for AI regulation systems,\nand help predict the outcome of strategic LLM agents, should they be used to\naid regulation itself."
                },
                "authors": [
                    {
                        "name": "Alessio Buscemi"
                    },
                    {
                        "name": "Daniele Proverbio"
                    },
                    {
                        "name": "Paolo Bova"
                    },
                    {
                        "name": "Nataliya Balabanova"
                    },
                    {
                        "name": "Adeela Bashir"
                    },
                    {
                        "name": "Theodor Cimpeanu"
                    },
                    {
                        "name": "Henrique Correia da Fonseca"
                    },
                    {
                        "name": "Manh Hong Duong"
                    },
                    {
                        "name": "Elias Fernandez Domingos"
                    },
                    {
                        "name": "Antonio M. Fernandes"
                    },
                    {
                        "name": "Marcus Krellner"
                    },
                    {
                        "name": "Ndidi Bianca Ogbo"
                    },
                    {
                        "name": "Simon T. Powers"
                    },
                    {
                        "name": "Fernando P. Santos"
                    },
                    {
                        "name": "Zia Ush Shamszaman"
                    },
                    {
                        "name": "Zhao Song"
                    },
                    {
                        "name": "Alessandro Di Stefano"
                    },
                    {
                        "name": "The Anh Han"
                    }
                ],
                "author_detail": {
                    "name": "The Anh Han"
                },
                "author": "The Anh Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08640v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08640v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.CD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05439v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05439v2",
                "updated": "2025-04-11T15:33:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    33,
                    14,
                    4,
                    101,
                    0
                ],
                "published": "2025-03-07T14:10:10Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    14,
                    10,
                    10,
                    4,
                    66,
                    0
                ],
                "title": "An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for\n  Robust Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study of Conformal Prediction in LLM with ASP Scaffolds for\n  Robust Reasoning"
                },
                "summary": "In this paper, we examine the use of Conformal Language Modelling (CLM)\nalongside Answer Set Programming (ASP) to enhance the performance of standard\nopen-weight LLMs on complex multi-step reasoning tasks. Using the StepGame\ndataset, which requires spatial reasoning, we apply CLM to generate sets of ASP\nprograms from an LLM, providing statistical guarantees on the correctness of\nthe outputs. Experimental results show that CLM significantly outperforms\nbaseline models that use standard sampling methods, achieving substantial\naccuracy improvements across different levels of reasoning complexity.\nAdditionally, the LLM-as-Judge metric enhances CLM's performance, especially in\nassessing structurally and logically correct ASP outputs. However, calibrating\nCLM with diverse calibration sets did not improve generalizability for tasks\nrequiring much longer reasoning steps, indicating limitations in handling more\ncomplex tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we examine the use of Conformal Language Modelling (CLM)\nalongside Answer Set Programming (ASP) to enhance the performance of standard\nopen-weight LLMs on complex multi-step reasoning tasks. Using the StepGame\ndataset, which requires spatial reasoning, we apply CLM to generate sets of ASP\nprograms from an LLM, providing statistical guarantees on the correctness of\nthe outputs. Experimental results show that CLM significantly outperforms\nbaseline models that use standard sampling methods, achieving substantial\naccuracy improvements across different levels of reasoning complexity.\nAdditionally, the LLM-as-Judge metric enhances CLM's performance, especially in\nassessing structurally and logically correct ASP outputs. However, calibrating\nCLM with diverse calibration sets did not improve generalizability for tasks\nrequiring much longer reasoning steps, indicating limitations in handling more\ncomplex tasks."
                },
                "authors": [
                    {
                        "name": "Navdeep Kaur"
                    },
                    {
                        "name": "Lachlan McPheat"
                    },
                    {
                        "name": "Alessandra Russo"
                    },
                    {
                        "name": "Anthony G Cohn"
                    },
                    {
                        "name": "Pranava Madhyastha"
                    }
                ],
                "author_detail": {
                    "name": "Pranava Madhyastha"
                },
                "author": "Pranava Madhyastha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05439v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05439v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08621v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08621v1",
                "updated": "2025-04-11T15:25:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    25,
                    50,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T15:25:50Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    25,
                    50,
                    4,
                    101,
                    0
                ],
                "title": "MooseAgent: A LLM Based Multi-agent Framework for Automating Moose\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MooseAgent: A LLM Based Multi-agent Framework for Automating Moose\n  Simulation"
                },
                "summary": "The Finite Element Method (FEM) is widely used in engineering and scientific\ncomputing, but its pre-processing, solver configuration, and post-processing\nstages are often time-consuming and require specialized knowledge. This paper\nproposes an automated solution framework, MooseAgent, for the multi-physics\nsimulation framework MOOSE, which combines large-scale pre-trained language\nmodels (LLMs) with a multi-agent system. The framework uses LLMs to understand\nuser-described simulation requirements in natural language and employs task\ndecomposition and multi-round iterative verification strategies to\nautomatically generate MOOSE input files. To improve accuracy and reduce model\nhallucinations, the system builds and utilizes a vector database containing\nannotated MOOSE input cards and function documentation. We conducted\nexperimental evaluations on several typical cases, including heat transfer,\nmechanics, phase field, and multi-physics coupling. The results show that\nMooseAgent can automate the MOOSE simulation process to a certain extent,\nespecially demonstrating a high success rate when dealing with relatively\nsimple single-physics problems. The main contribution of this research is the\nproposal of a multi-agent automated framework for MOOSE, which validates its\npotential in simplifying finite element simulation processes and lowering the\nuser barrier, providing new ideas for the development of intelligent finite\nelement simulation software. The code for the MooseAgent framework proposed in\nthis paper has been open-sourced and is available at\nhttps://github.com/taozhan18/MooseAgent",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Finite Element Method (FEM) is widely used in engineering and scientific\ncomputing, but its pre-processing, solver configuration, and post-processing\nstages are often time-consuming and require specialized knowledge. This paper\nproposes an automated solution framework, MooseAgent, for the multi-physics\nsimulation framework MOOSE, which combines large-scale pre-trained language\nmodels (LLMs) with a multi-agent system. The framework uses LLMs to understand\nuser-described simulation requirements in natural language and employs task\ndecomposition and multi-round iterative verification strategies to\nautomatically generate MOOSE input files. To improve accuracy and reduce model\nhallucinations, the system builds and utilizes a vector database containing\nannotated MOOSE input cards and function documentation. We conducted\nexperimental evaluations on several typical cases, including heat transfer,\nmechanics, phase field, and multi-physics coupling. The results show that\nMooseAgent can automate the MOOSE simulation process to a certain extent,\nespecially demonstrating a high success rate when dealing with relatively\nsimple single-physics problems. The main contribution of this research is the\nproposal of a multi-agent automated framework for MOOSE, which validates its\npotential in simplifying finite element simulation processes and lowering the\nuser barrier, providing new ideas for the development of intelligent finite\nelement simulation software. The code for the MooseAgent framework proposed in\nthis paper has been open-sourced and is available at\nhttps://github.com/taozhan18/MooseAgent"
                },
                "authors": [
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Zhenhai Liu"
                    },
                    {
                        "name": "Yong Xin"
                    },
                    {
                        "name": "Yongjun Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Yongjun Jiao"
                },
                "author": "Yongjun Jiao",
                "arxiv_comment": "7 pages, 2 Figs",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08621v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08621v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08619v1",
                "updated": "2025-04-11T15:24:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    24,
                    23,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T15:24:23Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    24,
                    23,
                    4,
                    101,
                    0
                ],
                "title": "Analyzing 16,193 LLM Papers for Fun and Profits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing 16,193 LLM Papers for Fun and Profits"
                },
                "summary": "Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are reshaping the landscape of computer science\nresearch, driving significant shifts in research priorities across diverse\nconferences and fields. This study provides a comprehensive analysis of the\npublication trend of LLM-related papers in 77 top-tier computer science\nconferences over the past six years (2019-2024). We approach this analysis from\nfour distinct perspectives: (1) We investigate how LLM research is driving\ntopic shifts within major conferences. (2) We adopt a topic modeling approach\nto identify various areas of LLM-related topic growth and reveal the topics of\nconcern at different conferences. (3) We explore distinct contribution patterns\nof academic and industrial institutions. (4) We study the influence of national\norigins on LLM development trajectories. Synthesizing the findings from these\ndiverse analytical angles, we derive ten key insights that illuminate the\ndynamics and evolution of the LLM research ecosystem."
                },
                "authors": [
                    {
                        "name": "Zhiqiu Xia"
                    },
                    {
                        "name": "Lang Zhu"
                    },
                    {
                        "name": "Bingzhe Li"
                    },
                    {
                        "name": "Feng Chen"
                    },
                    {
                        "name": "Qiannan Li"
                    },
                    {
                        "name": "Hang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Liu"
                },
                "author": "Hang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07574v2",
                "updated": "2025-04-11T15:06:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    15,
                    6,
                    17,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:17:45Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    17,
                    45,
                    3,
                    100,
                    0
                ],
                "title": "Malware analysis assisted by AI with R2AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Malware analysis assisted by AI with R2AI"
                },
                "summary": "This research studies the quality, speed and cost of malware analysis\nassisted by artificial intelligence. It focuses on Linux and IoT malware of\n2024-2025, and uses r2ai, the AI extension of Radare2's disassembler. Not all\nmalware and not all LLMs are equivalent but the study shows excellent results\nwith Claude 3.5 and 3.7 Sonnet. Despite a few errors, the quality of analysis\nis overall equal or better than without AI assistance. For good results, the AI\ncannot operate alone and must constantly be guided by an experienced analyst.\nThe gain of speed is largely visible with AI assistance, even when taking\naccount the time to understand AI's hallucinations, exaggerations and\nomissions. The cost is usually noticeably lower than the salary of a malware\nanalyst, but attention and guidance is needed to keep it under control in cases\nwhere the AI would naturally loop without showing progress.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research studies the quality, speed and cost of malware analysis\nassisted by artificial intelligence. It focuses on Linux and IoT malware of\n2024-2025, and uses r2ai, the AI extension of Radare2's disassembler. Not all\nmalware and not all LLMs are equivalent but the study shows excellent results\nwith Claude 3.5 and 3.7 Sonnet. Despite a few errors, the quality of analysis\nis overall equal or better than without AI assistance. For good results, the AI\ncannot operate alone and must constantly be guided by an experienced analyst.\nThe gain of speed is largely visible with AI assistance, even when taking\naccount the time to understand AI's hallucinations, exaggerations and\nomissions. The cost is usually noticeably lower than the salary of a malware\nanalyst, but attention and guidance is needed to keep it under control in cases\nwhere the AI would naturally loop without showing progress."
                },
                "authors": [
                    {
                        "name": "Axelle Apvrille"
                    },
                    {
                        "name": "Daniel Nakov"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Nakov"
                },
                "author": "Daniel Nakov",
                "arxiv_comment": "11 pages;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05067v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05067v2",
                "updated": "2025-04-11T14:58:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    14,
                    58,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-07T13:36:03Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    13,
                    36,
                    3,
                    0,
                    97,
                    0
                ],
                "title": "\"Security for Everyone\" in Finite Blocklength IRS-aided Systems With\n  Perfect and Imperfect CSI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Security for Everyone\" in Finite Blocklength IRS-aided Systems With\n  Perfect and Imperfect CSI"
                },
                "summary": "Provisioning secrecy for all users, given the heterogeneity in their channel\nconditions, locations, and the unknown location of the attacker/eavesdropper,\nis challenging and not always feasible. The problem is even more difficult\nunder finite blocklength constraints that are popular in ultra-reliable\nlow-latency communication (URLLC) and massive machine-type communications\n(mMTC). This work takes the first step to guarantee secrecy for all URLLC/mMTC\nusers in the finite blocklength regime (FBR) where intelligent reflecting\nsurfaces (IRS) are used to enhance legitimate users' reception and thwart the\npotential eavesdropper (Eve) from intercepting. To that end, we aim to maximize\nthe minimum secrecy rate (SR) among all users by jointly optimizing the\ntransmitter's beamforming and IRS's passive reflective elements (PREs) under\nthe FBR latency constraints. The resulting optimization problem is non-convex\nand even more complicated under imperfect channel state information (CSI). To\ntackle it, we linearize the objective function, and decompose the problem into\nsequential subproblems. When perfect CSI is not available, we use the\nsuccessive convex approximation (SCA) approach to transform imperfect\nCSI-related semi-infinite constraints into finite linear matrix inequalities\n(LMI). We prove that our proposed algorithm converges to a locally optimal\nsolution with low computational complexity thanks to our closed-form\nlinearization approach. This makes the solution scalable for large IRS\ndeployments. Extensive simulations with practical settings show that our\napproach can ensure secure communication for all users while satisfying FBR\nconstraints even with only imperfect CSI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Provisioning secrecy for all users, given the heterogeneity in their channel\nconditions, locations, and the unknown location of the attacker/eavesdropper,\nis challenging and not always feasible. The problem is even more difficult\nunder finite blocklength constraints that are popular in ultra-reliable\nlow-latency communication (URLLC) and massive machine-type communications\n(mMTC). This work takes the first step to guarantee secrecy for all URLLC/mMTC\nusers in the finite blocklength regime (FBR) where intelligent reflecting\nsurfaces (IRS) are used to enhance legitimate users' reception and thwart the\npotential eavesdropper (Eve) from intercepting. To that end, we aim to maximize\nthe minimum secrecy rate (SR) among all users by jointly optimizing the\ntransmitter's beamforming and IRS's passive reflective elements (PREs) under\nthe FBR latency constraints. The resulting optimization problem is non-convex\nand even more complicated under imperfect channel state information (CSI). To\ntackle it, we linearize the objective function, and decompose the problem into\nsequential subproblems. When perfect CSI is not available, we use the\nsuccessive convex approximation (SCA) approach to transform imperfect\nCSI-related semi-infinite constraints into finite linear matrix inequalities\n(LMI). We prove that our proposed algorithm converges to a locally optimal\nsolution with low computational complexity thanks to our closed-form\nlinearization approach. This makes the solution scalable for large IRS\ndeployments. Extensive simulations with practical settings show that our\napproach can ensure secure communication for all users while satisfying FBR\nconstraints even with only imperfect CSI."
                },
                "authors": [
                    {
                        "name": "Monir Abughalwa"
                    },
                    {
                        "name": "Diep N. Nguyen"
                    },
                    {
                        "name": "Dinh Thai Hoang"
                    },
                    {
                        "name": "Van-Dinh Nguyen"
                    },
                    {
                        "name": "Ming Zeng"
                    },
                    {
                        "name": "Quoc-Viet Pham"
                    },
                    {
                        "name": "Eryk Dutkiewicz"
                    }
                ],
                "author_detail": {
                    "name": "Eryk Dutkiewicz"
                },
                "author": "Eryk Dutkiewicz",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2504.05048",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05067v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08585v1",
                "updated": "2025-04-11T14:39:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    14,
                    39,
                    25,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T14:39:25Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    14,
                    39,
                    25,
                    4,
                    101,
                    0
                ],
                "title": "Ready, Bid, Go! On-Demand Delivery Using Fleets of Drones with Unknown,\n  Heterogeneous Energy Storage Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ready, Bid, Go! On-Demand Delivery Using Fleets of Drones with Unknown,\n  Heterogeneous Energy Storage Constraints"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) are expected to transform logistics, reducing\ndelivery time, costs, and emissions. This study addresses an on-demand delivery\n, in which fleets of UAVs are deployed to fulfil orders that arrive\nstochastically. Unlike previous work, it considers UAVs with heterogeneous,\nunknown energy storage capacities and assumes no knowledge of the energy\nconsumption models. We propose a decentralised deployment strategy that\ncombines auction-based task allocation with online learning. Each UAV\nindependently decides whether to bid for orders based on its energy storage\ncharge level, the parcel mass, and delivery distance. Over time, it refines its\npolicy to bid only for orders within its capability. Simulations using\nrealistic UAV energy models reveal that, counter-intuitively, assigning orders\nto the least confident bidders reduces delivery times and increases the number\nof successfully fulfilled orders. This strategy is shown to outperform\nthreshold-based methods which require UAVs to exceed specific charge levels at\ndeployment. We propose a variant of the strategy which uses learned policies\nfor forecasting. This enables UAVs with insufficient charge levels to commit to\nfulfilling orders at specific future times, helping to prioritise early orders.\nOur work provides new insights into long-term deployment of UAV swarms,\nhighlighting the advantages of decentralised energy-aware decision-making\ncoupled with online learning in real-world dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) are expected to transform logistics, reducing\ndelivery time, costs, and emissions. This study addresses an on-demand delivery\n, in which fleets of UAVs are deployed to fulfil orders that arrive\nstochastically. Unlike previous work, it considers UAVs with heterogeneous,\nunknown energy storage capacities and assumes no knowledge of the energy\nconsumption models. We propose a decentralised deployment strategy that\ncombines auction-based task allocation with online learning. Each UAV\nindependently decides whether to bid for orders based on its energy storage\ncharge level, the parcel mass, and delivery distance. Over time, it refines its\npolicy to bid only for orders within its capability. Simulations using\nrealistic UAV energy models reveal that, counter-intuitively, assigning orders\nto the least confident bidders reduces delivery times and increases the number\nof successfully fulfilled orders. This strategy is shown to outperform\nthreshold-based methods which require UAVs to exceed specific charge levels at\ndeployment. We propose a variant of the strategy which uses learned policies\nfor forecasting. This enables UAVs with insufficient charge levels to commit to\nfulfilling orders at specific future times, helping to prioritise early orders.\nOur work provides new insights into long-term deployment of UAV swarms,\nhighlighting the advantages of decentralised energy-aware decision-making\ncoupled with online learning in real-world dynamic environments."
                },
                "authors": [
                    {
                        "name": "Mohamed S. Talamali"
                    },
                    {
                        "name": "Genki Miyauchi"
                    },
                    {
                        "name": "Thomas Watteyne"
                    },
                    {
                        "name": "Micael S. Couceiro"
                    },
                    {
                        "name": "Roderich Gross"
                    }
                ],
                "author_detail": {
                    "name": "Roderich Gross"
                },
                "author": "Roderich Gross",
                "arxiv_comment": "The 24th International Conference on Autonomous Agents and Multiagent\n  Systems (AAMAS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08584v1",
                "updated": "2025-04-11T14:38:09Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    14,
                    38,
                    9,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T14:38:09Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    14,
                    38,
                    9,
                    4,
                    101,
                    0
                ],
                "title": "Boosting multi-demographic federated learning for chest x-ray analysis\n  using general-purpose self-supervised representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting multi-demographic federated learning for chest x-ray analysis\n  using general-purpose self-supervised representations"
                },
                "summary": "Reliable artificial intelligence (AI) models for medical image analysis often\ndepend on large and diverse labeled datasets. Federated learning (FL) offers a\ndecentralized and privacy-preserving approach to training but struggles in\nhighly non-independent and identically distributed (non-IID) settings, where\ninstitutions with more representative data may experience degraded performance.\nMoreover, existing large-scale FL studies have been limited to adult datasets,\nneglecting the unique challenges posed by pediatric data, which introduces\nadditional non-IID variability. To address these limitations, we analyzed\nn=398,523 adult chest radiographs from diverse institutions across multiple\ncountries and n=9,125 pediatric images, leveraging transfer learning from\ngeneral-purpose self-supervised image representations to classify pneumonia and\ncases with no abnormality. Using state-of-the-art vision transformers, we found\nthat FL improved performance only for smaller adult datasets (P<0.001) but\ndegraded performance for larger datasets (P<0.064) and pediatric cases\n(P=0.242). However, equipping FL with self-supervised weights significantly\nenhanced outcomes across pediatric cases (P=0.031) and most adult datasets\n(P<0.008), except the largest dataset (P=0.052). These findings underscore the\npotential of easily deployable general-purpose self-supervised image\nrepresentations to address non-IID challenges in clinical FL applications and\nhighlight their promise for enhancing patient outcomes and advancing pediatric\nhealthcare, where data scarcity and variability remain persistent obstacles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable artificial intelligence (AI) models for medical image analysis often\ndepend on large and diverse labeled datasets. Federated learning (FL) offers a\ndecentralized and privacy-preserving approach to training but struggles in\nhighly non-independent and identically distributed (non-IID) settings, where\ninstitutions with more representative data may experience degraded performance.\nMoreover, existing large-scale FL studies have been limited to adult datasets,\nneglecting the unique challenges posed by pediatric data, which introduces\nadditional non-IID variability. To address these limitations, we analyzed\nn=398,523 adult chest radiographs from diverse institutions across multiple\ncountries and n=9,125 pediatric images, leveraging transfer learning from\ngeneral-purpose self-supervised image representations to classify pneumonia and\ncases with no abnormality. Using state-of-the-art vision transformers, we found\nthat FL improved performance only for smaller adult datasets (P<0.001) but\ndegraded performance for larger datasets (P<0.064) and pediatric cases\n(P=0.242). However, equipping FL with self-supervised weights significantly\nenhanced outcomes across pediatric cases (P=0.031) and most adult datasets\n(P<0.008), except the largest dataset (P=0.052). These findings underscore the\npotential of easily deployable general-purpose self-supervised image\nrepresentations to address non-IID challenges in clinical FL applications and\nhighlight their promise for enhancing patient outcomes and advancing pediatric\nhealthcare, where data scarcity and variability remain persistent obstacles."
                },
                "authors": [
                    {
                        "name": "Mahshad Lotfinia"
                    },
                    {
                        "name": "Arash Tayebiarasteh"
                    },
                    {
                        "name": "Samaneh Samiei"
                    },
                    {
                        "name": "Mehdi Joodaki"
                    },
                    {
                        "name": "Soroosh Tayebi Arasteh"
                    }
                ],
                "author_detail": {
                    "name": "Soroosh Tayebi Arasteh"
                },
                "author": "Soroosh Tayebi Arasteh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07703v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07703v3",
                "updated": "2025-04-11T14:12:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    14,
                    12,
                    58,
                    4,
                    101,
                    0
                ],
                "published": "2024-09-12T02:08:00Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    2,
                    8,
                    0,
                    3,
                    256,
                    0
                ],
                "title": "DSBench: How Far Are Data Science Agents from Becoming Data Science\n  Experts?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSBench: How Far Are Data Science Agents from Becoming Data Science\n  Experts?"
                },
                "summary": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have\ndemonstrated impressive language/vision reasoning abilities, igniting the\nrecent trend of building agents for targeted applications such as shopping\nassistants or AI software engineers. Recently, many data science benchmarks\nhave been proposed to investigate their performance in the data science domain.\nHowever, existing data science benchmarks still fall short when compared to\nreal-world data science applications due to their simplified settings. To\nbridge this gap, we introduce DSBench, a comprehensive benchmark designed to\nevaluate data science agents with realistic tasks. This benchmark includes 466\ndata analysis tasks and 74 data modeling tasks, sourced from Eloquence and\nKaggle competitions. DSBench offers a realistic setting by encompassing long\ncontexts, multimodal task backgrounds, reasoning with large data files and\nmulti-table structures, and performing end-to-end data modeling tasks. Our\nevaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle\nwith most tasks, with the best agent solving only 34.12% of data analysis tasks\nand achieving a 34.74% Relative Performance Gap (RPG). These findings\nunderscore the need for further advancements in developing more practical,\nintelligent, and autonomous data science agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) have\ndemonstrated impressive language/vision reasoning abilities, igniting the\nrecent trend of building agents for targeted applications such as shopping\nassistants or AI software engineers. Recently, many data science benchmarks\nhave been proposed to investigate their performance in the data science domain.\nHowever, existing data science benchmarks still fall short when compared to\nreal-world data science applications due to their simplified settings. To\nbridge this gap, we introduce DSBench, a comprehensive benchmark designed to\nevaluate data science agents with realistic tasks. This benchmark includes 466\ndata analysis tasks and 74 data modeling tasks, sourced from Eloquence and\nKaggle competitions. DSBench offers a realistic setting by encompassing long\ncontexts, multimodal task backgrounds, reasoning with large data files and\nmulti-table structures, and performing end-to-end data modeling tasks. Our\nevaluation of state-of-the-art LLMs, LVLMs, and agents shows that they struggle\nwith most tasks, with the best agent solving only 34.12% of data analysis tasks\nand achieving a 34.74% Relative Performance Gap (RPG). These findings\nunderscore the need for further advancements in developing more practical,\nintelligent, and autonomous data science agents."
                },
                "authors": [
                    {
                        "name": "Liqiang Jing"
                    },
                    {
                        "name": "Zhehui Huang"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Wenlin Yao"
                    },
                    {
                        "name": "Wenhao Yu"
                    },
                    {
                        "name": "Kaixin Ma"
                    },
                    {
                        "name": "Hongming Zhang"
                    },
                    {
                        "name": "Xinya Du"
                    },
                    {
                        "name": "Dong Yu"
                    }
                ],
                "author_detail": {
                    "name": "Dong Yu"
                },
                "author": "Dong Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07703v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07703v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08525v1",
                "updated": "2025-04-11T13:38:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    13,
                    38,
                    36,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T13:38:36Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    13,
                    38,
                    36,
                    4,
                    101,
                    0
                ],
                "title": "Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM\n  Agent Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM\n  Agent Tasks"
                },
                "summary": "Large Language Models (LLMs) are increasingly used as autonomous agents for\nmulti-step tasks. However, most existing frameworks fail to maintain a\nstructured understanding of the task state, often relying on linear prompt\nconcatenation or shallow memory buffers. This leads to brittle performance,\nfrequent hallucinations, and poor long-range coherence. In this work, we\npropose the Task Memory Engine (TME), a lightweight and structured memory\nmodule that tracks task execution using a hierarchical Task Memory Tree (TMT).\nEach node in the tree corresponds to a task step, storing relevant input,\noutput, status, and sub-task relationships. We introduce a prompt synthesis\nmethod that dynamically generates LLM prompts based on the active node path,\nsignificantly improving execution consistency and contextual grounding. Through\ncase studies and comparative experiments on multi-step agent tasks, we\ndemonstrate that TME leads to better task completion accuracy and more\ninterpretable behavior with minimal implementation overhead. The full\nimplementation of TME is available at\nhttps://github.com/biubiutomato/TME-Agent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used as autonomous agents for\nmulti-step tasks. However, most existing frameworks fail to maintain a\nstructured understanding of the task state, often relying on linear prompt\nconcatenation or shallow memory buffers. This leads to brittle performance,\nfrequent hallucinations, and poor long-range coherence. In this work, we\npropose the Task Memory Engine (TME), a lightweight and structured memory\nmodule that tracks task execution using a hierarchical Task Memory Tree (TMT).\nEach node in the tree corresponds to a task step, storing relevant input,\noutput, status, and sub-task relationships. We introduce a prompt synthesis\nmethod that dynamically generates LLM prompts based on the active node path,\nsignificantly improving execution consistency and contextual grounding. Through\ncase studies and comparative experiments on multi-step agent tasks, we\ndemonstrate that TME leads to better task completion accuracy and more\ninterpretable behavior with minimal implementation overhead. The full\nimplementation of TME is available at\nhttps://github.com/biubiutomato/TME-Agent."
                },
                "authors": [
                    {
                        "name": "Ye Ye"
                    }
                ],
                "author_detail": {
                    "name": "Ye Ye"
                },
                "author": "Ye Ye",
                "arxiv_comment": "14 pages, 5 figures. Preprint prepared for future submission.\n  Includes implementation and token-efficiency analysis. Code at\n  https://github.com/biubiutomato/TME-Agent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.6; I.2.8; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08508v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08508v1",
                "updated": "2025-04-11T13:21:33Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    13,
                    21,
                    33,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T13:21:33Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    13,
                    21,
                    33,
                    4,
                    101,
                    0
                ],
                "title": "An Early Experience with Confidential Computing Architecture for\n  On-Device Model Protection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Early Experience with Confidential Computing Architecture for\n  On-Device Model Protection"
                },
                "summary": "Deploying machine learning (ML) models on user devices can improve privacy\n(by keeping data local) and reduce inference latency. Trusted Execution\nEnvironments (TEEs) are a practical solution for protecting proprietary models,\nyet existing TEE solutions have architectural constraints that hinder on-device\nmodel deployment. Arm Confidential Computing Architecture (CCA), a new Arm\nextension, addresses several of these limitations and shows promise as a secure\nplatform for on-device ML. In this paper, we evaluate the performance-privacy\ntrade-offs of deploying models within CCA, highlighting its potential to enable\nconfidential and efficient ML applications. Our evaluations show that CCA can\nachieve an overhead of, at most, 22% in running models of different sizes and\napplications, including image classification, voice recognition, and chat\nassistants. This performance overhead comes with privacy benefits; for example,\nour framework can successfully protect the model against membership inference\nattack by an 8.3% reduction in the adversary's success rate. To support further\nresearch and early adoption, we make our code and methodology publicly\navailable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying machine learning (ML) models on user devices can improve privacy\n(by keeping data local) and reduce inference latency. Trusted Execution\nEnvironments (TEEs) are a practical solution for protecting proprietary models,\nyet existing TEE solutions have architectural constraints that hinder on-device\nmodel deployment. Arm Confidential Computing Architecture (CCA), a new Arm\nextension, addresses several of these limitations and shows promise as a secure\nplatform for on-device ML. In this paper, we evaluate the performance-privacy\ntrade-offs of deploying models within CCA, highlighting its potential to enable\nconfidential and efficient ML applications. Our evaluations show that CCA can\nachieve an overhead of, at most, 22% in running models of different sizes and\napplications, including image classification, voice recognition, and chat\nassistants. This performance overhead comes with privacy benefits; for example,\nour framework can successfully protect the model against membership inference\nattack by an 8.3% reduction in the adversary's success rate. To support further\nresearch and early adoption, we make our code and methodology publicly\navailable."
                },
                "authors": [
                    {
                        "name": "Sina Abdollahi"
                    },
                    {
                        "name": "Mohammad Maheri"
                    },
                    {
                        "name": "Sandra Siby"
                    },
                    {
                        "name": "Marios Kogias"
                    },
                    {
                        "name": "Hamed Haddadi"
                    }
                ],
                "author_detail": {
                    "name": "Hamed Haddadi"
                },
                "author": "Hamed Haddadi",
                "arxiv_comment": "Accepted to the 8th Workshop on System Software for Trusted Execution\n  (SysTEX 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08508v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08508v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06553v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06553v3",
                "updated": "2025-04-11T12:57:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    57,
                    13,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-09T03:22:52Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    3,
                    22,
                    52,
                    2,
                    99,
                    0
                ],
                "title": "ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis"
                },
                "summary": "While recent work in scene reconstruction and understanding has made strides\nin grounding natural language to physical 3D environments, it is still\nchallenging to ground abstract, high-level instructions to a 3D scene.\nHigh-level instructions might not explicitly invoke semantic elements in the\nscene, and even the process of breaking a high-level task into a set of more\nconcrete subtasks, a process called hierarchical task analysis, is\nenvironment-dependent. In this work, we propose ASHiTA, the first framework\nthat generates a task hierarchy grounded to a 3D scene graph by breaking down\nhigh-level tasks into grounded subtasks. ASHiTA alternates LLM-assisted\nhierarchical task analysis, to generate the task breakdown, with task-driven 3D\nscene graph construction to generate a suitable representation of the\nenvironment. Our experiments show that ASHiTA performs significantly better\nthan LLM baselines in breaking down high-level tasks into environment-dependent\nsubtasks and is additionally able to achieve grounding performance comparable\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent work in scene reconstruction and understanding has made strides\nin grounding natural language to physical 3D environments, it is still\nchallenging to ground abstract, high-level instructions to a 3D scene.\nHigh-level instructions might not explicitly invoke semantic elements in the\nscene, and even the process of breaking a high-level task into a set of more\nconcrete subtasks, a process called hierarchical task analysis, is\nenvironment-dependent. In this work, we propose ASHiTA, the first framework\nthat generates a task hierarchy grounded to a 3D scene graph by breaking down\nhigh-level tasks into grounded subtasks. ASHiTA alternates LLM-assisted\nhierarchical task analysis, to generate the task breakdown, with task-driven 3D\nscene graph construction to generate a suitable representation of the\nenvironment. Our experiments show that ASHiTA performs significantly better\nthan LLM baselines in breaking down high-level tasks into environment-dependent\nsubtasks and is additionally able to achieve grounding performance comparable\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yun Chang"
                    },
                    {
                        "name": "Leonor Fermoselle"
                    },
                    {
                        "name": "Duy Ta"
                    },
                    {
                        "name": "Bernadette Bucher"
                    },
                    {
                        "name": "Luca Carlone"
                    },
                    {
                        "name": "Jiuguang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jiuguang Wang"
                },
                "author": "Jiuguang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06553v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06553v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08490v1",
                "updated": "2025-04-11T12:42:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    42,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T12:42:01Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    12,
                    42,
                    1,
                    4,
                    101,
                    0
                ],
                "title": "Adopting Large Language Models to Automated System Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adopting Large Language Models to Automated System Integration"
                },
                "summary": "Modern enterprise computing systems integrate numerous subsystems to resolve\na common task by yielding emergent behavior. A widespread approach is using\nservices implemented with Web technologies like REST or OpenAPI, which offer an\ninteraction mechanism and service documentation standard, respectively. Each\nservice represents a specific business functionality, allowing encapsulation\nand easier maintenance. Despite the reduced maintenance costs on an individual\nservice level, increased integration complexity arises. Consequently, automated\nservice composition approaches have arisen to mitigate this issue.\nNevertheless, these approaches have not achieved high acceptance in practice\ndue to their reliance on complex formal modeling. Within this Ph.D. thesis, we\nanalyze the application of Large Language Models (LLMs) to automatically\nintegrate the services based on a natural language input. The result is a\nreusable service composition, e.g., as program code. While not always\ngenerating entirely correct results, the result can still be helpful by\nproviding integration engineers with a close approximation of a suitable\nsolution, which requires little effort to become operational. Our research\ninvolves (i) introducing a software architecture for automated service\ncomposition using LLMs, (ii) analyzing Retrieval Augmented Generation (RAG) for\nservice discovery, (iii) proposing a novel natural language query-based\nbenchmark for service discovery, and (iv) extending the benchmark to complete\nservice composition scenarios. We have presented our software architecture as\nCompositio Prompto, the analysis of RAG for service discovery, and submitted a\nproposal for the service discovery benchmark. Open topics are primarily the\nextension of the service discovery benchmark to service composition scenarios\nand the improvements of the service composition generation, e.g., using\nfine-tuning or LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern enterprise computing systems integrate numerous subsystems to resolve\na common task by yielding emergent behavior. A widespread approach is using\nservices implemented with Web technologies like REST or OpenAPI, which offer an\ninteraction mechanism and service documentation standard, respectively. Each\nservice represents a specific business functionality, allowing encapsulation\nand easier maintenance. Despite the reduced maintenance costs on an individual\nservice level, increased integration complexity arises. Consequently, automated\nservice composition approaches have arisen to mitigate this issue.\nNevertheless, these approaches have not achieved high acceptance in practice\ndue to their reliance on complex formal modeling. Within this Ph.D. thesis, we\nanalyze the application of Large Language Models (LLMs) to automatically\nintegrate the services based on a natural language input. The result is a\nreusable service composition, e.g., as program code. While not always\ngenerating entirely correct results, the result can still be helpful by\nproviding integration engineers with a close approximation of a suitable\nsolution, which requires little effort to become operational. Our research\ninvolves (i) introducing a software architecture for automated service\ncomposition using LLMs, (ii) analyzing Retrieval Augmented Generation (RAG) for\nservice discovery, (iii) proposing a novel natural language query-based\nbenchmark for service discovery, and (iv) extending the benchmark to complete\nservice composition scenarios. We have presented our software architecture as\nCompositio Prompto, the analysis of RAG for service discovery, and submitted a\nproposal for the service discovery benchmark. Open topics are primarily the\nextension of the service discovery benchmark to service composition scenarios\nand the improvements of the service composition generation, e.g., using\nfine-tuning or LLM agents."
                },
                "authors": [
                    {
                        "name": "Robin D. Pesl"
                    }
                ],
                "author_detail": {
                    "name": "Robin D. Pesl"
                },
                "author": "Robin D. Pesl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08451v1",
                "updated": "2025-04-11T11:27:29Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    11,
                    27,
                    29,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T11:27:29Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    11,
                    27,
                    29,
                    4,
                    101,
                    0
                ],
                "title": "Muon-Accelerated Attention Distillation for Real-Time Edge Synthesis via\n  Optimized Latent Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Muon-Accelerated Attention Distillation for Real-Time Edge Synthesis via\n  Optimized Latent Diffusion"
                },
                "summary": "Recent advances in visual synthesis have leveraged diffusion models and\nattention mechanisms to achieve high-fidelity artistic style transfer and\nphotorealistic text-to-image generation. However, real-time deployment on edge\ndevices remains challenging due to computational and memory constraints. We\npropose Muon-AD, a co-designed framework that integrates the Muon optimizer\nwith attention distillation for real-time edge synthesis. By eliminating\ngradient conflicts through orthogonal parameter updates and dynamic pruning,\nMuon-AD achieves 3.2 times faster convergence compared to Stable\nDiffusion-TensorRT, while maintaining synthesis quality (15% lower FID, 4%\nhigher SSIM). Our framework reduces peak memory to 7GB on Jetson Orin and\nenables 24FPS real-time generation through mixed-precision quantization and\ncurriculum learning. Extensive experiments on COCO-Stuff and ImageNet-Texture\ndemonstrate Muon-AD's Pareto-optimal efficiency-quality trade-offs. Here, we\nshow a 65% reduction in communication overhead during distributed training and\nreal-time 10s/image generation on edge GPUs. These advancements pave the way\nfor democratizing high-quality visual synthesis in resource-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in visual synthesis have leveraged diffusion models and\nattention mechanisms to achieve high-fidelity artistic style transfer and\nphotorealistic text-to-image generation. However, real-time deployment on edge\ndevices remains challenging due to computational and memory constraints. We\npropose Muon-AD, a co-designed framework that integrates the Muon optimizer\nwith attention distillation for real-time edge synthesis. By eliminating\ngradient conflicts through orthogonal parameter updates and dynamic pruning,\nMuon-AD achieves 3.2 times faster convergence compared to Stable\nDiffusion-TensorRT, while maintaining synthesis quality (15% lower FID, 4%\nhigher SSIM). Our framework reduces peak memory to 7GB on Jetson Orin and\nenables 24FPS real-time generation through mixed-precision quantization and\ncurriculum learning. Extensive experiments on COCO-Stuff and ImageNet-Texture\ndemonstrate Muon-AD's Pareto-optimal efficiency-quality trade-offs. Here, we\nshow a 65% reduction in communication overhead during distributed training and\nreal-time 10s/image generation on edge GPUs. These advancements pave the way\nfor democratizing high-quality visual synthesis in resource-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Weiye Chen"
                    },
                    {
                        "name": "Qingen Zhu"
                    },
                    {
                        "name": "Qian Long"
                    }
                ],
                "author_detail": {
                    "name": "Qian Long"
                },
                "author": "Qian Long",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.19887v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.19887v3",
                "updated": "2025-04-11T11:23:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    11,
                    23,
                    24,
                    4,
                    101,
                    0
                ],
                "published": "2025-03-25T17:51:50Z",
                "published_parsed": [
                    2025,
                    3,
                    25,
                    17,
                    51,
                    50,
                    1,
                    84,
                    0
                ],
                "title": "AI threats to national security can be countered by an incident regime",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI threats to national security can be countered by an incident regime"
                },
                "summary": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a timely proposal for a legally mandated post-deployment AI incident\nregime that aims to counter potential national security threats from AI\nsystems. We start the paper by introducing the concept of 'security-critical'\nto describe doctors that pose extreme risks to national security, before\narguing that 'security-critical' describes civilian nuclear power, aviation,\nlife science dual-use research of concern, and frontier AI development. We then\npresent in detail our AI incident regime proposal, justifying each component of\nthe proposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent progress in AI capabilities has heightened concerns that AI systems\ncould pose a threat to national security, for example, by making it easier for\nmalicious actors to perform cyberattacks on critical national infrastructure,\nor through loss of control of autonomous AI systems. In parallel, federal\nlegislators in the US have proposed nascent 'AI incident regimes' to identify\nand counter similar threats. In this paper, we consolidate these two trends and\npresent a timely proposal for a legally mandated post-deployment AI incident\nregime that aims to counter potential national security threats from AI\nsystems. We start the paper by introducing the concept of 'security-critical'\nto describe doctors that pose extreme risks to national security, before\narguing that 'security-critical' describes civilian nuclear power, aviation,\nlife science dual-use research of concern, and frontier AI development. We then\npresent in detail our AI incident regime proposal, justifying each component of\nthe proposal by demonstrating its similarity to US domestic incident regimes in\nother 'security-critical' sectors. Finally, we sketch a hypothetical scenario\nwhere our proposed AI incident regime deals with an AI cyber incident. Our\nproposed AI incident regime is split into three phases. The first phase\nrevolves around a novel operationalization of what counts as an 'AI incident'\nand we suggest that AI providers must create a 'national security case' before\ndeploying a frontier AI system. The second and third phases spell out that AI\nproviders should notify a government agency about incidents, and that the\ngovernment agency should be involved in amending AI providers' security and\nsafety procedures, in order to counter future threats to national security."
                },
                "authors": [
                    {
                        "name": "Alejandro Ortega"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Ortega"
                },
                "author": "Alejandro Ortega",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.19887v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.19887v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08449v1",
                "updated": "2025-04-11T11:18:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    11,
                    18,
                    57,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T11:18:57Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    11,
                    18,
                    57,
                    4,
                    101,
                    0
                ],
                "title": "Ego4o: Egocentric Human Motion Capture and Understanding from\n  Multi-Modal Input",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ego4o: Egocentric Human Motion Capture and Understanding from\n  Multi-Modal Input"
                },
                "summary": "This work focuses on tracking and understanding human motion using consumer\nwearable devices, such as VR/AR headsets, smart glasses, cellphones, and\nsmartwatches. These devices provide diverse, multi-modal sensor inputs,\nincluding egocentric images, and 1-3 sparse IMU sensors in varied combinations.\nMotion descriptions can also accompany these signals. The diverse input\nmodalities and their intermittent availability pose challenges for consistent\nmotion capture and understanding. In this work, we present Ego4o (o for omni),\na new framework for simultaneous human motion capture and understanding from\nmulti-modal egocentric inputs. This method maintains performance with partial\ninputs while achieving better results when multiple modalities are combined.\nFirst, the IMU sensor inputs, the optional egocentric image, and text\ndescription of human motion are encoded into the latent space of a motion\nVQ-VAE. Next, the latent vectors are sent to the VQ-VAE decoder and optimized\nto track human motion. When motion descriptions are unavailable, the latent\nvectors can be input into a multi-modal LLM to generate human motion\ndescriptions, which can further enhance motion capture accuracy. Quantitative\nand qualitative evaluations demonstrate the effectiveness of our method in\npredicting accurate human motion and high-quality motion descriptions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work focuses on tracking and understanding human motion using consumer\nwearable devices, such as VR/AR headsets, smart glasses, cellphones, and\nsmartwatches. These devices provide diverse, multi-modal sensor inputs,\nincluding egocentric images, and 1-3 sparse IMU sensors in varied combinations.\nMotion descriptions can also accompany these signals. The diverse input\nmodalities and their intermittent availability pose challenges for consistent\nmotion capture and understanding. In this work, we present Ego4o (o for omni),\na new framework for simultaneous human motion capture and understanding from\nmulti-modal egocentric inputs. This method maintains performance with partial\ninputs while achieving better results when multiple modalities are combined.\nFirst, the IMU sensor inputs, the optional egocentric image, and text\ndescription of human motion are encoded into the latent space of a motion\nVQ-VAE. Next, the latent vectors are sent to the VQ-VAE decoder and optimized\nto track human motion. When motion descriptions are unavailable, the latent\nvectors can be input into a multi-modal LLM to generate human motion\ndescriptions, which can further enhance motion capture accuracy. Quantitative\nand qualitative evaluations demonstrate the effectiveness of our method in\npredicting accurate human motion and high-quality motion descriptions."
                },
                "authors": [
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Rishabh Dabral"
                    },
                    {
                        "name": "Diogo Luvizon"
                    },
                    {
                        "name": "Zhe Cao"
                    },
                    {
                        "name": "Lingjie Liu"
                    },
                    {
                        "name": "Thabo Beeler"
                    },
                    {
                        "name": "Christian Theobalt"
                    }
                ],
                "author_detail": {
                    "name": "Christian Theobalt"
                },
                "author": "Christian Theobalt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07199v2",
                "updated": "2025-04-11T10:14:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    14,
                    39,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-09T18:26:46Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    18,
                    26,
                    46,
                    2,
                    99,
                    0
                ],
                "title": "SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject\n  Tagging for a National Technical Library's Open-Access Catalog",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SemEval-2025 Task 5: LLMs4Subjects -- LLM-based Automated Subject\n  Tagging for a National Technical Library's Open-Access Catalog"
                },
                "summary": "We present SemEval-2025 Task 5: LLMs4Subjects, a shared task on automated\nsubject tagging for scientific and technical records in English and German\nusing the GND taxonomy. Participants developed LLM-based systems to recommend\ntop-k subjects, evaluated through quantitative metrics (precision, recall,\nF1-score) and qualitative assessments by subject specialists. Results highlight\nthe effectiveness of LLM ensembles, synthetic data generation, and multilingual\nprocessing, offering insights into applying LLMs for digital library\nclassification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present SemEval-2025 Task 5: LLMs4Subjects, a shared task on automated\nsubject tagging for scientific and technical records in English and German\nusing the GND taxonomy. Participants developed LLM-based systems to recommend\ntop-k subjects, evaluated through quantitative metrics (precision, recall,\nF1-score) and qualitative assessments by subject specialists. Results highlight\nthe effectiveness of LLM ensembles, synthetic data generation, and multilingual\nprocessing, offering insights into applying LLMs for digital library\nclassification."
                },
                "authors": [
                    {
                        "name": "Jennifer D'Souza"
                    },
                    {
                        "name": "Sameer Sadruddin"
                    },
                    {
                        "name": "Holger Israel"
                    },
                    {
                        "name": "Mathias Begoin"
                    },
                    {
                        "name": "Diana Slawig"
                    }
                ],
                "author_detail": {
                    "name": "Diana Slawig"
                },
                "author": "Diana Slawig",
                "arxiv_comment": "10 pages, 4 figures, Accepted as SemEval 2025 Task 5 description\n  paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08400v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08400v1",
                "updated": "2025-04-11T10:04:12Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    4,
                    12,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T10:04:12Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    4,
                    12,
                    4,
                    101,
                    0
                ],
                "title": "A Reproducibility Study of Graph-Based Legal Case Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Reproducibility Study of Graph-Based Legal Case Retrieval"
                },
                "summary": "Legal retrieval is a widely studied area in Information Retrieval (IR) and a\nkey task in this domain is retrieving relevant cases based on a given query\ncase, often done by applying language models as encoders to model case\nsimilarity. Recently, Tang et al. proposed CaseLink, a novel graph-based method\nfor legal case retrieval, which models both cases and legal charges as nodes in\na network, with edges representing relationships such as references and shared\nsemantics. This approach offers a new perspective on the task by capturing\nhigher-order relationships of cases going beyond the stand-alone level of\ndocuments. However, while this shift in approaching legal case retrieval is a\npromising direction in an understudied area of graph-based legal IR, challenges\nin reproducing novel results have recently been highlighted, with multiple\nstudies reporting difficulties in reproducing previous findings. Thus, in this\nwork we reproduce CaseLink, a graph-based legal case retrieval method, to\nsupport future research in this area of IR. In particular, we aim to assess its\nreliability and generalizability by (i) first reproducing the original study\nsetup and (ii) applying the approach to an additional dataset. We then build\nupon the original implementations by (iii) evaluating the approach's\nperformance when using a more sophisticated graph data representation and (iv)\nusing an open large language model (LLM) in the pipeline to address limitations\nthat are known to result from using closed models accessed via an API. Our\nfindings aim to improve the understanding of graph-based approaches in legal IR\nand contribute to improving reproducibility in the field. To achieve this, we\nshare all our implementations and experimental artifacts with the community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Legal retrieval is a widely studied area in Information Retrieval (IR) and a\nkey task in this domain is retrieving relevant cases based on a given query\ncase, often done by applying language models as encoders to model case\nsimilarity. Recently, Tang et al. proposed CaseLink, a novel graph-based method\nfor legal case retrieval, which models both cases and legal charges as nodes in\na network, with edges representing relationships such as references and shared\nsemantics. This approach offers a new perspective on the task by capturing\nhigher-order relationships of cases going beyond the stand-alone level of\ndocuments. However, while this shift in approaching legal case retrieval is a\npromising direction in an understudied area of graph-based legal IR, challenges\nin reproducing novel results have recently been highlighted, with multiple\nstudies reporting difficulties in reproducing previous findings. Thus, in this\nwork we reproduce CaseLink, a graph-based legal case retrieval method, to\nsupport future research in this area of IR. In particular, we aim to assess its\nreliability and generalizability by (i) first reproducing the original study\nsetup and (ii) applying the approach to an additional dataset. We then build\nupon the original implementations by (iii) evaluating the approach's\nperformance when using a more sophisticated graph data representation and (iv)\nusing an open large language model (LLM) in the pipeline to address limitations\nthat are known to result from using closed models accessed via an API. Our\nfindings aim to improve the understanding of graph-based approaches in legal IR\nand contribute to improving reproducibility in the field. To achieve this, we\nshare all our implementations and experimental artifacts with the community."
                },
                "authors": [
                    {
                        "name": "Gregor Donabauer"
                    },
                    {
                        "name": "Udo Kruschwitz"
                    }
                ],
                "author_detail": {
                    "name": "Udo Kruschwitz"
                },
                "author": "Udo Kruschwitz",
                "arxiv_comment": "Preprint accepted at SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08400v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08400v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08399v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08399v1",
                "updated": "2025-04-11T10:03:55Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    3,
                    55,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T10:03:55Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    3,
                    55,
                    4,
                    101,
                    0
                ],
                "title": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in\n  Large Language Models"
                },
                "summary": "There is a growing interest in assessing the personality traits of Large\nlanguage models (LLMs). However, traditional personality assessments based on\nself-report questionnaires may fail to capture their true behavioral nuances\ndue to inherent biases and meta-knowledge contamination. This paper introduces\na novel multi-observer framework for LLM personality assessment that draws\ninspiration from informant-report methods in psychology. Instead of relying\nsolely on self-assessments, our approach employs multiple observer agents\nconfigured with a specific relationship context (e.g., family, friend, or\nworkplace) to simulate interactive scenarios with a subject LLM. These\nobservers engage in dialogues and subsequently provide ratings across the Big\nFive personality dimensions. Our experiments reveal that LLMs possess\nsystematic biases in self-report personality ratings. Moreover, aggregating\nobserver ratings effectively reduces non-systematic biases and achieves optimal\nreliability with 5-7 observers. The findings highlight the significant impact\nof relationship context on personality perception and demonstrate that a\nmulti-observer paradigm yields a more robust and context-sensitive evaluation\nof LLM personality traits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "There is a growing interest in assessing the personality traits of Large\nlanguage models (LLMs). However, traditional personality assessments based on\nself-report questionnaires may fail to capture their true behavioral nuances\ndue to inherent biases and meta-knowledge contamination. This paper introduces\na novel multi-observer framework for LLM personality assessment that draws\ninspiration from informant-report methods in psychology. Instead of relying\nsolely on self-assessments, our approach employs multiple observer agents\nconfigured with a specific relationship context (e.g., family, friend, or\nworkplace) to simulate interactive scenarios with a subject LLM. These\nobservers engage in dialogues and subsequently provide ratings across the Big\nFive personality dimensions. Our experiments reveal that LLMs possess\nsystematic biases in self-report personality ratings. Moreover, aggregating\nobserver ratings effectively reduces non-systematic biases and achieves optimal\nreliability with 5-7 observers. The findings highlight the significant impact\nof relationship context on personality perception and demonstrate that a\nmulti-observer paradigm yields a more robust and context-sensitive evaluation\nof LLM personality traits."
                },
                "authors": [
                    {
                        "name": "Yin Jou Huang"
                    },
                    {
                        "name": "Rafik Hadfi"
                    }
                ],
                "author_detail": {
                    "name": "Rafik Hadfi"
                },
                "author": "Rafik Hadfi",
                "arxiv_comment": "13 pages, 5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08399v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08399v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.10727v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.10727v3",
                "updated": "2025-04-11T10:01:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    10,
                    1,
                    13,
                    4,
                    101,
                    0
                ],
                "published": "2024-01-19T14:44:37Z",
                "published_parsed": [
                    2024,
                    1,
                    19,
                    14,
                    44,
                    37,
                    4,
                    19,
                    0
                ],
                "title": "MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning"
                },
                "summary": "Recently, the astonishing performance of large language models (LLMs) in\nnatural language comprehension and generation tasks triggered lots of\nexploration of using them as central controllers to build agent systems.\nMultiple studies focus on bridging the LLMs to external tools to extend the\napplication scenarios. However, the current LLMs' ability to perceive tool use\nis limited to a single text query, which may result in ambiguity in\nunderstanding the users' real intentions. LLMs are expected to eliminate that\nby perceiving the information in the visual- or auditory-grounded instructions.\nTherefore, in this paper, we propose MLLM-Tool, a system incorporating\nopen-source LLMs and multi-modal encoders so that the learned LLMs can be\nconscious of multi-modal input instruction and then select the function-matched\ntool correctly. To facilitate the evaluation of the model's capability, we\ncollect a dataset featuring multi-modal input tools from HuggingFace. Another\nessential feature of our dataset is that it also contains multiple potential\nchoices for the same instruction due to the existence of identical functions\nand synonymous functions, which provides more potential solutions for the same\nquery. The experiments reveal that our MLLM-Tool is capable of recommending\nappropriate tools for multi-modal instructions. Codes and data are available at\nhttps://github.com/MLLM-Tool/MLLM-Tool.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, the astonishing performance of large language models (LLMs) in\nnatural language comprehension and generation tasks triggered lots of\nexploration of using them as central controllers to build agent systems.\nMultiple studies focus on bridging the LLMs to external tools to extend the\napplication scenarios. However, the current LLMs' ability to perceive tool use\nis limited to a single text query, which may result in ambiguity in\nunderstanding the users' real intentions. LLMs are expected to eliminate that\nby perceiving the information in the visual- or auditory-grounded instructions.\nTherefore, in this paper, we propose MLLM-Tool, a system incorporating\nopen-source LLMs and multi-modal encoders so that the learned LLMs can be\nconscious of multi-modal input instruction and then select the function-matched\ntool correctly. To facilitate the evaluation of the model's capability, we\ncollect a dataset featuring multi-modal input tools from HuggingFace. Another\nessential feature of our dataset is that it also contains multiple potential\nchoices for the same instruction due to the existence of identical functions\nand synonymous functions, which provides more potential solutions for the same\nquery. The experiments reveal that our MLLM-Tool is capable of recommending\nappropriate tools for multi-modal instructions. Codes and data are available at\nhttps://github.com/MLLM-Tool/MLLM-Tool."
                },
                "authors": [
                    {
                        "name": "Chenyu Wang"
                    },
                    {
                        "name": "Weixin Luo"
                    },
                    {
                        "name": "Sixun Dong"
                    },
                    {
                        "name": "Xiaohua Xuan"
                    },
                    {
                        "name": "Zhengxin Li"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Shenghua Gao"
                    }
                ],
                "author_detail": {
                    "name": "Shenghua Gao"
                },
                "author": "Shenghua Gao",
                "arxiv_comment": "WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.10727v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.10727v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07557v2",
                "updated": "2025-04-11T09:54:57Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    54,
                    57,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T08:38:39Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    8,
                    38,
                    39,
                    3,
                    100,
                    0
                ],
                "title": "Using LLMs for Analyzing AIS Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using LLMs for Analyzing AIS Data"
                },
                "summary": "Recent research in Large Language Models (LLMs), has had a profound impact\nacross various fields, including mobility data science. This paper explores the\nand experiment with different approaches to using LLMs for analyzing AIS data.\nWe propose a set of carefully designed queries to assess the reasoning\ncapabilities of LLMs in this kind of tasks. Further, we experiment with four\ndifferent methods: (1) using LLMs as a natural language interface to a spatial\ndatabase, (2) reasoning on raw data, (3) reasoning on compressed trajectories,\nand (4) reasoning on semantic trajectories. We investigate the strengths and\nweaknesses for the four methods, and discuss the findings. The goal is to\nprovide valuable insights for both researchers and practitioners on selecting\nthe most appropriate LLM-based method depending on their specific data analysis\nobjectives.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent research in Large Language Models (LLMs), has had a profound impact\nacross various fields, including mobility data science. This paper explores the\nand experiment with different approaches to using LLMs for analyzing AIS data.\nWe propose a set of carefully designed queries to assess the reasoning\ncapabilities of LLMs in this kind of tasks. Further, we experiment with four\ndifferent methods: (1) using LLMs as a natural language interface to a spatial\ndatabase, (2) reasoning on raw data, (3) reasoning on compressed trajectories,\nand (4) reasoning on semantic trajectories. We investigate the strengths and\nweaknesses for the four methods, and discuss the findings. The goal is to\nprovide valuable insights for both researchers and practitioners on selecting\nthe most appropriate LLM-based method depending on their specific data analysis\nobjectives."
                },
                "authors": [
                    {
                        "name": "Gaspard Merten"
                    },
                    {
                        "name": "Gilles Dejaegere"
                    },
                    {
                        "name": "Mahmoud Sakr"
                    }
                ],
                "author_detail": {
                    "name": "Mahmoud Sakr"
                },
                "author": "Mahmoud Sakr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04893v2",
                "updated": "2025-04-11T09:50:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    50,
                    50,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-07T10:01:38Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    10,
                    1,
                    38,
                    0,
                    97,
                    0
                ],
                "title": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal\n  Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCAM: A Real-World Typographic Robustness Evaluation for Multimodal\n  Foundation Models"
                },
                "summary": "Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with\nthe code for evaluations at https://github.com/Bliss-e-V/SCAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Typographic attacks exploit the interplay between text and visual content in\nmultimodal foundation models, causing misclassifications when misleading text\nis embedded within images. However, existing datasets are limited in size and\ndiversity, making it difficult to study such vulnerabilities. In this paper, we\nintroduce SCAM, the largest and most diverse dataset of real-world typographic\nattack images to date, containing 1,162 images across hundreds of object\ncategories and attack words. Through extensive benchmarking of Vision-Language\nModels (VLMs) on SCAM, we demonstrate that typographic attacks significantly\ndegrade performance, and identify that training data and model architecture\ninfluence the susceptibility to these attacks. Our findings reveal that\ntypographic attacks persist in state-of-the-art Large Vision-Language Models\n(LVLMs) due to the choice of their vision encoder, though larger Large Language\nModels (LLMs) backbones help mitigate their vulnerability. Additionally, we\ndemonstrate that synthetic attacks closely resemble real-world (handwritten)\nattacks, validating their use in research. Our work provides a comprehensive\nresource and empirical insights to facilitate future research toward robust and\ntrustworthy multimodal AI systems. We publicly release the datasets introduced\nin this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with\nthe code for evaluations at https://github.com/Bliss-e-V/SCAM."
                },
                "authors": [
                    {
                        "name": "Justus Westerhoff"
                    },
                    {
                        "name": "Erblina Purelku"
                    },
                    {
                        "name": "Jakob Hackstein"
                    },
                    {
                        "name": "Leo Pinetzki"
                    },
                    {
                        "name": "Lorenz Hufe"
                    }
                ],
                "author_detail": {
                    "name": "Lorenz Hufe"
                },
                "author": "Lorenz Hufe",
                "arxiv_comment": "Submitted to CVPR 2025 Workshop EVAL-FoMo-2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08389v1",
                "updated": "2025-04-11T09:42:46Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    42,
                    46,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:42:46Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    42,
                    46,
                    4,
                    101,
                    0
                ],
                "title": "Light-YOLOv8-Flame: A Lightweight High-Performance Flame Detection\n  Algorithm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Light-YOLOv8-Flame: A Lightweight High-Performance Flame Detection\n  Algorithm"
                },
                "summary": "Fire detection algorithms, particularly those based on computer vision,\nencounter significant challenges such as high computational costs and delayed\nresponse times, which hinder their application in real-time systems. To address\nthese limitations, this paper introduces Light-YOLOv8-Flame, a lightweight\nflame detection algorithm specifically designed for fast and efficient\nreal-time deployment. The proposed model enhances the YOLOv8 architecture\nthrough the substitution of the original C2f module with the FasterNet Block\nmodule. This new block combines Partial Convolution (PConv) and Convolution\n(Conv) layers, reducing both computational complexity and model size. A dataset\ncomprising 7,431 images, representing both flame and non-flame scenarios, was\ncollected and augmented for training purposes. Experimental findings indicate\nthat the modified YOLOv8 model achieves a 0.78% gain in mean average precision\n(mAP) and a 2.05% boost in recall, while reducing the parameter count by\n25.34%, with only a marginal decrease in precision by 0.82%. These findings\nhighlight that Light-YOLOv8-Flame offers enhanced detection performance and\nspeed, making it well-suited for real-time fire detection on\nresource-constrained devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fire detection algorithms, particularly those based on computer vision,\nencounter significant challenges such as high computational costs and delayed\nresponse times, which hinder their application in real-time systems. To address\nthese limitations, this paper introduces Light-YOLOv8-Flame, a lightweight\nflame detection algorithm specifically designed for fast and efficient\nreal-time deployment. The proposed model enhances the YOLOv8 architecture\nthrough the substitution of the original C2f module with the FasterNet Block\nmodule. This new block combines Partial Convolution (PConv) and Convolution\n(Conv) layers, reducing both computational complexity and model size. A dataset\ncomprising 7,431 images, representing both flame and non-flame scenarios, was\ncollected and augmented for training purposes. Experimental findings indicate\nthat the modified YOLOv8 model achieves a 0.78% gain in mean average precision\n(mAP) and a 2.05% boost in recall, while reducing the parameter count by\n25.34%, with only a marginal decrease in precision by 0.82%. These findings\nhighlight that Light-YOLOv8-Flame offers enhanced detection performance and\nspeed, making it well-suited for real-time fire detection on\nresource-constrained devices."
                },
                "authors": [
                    {
                        "name": "Jiawei Lan"
                    },
                    {
                        "name": "Zhibiao Wang"
                    },
                    {
                        "name": "Haoyang Yu"
                    },
                    {
                        "name": "Ye Tao"
                    },
                    {
                        "name": "Wenhua Cui"
                    }
                ],
                "author_detail": {
                    "name": "Wenhua Cui"
                },
                "author": "Wenhua Cui",
                "arxiv_comment": "12 pages, 19 figures, 6 tables. Submitted to Engineering Letters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.08576v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.08576v2",
                "updated": "2025-04-11T09:41:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    41,
                    45,
                    4,
                    101,
                    0
                ],
                "published": "2025-02-12T17:10:34Z",
                "published_parsed": [
                    2025,
                    2,
                    12,
                    17,
                    10,
                    34,
                    2,
                    43,
                    0
                ],
                "title": "Mapping the Landscape of Generative AI in Network Monitoring and\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mapping the Landscape of Generative AI in Network Monitoring and\n  Management"
                },
                "summary": "Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and\nDiffusion Models have recently gained widespread attention from both the\nresearch and the industrial communities. This survey explores their application\nin network monitoring and management, focusing on prominent use cases, as well\nas challenges and opportunities. We discuss how network traffic generation and\nclassification, network intrusion detection, networked system log analysis, and\nnetwork digital assistance can benefit from the use of GenAI models.\nAdditionally, we provide an overview of the available GenAI models, datasets\nfor large-scale training phases, and platforms for the development of such\nmodels. Finally, we discuss research directions that potentially mitigate the\nroadblocks to the adoption of GenAI for network monitoring and management. Our\ninvestigation aims to map the current landscape and pave the way for future\nresearch in leveraging GenAI for network monitoring and management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and\nDiffusion Models have recently gained widespread attention from both the\nresearch and the industrial communities. This survey explores their application\nin network monitoring and management, focusing on prominent use cases, as well\nas challenges and opportunities. We discuss how network traffic generation and\nclassification, network intrusion detection, networked system log analysis, and\nnetwork digital assistance can benefit from the use of GenAI models.\nAdditionally, we provide an overview of the available GenAI models, datasets\nfor large-scale training phases, and platforms for the development of such\nmodels. Finally, we discuss research directions that potentially mitigate the\nroadblocks to the adoption of GenAI for network monitoring and management. Our\ninvestigation aims to map the current landscape and pave the way for future\nresearch in leveraging GenAI for network monitoring and management."
                },
                "authors": [
                    {
                        "name": "Giampaolo Bovenzi"
                    },
                    {
                        "name": "Francesco Cerasuolo"
                    },
                    {
                        "name": "Domenico Ciuonzo"
                    },
                    {
                        "name": "Davide Di Monda"
                    },
                    {
                        "name": "Idio Guarino"
                    },
                    {
                        "name": "Antonio Montieri"
                    },
                    {
                        "name": "Valerio Persico"
                    },
                    {
                        "name": "Antonio Pescap"
                    }
                ],
                "author_detail": {
                    "name": "Antonio Pescap"
                },
                "author": "Antonio Pescap",
                "arxiv_doi": "10.1109/TNSM.2025.3543022",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TNSM.2025.3543022",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.08576v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.08576v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "32 pages, 9 figure, 10 tables",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2; I.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08378v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08378v1",
                "updated": "2025-04-11T09:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T09:26:47Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "title": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and\n  Flash"
                },
                "summary": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly being deployed on mobile\ndevices, but the limited DRAM capacity constrains the deployable model size.\nThis paper introduces ActiveFlow, the first LLM inference framework that can\nachieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the\nscaling up of deployable model sizes. The framework is based on the novel\nconcept of active weight DRAM-flash swapping and incorporates three novel\ntechniques: (1) Cross-layer active weights preloading. It uses the activations\nfrom the current layer to predict the active weights of several subsequent\nlayers, enabling computation and data loading to overlap, as well as\nfacilitating large I/O transfers. (2) Sparsity-aware self-distillation. It\nadjusts the active weights to align with the dense-model output distribution,\ncompensating for approximations introduced by contextual sparsity. (3) Active\nweight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation\namong the hot weight cache, preloaded active weights, and computation-involved\nweights based on available memory. Results show ActiveFlow achieves the\nperformance-cost Pareto frontier compared to existing efficiency optimization\nmethods."
                },
                "authors": [
                    {
                        "name": "Fucheng Jia"
                    },
                    {
                        "name": "Zewen Wu"
                    },
                    {
                        "name": "Shiqi Jiang"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Yunxin Liu"
                    },
                    {
                        "name": "Ju Ren"
                    },
                    {
                        "name": "Deyu Zhang"
                    },
                    {
                        "name": "Ting Cao"
                    }
                ],
                "author_detail": {
                    "name": "Ting Cao"
                },
                "author": "Ting Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08378v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08378v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17365v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17365v2",
                "updated": "2025-04-11T09:18:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    9,
                    18,
                    53,
                    4,
                    101,
                    0
                ],
                "published": "2025-02-01T06:58:27Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    6,
                    58,
                    27,
                    5,
                    32,
                    0
                ],
                "title": "How Effective Is Constitutional AI in Small LLMs? A Study on DeepSeek-R1\n  and Its Peers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Effective Is Constitutional AI in Small LLMs? A Study on DeepSeek-R1\n  and Its Peers"
                },
                "summary": "Recent incidents highlight safety risks in Large Language Models (LLMs),\nmotivating research into alignment methods like Constitutional AI (CAI). This\npaper explores CAI's self-critique mechanism on small, uncensored 7-9B\nparameter models: DeepSeek-R1-8B, Gemma-2-9B, Llama 3.1-8B, and Qwen2.5-7B. We\nshow that while Llama-based models exhibited significant harm reduction through\nself-critique, other architectures demonstrated less improvement in harm\ndetection after abliteration. These results suggest CAI's effectiveness may\nvary depending on model architecture and reasoning capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent incidents highlight safety risks in Large Language Models (LLMs),\nmotivating research into alignment methods like Constitutional AI (CAI). This\npaper explores CAI's self-critique mechanism on small, uncensored 7-9B\nparameter models: DeepSeek-R1-8B, Gemma-2-9B, Llama 3.1-8B, and Qwen2.5-7B. We\nshow that while Llama-based models exhibited significant harm reduction through\nself-critique, other architectures demonstrated less improvement in harm\ndetection after abliteration. These results suggest CAI's effectiveness may\nvary depending on model architecture and reasoning capabilities."
                },
                "authors": [
                    {
                        "name": "Antonio-Gabriel Chacn Menke"
                    },
                    {
                        "name": "Phan Xuan Tan"
                    }
                ],
                "author_detail": {
                    "name": "Phan Xuan Tan"
                },
                "arxiv_affiliation": "Shibaura Institute of Technology",
                "author": "Phan Xuan Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17365v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17365v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07583v2",
                "updated": "2025-04-11T08:22:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    8,
                    22,
                    54,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:24:54Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    24,
                    54,
                    3,
                    100,
                    0
                ],
                "title": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with\n  Question Answering"
                },
                "summary": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the steady progress in machine translation evaluation, existing\nautomatic metrics struggle to capture how well meaning is preserved beyond\nsentence boundaries. We posit that reliance on a single intrinsic quality\nscore, trained to mimic human judgments, might be insufficient for evaluating\ntranslations of long, complex passages, and a more ``pragmatic'' approach that\nassesses how accurately key information is conveyed by a translation in context\nis needed. We introduce TREQA (Translation Evaluation via Question-Answering),\na framework that extrinsically evaluates translation quality by assessing how\naccurately candidate translations answer reading comprehension questions that\ntarget key information in the original source or reference texts. In\nchallenging domains that require long-range understanding, such as literary\ntexts, we show that TREQA is competitive with and, in some cases, outperforms\nstate-of-the-art neural and LLM-based metrics in ranking alternative\nparagraph-level translations, despite never being explicitly optimized to\ncorrelate with human judgments. Furthermore, the generated questions and\nanswers offer interpretability: empirical analysis shows that they effectively\ntarget translation errors identified by experts in evaluated datasets. Our code\nis available at https://github.com/deep-spin/treqa"
                },
                "authors": [
                    {
                        "name": "Patrick Fernandes"
                    },
                    {
                        "name": "Sweta Agrawal"
                    },
                    {
                        "name": "Emmanouil Zaranis"
                    },
                    {
                        "name": "Andr F. T. Martins"
                    },
                    {
                        "name": "Graham Neubig"
                    }
                ],
                "author_detail": {
                    "name": "Graham Neubig"
                },
                "author": "Graham Neubig",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02623v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02623v2",
                "updated": "2025-04-11T08:14:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    8,
                    14,
                    19,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-03T14:21:33Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    14,
                    21,
                    33,
                    3,
                    93,
                    0
                ],
                "title": "Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents\n  through Related and Dynamic Missions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Mission Tool Bench: Assessing the Robustness of LLM based Agents\n  through Related and Dynamic Missions"
                },
                "summary": "Large language models (LLMs) demonstrate strong potential as agents for tool\ninvocation due to their advanced comprehension and planning capabilities. Users\nincreasingly rely on LLM-based agents to solve complex missions through\niterative interactions. However, existing benchmarks predominantly access\nagents in single-mission scenarios, failing to capture real-world complexity.\nTo bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark,\neach test case comprises multiple interrelated missions. This design requires\nagents to dynamically adapt to evolving demands. Moreover, the proposed\nbenchmark explores all possible mission-switching patterns within a fixed\nmission number. Specifically, we propose a multi-agent data generation\nframework to construct the benchmark. We also propose a novel method to\nevaluate the accuracy and efficiency of agent decisions with dynamic decision\ntrees. Experiments on diverse open-source and closed-source LLMs reveal\ncritical factors influencing agent robustness and provide actionable insights\nto the tool invocation society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate strong potential as agents for tool\ninvocation due to their advanced comprehension and planning capabilities. Users\nincreasingly rely on LLM-based agents to solve complex missions through\niterative interactions. However, existing benchmarks predominantly access\nagents in single-mission scenarios, failing to capture real-world complexity.\nTo bridge this gap, we propose the Multi-Mission Tool Bench. In the benchmark,\neach test case comprises multiple interrelated missions. This design requires\nagents to dynamically adapt to evolving demands. Moreover, the proposed\nbenchmark explores all possible mission-switching patterns within a fixed\nmission number. Specifically, we propose a multi-agent data generation\nframework to construct the benchmark. We also propose a novel method to\nevaluate the accuracy and efficiency of agent decisions with dynamic decision\ntrees. Experiments on diverse open-source and closed-source LLMs reveal\ncritical factors influencing agent robustness and provide actionable insights\nto the tool invocation society."
                },
                "authors": [
                    {
                        "name": "Peijie Yu"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Jinjian Li"
                    },
                    {
                        "name": "Zelong Zhang"
                    },
                    {
                        "name": "Haorui Wang"
                    },
                    {
                        "name": "Xiao Feng"
                    },
                    {
                        "name": "Feng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Feng Zhang"
                },
                "author": "Feng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02623v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02623v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08337v1",
                "updated": "2025-04-11T08:06:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    8,
                    6,
                    59,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T08:06:59Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    8,
                    6,
                    59,
                    4,
                    101,
                    0
                ],
                "title": "Trabant: A Serverless Architecture for Multi-Tenant Orbital Edge\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trabant: A Serverless Architecture for Multi-Tenant Orbital Edge\n  Computing"
                },
                "summary": "Orbital edge computing reduces the data transmission needs of Earth\nobservation satellites by processing sensor data on-board, allowing\nnear-real-time insights while minimizing downlink costs. However, current\norbital edge computing architectures are inflexible, requiring custom mission\nplanning and high upfront development costs. In this paper, we propose a novel\napproach: shared Earth observation satellites that are operated by a central\nprovider but used by multiple tenants. Each tenant can execute their own logic\non-board the satellite to filter, prioritize, and analyze sensor data.\n  We introduce Trabant, a serverless architecture for shared satellite\nplatforms, leveraging the Function-as-a-Service (FaaS) paradigm and\ntime-shifted computing. This architecture abstracts operational complexities,\nenabling dynamic scheduling under satellite resource constraints, reducing\ndeployment overhead, and aligning event-driven satellite observations with\nintermittent computation. We present the design of Trabant, demonstrate its\ncapabilities with a proof-of-concept prototype, and evaluate it using real\nsatellite computing telemetry data. Our findings suggest that Trabant can\nsignificantly reduce mission planning overheads, offering a scalable and\nefficient platform for diverse Earth observation missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orbital edge computing reduces the data transmission needs of Earth\nobservation satellites by processing sensor data on-board, allowing\nnear-real-time insights while minimizing downlink costs. However, current\norbital edge computing architectures are inflexible, requiring custom mission\nplanning and high upfront development costs. In this paper, we propose a novel\napproach: shared Earth observation satellites that are operated by a central\nprovider but used by multiple tenants. Each tenant can execute their own logic\non-board the satellite to filter, prioritize, and analyze sensor data.\n  We introduce Trabant, a serverless architecture for shared satellite\nplatforms, leveraging the Function-as-a-Service (FaaS) paradigm and\ntime-shifted computing. This architecture abstracts operational complexities,\nenabling dynamic scheduling under satellite resource constraints, reducing\ndeployment overhead, and aligning event-driven satellite observations with\nintermittent computation. We present the design of Trabant, demonstrate its\ncapabilities with a proof-of-concept prototype, and evaluate it using real\nsatellite computing telemetry data. Our findings suggest that Trabant can\nsignificantly reduce mission planning overheads, offering a scalable and\nefficient platform for diverse Earth observation missions."
                },
                "authors": [
                    {
                        "name": "Tobias Pfandzelter"
                    },
                    {
                        "name": "Nikita Bauer"
                    },
                    {
                        "name": "Alexander Leis"
                    },
                    {
                        "name": "Corentin Perdrizet"
                    },
                    {
                        "name": "Felix Trautwein"
                    },
                    {
                        "name": "Trever Schirmer"
                    },
                    {
                        "name": "Osama Abboud"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08329v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08329v1",
                "updated": "2025-04-11T07:51:58Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    51,
                    58,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T07:51:58Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    51,
                    58,
                    4,
                    101,
                    0
                ],
                "title": "MedRep: Medical Concept Representation for General Electronic Health\n  Record Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedRep: Medical Concept Representation for General Electronic Health\n  Record Foundation Models"
                },
                "summary": "Electronic health record (EHR) foundation models have been an area ripe for\nexploration with their improved performance in various medical tasks. Despite\nthe rapid advances, there exists a fundamental limitation: Processing unseen\nmedical codes out of the vocabulary. This problem limits the generality of EHR\nfoundation models and the integration of models trained with different\nvocabularies. To deal with this problem, we propose MedRep for EHR foundation\nmodels based on the observational medical outcome partnership (OMOP) common\ndata model (CDM), providing the integrated medical concept representations and\nthe basic data augmentation strategy for patient trajectories. For concept\nrepresentation learning, we enrich the information of each concept with a\nminimal definition through large language model (LLM) prompts and enhance the\ntext-based representations through graph ontology of OMOP vocabulary.\nTrajectory augmentation randomly replaces selected concepts with other similar\nconcepts that have closely related representations to let the model practice\nwith the concepts out-of-vocabulary. Finally, we demonstrate that EHR\nfoundation models trained with MedRep better maintain the prediction\nperformance in external datasets. Our code implementation is publicly available\nat https://github.com/kicarussays/MedRep.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Electronic health record (EHR) foundation models have been an area ripe for\nexploration with their improved performance in various medical tasks. Despite\nthe rapid advances, there exists a fundamental limitation: Processing unseen\nmedical codes out of the vocabulary. This problem limits the generality of EHR\nfoundation models and the integration of models trained with different\nvocabularies. To deal with this problem, we propose MedRep for EHR foundation\nmodels based on the observational medical outcome partnership (OMOP) common\ndata model (CDM), providing the integrated medical concept representations and\nthe basic data augmentation strategy for patient trajectories. For concept\nrepresentation learning, we enrich the information of each concept with a\nminimal definition through large language model (LLM) prompts and enhance the\ntext-based representations through graph ontology of OMOP vocabulary.\nTrajectory augmentation randomly replaces selected concepts with other similar\nconcepts that have closely related representations to let the model practice\nwith the concepts out-of-vocabulary. Finally, we demonstrate that EHR\nfoundation models trained with MedRep better maintain the prediction\nperformance in external datasets. Our code implementation is publicly available\nat https://github.com/kicarussays/MedRep."
                },
                "authors": [
                    {
                        "name": "Junmo Kim"
                    },
                    {
                        "name": "Namkyeong Lee"
                    },
                    {
                        "name": "Jiwon Kim"
                    },
                    {
                        "name": "Kwangsoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Kwangsoo Kim"
                },
                "author": "Kwangsoo Kim",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08329v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08329v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07866v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07866v2",
                "updated": "2025-04-11T07:47:04Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    47,
                    4,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T15:41:51Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    41,
                    51,
                    3,
                    100,
                    0
                ],
                "title": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pangu Ultra: Pushing the Limits of Dense Large Language Models on Ascend\n  NPUs"
                },
                "summary": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\nparameters and dense Transformer modules trained on Ascend Neural Processing\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\nadvances in pushing the scale and capability of LLM in recent years, training\nsuch a large-scale model still involves significant optimization and system\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\nnormalization, which effectively eliminates loss spikes during the training\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\nhigh-quality tokens and further enhance its reasoning capabilities during\npost-training. To perform such large-scale training efficiently, we utilize\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\nmodel structure contains much more parameters. Our exploration demonstrates\nthat Ascend NPUs are capable of efficiently and effectively training dense\nmodels with more than 100 billion parameters. Our model and system will be\navailable for our commercial customers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present Pangu Ultra, a Large Language Model (LLM) with 135 billion\nparameters and dense Transformer modules trained on Ascend Neural Processing\nUnits (NPUs). Although the field of LLM has been witnessing unprecedented\nadvances in pushing the scale and capability of LLM in recent years, training\nsuch a large-scale model still involves significant optimization and system\nchallenges. To stabilize the training process, we propose depth-scaled sandwich\nnormalization, which effectively eliminates loss spikes during the training\nprocess of deep models. We pre-train our model on 13.2 trillion diverse and\nhigh-quality tokens and further enhance its reasoning capabilities during\npost-training. To perform such large-scale training efficiently, we utilize\n8,192 Ascend NPUs with a series of system optimizations. Evaluations on\nmultiple diverse benchmarks indicate that Pangu Ultra significantly advances\nthe state-of-the-art capabilities of dense LLMs such as Llama 405B and Mistral\nLarge 2, and even achieves competitive results with DeepSeek-R1, whose sparse\nmodel structure contains much more parameters. Our exploration demonstrates\nthat Ascend NPUs are capable of efficiently and effectively training dense\nmodels with more than 100 billion parameters. Our model and system will be\navailable for our commercial customers."
                },
                "authors": [
                    {
                        "name": "Yichun Yin"
                    },
                    {
                        "name": "Wenyong Huang"
                    },
                    {
                        "name": "Kaikai Song"
                    },
                    {
                        "name": "Yehui Tang"
                    },
                    {
                        "name": "Xueyu Wu"
                    },
                    {
                        "name": "Wei Guo"
                    },
                    {
                        "name": "Peng Guo"
                    },
                    {
                        "name": "Yaoyuan Wang"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Yasheng Wang"
                    },
                    {
                        "name": "Dong Li"
                    },
                    {
                        "name": "Can Chen"
                    },
                    {
                        "name": "Dandan Tu"
                    },
                    {
                        "name": "Yin Li"
                    },
                    {
                        "name": "Fisher Yu"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Yunhe Wang"
                    },
                    {
                        "name": "Baojun Wang"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Boxiao Liu"
                    },
                    {
                        "name": "Changzheng Zhang"
                    },
                    {
                        "name": "Duyu Tang"
                    },
                    {
                        "name": "Fei Mi"
                    },
                    {
                        "name": "Hui Jin"
                    },
                    {
                        "name": "Jiansheng Wei"
                    },
                    {
                        "name": "Jiarui Qin"
                    },
                    {
                        "name": "Jinpeng Li"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Liqun Deng"
                    },
                    {
                        "name": "Lin Li"
                    },
                    {
                        "name": "Minghui Xu"
                    },
                    {
                        "name": "Naifu Zhang"
                    },
                    {
                        "name": "Nianzu Zheng"
                    },
                    {
                        "name": "Qiang Li"
                    },
                    {
                        "name": "Rongju Ruan"
                    },
                    {
                        "name": "Shengjun Cheng"
                    },
                    {
                        "name": "Tianyu Guo"
                    },
                    {
                        "name": "Wei He"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Wulong Liu"
                    },
                    {
                        "name": "Xinyi Dai"
                    },
                    {
                        "name": "Yonghan Dong"
                    },
                    {
                        "name": "Yu Pan"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Yujun Li"
                    },
                    {
                        "name": "Yunsheng Ni"
                    },
                    {
                        "name": "Zhe Liu"
                    },
                    {
                        "name": "Zhenhe Zhang"
                    },
                    {
                        "name": "Zhicheng Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Liu"
                },
                "author": "Zhicheng Liu",
                "arxiv_comment": "fix conflicts of latex pacakges",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07866v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07866v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08312v1",
                "updated": "2025-04-11T07:29:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    29,
                    56,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T07:29:56Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    29,
                    56,
                    4,
                    101,
                    0
                ],
                "title": "SortBench: Benchmarking LLMs based on their ability to sort lists",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SortBench: Benchmarking LLMs based on their ability to sort lists"
                },
                "summary": "Sorting is a tedious but simple task for human intelligence and can be solved\nfairly easily algorithmically. However, for Large Language Models (LLMs) this\ntask is surprisingly hard, as some properties of sorting are among known\nweaknesses of LLMs: being faithful to the input data, logical comparisons\nbetween values, and strictly differentiating between syntax (used for sorting)\nand semantics (typically learned by embeddings). Within this paper, we describe\nthe new SortBench benchmark for LLMs that comes with different difficulties and\nthat can be easily scaled in terms of difficulty. We apply this benchmark to\nseven state-of-the-art LLMs, including current test-time reasoning models. Our\nresults show that while the o3-mini model is very capable at sorting in\ngeneral, even this can be fooled if strings are defined to mix syntactical and\nsemantical aspects, e.g., by asking to sort numbers written-out as word.\nFurthermore, all models have problems with the faithfulness to the input of\nlong lists, i.e., they drop items and add new ones. Our results also show that\ntest-time reasoning has a tendency to overthink problems which leads to\nperformance degradation. Finally, models without test-time reasoning like\nGPT-4o are not much worse than reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sorting is a tedious but simple task for human intelligence and can be solved\nfairly easily algorithmically. However, for Large Language Models (LLMs) this\ntask is surprisingly hard, as some properties of sorting are among known\nweaknesses of LLMs: being faithful to the input data, logical comparisons\nbetween values, and strictly differentiating between syntax (used for sorting)\nand semantics (typically learned by embeddings). Within this paper, we describe\nthe new SortBench benchmark for LLMs that comes with different difficulties and\nthat can be easily scaled in terms of difficulty. We apply this benchmark to\nseven state-of-the-art LLMs, including current test-time reasoning models. Our\nresults show that while the o3-mini model is very capable at sorting in\ngeneral, even this can be fooled if strings are defined to mix syntactical and\nsemantical aspects, e.g., by asking to sort numbers written-out as word.\nFurthermore, all models have problems with the faithfulness to the input of\nlong lists, i.e., they drop items and add new ones. Our results also show that\ntest-time reasoning has a tendency to overthink problems which leads to\nperformance degradation. Finally, models without test-time reasoning like\nGPT-4o are not much worse than reasoning models."
                },
                "authors": [
                    {
                        "name": "Steffen Herbold"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Herbold"
                },
                "author": "Steffen Herbold",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.14985v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.14985v2",
                "updated": "2025-04-11T07:26:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    26,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2023-05-24T10:19:57Z",
                "published_parsed": [
                    2023,
                    5,
                    24,
                    10,
                    19,
                    57,
                    2,
                    144,
                    0
                ],
                "title": "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IdealGPT: Iteratively Decomposing Vision and Language Reasoning via\n  Large Language Models"
                },
                "summary": "The field of vision-and-language (VL) understanding has made unprecedented\nprogress with end-to-end large pre-trained VL models (VLMs). However, they\nstill fall short in zero-shot reasoning tasks that require multi-step\ninferencing. To achieve this goal, previous works resort to a\ndivide-and-conquer pipeline. In this paper, we argue that previous efforts have\nseveral inherent shortcomings: 1) They rely on domain-specific sub-question\ndecomposing models. 2) They force models to predict the final answer even if\nthe sub-questions or sub-answers provide insufficient information. We address\nthese limitations via IdealGPT, a framework that iteratively decomposes VL\nreasoning using large language models (LLMs). Specifically, IdealGPT utilizes\nan LLM to generate sub-questions, a VLM to provide corresponding sub-answers,\nand another LLM to reason to achieve the final answer. These three modules\nperform the divide-and-conquer procedure iteratively until the model is\nconfident about the final answer to the main question. We evaluate IdealGPT on\nmultiple challenging VL reasoning tasks under a zero-shot setting. In\nparticular, our IdealGPT outperforms the best existing GPT-4-like models by an\nabsolute 10% on VCR and 15% on SNLI-VE. Code is available at\nhttps://github.com/Hxyou/IdealGPT",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The field of vision-and-language (VL) understanding has made unprecedented\nprogress with end-to-end large pre-trained VL models (VLMs). However, they\nstill fall short in zero-shot reasoning tasks that require multi-step\ninferencing. To achieve this goal, previous works resort to a\ndivide-and-conquer pipeline. In this paper, we argue that previous efforts have\nseveral inherent shortcomings: 1) They rely on domain-specific sub-question\ndecomposing models. 2) They force models to predict the final answer even if\nthe sub-questions or sub-answers provide insufficient information. We address\nthese limitations via IdealGPT, a framework that iteratively decomposes VL\nreasoning using large language models (LLMs). Specifically, IdealGPT utilizes\nan LLM to generate sub-questions, a VLM to provide corresponding sub-answers,\nand another LLM to reason to achieve the final answer. These three modules\nperform the divide-and-conquer procedure iteratively until the model is\nconfident about the final answer to the main question. We evaluate IdealGPT on\nmultiple challenging VL reasoning tasks under a zero-shot setting. In\nparticular, our IdealGPT outperforms the best existing GPT-4-like models by an\nabsolute 10% on VCR and 15% on SNLI-VE. Code is available at\nhttps://github.com/Hxyou/IdealGPT"
                },
                "authors": [
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Zhecan Wang"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Long Chen"
                    },
                    {
                        "name": "Gengyu Wang"
                    },
                    {
                        "name": "Hammad A. Ayyubi"
                    },
                    {
                        "name": "Kai-Wei Chang"
                    },
                    {
                        "name": "Shih-Fu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shih-Fu Chang"
                },
                "author": "Shih-Fu Chang",
                "arxiv_comment": "13 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.14985v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.14985v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07881v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07881v2",
                "updated": "2025-04-11T07:26:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    26,
                    43,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T15:55:34Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    15,
                    55,
                    34,
                    3,
                    100,
                    0
                ],
                "title": "An LLM-Driven Multi-Agent Debate System for Mendelian Diseases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An LLM-Driven Multi-Agent Debate System for Mendelian Diseases"
                },
                "summary": "Accurate diagnosis of Mendelian diseases is crucial for precision therapy and\nassistance in preimplantation genetic diagnosis. However, existing methods\noften fall short of clinical standards or depend on extensive datasets to build\npretrained machine learning models. To address this, we introduce an innovative\nLLM-Driven multi-agent debate system (MD2GPS) with natural language\nexplanations of the diagnostic results. It utilizes a language model to\ntransform results from data-driven and knowledge-driven agents into natural\nlanguage, then fostering a debate between these two specialized agents. This\nsystem has been tested on 1,185 samples across four independent datasets,\nenhancing the TOP1 accuracy from 42.9% to 66% on average. Additionally, in a\nchallenging cohort of 72 cases, MD2GPS identified potential pathogenic genes in\n12 patients, reducing the diagnostic time by 90%. The methods within each\nmodule of this multi-agent debate system are also replaceable, facilitating its\nadaptation for diagnosing and researching other complex diseases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate diagnosis of Mendelian diseases is crucial for precision therapy and\nassistance in preimplantation genetic diagnosis. However, existing methods\noften fall short of clinical standards or depend on extensive datasets to build\npretrained machine learning models. To address this, we introduce an innovative\nLLM-Driven multi-agent debate system (MD2GPS) with natural language\nexplanations of the diagnostic results. It utilizes a language model to\ntransform results from data-driven and knowledge-driven agents into natural\nlanguage, then fostering a debate between these two specialized agents. This\nsystem has been tested on 1,185 samples across four independent datasets,\nenhancing the TOP1 accuracy from 42.9% to 66% on average. Additionally, in a\nchallenging cohort of 72 cases, MD2GPS identified potential pathogenic genes in\n12 patients, reducing the diagnostic time by 90%. The methods within each\nmodule of this multi-agent debate system are also replaceable, facilitating its\nadaptation for diagnosing and researching other complex diseases."
                },
                "authors": [
                    {
                        "name": "Xinyang Zhou"
                    },
                    {
                        "name": "Yongyong Ren"
                    },
                    {
                        "name": "Qianqian Zhao"
                    },
                    {
                        "name": "Daoyi Huang"
                    },
                    {
                        "name": "Xinbo Wang"
                    },
                    {
                        "name": "Tingting Zhao"
                    },
                    {
                        "name": "Zhixing Zhu"
                    },
                    {
                        "name": "Wenyuan He"
                    },
                    {
                        "name": "Shuyuan Li"
                    },
                    {
                        "name": "Yan Xu"
                    },
                    {
                        "name": "Yu Sun"
                    },
                    {
                        "name": "Yongguo Yu"
                    },
                    {
                        "name": "Shengnan Wu"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Guangjun Yu"
                    },
                    {
                        "name": "Dake He"
                    },
                    {
                        "name": "Bo Ban"
                    },
                    {
                        "name": "Hui Lu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Lu"
                },
                "author": "Hui Lu",
                "arxiv_comment": "21 pages, 5 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07881v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07881v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02228v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02228v3",
                "updated": "2025-04-11T07:20:47Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    20,
                    47,
                    4,
                    101,
                    0
                ],
                "published": "2024-05-03T16:38:51Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    16,
                    38,
                    51,
                    4,
                    124,
                    0
                ],
                "title": "Attribution in Scientific Literature: New Benchmark and Methods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attribution in Scientific Literature: New Benchmark and Methods"
                },
                "summary": "Large language models (LLMs) present a promising yet challenging frontier for\nautomated source citation in scientific communication. Previous approaches to\ncitation generation have been limited by citation ambiguity and LLM\novergeneralization. We introduce REASONS, a novel dataset with sentence-level\nannotations across 12 scientific domains from arXiv. Our evaluation framework\ncovers two key citation scenarios: indirect queries (matching sentences to\npaper titles) and direct queries (author attribution), both enhanced with\ncontextual metadata. We conduct extensive experiments with models such as\nGPT-O1, GPT-4O, GPT-3.5, DeepSeek, and other smaller models like Perplexity AI\n(7B). While top-tier LLMs achieve high performance in sentence attribution,\nthey struggle with high hallucination rates, a key metric for scientific\nreliability. Our metadata-augmented approach reduces hallucination rates across\nall tasks, offering a promising direction for improvement. Retrieval-augmented\ngeneration (RAG) with Mistral improves performance in indirect queries,\nreducing hallucination rates by 42% and maintaining competitive precision with\nlarger models. However, adversarial testing highlights challenges in linking\npaper titles to abstracts, revealing fundamental limitations in current LLMs.\nREASONS provides a challenging benchmark for developing reliable and\ntrustworthy LLMs in scientific applications",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) present a promising yet challenging frontier for\nautomated source citation in scientific communication. Previous approaches to\ncitation generation have been limited by citation ambiguity and LLM\novergeneralization. We introduce REASONS, a novel dataset with sentence-level\nannotations across 12 scientific domains from arXiv. Our evaluation framework\ncovers two key citation scenarios: indirect queries (matching sentences to\npaper titles) and direct queries (author attribution), both enhanced with\ncontextual metadata. We conduct extensive experiments with models such as\nGPT-O1, GPT-4O, GPT-3.5, DeepSeek, and other smaller models like Perplexity AI\n(7B). While top-tier LLMs achieve high performance in sentence attribution,\nthey struggle with high hallucination rates, a key metric for scientific\nreliability. Our metadata-augmented approach reduces hallucination rates across\nall tasks, offering a promising direction for improvement. Retrieval-augmented\ngeneration (RAG) with Mistral improves performance in indirect queries,\nreducing hallucination rates by 42% and maintaining competitive precision with\nlarger models. However, adversarial testing highlights challenges in linking\npaper titles to abstracts, revealing fundamental limitations in current LLMs.\nREASONS provides a challenging benchmark for developing reliable and\ntrustworthy LLMs in scientific applications"
                },
                "authors": [
                    {
                        "name": "Yash Saxena"
                    },
                    {
                        "name": "Deepa Tilwani"
                    },
                    {
                        "name": "Ali Mohammadi"
                    },
                    {
                        "name": "Edward Raff"
                    },
                    {
                        "name": "Amit Sheth"
                    },
                    {
                        "name": "Srinivasan Parthasarathy"
                    },
                    {
                        "name": "Manas Gaur"
                    }
                ],
                "author_detail": {
                    "name": "Manas Gaur"
                },
                "author": "Manas Gaur",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02228v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02228v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08300v1",
                "updated": "2025-04-11T07:04:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    4,
                    44,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T07:04:44Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    7,
                    4,
                    44,
                    4,
                    101,
                    0
                ],
                "title": "Large language models could be rote learners",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models could be rote learners"
                },
                "summary": "Multiple-choice question (MCQ) benchmarks are widely used for evaluating\nLarge Language Models (LLMs), yet their reliability is undermined by benchmark\ncontamination. In this study, we reframe contamination as an inherent aspect of\nlearning and seek to disentangle genuine capability acquisition from\nsuperficial memorization in LLM evaluation. First, by analyzing model\nperformance under different memorization conditions, we uncover a\ncounterintuitive trend: LLMs perform worse on memorized MCQs than on\nnon-memorized ones, indicating the coexistence of two distinct learning\nphenomena, i.e., rote memorization and genuine capability learning. To\ndisentangle them, we propose TrinEval, a novel evaluation framework that\nreformulates MCQs into an alternative trinity format, reducing memorization\nwhile preserving knowledge assessment. Experiments validate TrinEval's\neffectiveness in reformulation, and its evaluation reveals that common LLMs may\nmemorize by rote 20.5% of knowledge points (in MMLU on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiple-choice question (MCQ) benchmarks are widely used for evaluating\nLarge Language Models (LLMs), yet their reliability is undermined by benchmark\ncontamination. In this study, we reframe contamination as an inherent aspect of\nlearning and seek to disentangle genuine capability acquisition from\nsuperficial memorization in LLM evaluation. First, by analyzing model\nperformance under different memorization conditions, we uncover a\ncounterintuitive trend: LLMs perform worse on memorized MCQs than on\nnon-memorized ones, indicating the coexistence of two distinct learning\nphenomena, i.e., rote memorization and genuine capability learning. To\ndisentangle them, we propose TrinEval, a novel evaluation framework that\nreformulates MCQs into an alternative trinity format, reducing memorization\nwhile preserving knowledge assessment. Experiments validate TrinEval's\neffectiveness in reformulation, and its evaluation reveals that common LLMs may\nmemorize by rote 20.5% of knowledge points (in MMLU on average)."
                },
                "authors": [
                    {
                        "name": "Yuyang Xu"
                    },
                    {
                        "name": "Renjun Hu"
                    },
                    {
                        "name": "Haochao Ying"
                    },
                    {
                        "name": "Jian Wu"
                    },
                    {
                        "name": "Xing Shi"
                    },
                    {
                        "name": "Wei Lin"
                    }
                ],
                "author_detail": {
                    "name": "Wei Lin"
                },
                "author": "Wei Lin",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07951v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07951v2",
                "updated": "2025-04-11T06:35:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    6,
                    35,
                    42,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T17:57:28Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    17,
                    57,
                    28,
                    3,
                    100,
                    0
                ],
                "title": "Scaling Laws for Native Multimodal Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws for Native Multimodal Models"
                },
                "summary": "Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building general-purpose models that can effectively perceive the world\nthrough multimodal signals has been a long-standing goal. Current approaches\ninvolve integrating separately pre-trained components, such as connecting\nvision encoders to LLMs and continuing multimodal training. While such\napproaches exhibit remarkable sample efficiency, it remains an open question\nwhether such late-fusion architectures are inherently superior. In this work,\nwe revisit the architectural design of native multimodal models (NMMs)--those\ntrained from the ground up on all modalities--and conduct an extensive scaling\nlaws study, spanning 457 trained models with different architectures and\ntraining mixtures. Our investigation reveals no inherent advantage to\nlate-fusion architectures over early-fusion ones, which do not rely on image\nencoders. On the contrary, early-fusion exhibits stronger performance at lower\nparameter counts, is more efficient to train, and is easier to deploy.\nMotivated by the strong performance of the early-fusion architectures, we show\nthat incorporating Mixture of Experts (MoEs) allows for models that learn\nmodality-specific weights, significantly enhancing performance."
                },
                "authors": [
                    {
                        "name": "Mustafa Shukor"
                    },
                    {
                        "name": "Enrico Fini"
                    },
                    {
                        "name": "Victor Guilherme Turrisi da Costa"
                    },
                    {
                        "name": "Matthieu Cord"
                    },
                    {
                        "name": "Joshua Susskind"
                    },
                    {
                        "name": "Alaaeldin El-Nouby"
                    }
                ],
                "author_detail": {
                    "name": "Alaaeldin El-Nouby"
                },
                "author": "Alaaeldin El-Nouby",
                "arxiv_comment": "31 pages, 26 figures, 13 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07951v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07951v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08281v1",
                "updated": "2025-04-11T06:30:16Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    6,
                    30,
                    16,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T06:30:16Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    6,
                    30,
                    16,
                    4,
                    101,
                    0
                ],
                "title": "ELSA: A Style Aligned Dataset for Emotionally Intelligent Language\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ELSA: A Style Aligned Dataset for Emotionally Intelligent Language\n  Generation"
                },
                "summary": "Advancements in emotion aware language processing increasingly shape vital\nNLP applications ranging from conversational AI and affective computing to\ncomputational psychology and creative content generation. Existing emotion\ndatasets either lack emotional granularity or fail to capture necessary\nstylistic diversity, limiting the advancement of effective emotion conditioned\ntext generation systems. Seeking to bridge this crucial gap between granularity\nand style diversity, this paper introduces a novel systematically constructed\ndataset named ELSA Emotion and Language Style Alignment Dataset leveraging fine\ngrained emotion taxonomies adapted from existing sources such as dair ai\nemotion dataset and GoEmotions taxonomy. This dataset comprises multiple\nemotionally nuanced variations of original sentences regenerated across\ndistinct contextual styles such as conversational, formal, poetic, and\nnarrative, using advanced Large Language Models LLMs. Rigorous computational\nevaluation using metrics such as perplexity, embedding variance, readability,\nlexical diversity, and semantic coherence measures validates the datasets\nemotional authenticity, linguistic fluency, and textual diversity.\nComprehensive metric analyses affirm its potential to support deeper\nexplorations into emotion conditioned style adaptive text generation. By\nenabling precision tuned emotionally nuanced language modeling, our dataset\ncreates fertile ground for research on fine grained emotional control, prompt\ndriven explanation, interpretability, and style adaptive expressive language\ngeneration with LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in emotion aware language processing increasingly shape vital\nNLP applications ranging from conversational AI and affective computing to\ncomputational psychology and creative content generation. Existing emotion\ndatasets either lack emotional granularity or fail to capture necessary\nstylistic diversity, limiting the advancement of effective emotion conditioned\ntext generation systems. Seeking to bridge this crucial gap between granularity\nand style diversity, this paper introduces a novel systematically constructed\ndataset named ELSA Emotion and Language Style Alignment Dataset leveraging fine\ngrained emotion taxonomies adapted from existing sources such as dair ai\nemotion dataset and GoEmotions taxonomy. This dataset comprises multiple\nemotionally nuanced variations of original sentences regenerated across\ndistinct contextual styles such as conversational, formal, poetic, and\nnarrative, using advanced Large Language Models LLMs. Rigorous computational\nevaluation using metrics such as perplexity, embedding variance, readability,\nlexical diversity, and semantic coherence measures validates the datasets\nemotional authenticity, linguistic fluency, and textual diversity.\nComprehensive metric analyses affirm its potential to support deeper\nexplorations into emotion conditioned style adaptive text generation. By\nenabling precision tuned emotionally nuanced language modeling, our dataset\ncreates fertile ground for research on fine grained emotional control, prompt\ndriven explanation, interpretability, and style adaptive expressive language\ngeneration with LLMs."
                },
                "authors": [
                    {
                        "name": "Vishal Gandhi"
                    },
                    {
                        "name": "Sagar Gandhi"
                    }
                ],
                "author_detail": {
                    "name": "Sagar Gandhi"
                },
                "author": "Sagar Gandhi",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02281v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02281v2",
                "updated": "2025-04-11T06:05:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    6,
                    5,
                    40,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-03T05:08:04Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    5,
                    8,
                    4,
                    3,
                    93,
                    0
                ],
                "title": "Parallel Market Environments for FinRL Contests",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Market Environments for FinRL Contests"
                },
                "summary": "Financial reinforcement learning has attracted lots of attention recently.\nFrom 2023 to 2025, we have organized three FinRL Contests featuring different\nfinancial tasks. Large language models have a strong capability to process\nfinancial documents. By integrating LLM-generated signals into the state,\ntrading agents can take smarter actions based on both structured market data\nand unstructured financial documents. In this paper, we summarize the parallel\nmarket environments for tasks used in FinRL Contests 2023-2025. To address the\nsampling bottleneck during training, we introduce GPU-optimized parallel market\nenvironments to address the sampling bottleneck. In particular, two new tasks\nincorporate LLM-generated signals and all tasks support massively parallel\nsimulation. Contestants have used these market environments to train robust and\npowerful trading agents for both stock and cryptocurrency trading tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial reinforcement learning has attracted lots of attention recently.\nFrom 2023 to 2025, we have organized three FinRL Contests featuring different\nfinancial tasks. Large language models have a strong capability to process\nfinancial documents. By integrating LLM-generated signals into the state,\ntrading agents can take smarter actions based on both structured market data\nand unstructured financial documents. In this paper, we summarize the parallel\nmarket environments for tasks used in FinRL Contests 2023-2025. To address the\nsampling bottleneck during training, we introduce GPU-optimized parallel market\nenvironments to address the sampling bottleneck. In particular, two new tasks\nincorporate LLM-generated signals and all tasks support massively parallel\nsimulation. Contestants have used these market environments to train robust and\npowerful trading agents for both stock and cryptocurrency trading tasks."
                },
                "authors": [
                    {
                        "name": "Keyi Wang"
                    },
                    {
                        "name": "Kairong Xiao"
                    },
                    {
                        "name": "Xiao-Yang Liu Yanglet"
                    }
                ],
                "author_detail": {
                    "name": "Xiao-Yang Liu Yanglet"
                },
                "author": "Xiao-Yang Liu Yanglet",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02281v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02281v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09953v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09953v3",
                "updated": "2025-04-11T05:41:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    41,
                    19,
                    4,
                    101,
                    0
                ],
                "published": "2024-06-14T11:58:51Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    11,
                    58,
                    51,
                    4,
                    166,
                    0
                ],
                "title": "DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm\n  Cooperative Planning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm\n  Cooperative Planning"
                },
                "summary": "Dual-arm robots offer enhanced versatility and efficiency over single-arm\ncounterparts by enabling concurrent manipulation of multiple objects or\ncooperative execution of tasks using both arms. However, the coordination of\ndual-arm systems for long-horizon tasks continues to pose significant\nchallenges, stemming from the intricate temporal and spatial dependencies among\nsub-tasks, necessitating intelligent decisions regarding the allocation of\nactions between arms and their optimal execution order. Existing task planning\nmethods predominantly focus on single-arm robots or rely on predefined bimanual\noperations to use large language models (LLMs) generate task sequence with\nlinear temporal dependency, failing to fully leverage the capabilities of\ndual-arm systems. To address this limitation, we introduce DAG-Plan, a\nstructured task planning framework tailored for dual-arm robots. DAG-Plan\nharnesses LLMs to decompose intricate tasks into actionable sub-tasks\nrepresented as nodes within a directed acyclic graph (DAG). Critically,\nDAG-Plan dynamically assigns these sub-tasks to the appropriate arm based on\nreal-time environmental observations, enabling parallel and adaptive execution.\nWe evaluate DAG-Plan on the Dual-Arm Kitchen Benchmark, comprising 5 sequential\ntasks with 44 sub-tasks. Extensive experiments demonstrate the superiority of\nDAG-Plan over directly using LLM to generate linear task sequence, achieving\n52.8% higher efficiency compared to the single-arm task planning and 48% higher\nsuccess rate of the dual-arm task planning. Compared to iterative methods,\nDAG-Plan improving execution efficiency 84.1% due to its fewer query time. More\ndemos and information are available on https://sites.google.com/view/dag-plan.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dual-arm robots offer enhanced versatility and efficiency over single-arm\ncounterparts by enabling concurrent manipulation of multiple objects or\ncooperative execution of tasks using both arms. However, the coordination of\ndual-arm systems for long-horizon tasks continues to pose significant\nchallenges, stemming from the intricate temporal and spatial dependencies among\nsub-tasks, necessitating intelligent decisions regarding the allocation of\nactions between arms and their optimal execution order. Existing task planning\nmethods predominantly focus on single-arm robots or rely on predefined bimanual\noperations to use large language models (LLMs) generate task sequence with\nlinear temporal dependency, failing to fully leverage the capabilities of\ndual-arm systems. To address this limitation, we introduce DAG-Plan, a\nstructured task planning framework tailored for dual-arm robots. DAG-Plan\nharnesses LLMs to decompose intricate tasks into actionable sub-tasks\nrepresented as nodes within a directed acyclic graph (DAG). Critically,\nDAG-Plan dynamically assigns these sub-tasks to the appropriate arm based on\nreal-time environmental observations, enabling parallel and adaptive execution.\nWe evaluate DAG-Plan on the Dual-Arm Kitchen Benchmark, comprising 5 sequential\ntasks with 44 sub-tasks. Extensive experiments demonstrate the superiority of\nDAG-Plan over directly using LLM to generate linear task sequence, achieving\n52.8% higher efficiency compared to the single-arm task planning and 48% higher\nsuccess rate of the dual-arm task planning. Compared to iterative methods,\nDAG-Plan improving execution efficiency 84.1% due to its fewer query time. More\ndemos and information are available on https://sites.google.com/view/dag-plan."
                },
                "authors": [
                    {
                        "name": "Zeyu Gao"
                    },
                    {
                        "name": "Yao Mu"
                    },
                    {
                        "name": "Jinye Qu"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Shijia Peng"
                    },
                    {
                        "name": "Chengkai Hou"
                    },
                    {
                        "name": "Lingyue Guo"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Shanghang Zhang"
                    },
                    {
                        "name": "Yanfeng Lu"
                    }
                ],
                "author_detail": {
                    "name": "Yanfeng Lu"
                },
                "author": "Yanfeng Lu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09953v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09953v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06943v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06943v2",
                "updated": "2025-04-11T05:34:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    34,
                    20,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-09T14:51:02Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    14,
                    51,
                    2,
                    2,
                    99,
                    0
                ],
                "title": "Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations,\n  Architectural Components, and Cognitive Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations,\n  Architectural Components, and Cognitive Integration"
                },
                "summary": "Agents powered by Large Language Models (LLMs) have recently demonstrated\nimpressive capabilities in various tasks. Still, they face limitations in tasks\nrequiring specific, structured knowledge, flexibility, or accountable\ndecision-making. While agents are capable of perceiving their environments,\nforming inferences, planning, and executing actions towards goals, they often\nface issues such as hallucinations and lack of contextual memory across\ninteractions. This paper explores how Case-Based Reasoning (CBR), a strategy\nthat solves new problems by referencing past experiences, can be integrated\ninto LLM agent frameworks. This integration allows LLMs to leverage explicit\nknowledge, enhancing their effectiveness. We systematically review the\ntheoretical foundations of these enhanced agents, identify critical framework\ncomponents, and formulate a mathematical model for the CBR processes of case\nretrieval, adaptation, and learning. We also evaluate CBR-enhanced agents\nagainst other methods like Chain-of-Thought reasoning and standard\nRetrieval-Augmented Generation, analyzing their relative strengths. Moreover,\nwe explore how leveraging CBR's cognitive dimensions (including\nself-reflection, introspection, and curiosity) via goal-driven autonomy\nmechanisms can further enhance the LLM agent capabilities. Contributing to the\nongoing research on neuro-symbolic hybrid systems, this work posits CBR as a\nviable technique for enhancing the reasoning skills and cognitive aspects of\nautonomous LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agents powered by Large Language Models (LLMs) have recently demonstrated\nimpressive capabilities in various tasks. Still, they face limitations in tasks\nrequiring specific, structured knowledge, flexibility, or accountable\ndecision-making. While agents are capable of perceiving their environments,\nforming inferences, planning, and executing actions towards goals, they often\nface issues such as hallucinations and lack of contextual memory across\ninteractions. This paper explores how Case-Based Reasoning (CBR), a strategy\nthat solves new problems by referencing past experiences, can be integrated\ninto LLM agent frameworks. This integration allows LLMs to leverage explicit\nknowledge, enhancing their effectiveness. We systematically review the\ntheoretical foundations of these enhanced agents, identify critical framework\ncomponents, and formulate a mathematical model for the CBR processes of case\nretrieval, adaptation, and learning. We also evaluate CBR-enhanced agents\nagainst other methods like Chain-of-Thought reasoning and standard\nRetrieval-Augmented Generation, analyzing their relative strengths. Moreover,\nwe explore how leveraging CBR's cognitive dimensions (including\nself-reflection, introspection, and curiosity) via goal-driven autonomy\nmechanisms can further enhance the LLM agent capabilities. Contributing to the\nongoing research on neuro-symbolic hybrid systems, this work posits CBR as a\nviable technique for enhancing the reasoning skills and cognitive aspects of\nautonomous LLM agents."
                },
                "authors": [
                    {
                        "name": "Kostas Hatalis"
                    },
                    {
                        "name": "Despina Christou"
                    },
                    {
                        "name": "Vyshnavi Kondapalli"
                    }
                ],
                "author_detail": {
                    "name": "Vyshnavi Kondapalli"
                },
                "author": "Vyshnavi Kondapalli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06943v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06943v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08864v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08864v3",
                "updated": "2025-04-11T05:27:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    27,
                    8,
                    4,
                    101,
                    0
                ],
                "published": "2024-12-12T01:52:25Z",
                "published_parsed": [
                    2024,
                    12,
                    12,
                    1,
                    52,
                    25,
                    3,
                    347,
                    0
                ],
                "title": "A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning\n  Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Graph-Based Synthetic Data Pipeline for Scaling High-Quality Reasoning\n  Instructions"
                },
                "summary": "Synthesizing high-quality reasoning data for continual training has been\nproven to be effective in enhancing the performance of Large Language Models\n(LLMs). However, previous synthetic approaches struggle to easily scale up data\nand incur high costs in the pursuit of high quality. In this paper, we propose\nthe Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable\nframework for high-quality reasoning data synthesis. Inspired by knowledge\ngraphs, we extracted knowledge points from seed data and constructed a\nknowledge point relationships graph to explore their interconnections. By\nexploring the implicit relationships among knowledge, our method achieves\n$\\times$255 data expansion. Furthermore, GSDP led by open-source models,\nachieves synthesis quality comparable to GPT-4-0613 while maintaining\n$\\times$100 lower costs. To tackle the most challenging mathematical reasoning\ntask, we present the GSDP-MATH dataset comprising over 1.91 million pairs of\nmath problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on\nMistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating\nthe effectiveness of our method. The dataset and models will be released at\nhttps://github.com/Jayce1kk/GSDP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthesizing high-quality reasoning data for continual training has been\nproven to be effective in enhancing the performance of Large Language Models\n(LLMs). However, previous synthetic approaches struggle to easily scale up data\nand incur high costs in the pursuit of high quality. In this paper, we propose\nthe Graph-based Synthetic Data Pipeline (GSDP), an economical and scalable\nframework for high-quality reasoning data synthesis. Inspired by knowledge\ngraphs, we extracted knowledge points from seed data and constructed a\nknowledge point relationships graph to explore their interconnections. By\nexploring the implicit relationships among knowledge, our method achieves\n$\\times$255 data expansion. Furthermore, GSDP led by open-source models,\nachieves synthesis quality comparable to GPT-4-0613 while maintaining\n$\\times$100 lower costs. To tackle the most challenging mathematical reasoning\ntask, we present the GSDP-MATH dataset comprising over 1.91 million pairs of\nmath problems and answers. After fine-tuning on GSDP-MATH, GSDP-7B based on\nMistral-7B achieves 37.7% accuracy on MATH and 78.4% on GSM8K, demonstrating\nthe effectiveness of our method. The dataset and models will be released at\nhttps://github.com/Jayce1kk/GSDP."
                },
                "authors": [
                    {
                        "name": "Jiankang Wang"
                    },
                    {
                        "name": "Jianjun Xu"
                    },
                    {
                        "name": "Xiaorui Wang"
                    },
                    {
                        "name": "Yuxin Wang"
                    },
                    {
                        "name": "Mengting Xing"
                    },
                    {
                        "name": "Shancheng Fang"
                    },
                    {
                        "name": "Zhineng Chen"
                    },
                    {
                        "name": "Hongtao Xie"
                    },
                    {
                        "name": "Yongdong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongdong Zhang"
                },
                "author": "Yongdong Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08864v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08864v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08260v1",
                "updated": "2025-04-11T05:11:40Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    11,
                    40,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T05:11:40Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    11,
                    40,
                    4,
                    101,
                    0
                ],
                "title": "Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in\n  Healthcare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in\n  Healthcare"
                },
                "summary": "Generative agents have been increasingly used to simulate human behaviour in\nsilico, driven by large language models (LLMs). These simulacra serve as\nsandboxes for studying human behaviour without compromising privacy or safety.\nHowever, it remains unclear whether such agents can truly represent real\nindividuals. This work compares survey data from the Understanding America\nStudy (UAS) on healthcare decision-making with simulated responses from\ngenerative agents. Using demographic-based prompt engineering, we create\ndigital twins of survey respondents and analyse how well different LLMs\nreproduce real-world behaviours. Our findings show that some LLMs fail to\nreflect realistic decision-making, such as predicting universal vaccine\nacceptance. However, Llama 3 captures variations across race and Income more\naccurately but also introduces biases not present in the UAS data. This study\nhighlights the potential of generative agents for behavioural research while\nunderscoring the risks of bias from both LLMs and prompting strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative agents have been increasingly used to simulate human behaviour in\nsilico, driven by large language models (LLMs). These simulacra serve as\nsandboxes for studying human behaviour without compromising privacy or safety.\nHowever, it remains unclear whether such agents can truly represent real\nindividuals. This work compares survey data from the Understanding America\nStudy (UAS) on healthcare decision-making with simulated responses from\ngenerative agents. Using demographic-based prompt engineering, we create\ndigital twins of survey respondents and analyse how well different LLMs\nreproduce real-world behaviours. Our findings show that some LLMs fail to\nreflect realistic decision-making, such as predicting universal vaccine\nacceptance. However, Llama 3 captures variations across race and Income more\naccurately but also introduces biases not present in the UAS data. This study\nhighlights the potential of generative agents for behavioural research while\nunderscoring the risks of bias from both LLMs and prompting strategies."
                },
                "authors": [
                    {
                        "name": "Yonchanok Khaokaew"
                    },
                    {
                        "name": "Flora D. Salim"
                    },
                    {
                        "name": "Andreas Zfle"
                    },
                    {
                        "name": "Hao Xue"
                    },
                    {
                        "name": "Taylor Anderson"
                    },
                    {
                        "name": "Matthew Scotch"
                    },
                    {
                        "name": "David J Heslop"
                    }
                ],
                "author_detail": {
                    "name": "David J Heslop"
                },
                "author": "David J Heslop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20052v2",
                "updated": "2025-04-11T05:07:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    5,
                    7,
                    41,
                    4,
                    101,
                    0
                ],
                "published": "2024-09-30T07:57:13Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    7,
                    57,
                    13,
                    0,
                    274,
                    0
                ],
                "title": "Mitigating Propensity Bias of Large Language Models for Recommender\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Propensity Bias of Large Language Models for Recommender\n  Systems"
                },
                "summary": "The rapid development of Large Language Models (LLMs) creates new\nopportunities for recommender systems, especially by exploiting the side\ninformation (e.g., descriptions and analyses of items) generated by these\nmodels. However, aligning this side information with collaborative information\nfrom historical interactions poses significant challenges. The inherent biases\nwithin LLMs can skew recommendations, resulting in distorted and potentially\nunfair user experiences. On the other hand, propensity bias causes side\ninformation to be aligned in such a way that it often tends to represent all\ninputs in a low-dimensional subspace, leading to a phenomenon known as\ndimensional collapse, which severely restricts the recommender system's ability\nto capture user preferences and behaviours. To address these issues, we\nintroduce a novel framework named Counterfactual LLM Recommendation (CLLMR).\nSpecifically, we propose a spectrum-based side information encoder that\nimplicitly embeds structural information from historical interactions into the\nside information representation, thereby circumventing the risk of dimension\ncollapse. Furthermore, our CLLMR approach explores the causal relationships\ninherent in LLM-based recommender systems. By leveraging counterfactual\ninference, we counteract the biases introduced by LLMs. Extensive experiments\ndemonstrate that our CLLMR approach consistently enhances the performance of\nvarious recommender models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development of Large Language Models (LLMs) creates new\nopportunities for recommender systems, especially by exploiting the side\ninformation (e.g., descriptions and analyses of items) generated by these\nmodels. However, aligning this side information with collaborative information\nfrom historical interactions poses significant challenges. The inherent biases\nwithin LLMs can skew recommendations, resulting in distorted and potentially\nunfair user experiences. On the other hand, propensity bias causes side\ninformation to be aligned in such a way that it often tends to represent all\ninputs in a low-dimensional subspace, leading to a phenomenon known as\ndimensional collapse, which severely restricts the recommender system's ability\nto capture user preferences and behaviours. To address these issues, we\nintroduce a novel framework named Counterfactual LLM Recommendation (CLLMR).\nSpecifically, we propose a spectrum-based side information encoder that\nimplicitly embeds structural information from historical interactions into the\nside information representation, thereby circumventing the risk of dimension\ncollapse. Furthermore, our CLLMR approach explores the causal relationships\ninherent in LLM-based recommender systems. By leveraging counterfactual\ninference, we counteract the biases introduced by LLMs. Extensive experiments\ndemonstrate that our CLLMR approach consistently enhances the performance of\nvarious recommender models."
                },
                "authors": [
                    {
                        "name": "Guixian Zhang"
                    },
                    {
                        "name": "Guan Yuan"
                    },
                    {
                        "name": "Debo Cheng"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Jiuyong Li"
                    },
                    {
                        "name": "Shichao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shichao Zhang"
                },
                "author": "Shichao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.07137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.07137v2",
                "updated": "2025-04-11T04:58:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    4,
                    58,
                    42,
                    4,
                    101,
                    0
                ],
                "published": "2025-03-10T10:08:55Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    10,
                    8,
                    55,
                    0,
                    69,
                    0
                ],
                "title": "A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Comprehensive Survey of Mixture-of-Experts: Algorithms, Theory, and\n  Applications"
                },
                "summary": "Artificial intelligence (AI) has achieved astonishing successes in many\ndomains, especially with the recent breakthroughs in the development of\nfoundational large models. These large models, leveraging their extensive\ntraining data, provide versatile solutions for a wide range of downstream\ntasks. However, as modern datasets become increasingly diverse and complex, the\ndevelopment of large AI models faces two major challenges: (1) the enormous\nconsumption of computational resources and deployment difficulties, and (2) the\ndifficulty in fitting heterogeneous and complex data, which limits the\nusability of the models. Mixture of Experts (MoE) models has recently attracted\nmuch attention in addressing these challenges, by dynamically selecting and\nactivating the most relevant sub-models to process input data. It has been\nshown that MoEs can significantly improve model performance and efficiency with\nfewer resources, particularly excelling in handling large-scale, multimodal\ndata. Given the tremendous potential MoE has demonstrated across various\ndomains, it is urgent to provide a comprehensive summary of recent advancements\nof MoEs in many important fields. Existing surveys on MoE have their\nlimitations, e.g., being outdated or lacking discussion on certain key areas,\nand we aim to address these gaps. In this paper, we first introduce the basic\ndesign of MoE, including gating functions, expert networks, routing mechanisms,\ntraining strategies, and system design. We then explore the algorithm design of\nMoE in important machine learning paradigms such as continual learning,\nmeta-learning, multi-task learning, and reinforcement learning. Additionally,\nwe summarize theoretical studies aimed at understanding MoE and review its\napplications in computer vision and natural language processing. Finally, we\ndiscuss promising future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial intelligence (AI) has achieved astonishing successes in many\ndomains, especially with the recent breakthroughs in the development of\nfoundational large models. These large models, leveraging their extensive\ntraining data, provide versatile solutions for a wide range of downstream\ntasks. However, as modern datasets become increasingly diverse and complex, the\ndevelopment of large AI models faces two major challenges: (1) the enormous\nconsumption of computational resources and deployment difficulties, and (2) the\ndifficulty in fitting heterogeneous and complex data, which limits the\nusability of the models. Mixture of Experts (MoE) models has recently attracted\nmuch attention in addressing these challenges, by dynamically selecting and\nactivating the most relevant sub-models to process input data. It has been\nshown that MoEs can significantly improve model performance and efficiency with\nfewer resources, particularly excelling in handling large-scale, multimodal\ndata. Given the tremendous potential MoE has demonstrated across various\ndomains, it is urgent to provide a comprehensive summary of recent advancements\nof MoEs in many important fields. Existing surveys on MoE have their\nlimitations, e.g., being outdated or lacking discussion on certain key areas,\nand we aim to address these gaps. In this paper, we first introduce the basic\ndesign of MoE, including gating functions, expert networks, routing mechanisms,\ntraining strategies, and system design. We then explore the algorithm design of\nMoE in important machine learning paradigms such as continual learning,\nmeta-learning, multi-task learning, and reinforcement learning. Additionally,\nwe summarize theoretical studies aimed at understanding MoE and review its\napplications in computer vision and natural language processing. Finally, we\ndiscuss promising future research directions."
                },
                "authors": [
                    {
                        "name": "Siyuan Mu"
                    },
                    {
                        "name": "Sen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Sen Lin"
                },
                "author": "Sen Lin",
                "arxiv_comment": "29 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.07137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.07137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08256v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08256v1",
                "updated": "2025-04-11T04:55:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    4,
                    55,
                    50,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T04:55:50Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    4,
                    55,
                    50,
                    4,
                    101,
                    0
                ],
                "title": "RAG-VR: Leveraging Retrieval-Augmented Generation for 3D Question\n  Answering in VR Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG-VR: Leveraging Retrieval-Augmented Generation for 3D Question\n  Answering in VR Environments"
                },
                "summary": "Recent advances in large language models (LLMs) provide new opportunities for\ncontext understanding in virtual reality (VR). However, VR contexts are often\nhighly localized and personalized, limiting the effectiveness of\ngeneral-purpose LLMs. To address this challenge, we present RAG-VR, the first\n3D question-answering system for VR that incorporates retrieval-augmented\ngeneration (RAG), which augments an LLM with external knowledge retrieved from\na localized knowledge database to improve the answer quality. RAG-VR includes a\npipeline for extracting comprehensive knowledge about virtual environments and\nuser conditions for accurate answer generation. To ensure efficient retrieval,\nRAG-VR offloads the retrieval process to a nearby edge server and uses only\nessential information during retrieval. Moreover, we train the retriever to\neffectively distinguish among relevant, irrelevant, and hard-to-differentiate\ninformation in relation to questions. RAG-VR improves answer accuracy by\n17.9%-41.8% and reduces end-to-end latency by 34.5%-47.3% compared with two\nbaseline systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) provide new opportunities for\ncontext understanding in virtual reality (VR). However, VR contexts are often\nhighly localized and personalized, limiting the effectiveness of\ngeneral-purpose LLMs. To address this challenge, we present RAG-VR, the first\n3D question-answering system for VR that incorporates retrieval-augmented\ngeneration (RAG), which augments an LLM with external knowledge retrieved from\na localized knowledge database to improve the answer quality. RAG-VR includes a\npipeline for extracting comprehensive knowledge about virtual environments and\nuser conditions for accurate answer generation. To ensure efficient retrieval,\nRAG-VR offloads the retrieval process to a nearby edge server and uses only\nessential information during retrieval. Moreover, we train the retriever to\neffectively distinguish among relevant, irrelevant, and hard-to-differentiate\ninformation in relation to questions. RAG-VR improves answer accuracy by\n17.9%-41.8% and reduces end-to-end latency by 34.5%-47.3% compared with two\nbaseline systems."
                },
                "authors": [
                    {
                        "name": "Shiyi Ding"
                    },
                    {
                        "name": "Ying Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ying Chen"
                },
                "author": "Ying Chen",
                "arxiv_comment": "Proceedings of the 2025 IEEE Conference on Virtual Reality and 3D\n  User Interfaces (VR), March 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08256v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08256v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08255v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08255v1",
                "updated": "2025-04-11T04:37:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    4,
                    37,
                    52,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T04:37:52Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    4,
                    37,
                    52,
                    4,
                    101,
                    0
                ],
                "title": "CICV5G: A 5G Communication Delay Dataset for PnC in Cloud-based\n  Intelligent Connected Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CICV5G: A 5G Communication Delay Dataset for PnC in Cloud-based\n  Intelligent Connected Vehicles"
                },
                "summary": "Cloud-based intelligent connected vehicles (CICVs) leverage cloud computing\nand vehicle-to-everything (V2X) to enable efficient information exchange and\ncooperative control. However, communication delay is a critical factor in\nvehicle-cloud interactions, potentially deteriorating the planning and control\n(PnC) performance of CICVs. To explore whether the new generation of\ncommunication technology, 5G, can support the PnC of CICVs, we present CICV5G,\na publicly available 5G communication delay dataset for the PnC of CICVs. This\ndataset offers real-time delay variations across diverse traffic environments,\nvelocity, data transmission frequencies, and network conditions. It contains\nover 300,000 records, with each record consists of the network performance\nindicators (e.g., cell ID, reference signal received power, and signal-to-noise\nratio) and PnC related data (e.g., position). Based on the CICV5G, we compare\nthe performance of CICVs with that of autonomous vehicles and examine how delay\nimpacts the PnC of CICVs. The object of this dataset is to support research in\ndeveloping more accurate communication models and to provide a valuable\nreference for scheme development and network deployment for CICVs. To ensure\nthat the research community can benefit from this work, our dataset and\naccompanying code are made publicly available.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cloud-based intelligent connected vehicles (CICVs) leverage cloud computing\nand vehicle-to-everything (V2X) to enable efficient information exchange and\ncooperative control. However, communication delay is a critical factor in\nvehicle-cloud interactions, potentially deteriorating the planning and control\n(PnC) performance of CICVs. To explore whether the new generation of\ncommunication technology, 5G, can support the PnC of CICVs, we present CICV5G,\na publicly available 5G communication delay dataset for the PnC of CICVs. This\ndataset offers real-time delay variations across diverse traffic environments,\nvelocity, data transmission frequencies, and network conditions. It contains\nover 300,000 records, with each record consists of the network performance\nindicators (e.g., cell ID, reference signal received power, and signal-to-noise\nratio) and PnC related data (e.g., position). Based on the CICV5G, we compare\nthe performance of CICVs with that of autonomous vehicles and examine how delay\nimpacts the PnC of CICVs. The object of this dataset is to support research in\ndeveloping more accurate communication models and to provide a valuable\nreference for scheme development and network deployment for CICVs. To ensure\nthat the research community can benefit from this work, our dataset and\naccompanying code are made publicly available."
                },
                "authors": [
                    {
                        "name": "Xinrui Zhang"
                    },
                    {
                        "name": "Peizhi Zhang"
                    },
                    {
                        "name": "Junpeng Huang"
                    },
                    {
                        "name": "Haojie Feng"
                    },
                    {
                        "name": "Yining Ma"
                    },
                    {
                        "name": "Feng Shen"
                    },
                    {
                        "name": "Lu Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Lu Xiong"
                },
                "author": "Lu Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08255v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08255v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11858v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11858v2",
                "updated": "2025-04-11T04:26:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    4,
                    26,
                    42,
                    4,
                    101,
                    0
                ],
                "published": "2025-01-21T03:22:10Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    22,
                    10,
                    1,
                    21,
                    0
                ],
                "title": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmbodiedEval: Evaluate Multimodal LLMs as Embodied Agents"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have shown significant advancements,\nproviding a promising future for embodied agents. Existing benchmarks for\nevaluating MLLMs primarily utilize static images or videos, limiting\nassessments to non-interactive scenarios. Meanwhile, existing embodied AI\nbenchmarks are task-specific and not diverse enough, which do not adequately\nevaluate the embodied capabilities of MLLMs. To address this, we propose\nEmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs\nwith embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied\n3D scenes, each of which is rigorously selected and annotated. It covers a\nbroad spectrum of existing embodied AI tasks with significantly enhanced\ndiversity, all within a unified simulation and evaluation framework tailored\nfor MLLMs. The tasks are organized into five categories: navigation, object\ninteraction, social interaction, attribute question answering, and spatial\nquestion answering to assess different capabilities of the agents. We evaluated\nthe state-of-the-art MLLMs on EmbodiedEval and found that they have a\nsignificant shortfall compared to human level on embodied tasks. Our analysis\ndemonstrates the limitations of existing MLLMs in embodied capabilities,\nproviding insights for their future development. We open-source all evaluation\ndata and simulation framework at https://github.com/thunlp/EmbodiedEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have shown significant advancements,\nproviding a promising future for embodied agents. Existing benchmarks for\nevaluating MLLMs primarily utilize static images or videos, limiting\nassessments to non-interactive scenarios. Meanwhile, existing embodied AI\nbenchmarks are task-specific and not diverse enough, which do not adequately\nevaluate the embodied capabilities of MLLMs. To address this, we propose\nEmbodiedEval, a comprehensive and interactive evaluation benchmark for MLLMs\nwith embodied tasks. EmbodiedEval features 328 distinct tasks within 125 varied\n3D scenes, each of which is rigorously selected and annotated. It covers a\nbroad spectrum of existing embodied AI tasks with significantly enhanced\ndiversity, all within a unified simulation and evaluation framework tailored\nfor MLLMs. The tasks are organized into five categories: navigation, object\ninteraction, social interaction, attribute question answering, and spatial\nquestion answering to assess different capabilities of the agents. We evaluated\nthe state-of-the-art MLLMs on EmbodiedEval and found that they have a\nsignificant shortfall compared to human level on embodied tasks. Our analysis\ndemonstrates the limitations of existing MLLMs in embodied capabilities,\nproviding insights for their future development. We open-source all evaluation\ndata and simulation framework at https://github.com/thunlp/EmbodiedEval."
                },
                "authors": [
                    {
                        "name": "Zhili Cheng"
                    },
                    {
                        "name": "Yuge Tu"
                    },
                    {
                        "name": "Ran Li"
                    },
                    {
                        "name": "Shiqi Dai"
                    },
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Jiahao Li"
                    },
                    {
                        "name": "Yang Shi"
                    },
                    {
                        "name": "Tianyu Yu"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Lei Shi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11858v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11858v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15655v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15655v3",
                "updated": "2025-04-11T04:17:44Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    4,
                    17,
                    44,
                    4,
                    101,
                    0
                ],
                "published": "2024-12-20T08:13:05Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    8,
                    13,
                    5,
                    4,
                    355,
                    0
                ],
                "title": "MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical\n  Speech-to-Formula",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical\n  Speech-to-Formula"
                },
                "summary": "In various academic and professional settings, such as mathematics lectures\nor research presentations, it is often necessary to convey mathematical\nexpressions orally. However, reading mathematical expressions aloud without\naccompanying visuals can significantly hinder comprehension, especially for\nthose who are hearing-impaired or rely on subtitles due to language barriers.\nFor instance, when a presenter reads Euler's Formula, current Automatic Speech\nRecognition (ASR) models often produce a verbose and error-prone textual\ndescription (e.g., e to the power of i x equals cosine of x plus i\n$\\textit{side}$ of x), instead of the concise $\\LaTeX{}$ format (i.e., $ e^{ix}\n= \\cos(x) + i\\sin(x) $), which hampers clear understanding and communication.\nTo address this issue, we introduce MathSpeech, a novel pipeline that\nintegrates ASR models with small Language Models (sLMs) to correct errors in\nmathematical expressions and accurately convert spoken expressions into\nstructured $\\LaTeX{}$ representations. Evaluated on a new dataset derived from\nlecture recordings, MathSpeech demonstrates $\\LaTeX{}$ generation capabilities\ncomparable to leading commercial Large Language Models (LLMs), while leveraging\nfine-tuned small language models of only 120M parameters. Specifically, in\nterms of CER, BLEU, and ROUGE scores for $\\LaTeX{}$ translation, MathSpeech\ndemonstrated significantly superior capabilities compared to GPT-4o. We\nobserved a decrease in CER from 0.390 to 0.298, and higher ROUGE/BLEU scores\ncompared to GPT-4o.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In various academic and professional settings, such as mathematics lectures\nor research presentations, it is often necessary to convey mathematical\nexpressions orally. However, reading mathematical expressions aloud without\naccompanying visuals can significantly hinder comprehension, especially for\nthose who are hearing-impaired or rely on subtitles due to language barriers.\nFor instance, when a presenter reads Euler's Formula, current Automatic Speech\nRecognition (ASR) models often produce a verbose and error-prone textual\ndescription (e.g., e to the power of i x equals cosine of x plus i\n$\\textit{side}$ of x), instead of the concise $\\LaTeX{}$ format (i.e., $ e^{ix}\n= \\cos(x) + i\\sin(x) $), which hampers clear understanding and communication.\nTo address this issue, we introduce MathSpeech, a novel pipeline that\nintegrates ASR models with small Language Models (sLMs) to correct errors in\nmathematical expressions and accurately convert spoken expressions into\nstructured $\\LaTeX{}$ representations. Evaluated on a new dataset derived from\nlecture recordings, MathSpeech demonstrates $\\LaTeX{}$ generation capabilities\ncomparable to leading commercial Large Language Models (LLMs), while leveraging\nfine-tuned small language models of only 120M parameters. Specifically, in\nterms of CER, BLEU, and ROUGE scores for $\\LaTeX{}$ translation, MathSpeech\ndemonstrated significantly superior capabilities compared to GPT-4o. We\nobserved a decrease in CER from 0.390 to 0.298, and higher ROUGE/BLEU scores\ncompared to GPT-4o."
                },
                "authors": [
                    {
                        "name": "Sieun Hyeon"
                    },
                    {
                        "name": "Kyudan Jung"
                    },
                    {
                        "name": "Jaehee Won"
                    },
                    {
                        "name": "Nam-Joon Kim"
                    },
                    {
                        "name": "Hyun Gon Ryu"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Jaeyoung Do"
                    }
                ],
                "author_detail": {
                    "name": "Jaeyoung Do"
                },
                "author": "Jaeyoung Do",
                "arxiv_comment": "Accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15655v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15655v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08242v1",
                "updated": "2025-04-11T03:58:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    3,
                    58,
                    59,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T03:58:59Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    3,
                    58,
                    59,
                    4,
                    101,
                    0
                ],
                "title": "Jupiter: Fast and Resource-Efficient Collaborative Inference of\n  Generative LLMs on Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jupiter: Fast and Resource-Efficient Collaborative Inference of\n  Generative LLMs on Edge Devices"
                },
                "summary": "Generative large language models (LLMs) have garnered significant attention\ndue to their exceptional capabilities in various AI tasks. Traditionally\ndeployed in cloud datacenters, LLMs are now increasingly moving towards more\naccessible edge platforms to protect sensitive user data and ensure privacy\npreservation. The limited computational resources of individual edge devices,\nhowever, can result in excessively prolonged inference latency and overwhelmed\nmemory usage. While existing research has explored collaborative edge computing\nto break the resource wall of individual devices, these solutions yet suffer\nfrom massive communication overhead and under-utilization of edge resources.\nFurthermore, they focus exclusively on optimizing the prefill phase, neglecting\nthe crucial autoregressive decoding phase for generative LLMs. To address that,\nwe propose Jupiter, a fast, scalable, and resource-efficient collaborative edge\nAI system for generative LLM inference. Jupiter introduces a flexible pipelined\narchitecture as a principle and differentiates its system design according to\nthe differentiated characteristics of the prefill and decoding phases. For\nprefill phase, Jupiter submits a novel intra-sequence pipeline parallelism and\ndevelops a meticulous parallelism planning strategy to maximize resource\nefficiency; For decoding, Jupiter devises an effective outline-based pipeline\nparallel decoding mechanism combined with speculative decoding, which further\nmagnifies inference acceleration. Extensive evaluation based on realistic\nimplementation demonstrates that Jupiter remarkably outperforms\nstate-of-the-art approaches under various edge environment setups, achieving up\nto 26.1x end-to-end latency reduction while rendering on-par generation\nquality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative large language models (LLMs) have garnered significant attention\ndue to their exceptional capabilities in various AI tasks. Traditionally\ndeployed in cloud datacenters, LLMs are now increasingly moving towards more\naccessible edge platforms to protect sensitive user data and ensure privacy\npreservation. The limited computational resources of individual edge devices,\nhowever, can result in excessively prolonged inference latency and overwhelmed\nmemory usage. While existing research has explored collaborative edge computing\nto break the resource wall of individual devices, these solutions yet suffer\nfrom massive communication overhead and under-utilization of edge resources.\nFurthermore, they focus exclusively on optimizing the prefill phase, neglecting\nthe crucial autoregressive decoding phase for generative LLMs. To address that,\nwe propose Jupiter, a fast, scalable, and resource-efficient collaborative edge\nAI system for generative LLM inference. Jupiter introduces a flexible pipelined\narchitecture as a principle and differentiates its system design according to\nthe differentiated characteristics of the prefill and decoding phases. For\nprefill phase, Jupiter submits a novel intra-sequence pipeline parallelism and\ndevelops a meticulous parallelism planning strategy to maximize resource\nefficiency; For decoding, Jupiter devises an effective outline-based pipeline\nparallel decoding mechanism combined with speculative decoding, which further\nmagnifies inference acceleration. Extensive evaluation based on realistic\nimplementation demonstrates that Jupiter remarkably outperforms\nstate-of-the-art approaches under various edge environment setups, achieving up\nto 26.1x end-to-end latency reduction while rendering on-par generation\nquality."
                },
                "authors": [
                    {
                        "name": "Shengyuan Ye"
                    },
                    {
                        "name": "Bei Ouyang"
                    },
                    {
                        "name": "Liekang Zeng"
                    },
                    {
                        "name": "Tianyi Qian"
                    },
                    {
                        "name": "Xiaowen Chu"
                    },
                    {
                        "name": "Jian Tang"
                    },
                    {
                        "name": "Xu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xu Chen"
                },
                "author": "Xu Chen",
                "arxiv_comment": "Accepted by IEEE International Conference on Computer Communications\n  2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.13262v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.13262v2",
                "updated": "2025-04-11T03:43:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    3,
                    43,
                    11,
                    4,
                    101,
                    0
                ],
                "published": "2024-10-17T06:33:16Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    6,
                    33,
                    16,
                    3,
                    291,
                    0
                ],
                "title": "Membership Testing for Semantic Regular Expressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership Testing for Semantic Regular Expressions"
                },
                "summary": "SMORE (Chen et al., 2023) recently proposed the concept of semantic regular\nexpressions that extend the classical formalism with a primitive to query\nexternal oracles such as databases and large language models (LLMs). Such\npatterns can be used to identify lines of text containing references to\nsemantic concepts such as cities, celebrities, political entities, etc. The\nfocus in their paper was on automatically synthesizing semantic regular\nexpressions from positive and negative examples. In this paper, we study the\nmembership testing problem:\n  First, We present a two-pass NFA-based algorithm to determine whether a\nstring $w$ matches a semantic regular expression (SemRE) $r$ in $O(|r|^2 |w|^2\n+ |r| |w|^3)$ time, assuming the oracle responds to each query in unit time. In\ncommon situations, where oracle queries are not nested, we show that this\nprocedure runs in $O(|r|^2 |w|^2)$ time. Experiments with a prototype\nimplementation of this algorithm validate our theoretical analysis, and show\nthat the procedure massively outperforms a dynamic programming-based baseline,\nand incurs a $\\approx 2 \\times$ overhead over the time needed for interaction\nwith the oracle.\n  Next, We establish connections between SemRE membership testing and the\ntriangle finding problem from graph theory, which suggest that developing\nalgorithms which are simultaneously practical and asymptotically faster might\nbe challenging. Furthermore, algorithms for classical regular expressions\nprimarily aim to optimize their time and memory consumption. In contrast, an\nimportant consideration in our setting is to minimize the cost of invoking the\noracle. We demonstrate an $\\Omega(|w|^2)$ lower bound on the number of oracle\nqueries necessary to make this determination.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SMORE (Chen et al., 2023) recently proposed the concept of semantic regular\nexpressions that extend the classical formalism with a primitive to query\nexternal oracles such as databases and large language models (LLMs). Such\npatterns can be used to identify lines of text containing references to\nsemantic concepts such as cities, celebrities, political entities, etc. The\nfocus in their paper was on automatically synthesizing semantic regular\nexpressions from positive and negative examples. In this paper, we study the\nmembership testing problem:\n  First, We present a two-pass NFA-based algorithm to determine whether a\nstring $w$ matches a semantic regular expression (SemRE) $r$ in $O(|r|^2 |w|^2\n+ |r| |w|^3)$ time, assuming the oracle responds to each query in unit time. In\ncommon situations, where oracle queries are not nested, we show that this\nprocedure runs in $O(|r|^2 |w|^2)$ time. Experiments with a prototype\nimplementation of this algorithm validate our theoretical analysis, and show\nthat the procedure massively outperforms a dynamic programming-based baseline,\nand incurs a $\\approx 2 \\times$ overhead over the time needed for interaction\nwith the oracle.\n  Next, We establish connections between SemRE membership testing and the\ntriangle finding problem from graph theory, which suggest that developing\nalgorithms which are simultaneously practical and asymptotically faster might\nbe challenging. Furthermore, algorithms for classical regular expressions\nprimarily aim to optimize their time and memory consumption. In contrast, an\nimportant consideration in our setting is to minimize the cost of invoking the\noracle. We demonstrate an $\\Omega(|w|^2)$ lower bound on the number of oracle\nqueries necessary to make this determination."
                },
                "authors": [
                    {
                        "name": "Yifei Huang"
                    },
                    {
                        "name": "Matin Amini"
                    },
                    {
                        "name": "Alexis Le Glaunec"
                    },
                    {
                        "name": "Konstantinos Mamouras"
                    },
                    {
                        "name": "Mukund Raghothaman"
                    }
                ],
                "author_detail": {
                    "name": "Mukund Raghothaman"
                },
                "author": "Mukund Raghothaman",
                "arxiv_doi": "10.1145/3729300",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3729300",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.13262v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.13262v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14249v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14249v6",
                "updated": "2025-04-11T03:34:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    3,
                    34,
                    49,
                    4,
                    101,
                    0
                ],
                "published": "2025-01-24T05:27:46Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    5,
                    27,
                    46,
                    4,
                    24,
                    0
                ],
                "title": "Humanity's Last Exam",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humanity's Last Exam"
                },
                "summary": "Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 2,700\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarks are important tools for tracking the rapid advancements in large\nlanguage model (LLM) capabilities. However, benchmarks are not keeping pace in\ndifficulty: LLMs now achieve over 90\\% accuracy on popular benchmarks like\nMMLU, limiting informed measurement of state-of-the-art LLM capabilities. In\nresponse, we introduce Humanity's Last Exam (HLE), a multi-modal benchmark at\nthe frontier of human knowledge, designed to be the final closed-ended academic\nbenchmark of its kind with broad subject coverage. HLE consists of 2,700\nquestions across dozens of subjects, including mathematics, humanities, and the\nnatural sciences. HLE is developed globally by subject-matter experts and\nconsists of multiple-choice and short-answer questions suitable for automated\ngrading. Each question has a known solution that is unambiguous and easily\nverifiable, but cannot be quickly answered via internet retrieval.\nState-of-the-art LLMs demonstrate low accuracy and calibration on HLE,\nhighlighting a significant gap between current LLM capabilities and the expert\nhuman frontier on closed-ended academic questions. To inform research and\npolicymaking upon a clear understanding of model capabilities, we publicly\nrelease HLE at https://lastexam.ai."
                },
                "authors": [
                    {
                        "name": "Long Phan"
                    },
                    {
                        "name": "Alice Gatti"
                    },
                    {
                        "name": "Ziwen Han"
                    },
                    {
                        "name": "Nathaniel Li"
                    },
                    {
                        "name": "Josephina Hu"
                    },
                    {
                        "name": "Hugh Zhang"
                    },
                    {
                        "name": "Chen Bo Calvin Zhang"
                    },
                    {
                        "name": "Mohamed Shaaban"
                    },
                    {
                        "name": "John Ling"
                    },
                    {
                        "name": "Sean Shi"
                    },
                    {
                        "name": "Michael Choi"
                    },
                    {
                        "name": "Anish Agrawal"
                    },
                    {
                        "name": "Arnav Chopra"
                    },
                    {
                        "name": "Adam Khoja"
                    },
                    {
                        "name": "Ryan Kim"
                    },
                    {
                        "name": "Richard Ren"
                    },
                    {
                        "name": "Jason Hausenloy"
                    },
                    {
                        "name": "Oliver Zhang"
                    },
                    {
                        "name": "Mantas Mazeika"
                    },
                    {
                        "name": "Dmitry Dodonov"
                    },
                    {
                        "name": "Tung Nguyen"
                    },
                    {
                        "name": "Jaeho Lee"
                    },
                    {
                        "name": "Daron Anderson"
                    },
                    {
                        "name": "Mikhail Doroshenko"
                    },
                    {
                        "name": "Alun Cennyth Stokes"
                    },
                    {
                        "name": "Mobeen Mahmood"
                    },
                    {
                        "name": "Oleksandr Pokutnyi"
                    },
                    {
                        "name": "Oleg Iskra"
                    },
                    {
                        "name": "Jessica P. Wang"
                    },
                    {
                        "name": "John-Clark Levin"
                    },
                    {
                        "name": "Mstyslav Kazakov"
                    },
                    {
                        "name": "Fiona Feng"
                    },
                    {
                        "name": "Steven Y. Feng"
                    },
                    {
                        "name": "Haoran Zhao"
                    },
                    {
                        "name": "Michael Yu"
                    },
                    {
                        "name": "Varun Gangal"
                    },
                    {
                        "name": "Chelsea Zou"
                    },
                    {
                        "name": "Zihan Wang"
                    },
                    {
                        "name": "Serguei Popov"
                    },
                    {
                        "name": "Robert Gerbicz"
                    },
                    {
                        "name": "Geoff Galgon"
                    },
                    {
                        "name": "Johannes Schmitt"
                    },
                    {
                        "name": "Will Yeadon"
                    },
                    {
                        "name": "Yongki Lee"
                    },
                    {
                        "name": "Scott Sauers"
                    },
                    {
                        "name": "Alvaro Sanchez"
                    },
                    {
                        "name": "Fabian Giska"
                    },
                    {
                        "name": "Marc Roth"
                    },
                    {
                        "name": "Sren Riis"
                    },
                    {
                        "name": "Saiteja Utpala"
                    },
                    {
                        "name": "Noah Burns"
                    },
                    {
                        "name": "Gashaw M. Goshu"
                    },
                    {
                        "name": "Mohinder Maheshbhai Naiya"
                    },
                    {
                        "name": "Chidozie Agu"
                    },
                    {
                        "name": "Zachary Giboney"
                    },
                    {
                        "name": "Antrell Cheatom"
                    },
                    {
                        "name": "Francesco Fournier-Facio"
                    },
                    {
                        "name": "Sarah-Jane Crowson"
                    },
                    {
                        "name": "Lennart Finke"
                    },
                    {
                        "name": "Zerui Cheng"
                    },
                    {
                        "name": "Jennifer Zampese"
                    },
                    {
                        "name": "Ryan G. Hoerr"
                    },
                    {
                        "name": "Mark Nandor"
                    },
                    {
                        "name": "Hyunwoo Park"
                    },
                    {
                        "name": "Tim Gehrunger"
                    },
                    {
                        "name": "Jiaqi Cai"
                    },
                    {
                        "name": "Ben McCarty"
                    },
                    {
                        "name": "Alexis C Garretson"
                    },
                    {
                        "name": "Edwin Taylor"
                    },
                    {
                        "name": "Damien Sileo"
                    },
                    {
                        "name": "Qiuyu Ren"
                    },
                    {
                        "name": "Usman Qazi"
                    },
                    {
                        "name": "Lianghui Li"
                    },
                    {
                        "name": "Jungbae Nam"
                    },
                    {
                        "name": "John B. Wydallis"
                    },
                    {
                        "name": "Pavel Arkhipov"
                    },
                    {
                        "name": "Jack Wei Lun Shi"
                    },
                    {
                        "name": "Aras Bacho"
                    },
                    {
                        "name": "Chris G. Willcocks"
                    },
                    {
                        "name": "Hangrui Cao"
                    },
                    {
                        "name": "Sumeet Motwani"
                    },
                    {
                        "name": "Emily de Oliveira Santos"
                    },
                    {
                        "name": "Johannes Veith"
                    },
                    {
                        "name": "Edward Vendrow"
                    },
                    {
                        "name": "Doru Cojoc"
                    },
                    {
                        "name": "Kengo Zenitani"
                    },
                    {
                        "name": "Joshua Robinson"
                    },
                    {
                        "name": "Longke Tang"
                    },
                    {
                        "name": "Yuqi Li"
                    },
                    {
                        "name": "Joshua Vendrow"
                    },
                    {
                        "name": "Natanael Wildner Fraga"
                    },
                    {
                        "name": "Vladyslav Kuchkin"
                    },
                    {
                        "name": "Andrey Pupasov Maksimov"
                    },
                    {
                        "name": "Pierre Marion"
                    },
                    {
                        "name": "Denis Efremov"
                    },
                    {
                        "name": "Jayson Lynch"
                    },
                    {
                        "name": "Kaiqu Liang"
                    },
                    {
                        "name": "Aleksandar Mikov"
                    },
                    {
                        "name": "Andrew Gritsevskiy"
                    },
                    {
                        "name": "Julien Guillod"
                    },
                    {
                        "name": "Gzdenur Demir"
                    },
                    {
                        "name": "Dakotah Martinez"
                    },
                    {
                        "name": "Ben Pageler"
                    },
                    {
                        "name": "Kevin Zhou"
                    },
                    {
                        "name": "Saeed Soori"
                    },
                    {
                        "name": "Ori Press"
                    },
                    {
                        "name": "Henry Tang"
                    },
                    {
                        "name": "Paolo Rissone"
                    },
                    {
                        "name": "Sean R. Green"
                    },
                    {
                        "name": "Lina Brssel"
                    },
                    {
                        "name": "Moon Twayana"
                    },
                    {
                        "name": "Aymeric Dieuleveut"
                    },
                    {
                        "name": "Joseph Marvin Imperial"
                    },
                    {
                        "name": "Ameya Prabhu"
                    },
                    {
                        "name": "Jinzhou Yang"
                    },
                    {
                        "name": "Nick Crispino"
                    },
                    {
                        "name": "Arun Rao"
                    },
                    {
                        "name": "Dimitri Zvonkine"
                    },
                    {
                        "name": "Gabriel Loiseau"
                    },
                    {
                        "name": "Mikhail Kalinin"
                    },
                    {
                        "name": "Marco Lukas"
                    },
                    {
                        "name": "Ciprian Manolescu"
                    },
                    {
                        "name": "Nate Stambaugh"
                    },
                    {
                        "name": "Subrata Mishra"
                    },
                    {
                        "name": "Tad Hogg"
                    },
                    {
                        "name": "Carlo Bosio"
                    },
                    {
                        "name": "Brian P Coppola"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Jaehyeok Jin"
                    },
                    {
                        "name": "Rafael Sayous"
                    },
                    {
                        "name": "Stefan Ivanov"
                    },
                    {
                        "name": "Philippe Schwaller"
                    },
                    {
                        "name": "Shaipranesh Senthilkuma"
                    },
                    {
                        "name": "Andres M Bran"
                    },
                    {
                        "name": "Andres Algaba"
                    },
                    {
                        "name": "Kelsey Van den Houte"
                    },
                    {
                        "name": "Lynn Van Der Sypt"
                    },
                    {
                        "name": "Brecht Verbeken"
                    },
                    {
                        "name": "David Noever"
                    },
                    {
                        "name": "Alexei Kopylov"
                    },
                    {
                        "name": "Benjamin Myklebust"
                    },
                    {
                        "name": "Bikun Li"
                    },
                    {
                        "name": "Lisa Schut"
                    },
                    {
                        "name": "Evgenii Zheltonozhskii"
                    },
                    {
                        "name": "Qiaochu Yuan"
                    },
                    {
                        "name": "Derek Lim"
                    },
                    {
                        "name": "Richard Stanley"
                    },
                    {
                        "name": "Tong Yang"
                    },
                    {
                        "name": "John Maar"
                    },
                    {
                        "name": "Julian Wykowski"
                    },
                    {
                        "name": "Mart Oller"
                    },
                    {
                        "name": "Anmol Sahu"
                    },
                    {
                        "name": "Cesare Giulio Ardito"
                    },
                    {
                        "name": "Yuzheng Hu"
                    },
                    {
                        "name": "Ariel Ghislain Kemogne Kamdoum"
                    },
                    {
                        "name": "Alvin Jin"
                    },
                    {
                        "name": "Tobias Garcia Vilchis"
                    },
                    {
                        "name": "Yuexuan Zu"
                    },
                    {
                        "name": "Martin Lackner"
                    },
                    {
                        "name": "James Koppel"
                    },
                    {
                        "name": "Gongbo Sun"
                    },
                    {
                        "name": "Daniil S. Antonenko"
                    },
                    {
                        "name": "Steffi Chern"
                    },
                    {
                        "name": "Bingchen Zhao"
                    },
                    {
                        "name": "Pierrot Arsene"
                    },
                    {
                        "name": "Joseph M Cavanagh"
                    },
                    {
                        "name": "Daofeng Li"
                    },
                    {
                        "name": "Jiawei Shen"
                    },
                    {
                        "name": "Donato Crisostomi"
                    },
                    {
                        "name": "Wenjin Zhang"
                    },
                    {
                        "name": "Ali Dehghan"
                    },
                    {
                        "name": "Sergey Ivanov"
                    },
                    {
                        "name": "David Perrella"
                    },
                    {
                        "name": "Nurdin Kaparov"
                    },
                    {
                        "name": "Allen Zang"
                    },
                    {
                        "name": "Ilia Sucholutsky"
                    },
                    {
                        "name": "Arina Kharlamova"
                    },
                    {
                        "name": "Daniil Orel"
                    },
                    {
                        "name": "Vladislav Poritski"
                    },
                    {
                        "name": "Shalev Ben-David"
                    },
                    {
                        "name": "Zachary Berger"
                    },
                    {
                        "name": "Parker Whitfill"
                    },
                    {
                        "name": "Michael Foster"
                    },
                    {
                        "name": "Daniel Munro"
                    },
                    {
                        "name": "Linh Ho"
                    },
                    {
                        "name": "Shankar Sivarajan"
                    },
                    {
                        "name": "Dan Bar Hava"
                    },
                    {
                        "name": "Aleksey Kuchkin"
                    },
                    {
                        "name": "David Holmes"
                    },
                    {
                        "name": "Alexandra Rodriguez-Romero"
                    },
                    {
                        "name": "Frank Sommerhage"
                    },
                    {
                        "name": "Anji Zhang"
                    },
                    {
                        "name": "Richard Moat"
                    },
                    {
                        "name": "Keith Schneider"
                    },
                    {
                        "name": "Zakayo Kazibwe"
                    },
                    {
                        "name": "Don Clarke"
                    },
                    {
                        "name": "Dae Hyun Kim"
                    },
                    {
                        "name": "Felipe Meneguitti Dias"
                    },
                    {
                        "name": "Sara Fish"
                    },
                    {
                        "name": "Veit Elser"
                    },
                    {
                        "name": "Tobias Kreiman"
                    },
                    {
                        "name": "Victor Efren Guadarrama Vilchis"
                    },
                    {
                        "name": "Immo Klose"
                    },
                    {
                        "name": "Ujjwala Anantheswaran"
                    },
                    {
                        "name": "Adam Zweiger"
                    },
                    {
                        "name": "Kaivalya Rawal"
                    },
                    {
                        "name": "Jeffery Li"
                    },
                    {
                        "name": "Jeremy Nguyen"
                    },
                    {
                        "name": "Nicolas Daans"
                    },
                    {
                        "name": "Haline Heidinger"
                    },
                    {
                        "name": "Maksim Radionov"
                    },
                    {
                        "name": "Vclav Rozho"
                    },
                    {
                        "name": "Vincent Ginis"
                    },
                    {
                        "name": "Christian Stump"
                    },
                    {
                        "name": "Niv Cohen"
                    },
                    {
                        "name": "Rafa Powiata"
                    },
                    {
                        "name": "Josef Tkadlec"
                    },
                    {
                        "name": "Alan Goldfarb"
                    },
                    {
                        "name": "Chenguang Wang"
                    },
                    {
                        "name": "Piotr Padlewski"
                    },
                    {
                        "name": "Stanislaw Barzowski"
                    },
                    {
                        "name": "Kyle Montgomery"
                    },
                    {
                        "name": "Ryan Stendall"
                    },
                    {
                        "name": "Jamie Tucker-Foltz"
                    },
                    {
                        "name": "Jack Stade"
                    },
                    {
                        "name": "T. Ryan Rogers"
                    },
                    {
                        "name": "Tom Goertzen"
                    },
                    {
                        "name": "Declan Grabb"
                    },
                    {
                        "name": "Abhishek Shukla"
                    },
                    {
                        "name": "Alan Givr"
                    },
                    {
                        "name": "John Arnold Ambay"
                    },
                    {
                        "name": "Archan Sen"
                    },
                    {
                        "name": "Muhammad Fayez Aziz"
                    },
                    {
                        "name": "Mark H Inlow"
                    },
                    {
                        "name": "Hao He"
                    },
                    {
                        "name": "Ling Zhang"
                    },
                    {
                        "name": "Younesse Kaddar"
                    },
                    {
                        "name": "Ivar ngquist"
                    },
                    {
                        "name": "Yanxu Chen"
                    },
                    {
                        "name": "Harrison K Wang"
                    },
                    {
                        "name": "Kalyan Ramakrishnan"
                    },
                    {
                        "name": "Elliott Thornley"
                    },
                    {
                        "name": "Antonio Terpin"
                    },
                    {
                        "name": "Hailey Schoelkopf"
                    },
                    {
                        "name": "Eric Zheng"
                    },
                    {
                        "name": "Avishy Carmi"
                    },
                    {
                        "name": "Ethan D. L. Brown"
                    },
                    {
                        "name": "Kelin Zhu"
                    },
                    {
                        "name": "Max Bartolo"
                    },
                    {
                        "name": "Richard Wheeler"
                    },
                    {
                        "name": "Martin Stehberger"
                    },
                    {
                        "name": "Peter Bradshaw"
                    },
                    {
                        "name": "JP Heimonen"
                    },
                    {
                        "name": "Kaustubh Sridhar"
                    },
                    {
                        "name": "Ido Akov"
                    },
                    {
                        "name": "Jennifer Sandlin"
                    },
                    {
                        "name": "Yury Makarychev"
                    },
                    {
                        "name": "Joanna Tam"
                    },
                    {
                        "name": "Hieu Hoang"
                    },
                    {
                        "name": "David M. Cunningham"
                    },
                    {
                        "name": "Vladimir Goryachev"
                    },
                    {
                        "name": "Demosthenes Patramanis"
                    },
                    {
                        "name": "Michael Krause"
                    },
                    {
                        "name": "Andrew Redenti"
                    },
                    {
                        "name": "David Aldous"
                    },
                    {
                        "name": "Jesyin Lai"
                    },
                    {
                        "name": "Shannon Coleman"
                    },
                    {
                        "name": "Jiangnan Xu"
                    },
                    {
                        "name": "Sangwon Lee"
                    },
                    {
                        "name": "Ilias Magoulas"
                    },
                    {
                        "name": "Sandy Zhao"
                    },
                    {
                        "name": "Ning Tang"
                    },
                    {
                        "name": "Michael K. Cohen"
                    },
                    {
                        "name": "Orr Paradise"
                    },
                    {
                        "name": "Jan Hendrik Kirchner"
                    },
                    {
                        "name": "Maksym Ovchynnikov"
                    },
                    {
                        "name": "Jason O. Matos"
                    },
                    {
                        "name": "Adithya Shenoy"
                    },
                    {
                        "name": "Michael Wang"
                    },
                    {
                        "name": "Yuzhou Nie"
                    },
                    {
                        "name": "Anna Sztyber-Betley"
                    },
                    {
                        "name": "Paolo Faraboschi"
                    },
                    {
                        "name": "Robin Riblet"
                    },
                    {
                        "name": "Jonathan Crozier"
                    },
                    {
                        "name": "Shiv Halasyamani"
                    },
                    {
                        "name": "Shreyas Verma"
                    },
                    {
                        "name": "Prashant Joshi"
                    },
                    {
                        "name": "Eli Meril"
                    },
                    {
                        "name": "Ziqiao Ma"
                    },
                    {
                        "name": "Jrmy Androletti"
                    },
                    {
                        "name": "Raghav Singhal"
                    },
                    {
                        "name": "Jacob Platnick"
                    },
                    {
                        "name": "Volodymyr Nevirkovets"
                    },
                    {
                        "name": "Luke Basler"
                    },
                    {
                        "name": "Alexander Ivanov"
                    },
                    {
                        "name": "Seri Khoury"
                    },
                    {
                        "name": "Nils Gustafsson"
                    },
                    {
                        "name": "Marco Piccardo"
                    },
                    {
                        "name": "Hamid Mostaghimi"
                    },
                    {
                        "name": "Qijia Chen"
                    },
                    {
                        "name": "Virendra Singh"
                    },
                    {
                        "name": "Tran Quoc Khnh"
                    },
                    {
                        "name": "Paul Rosu"
                    },
                    {
                        "name": "Hannah Szlyk"
                    },
                    {
                        "name": "Zachary Brown"
                    },
                    {
                        "name": "Himanshu Narayan"
                    },
                    {
                        "name": "Aline Menezes"
                    },
                    {
                        "name": "Jonathan Roberts"
                    },
                    {
                        "name": "William Alley"
                    },
                    {
                        "name": "Kunyang Sun"
                    },
                    {
                        "name": "Arkil Patel"
                    },
                    {
                        "name": "Max Lamparth"
                    },
                    {
                        "name": "Anka Reuel"
                    },
                    {
                        "name": "Linwei Xin"
                    },
                    {
                        "name": "Hanmeng Xu"
                    },
                    {
                        "name": "Jacob Loader"
                    },
                    {
                        "name": "Freddie Martin"
                    },
                    {
                        "name": "Zixuan Wang"
                    },
                    {
                        "name": "Andrea Achilleos"
                    },
                    {
                        "name": "Thomas Preu"
                    },
                    {
                        "name": "Tomek Korbak"
                    },
                    {
                        "name": "Ida Bosio"
                    },
                    {
                        "name": "Fereshteh Kazemi"
                    },
                    {
                        "name": "Ziye Chen"
                    },
                    {
                        "name": "Bir Blint"
                    },
                    {
                        "name": "Eve J. Y. Lo"
                    },
                    {
                        "name": "Jiaqi Wang"
                    },
                    {
                        "name": "Maria Ins S. Nunes"
                    },
                    {
                        "name": "Jeremiah Milbauer"
                    },
                    {
                        "name": "M Saiful Bari"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Behzad Ansarinejad"
                    },
                    {
                        "name": "Yewen Sun"
                    },
                    {
                        "name": "Stephane Durand"
                    },
                    {
                        "name": "Hossam Elgnainy"
                    },
                    {
                        "name": "Guillaume Douville"
                    },
                    {
                        "name": "Daniel Tordera"
                    },
                    {
                        "name": "George Balabanian"
                    },
                    {
                        "name": "Hew Wolff"
                    },
                    {
                        "name": "Lynna Kvistad"
                    },
                    {
                        "name": "Hsiaoyun Milliron"
                    },
                    {
                        "name": "Ahmad Sakor"
                    },
                    {
                        "name": "Murat Eron"
                    },
                    {
                        "name": "Andrew Favre D. O."
                    },
                    {
                        "name": "Shailesh Shah"
                    },
                    {
                        "name": "Xiaoxiang Zhou"
                    },
                    {
                        "name": "Firuz Kamalov"
                    },
                    {
                        "name": "Sherwin Abdoli"
                    },
                    {
                        "name": "Tim Santens"
                    },
                    {
                        "name": "Shaul Barkan"
                    },
                    {
                        "name": "Allison Tee"
                    },
                    {
                        "name": "Robin Zhang"
                    },
                    {
                        "name": "Alessandro Tomasiello"
                    },
                    {
                        "name": "G. Bruno De Luca"
                    },
                    {
                        "name": "Shi-Zhuo Looi"
                    },
                    {
                        "name": "Vinh-Kha Le"
                    },
                    {
                        "name": "Noam Kolt"
                    },
                    {
                        "name": "Jiayi Pan"
                    },
                    {
                        "name": "Emma Rodman"
                    },
                    {
                        "name": "Jacob Drori"
                    },
                    {
                        "name": "Carl J Fossum"
                    },
                    {
                        "name": "Niklas Muennighoff"
                    },
                    {
                        "name": "Milind Jagota"
                    },
                    {
                        "name": "Ronak Pradeep"
                    },
                    {
                        "name": "Honglu Fan"
                    },
                    {
                        "name": "Jonathan Eicher"
                    },
                    {
                        "name": "Michael Chen"
                    },
                    {
                        "name": "Kushal Thaman"
                    },
                    {
                        "name": "William Merrill"
                    },
                    {
                        "name": "Moritz Firsching"
                    },
                    {
                        "name": "Carter Harris"
                    },
                    {
                        "name": "Stefan Ciobc"
                    },
                    {
                        "name": "Jason Gross"
                    },
                    {
                        "name": "Rohan Pandey"
                    },
                    {
                        "name": "Ilya Gusev"
                    },
                    {
                        "name": "Adam Jones"
                    },
                    {
                        "name": "Shashank Agnihotri"
                    },
                    {
                        "name": "Pavel Zhelnov"
                    },
                    {
                        "name": "Mohammadreza Mofayezi"
                    },
                    {
                        "name": "Alexander Piperski"
                    },
                    {
                        "name": "David K. Zhang"
                    },
                    {
                        "name": "Kostiantyn Dobarskyi"
                    },
                    {
                        "name": "Roman Leventov"
                    },
                    {
                        "name": "Ignat Soroko"
                    },
                    {
                        "name": "Joshua Duersch"
                    },
                    {
                        "name": "Vage Taamazyan"
                    },
                    {
                        "name": "Andrew Ho"
                    },
                    {
                        "name": "Wenjie Ma"
                    },
                    {
                        "name": "William Held"
                    },
                    {
                        "name": "Ruicheng Xian"
                    },
                    {
                        "name": "Armel Randy Zebaze"
                    },
                    {
                        "name": "Mohanad Mohamed"
                    },
                    {
                        "name": "Julian Noah Leser"
                    },
                    {
                        "name": "Michelle X Yuan"
                    },
                    {
                        "name": "Laila Yacar"
                    },
                    {
                        "name": "Johannes Lengler"
                    },
                    {
                        "name": "Katarzyna Olszewska"
                    },
                    {
                        "name": "Claudio Di Fratta"
                    },
                    {
                        "name": "Edson Oliveira"
                    },
                    {
                        "name": "Joseph W. Jackson"
                    },
                    {
                        "name": "Andy Zou"
                    },
                    {
                        "name": "Muthu Chidambaram"
                    },
                    {
                        "name": "Timothy Manik"
                    },
                    {
                        "name": "Hector Haffenden"
                    },
                    {
                        "name": "Dashiell Stander"
                    },
                    {
                        "name": "Ali Dasouqi"
                    },
                    {
                        "name": "Alexander Shen"
                    },
                    {
                        "name": "Bita Golshani"
                    },
                    {
                        "name": "David Stap"
                    },
                    {
                        "name": "Egor Kretov"
                    },
                    {
                        "name": "Mikalai Uzhou"
                    },
                    {
                        "name": "Alina Borisovna Zhidkovskaya"
                    },
                    {
                        "name": "Nick Winter"
                    },
                    {
                        "name": "Miguel Orbegozo Rodriguez"
                    },
                    {
                        "name": "Robert Lauff"
                    },
                    {
                        "name": "Dustin Wehr"
                    },
                    {
                        "name": "Colin Tang"
                    },
                    {
                        "name": "Zaki Hossain"
                    },
                    {
                        "name": "Shaun Phillips"
                    },
                    {
                        "name": "Fortuna Samuele"
                    },
                    {
                        "name": "Fredrik Ekstrm"
                    },
                    {
                        "name": "Angela Hammon"
                    },
                    {
                        "name": "Oam Patel"
                    },
                    {
                        "name": "Faraz Farhidi"
                    },
                    {
                        "name": "George Medley"
                    },
                    {
                        "name": "Forough Mohammadzadeh"
                    },
                    {
                        "name": "Madellene Peaflor"
                    },
                    {
                        "name": "Haile Kassahun"
                    },
                    {
                        "name": "Alena Friedrich"
                    },
                    {
                        "name": "Rayner Hernandez Perez"
                    },
                    {
                        "name": "Daniel Pyda"
                    },
                    {
                        "name": "Taom Sakal"
                    },
                    {
                        "name": "Omkar Dhamane"
                    },
                    {
                        "name": "Ali Khajegili Mirabadi"
                    },
                    {
                        "name": "Eric Hallman"
                    },
                    {
                        "name": "Kenchi Okutsu"
                    },
                    {
                        "name": "Mike Battaglia"
                    },
                    {
                        "name": "Mohammad Maghsoudimehrabani"
                    },
                    {
                        "name": "Alon Amit"
                    },
                    {
                        "name": "Dave Hulbert"
                    },
                    {
                        "name": "Roberto Pereira"
                    },
                    {
                        "name": "Simon Weber"
                    },
                    {
                        "name": "Handoko"
                    },
                    {
                        "name": "Anton Peristyy"
                    },
                    {
                        "name": "Stephen Malina"
                    },
                    {
                        "name": "Mustafa Mehkary"
                    },
                    {
                        "name": "Rami Aly"
                    },
                    {
                        "name": "Frank Reidegeld"
                    },
                    {
                        "name": "Anna-Katharina Dick"
                    },
                    {
                        "name": "Cary Friday"
                    },
                    {
                        "name": "Mukhwinder Singh"
                    },
                    {
                        "name": "Hassan Shapourian"
                    },
                    {
                        "name": "Wanyoung Kim"
                    },
                    {
                        "name": "Mariana Costa"
                    },
                    {
                        "name": "Hubeyb Gurdogan"
                    },
                    {
                        "name": "Harsh Kumar"
                    },
                    {
                        "name": "Chiara Ceconello"
                    },
                    {
                        "name": "Chao Zhuang"
                    },
                    {
                        "name": "Haon Park"
                    },
                    {
                        "name": "Micah Carroll"
                    },
                    {
                        "name": "Andrew R. Tawfeek"
                    },
                    {
                        "name": "Stefan Steinerberger"
                    },
                    {
                        "name": "Daattavya Aggarwal"
                    },
                    {
                        "name": "Michael Kirchhof"
                    },
                    {
                        "name": "Linjie Dai"
                    },
                    {
                        "name": "Evan Kim"
                    },
                    {
                        "name": "Johan Ferret"
                    },
                    {
                        "name": "Jainam Shah"
                    },
                    {
                        "name": "Yuzhou Wang"
                    },
                    {
                        "name": "Minghao Yan"
                    },
                    {
                        "name": "Krzysztof Burdzy"
                    },
                    {
                        "name": "Lixin Zhang"
                    },
                    {
                        "name": "Antonio Franca"
                    },
                    {
                        "name": "Diana T. Pham"
                    },
                    {
                        "name": "Kang Yong Loh"
                    },
                    {
                        "name": "Joshua Robinson"
                    },
                    {
                        "name": "Abram Jackson"
                    },
                    {
                        "name": "Paolo Giordano"
                    },
                    {
                        "name": "Philipp Petersen"
                    },
                    {
                        "name": "Adrian Cosma"
                    },
                    {
                        "name": "Jesus Colino"
                    },
                    {
                        "name": "Colin White"
                    },
                    {
                        "name": "Jacob Votava"
                    },
                    {
                        "name": "Vladimir Vinnikov"
                    },
                    {
                        "name": "Ethan Delaney"
                    },
                    {
                        "name": "Petr Spelda"
                    },
                    {
                        "name": "Vit Stritecky"
                    },
                    {
                        "name": "Syed M. Shahid"
                    },
                    {
                        "name": "Jean-Christophe Mourrat"
                    },
                    {
                        "name": "Lavr Vetoshkin"
                    },
                    {
                        "name": "Koen Sponselee"
                    },
                    {
                        "name": "Renas Bacho"
                    },
                    {
                        "name": "Zheng-Xin Yong"
                    },
                    {
                        "name": "Florencia de la Rosa"
                    },
                    {
                        "name": "Nathan Cho"
                    },
                    {
                        "name": "Xiuyu Li"
                    },
                    {
                        "name": "Guillaume Malod"
                    },
                    {
                        "name": "Orion Weller"
                    },
                    {
                        "name": "Guglielmo Albani"
                    },
                    {
                        "name": "Leon Lang"
                    },
                    {
                        "name": "Julien Laurendeau"
                    },
                    {
                        "name": "Dmitry Kazakov"
                    },
                    {
                        "name": "Fatimah Adesanya"
                    },
                    {
                        "name": "Julien Portier"
                    },
                    {
                        "name": "Lawrence Hollom"
                    },
                    {
                        "name": "Victor Souza"
                    },
                    {
                        "name": "Yuchen Anna Zhou"
                    },
                    {
                        "name": "Julien Degorre"
                    },
                    {
                        "name": "Yiit Yaln"
                    },
                    {
                        "name": "Gbenga Daniel Obikoya"
                    },
                    {
                        "name": "Rai"
                    },
                    {
                        "name": "Filippo Bigi"
                    },
                    {
                        "name": "M. C. Bosc"
                    },
                    {
                        "name": "Oleg Shumar"
                    },
                    {
                        "name": "Kaniuar Bacho"
                    },
                    {
                        "name": "Gabriel Recchia"
                    },
                    {
                        "name": "Mara Popescu"
                    },
                    {
                        "name": "Nikita Shulga"
                    },
                    {
                        "name": "Ngefor Mildred Tanwie"
                    },
                    {
                        "name": "Thomas C. H. Lux"
                    },
                    {
                        "name": "Ben Rank"
                    },
                    {
                        "name": "Colin Ni"
                    },
                    {
                        "name": "Matthew Brooks"
                    },
                    {
                        "name": "Alesia Yakimchyk"
                    },
                    {
                        "name": "Huanxu"
                    },
                    {
                        "name": "Liu"
                    },
                    {
                        "name": "Stefano Cavalleri"
                    },
                    {
                        "name": "Olle Hggstrm"
                    },
                    {
                        "name": "Emil Verkama"
                    },
                    {
                        "name": "Joshua Newbould"
                    },
                    {
                        "name": "Hans Gundlach"
                    },
                    {
                        "name": "Leonor Brito-Santana"
                    },
                    {
                        "name": "Brian Amaro"
                    },
                    {
                        "name": "Vivek Vajipey"
                    },
                    {
                        "name": "Rynaa Grover"
                    },
                    {
                        "name": "Ting Wang"
                    },
                    {
                        "name": "Yosi Kratish"
                    },
                    {
                        "name": "Wen-Ding Li"
                    },
                    {
                        "name": "Sivakanth Gopi"
                    },
                    {
                        "name": "Andrea Caciolai"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "Pablo Hernndez-Cmara"
                    },
                    {
                        "name": "Emanuele Rodol"
                    },
                    {
                        "name": "Jules Robins"
                    },
                    {
                        "name": "Dominic Williamson"
                    },
                    {
                        "name": "Vincent Cheng"
                    },
                    {
                        "name": "Brad Raynor"
                    },
                    {
                        "name": "Hao Qi"
                    },
                    {
                        "name": "Ben Segev"
                    },
                    {
                        "name": "Jingxuan Fan"
                    },
                    {
                        "name": "Sarah Martinson"
                    },
                    {
                        "name": "Erik Y. Wang"
                    },
                    {
                        "name": "Kaylie Hausknecht"
                    },
                    {
                        "name": "Michael P. Brenner"
                    },
                    {
                        "name": "Mao Mao"
                    },
                    {
                        "name": "Christoph Demian"
                    },
                    {
                        "name": "Peyman Kassani"
                    },
                    {
                        "name": "Xinyu Zhang"
                    },
                    {
                        "name": "David Avagian"
                    },
                    {
                        "name": "Eshawn Jessica Scipio"
                    },
                    {
                        "name": "Alon Ragoler"
                    },
                    {
                        "name": "Justin Tan"
                    },
                    {
                        "name": "Blake Sims"
                    },
                    {
                        "name": "Rebeka Plecnik"
                    },
                    {
                        "name": "Aaron Kirtland"
                    },
                    {
                        "name": "Omer Faruk Bodur"
                    },
                    {
                        "name": "D. P. Shinde"
                    },
                    {
                        "name": "Yan Carlos Leyva Labrador"
                    },
                    {
                        "name": "Zahra Adoul"
                    },
                    {
                        "name": "Mohamed Zekry"
                    },
                    {
                        "name": "Ali Karakoc"
                    },
                    {
                        "name": "Tania C. B. Santos"
                    },
                    {
                        "name": "Samir Shamseldeen"
                    },
                    {
                        "name": "Loukmane Karim"
                    },
                    {
                        "name": "Anna Liakhovitskaia"
                    },
                    {
                        "name": "Nate Resman"
                    },
                    {
                        "name": "Nicholas Farina"
                    },
                    {
                        "name": "Juan Carlos Gonzalez"
                    },
                    {
                        "name": "Gabe Maayan"
                    },
                    {
                        "name": "Earth Anderson"
                    },
                    {
                        "name": "Rodrigo De Oliveira Pena"
                    },
                    {
                        "name": "Elizabeth Kelley"
                    },
                    {
                        "name": "Hodjat Mariji"
                    },
                    {
                        "name": "Rasoul Pouriamanesh"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Ross Finocchio"
                    },
                    {
                        "name": "Ismail Alarab"
                    },
                    {
                        "name": "Joshua Cole"
                    },
                    {
                        "name": "Danyelle Ferreira"
                    },
                    {
                        "name": "Bryan Johnson"
                    },
                    {
                        "name": "Mohammad Safdari"
                    },
                    {
                        "name": "Liangti Dai"
                    },
                    {
                        "name": "Siriphan Arthornthurasuk"
                    },
                    {
                        "name": "Isaac C. McAlister"
                    },
                    {
                        "name": "Alejandro Jos Moyano"
                    },
                    {
                        "name": "Alexey Pronin"
                    },
                    {
                        "name": "Jing Fan"
                    },
                    {
                        "name": "Angel Ramirez-Trinidad"
                    },
                    {
                        "name": "Yana Malysheva"
                    },
                    {
                        "name": "Daphiny Pottmaier"
                    },
                    {
                        "name": "Omid Taheri"
                    },
                    {
                        "name": "Stanley Stepanic"
                    },
                    {
                        "name": "Samuel Perry"
                    },
                    {
                        "name": "Luke Askew"
                    },
                    {
                        "name": "Ral Adrin Huerta Rodrguez"
                    },
                    {
                        "name": "Ali M. R. Minissi"
                    },
                    {
                        "name": "Ricardo Lorena"
                    },
                    {
                        "name": "Krishnamurthy Iyer"
                    },
                    {
                        "name": "Arshad Anil Fasiludeen"
                    },
                    {
                        "name": "Ronald Clark"
                    },
                    {
                        "name": "Josh Ducey"
                    },
                    {
                        "name": "Matheus Piza"
                    },
                    {
                        "name": "Maja Somrak"
                    },
                    {
                        "name": "Eric Vergo"
                    },
                    {
                        "name": "Juehang Qin"
                    },
                    {
                        "name": "Benjmin Borbs"
                    },
                    {
                        "name": "Eric Chu"
                    },
                    {
                        "name": "Jack Lindsey"
                    },
                    {
                        "name": "Antoine Jallon"
                    },
                    {
                        "name": "I. M. J. McInnis"
                    },
                    {
                        "name": "Evan Chen"
                    },
                    {
                        "name": "Avi Semler"
                    },
                    {
                        "name": "Luk Gloor"
                    },
                    {
                        "name": "Tej Shah"
                    },
                    {
                        "name": "Marc Carauleanu"
                    },
                    {
                        "name": "Pascal Lauer"
                    },
                    {
                        "name": "Tran uc Huy"
                    },
                    {
                        "name": "Hossein Shahrtash"
                    },
                    {
                        "name": "Emilien Duc"
                    },
                    {
                        "name": "Lukas Lewark"
                    },
                    {
                        "name": "Assaf Brown"
                    },
                    {
                        "name": "Samuel Albanie"
                    },
                    {
                        "name": "Brian Weber"
                    },
                    {
                        "name": "Warren S. Vaz"
                    },
                    {
                        "name": "Pierre Clavier"
                    },
                    {
                        "name": "Yiyang Fan"
                    },
                    {
                        "name": "Gabriel Poesia Reis e Silva"
                    },
                    {
                        "name": "Long"
                    },
                    {
                        "name": "Lian"
                    },
                    {
                        "name": "Marcus Abramovitch"
                    },
                    {
                        "name": "Xi Jiang"
                    },
                    {
                        "name": "Sandra Mendoza"
                    },
                    {
                        "name": "Murat Islam"
                    },
                    {
                        "name": "Juan Gonzalez"
                    },
                    {
                        "name": "Vasilios Mavroudis"
                    },
                    {
                        "name": "Justin Xu"
                    },
                    {
                        "name": "Pawan Kumar"
                    },
                    {
                        "name": "Laxman Prasad Goswami"
                    },
                    {
                        "name": "Daniel Bugas"
                    },
                    {
                        "name": "Nasser Heydari"
                    },
                    {
                        "name": "Ferenc Jeanplong"
                    },
                    {
                        "name": "Thorben Jansen"
                    },
                    {
                        "name": "Antonella Pinto"
                    },
                    {
                        "name": "Archimedes Apronti"
                    },
                    {
                        "name": "Abdallah Galal"
                    },
                    {
                        "name": "Ng Ze-An"
                    },
                    {
                        "name": "Ankit Singh"
                    },
                    {
                        "name": "Tong Jiang"
                    },
                    {
                        "name": "Joan of Arc Xavier"
                    },
                    {
                        "name": "Kanu Priya Agarwal"
                    },
                    {
                        "name": "Mohammed Berkani"
                    },
                    {
                        "name": "Gang Zhang"
                    },
                    {
                        "name": "Zhehang Du"
                    },
                    {
                        "name": "Benedito Alves de Oliveira Junior"
                    },
                    {
                        "name": "Dmitry Malishev"
                    },
                    {
                        "name": "Nicolas Remy"
                    },
                    {
                        "name": "Taylor D. Hartman"
                    },
                    {
                        "name": "Tim Tarver"
                    },
                    {
                        "name": "Stephen Mensah"
                    },
                    {
                        "name": "Gautier Abou Loume"
                    },
                    {
                        "name": "Wiktor Morak"
                    },
                    {
                        "name": "Farzad Habibi"
                    },
                    {
                        "name": "Sarah Hoback"
                    },
                    {
                        "name": "Will Cai"
                    },
                    {
                        "name": "Javier Gimenez"
                    },
                    {
                        "name": "Roselynn Grace Montecillo"
                    },
                    {
                        "name": "Jakub ucki"
                    },
                    {
                        "name": "Russell Campbell"
                    },
                    {
                        "name": "Asankhaya Sharma"
                    },
                    {
                        "name": "Khalida Meer"
                    },
                    {
                        "name": "Shreen Gul"
                    },
                    {
                        "name": "Daniel Espinosa Gonzalez"
                    },
                    {
                        "name": "Xavier Alapont"
                    },
                    {
                        "name": "Alex Hoover"
                    },
                    {
                        "name": "Gunjan Chhablani"
                    },
                    {
                        "name": "Freddie Vargus"
                    },
                    {
                        "name": "Arunim Agarwal"
                    },
                    {
                        "name": "Yibo Jiang"
                    },
                    {
                        "name": "Deepakkumar Patil"
                    },
                    {
                        "name": "David Outevsky"
                    },
                    {
                        "name": "Kevin Joseph Scaria"
                    },
                    {
                        "name": "Rajat Maheshwari"
                    },
                    {
                        "name": "Abdelkader Dendane"
                    },
                    {
                        "name": "Priti Shukla"
                    },
                    {
                        "name": "Ashley Cartwright"
                    },
                    {
                        "name": "Sergei Bogdanov"
                    },
                    {
                        "name": "Niels Mndler"
                    },
                    {
                        "name": "Sren Mller"
                    },
                    {
                        "name": "Luca Arnaboldi"
                    },
                    {
                        "name": "Kunvar Thaman"
                    },
                    {
                        "name": "Muhammad Rehan Siddiqi"
                    },
                    {
                        "name": "Prajvi Saxena"
                    },
                    {
                        "name": "Himanshu Gupta"
                    },
                    {
                        "name": "Tony Fruhauff"
                    },
                    {
                        "name": "Glen Sherman"
                    },
                    {
                        "name": "Mtys Vincze"
                    },
                    {
                        "name": "Siranut Usawasutsakorn"
                    },
                    {
                        "name": "Dylan Ler"
                    },
                    {
                        "name": "Anil Radhakrishnan"
                    },
                    {
                        "name": "Innocent Enyekwe"
                    },
                    {
                        "name": "Sk Md Salauddin"
                    },
                    {
                        "name": "Jiang Muzhen"
                    },
                    {
                        "name": "Aleksandr Maksapetyan"
                    },
                    {
                        "name": "Vivien Rossbach"
                    },
                    {
                        "name": "Chris Harjadi"
                    },
                    {
                        "name": "Mohsen Bahaloohoreh"
                    },
                    {
                        "name": "Claire Sparrow"
                    },
                    {
                        "name": "Jasdeep Sidhu"
                    },
                    {
                        "name": "Sam Ali"
                    },
                    {
                        "name": "Song Bian"
                    },
                    {
                        "name": "John Lai"
                    },
                    {
                        "name": "Eric Singer"
                    },
                    {
                        "name": "Justine Leon Uro"
                    },
                    {
                        "name": "Greg Bateman"
                    },
                    {
                        "name": "Mohamed Sayed"
                    },
                    {
                        "name": "Ahmed Menshawy"
                    },
                    {
                        "name": "Darling Duclosel"
                    },
                    {
                        "name": "Dario Bezzi"
                    },
                    {
                        "name": "Yashaswini Jain"
                    },
                    {
                        "name": "Ashley Aaron"
                    },
                    {
                        "name": "Murat Tiryakioglu"
                    },
                    {
                        "name": "Sheeshram Siddh"
                    },
                    {
                        "name": "Keith Krenek"
                    },
                    {
                        "name": "Imad Ali Shah"
                    },
                    {
                        "name": "Jun Jin"
                    },
                    {
                        "name": "Scott Creighton"
                    },
                    {
                        "name": "Denis Peskoff"
                    },
                    {
                        "name": "Zienab EL-Wasif"
                    },
                    {
                        "name": "Ragavendran P V"
                    },
                    {
                        "name": "Michael Richmond"
                    },
                    {
                        "name": "Joseph McGowan"
                    },
                    {
                        "name": "Tejal Patwardhan"
                    },
                    {
                        "name": "Hao-Yu Sun"
                    },
                    {
                        "name": "Ting Sun"
                    },
                    {
                        "name": "Nikola Zubi"
                    },
                    {
                        "name": "Samuele Sala"
                    },
                    {
                        "name": "Stephen Ebert"
                    },
                    {
                        "name": "Jean Kaddour"
                    },
                    {
                        "name": "Manuel Schottdorf"
                    },
                    {
                        "name": "Dianzhuo Wang"
                    },
                    {
                        "name": "Gerol Petruzella"
                    },
                    {
                        "name": "Alex Meiburg"
                    },
                    {
                        "name": "Tilen Medved"
                    },
                    {
                        "name": "Ali ElSheikh"
                    },
                    {
                        "name": "S Ashwin Hebbar"
                    },
                    {
                        "name": "Lorenzo Vaquero"
                    },
                    {
                        "name": "Xianjun Yang"
                    },
                    {
                        "name": "Jason Poulos"
                    },
                    {
                        "name": "Vilm Zouhar"
                    },
                    {
                        "name": "Sergey Bogdanik"
                    },
                    {
                        "name": "Mingfang Zhang"
                    },
                    {
                        "name": "Jorge Sanz-Ros"
                    },
                    {
                        "name": "David Anugraha"
                    },
                    {
                        "name": "Yinwei Dai"
                    },
                    {
                        "name": "Anh N. Nhu"
                    },
                    {
                        "name": "Xue Wang"
                    },
                    {
                        "name": "Ali Anil Demircali"
                    },
                    {
                        "name": "Zhibai Jia"
                    },
                    {
                        "name": "Yuyin Zhou"
                    },
                    {
                        "name": "Juncheng Wu"
                    },
                    {
                        "name": "Mike He"
                    },
                    {
                        "name": "Nitin Chandok"
                    },
                    {
                        "name": "Aarush Sinha"
                    },
                    {
                        "name": "Gaoxiang Luo"
                    },
                    {
                        "name": "Long Le"
                    },
                    {
                        "name": "Mickal Noy"
                    },
                    {
                        "name": "Ioannis Pantidis"
                    },
                    {
                        "name": "Tianbo Qi"
                    },
                    {
                        "name": "Soham Sachin Purohit"
                    },
                    {
                        "name": "Letitia Parcalabescu"
                    },
                    {
                        "name": "Thai-Hoa Nguyen"
                    },
                    {
                        "name": "Genta Indra Winata"
                    },
                    {
                        "name": "Edoardo M. Ponti"
                    },
                    {
                        "name": "Hanchen Li"
                    },
                    {
                        "name": "Kaustubh Dhole"
                    },
                    {
                        "name": "Jongee Park"
                    },
                    {
                        "name": "Dario Abbondanza"
                    },
                    {
                        "name": "Yuanli Wang"
                    },
                    {
                        "name": "Anupam Nayak"
                    },
                    {
                        "name": "Diogo M. Caetano"
                    },
                    {
                        "name": "Antonio A. W. L. Wong"
                    },
                    {
                        "name": "Maria del Rio-Chanona"
                    },
                    {
                        "name": "Dniel Kondor"
                    },
                    {
                        "name": "Pieter Francois"
                    },
                    {
                        "name": "Ed Chalstrey"
                    },
                    {
                        "name": "Jakob Zsambok"
                    },
                    {
                        "name": "Dan Hoyer"
                    },
                    {
                        "name": "Jenny Reddish"
                    },
                    {
                        "name": "Jakob Hauser"
                    },
                    {
                        "name": "Francisco-Javier Rodrigo-Gins"
                    },
                    {
                        "name": "Suchandra Datta"
                    },
                    {
                        "name": "Maxwell Shepherd"
                    },
                    {
                        "name": "Thom Kamphuis"
                    },
                    {
                        "name": "Qizheng Zhang"
                    },
                    {
                        "name": "Hyunjun Kim"
                    },
                    {
                        "name": "Ruiji Sun"
                    },
                    {
                        "name": "Jianzhu Yao"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Satyapriya Krishna"
                    },
                    {
                        "name": "Sina Rismanchian"
                    },
                    {
                        "name": "Bonan Pu"
                    },
                    {
                        "name": "Francesco Pinto"
                    },
                    {
                        "name": "Yingheng Wang"
                    },
                    {
                        "name": "Kumar Shridhar"
                    },
                    {
                        "name": "Kalon J. Overholt"
                    },
                    {
                        "name": "Glib Briia"
                    },
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "David"
                    },
                    {
                        "name": "Soler Bartomeu"
                    },
                    {
                        "name": "Tony CY Pang"
                    },
                    {
                        "name": "Adam Wecker"
                    },
                    {
                        "name": "Yifan Xiong"
                    },
                    {
                        "name": "Fanfei Li"
                    },
                    {
                        "name": "Lukas S. Huber"
                    },
                    {
                        "name": "Joshua Jaeger"
                    },
                    {
                        "name": "Romano De Maddalena"
                    },
                    {
                        "name": "Xing Han L"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Claas Beger"
                    },
                    {
                        "name": "Patrick Tser Jern Kon"
                    },
                    {
                        "name": "Sean Li"
                    },
                    {
                        "name": "Vivek Sanker"
                    },
                    {
                        "name": "Ming Yin"
                    },
                    {
                        "name": "Yihao Liang"
                    },
                    {
                        "name": "Xinlu Zhang"
                    },
                    {
                        "name": "Ankit Agrawal"
                    },
                    {
                        "name": "Li S. Yifei"
                    },
                    {
                        "name": "Zechen Zhang"
                    },
                    {
                        "name": "Mu Cai"
                    },
                    {
                        "name": "Yasin Sonmez"
                    },
                    {
                        "name": "Costin Cozianu"
                    },
                    {
                        "name": "Changhao Li"
                    },
                    {
                        "name": "Alex Slen"
                    },
                    {
                        "name": "Shoubin Yu"
                    },
                    {
                        "name": "Hyun Kyu Park"
                    },
                    {
                        "name": "Gabriele Sarti"
                    },
                    {
                        "name": "Marcin Briaski"
                    },
                    {
                        "name": "Alessandro Stolfo"
                    },
                    {
                        "name": "Truong An Nguyen"
                    },
                    {
                        "name": "Mike Zhang"
                    },
                    {
                        "name": "Yotam Perlitz"
                    },
                    {
                        "name": "Jose Hernandez-Orallo"
                    },
                    {
                        "name": "Runjia Li"
                    },
                    {
                        "name": "Amin Shabani"
                    },
                    {
                        "name": "Felix Juefei-Xu"
                    },
                    {
                        "name": "Shikhar Dhingra"
                    },
                    {
                        "name": "Orr Zohar"
                    },
                    {
                        "name": "My Chiffon Nguyen"
                    },
                    {
                        "name": "Alexander Pondaven"
                    },
                    {
                        "name": "Abdurrahim Yilmaz"
                    },
                    {
                        "name": "Xuandong Zhao"
                    },
                    {
                        "name": "Chuanyang Jin"
                    },
                    {
                        "name": "Muyan Jiang"
                    },
                    {
                        "name": "Stefan Todoran"
                    },
                    {
                        "name": "Xinyao Han"
                    },
                    {
                        "name": "Jules Kreuer"
                    },
                    {
                        "name": "Brian Rabern"
                    },
                    {
                        "name": "Anna Plassart"
                    },
                    {
                        "name": "Martino Maggetti"
                    },
                    {
                        "name": "Luther Yap"
                    },
                    {
                        "name": "Robert Geirhos"
                    },
                    {
                        "name": "Jonathon Kean"
                    },
                    {
                        "name": "Dingsu Wang"
                    },
                    {
                        "name": "Sina Mollaei"
                    },
                    {
                        "name": "Chenkai Sun"
                    },
                    {
                        "name": "Yifan Yin"
                    },
                    {
                        "name": "Shiqi Wang"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Yaowen Chang"
                    },
                    {
                        "name": "Anjiang Wei"
                    },
                    {
                        "name": "Alice Bizeul"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Alexandre Oliveira Arrais"
                    },
                    {
                        "name": "Kushin Mukherjee"
                    },
                    {
                        "name": "Jorge Chamorro-Padial"
                    },
                    {
                        "name": "Jiachen Liu"
                    },
                    {
                        "name": "Xingyu Qu"
                    },
                    {
                        "name": "Junyi Guan"
                    },
                    {
                        "name": "Adam Bouyamourn"
                    },
                    {
                        "name": "Shuyu Wu"
                    },
                    {
                        "name": "Martyna Plomecka"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "Mengze Tang"
                    },
                    {
                        "name": "Jiaqi Deng"
                    },
                    {
                        "name": "Shreyas Subramanian"
                    },
                    {
                        "name": "Haocheng Xi"
                    },
                    {
                        "name": "Haoxuan Chen"
                    },
                    {
                        "name": "Weizhi Zhang"
                    },
                    {
                        "name": "Yinuo Ren"
                    },
                    {
                        "name": "Haoqin Tu"
                    },
                    {
                        "name": "Sejong Kim"
                    },
                    {
                        "name": "Yushun Chen"
                    },
                    {
                        "name": "Sara Vera Marjanovi"
                    },
                    {
                        "name": "Junwoo Ha"
                    },
                    {
                        "name": "Grzegorz Luczyna"
                    },
                    {
                        "name": "Jeff J. Ma"
                    },
                    {
                        "name": "Zewen Shen"
                    },
                    {
                        "name": "Dawn Song"
                    },
                    {
                        "name": "Cedegao E. Zhang"
                    },
                    {
                        "name": "Zhun Wang"
                    },
                    {
                        "name": "Gal Gendron"
                    },
                    {
                        "name": "Yunze Xiao"
                    },
                    {
                        "name": "Leo Smucker"
                    },
                    {
                        "name": "Erica Weng"
                    },
                    {
                        "name": "Kwok Hao Lee"
                    },
                    {
                        "name": "Zhe Ye"
                    },
                    {
                        "name": "Stefano Ermon"
                    },
                    {
                        "name": "Ignacio D. Lopez-Miguel"
                    },
                    {
                        "name": "Theo Knights"
                    },
                    {
                        "name": "Anthony Gitter"
                    },
                    {
                        "name": "Namkyu Park"
                    },
                    {
                        "name": "Boyi Wei"
                    },
                    {
                        "name": "Hongzheng Chen"
                    },
                    {
                        "name": "Kunal Pai"
                    },
                    {
                        "name": "Ahmed Elkhanany"
                    },
                    {
                        "name": "Han Lin"
                    },
                    {
                        "name": "Philipp D. Siedler"
                    },
                    {
                        "name": "Jichao Fang"
                    },
                    {
                        "name": "Ritwik Mishra"
                    },
                    {
                        "name": "Kroly Zsolnai-Fehr"
                    },
                    {
                        "name": "Xilin Jiang"
                    },
                    {
                        "name": "Shadab Khan"
                    },
                    {
                        "name": "Jun Yuan"
                    },
                    {
                        "name": "Rishab Kumar Jain"
                    },
                    {
                        "name": "Xi Lin"
                    },
                    {
                        "name": "Mike Peterson"
                    },
                    {
                        "name": "Zhe Wang"
                    },
                    {
                        "name": "Aditya Malusare"
                    },
                    {
                        "name": "Maosen Tang"
                    },
                    {
                        "name": "Isha Gupta"
                    },
                    {
                        "name": "Ivan Fosin"
                    },
                    {
                        "name": "Timothy Kang"
                    },
                    {
                        "name": "Barbara Dworakowska"
                    },
                    {
                        "name": "Kazuki Matsumoto"
                    },
                    {
                        "name": "Guangyao Zheng"
                    },
                    {
                        "name": "Gerben Sewuster"
                    },
                    {
                        "name": "Jorge Pretel Villanueva"
                    },
                    {
                        "name": "Ivan Rannev"
                    },
                    {
                        "name": "Igor Chernyavsky"
                    },
                    {
                        "name": "Jiale Chen"
                    },
                    {
                        "name": "Deepayan Banik"
                    },
                    {
                        "name": "Ben Racz"
                    },
                    {
                        "name": "Wenchao Dong"
                    },
                    {
                        "name": "Jianxin Wang"
                    },
                    {
                        "name": "Laila Bashmal"
                    },
                    {
                        "name": "Duarte V. Gonalves"
                    },
                    {
                        "name": "Wei Hu"
                    },
                    {
                        "name": "Kaushik Bar"
                    },
                    {
                        "name": "Ondrej Bohdal"
                    },
                    {
                        "name": "Atharv Singh Patlan"
                    },
                    {
                        "name": "Shehzaad Dhuliawala"
                    },
                    {
                        "name": "Caroline Geirhos"
                    },
                    {
                        "name": "Julien Wist"
                    },
                    {
                        "name": "Yuval Kansal"
                    },
                    {
                        "name": "Bingsen Chen"
                    },
                    {
                        "name": "Kutay Tire"
                    },
                    {
                        "name": "Atak Talay Ycel"
                    },
                    {
                        "name": "Brandon Christof"
                    },
                    {
                        "name": "Veerupaksh Singla"
                    },
                    {
                        "name": "Zijian Song"
                    },
                    {
                        "name": "Sanxing Chen"
                    },
                    {
                        "name": "Jiaxin Ge"
                    },
                    {
                        "name": "Kaustubh Ponkshe"
                    },
                    {
                        "name": "Isaac Park"
                    },
                    {
                        "name": "Tianneng Shi"
                    },
                    {
                        "name": "Martin Q. Ma"
                    },
                    {
                        "name": "Joshua Mak"
                    },
                    {
                        "name": "Sherwin Lai"
                    },
                    {
                        "name": "Antoine Moulin"
                    },
                    {
                        "name": "Zhuo Cheng"
                    },
                    {
                        "name": "Zhanda Zhu"
                    },
                    {
                        "name": "Ziyi Zhang"
                    },
                    {
                        "name": "Vaidehi Patil"
                    },
                    {
                        "name": "Ketan Jha"
                    },
                    {
                        "name": "Qiutong Men"
                    },
                    {
                        "name": "Jiaxuan Wu"
                    },
                    {
                        "name": "Tianchi Zhang"
                    },
                    {
                        "name": "Bruno Hebling Vieira"
                    },
                    {
                        "name": "Alham Fikri Aji"
                    },
                    {
                        "name": "Jae-Won Chung"
                    },
                    {
                        "name": "Mohammed Mahfoud"
                    },
                    {
                        "name": "Ha Thi Hoang"
                    },
                    {
                        "name": "Marc Sperzel"
                    },
                    {
                        "name": "Wei Hao"
                    },
                    {
                        "name": "Kristof Meding"
                    },
                    {
                        "name": "Sihan Xu"
                    },
                    {
                        "name": "Vassilis Kostakos"
                    },
                    {
                        "name": "Davide Manini"
                    },
                    {
                        "name": "Yueying Liu"
                    },
                    {
                        "name": "Christopher Toukmaji"
                    },
                    {
                        "name": "Jay Paek"
                    },
                    {
                        "name": "Eunmi Yu"
                    },
                    {
                        "name": "Arif Engin Demircali"
                    },
                    {
                        "name": "Zhiyi Sun"
                    },
                    {
                        "name": "Ivan Dewerpe"
                    },
                    {
                        "name": "Hongsen Qin"
                    },
                    {
                        "name": "Roman Pflugfelder"
                    },
                    {
                        "name": "James Bailey"
                    },
                    {
                        "name": "Johnathan Morris"
                    },
                    {
                        "name": "Ville Heilala"
                    },
                    {
                        "name": "Sybille Rosset"
                    },
                    {
                        "name": "Zishun Yu"
                    },
                    {
                        "name": "Peter E. Chen"
                    },
                    {
                        "name": "Woongyeong Yeo"
                    },
                    {
                        "name": "Eeshaan Jain"
                    },
                    {
                        "name": "Ryan Yang"
                    },
                    {
                        "name": "Sreekar Chigurupati"
                    },
                    {
                        "name": "Julia Chernyavsky"
                    },
                    {
                        "name": "Sai Prajwal Reddy"
                    },
                    {
                        "name": "Subhashini Venugopalan"
                    },
                    {
                        "name": "Hunar Batra"
                    },
                    {
                        "name": "Core Francisco Park"
                    },
                    {
                        "name": "Hieu Tran"
                    },
                    {
                        "name": "Guilherme Maximiano"
                    },
                    {
                        "name": "Genghan Zhang"
                    },
                    {
                        "name": "Yizhuo Liang"
                    },
                    {
                        "name": "Hu Shiyu"
                    },
                    {
                        "name": "Rongwu Xu"
                    },
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Siddharth Suresh"
                    },
                    {
                        "name": "Ziqi Liu"
                    },
                    {
                        "name": "Samaksh Gulati"
                    },
                    {
                        "name": "Songyang Zhang"
                    },
                    {
                        "name": "Peter Turchin"
                    },
                    {
                        "name": "Christopher W. Bartlett"
                    },
                    {
                        "name": "Christopher R. Scotese"
                    },
                    {
                        "name": "Phuong M. Cao"
                    },
                    {
                        "name": "Aakaash Nattanmai"
                    },
                    {
                        "name": "Gordon McKellips"
                    },
                    {
                        "name": "Anish Cheraku"
                    },
                    {
                        "name": "Asim Suhail"
                    },
                    {
                        "name": "Ethan Luo"
                    },
                    {
                        "name": "Marvin Deng"
                    },
                    {
                        "name": "Jason Luo"
                    },
                    {
                        "name": "Ashley Zhang"
                    },
                    {
                        "name": "Kavin Jindel"
                    },
                    {
                        "name": "Jay Paek"
                    },
                    {
                        "name": "Kasper Halevy"
                    },
                    {
                        "name": "Allen Baranov"
                    },
                    {
                        "name": "Michael Liu"
                    },
                    {
                        "name": "Advaith Avadhanam"
                    },
                    {
                        "name": "David Zhang"
                    },
                    {
                        "name": "Vincent Cheng"
                    },
                    {
                        "name": "Brad Ma"
                    },
                    {
                        "name": "Evan Fu"
                    },
                    {
                        "name": "Liam Do"
                    },
                    {
                        "name": "Joshua Lass"
                    },
                    {
                        "name": "Hubert Yang"
                    },
                    {
                        "name": "Surya Sunkari"
                    },
                    {
                        "name": "Vishruth Bharath"
                    },
                    {
                        "name": "Violet Ai"
                    },
                    {
                        "name": "James Leung"
                    },
                    {
                        "name": "Rishit Agrawal"
                    },
                    {
                        "name": "Alan Zhou"
                    },
                    {
                        "name": "Kevin Chen"
                    },
                    {
                        "name": "Tejas Kalpathi"
                    },
                    {
                        "name": "Ziqi Xu"
                    },
                    {
                        "name": "Gavin Wang"
                    },
                    {
                        "name": "Tyler Xiao"
                    },
                    {
                        "name": "Erik Maung"
                    },
                    {
                        "name": "Sam Lee"
                    },
                    {
                        "name": "Ryan Yang"
                    },
                    {
                        "name": "Roy Yue"
                    },
                    {
                        "name": "Ben Zhao"
                    },
                    {
                        "name": "Julia Yoon"
                    },
                    {
                        "name": "Sunny Sun"
                    },
                    {
                        "name": "Aryan Singh"
                    },
                    {
                        "name": "Ethan Luo"
                    },
                    {
                        "name": "Clark Peng"
                    },
                    {
                        "name": "Tyler Osbey"
                    },
                    {
                        "name": "Taozhi Wang"
                    },
                    {
                        "name": "Daryl Echeazu"
                    },
                    {
                        "name": "Hubert Yang"
                    },
                    {
                        "name": "Timothy Wu"
                    },
                    {
                        "name": "Spandan Patel"
                    },
                    {
                        "name": "Vidhi Kulkarni"
                    },
                    {
                        "name": "Vijaykaarti Sundarapandiyan"
                    },
                    {
                        "name": "Ashley Zhang"
                    },
                    {
                        "name": "Andrew Le"
                    },
                    {
                        "name": "Zafir Nasim"
                    },
                    {
                        "name": "Srikar Yalam"
                    },
                    {
                        "name": "Ritesh Kasamsetty"
                    },
                    {
                        "name": "Soham Samal"
                    },
                    {
                        "name": "Hubert Yang"
                    },
                    {
                        "name": "David Sun"
                    },
                    {
                        "name": "Nihar Shah"
                    },
                    {
                        "name": "Abhijeet Saha"
                    },
                    {
                        "name": "Alex Zhang"
                    },
                    {
                        "name": "Leon Nguyen"
                    },
                    {
                        "name": "Laasya Nagumalli"
                    },
                    {
                        "name": "Kaixin Wang"
                    },
                    {
                        "name": "Alan Zhou"
                    },
                    {
                        "name": "Aidan Wu"
                    },
                    {
                        "name": "Jason Luo"
                    },
                    {
                        "name": "Anwith Telluri"
                    },
                    {
                        "name": "Summer Yue"
                    },
                    {
                        "name": "Alexandr Wang"
                    },
                    {
                        "name": "Dan Hendrycks"
                    }
                ],
                "author_detail": {
                    "name": "Dan Hendrycks"
                },
                "arxiv_affiliation": "Quod",
                "author": "Dan Hendrycks",
                "arxiv_comment": "29 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14249v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14249v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08231v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08231v1",
                "updated": "2025-04-11T03:30:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    3,
                    30,
                    26,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T03:30:26Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    3,
                    30,
                    26,
                    4,
                    101,
                    0
                ],
                "title": "Out of Style: RAG's Fragility to Linguistic Variation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out of Style: RAG's Fragility to Linguistic Variation"
                },
                "summary": "Despite the impressive performance of Retrieval-augmented Generation (RAG)\nsystems across various NLP benchmarks, their robustness in handling real-world\nuser-LLM interaction queries remains largely underexplored. This presents a\ncritical gap for practical deployment, where user queries exhibit greater\nlinguistic variations and can trigger cascading errors across interdependent\nRAG components. In this work, we systematically analyze how varying four\nlinguistic dimensions (formality, readability, politeness, and grammatical\ncorrectness) impact RAG performance. We evaluate two retrieval models and nine\nLLMs, ranging from 3 to 72 billion parameters, across four information-seeking\nQuestion Answering (QA) datasets. Our results reveal that linguistic\nreformulations significantly impact both retrieval and generation stages,\nleading to a relative performance drop of up to 40.41% in Recall@5 scores for\nless formal queries and 38.86% in answer match scores for queries containing\ngrammatical errors. Notably, RAG systems exhibit greater sensitivity to such\nvariations compared to LLM-only generations, highlighting their vulnerability\nto error propagation due to linguistic shifts. These findings highlight the\nneed for improved robustness techniques to enhance reliability in diverse user\ninteractions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the impressive performance of Retrieval-augmented Generation (RAG)\nsystems across various NLP benchmarks, their robustness in handling real-world\nuser-LLM interaction queries remains largely underexplored. This presents a\ncritical gap for practical deployment, where user queries exhibit greater\nlinguistic variations and can trigger cascading errors across interdependent\nRAG components. In this work, we systematically analyze how varying four\nlinguistic dimensions (formality, readability, politeness, and grammatical\ncorrectness) impact RAG performance. We evaluate two retrieval models and nine\nLLMs, ranging from 3 to 72 billion parameters, across four information-seeking\nQuestion Answering (QA) datasets. Our results reveal that linguistic\nreformulations significantly impact both retrieval and generation stages,\nleading to a relative performance drop of up to 40.41% in Recall@5 scores for\nless formal queries and 38.86% in answer match scores for queries containing\ngrammatical errors. Notably, RAG systems exhibit greater sensitivity to such\nvariations compared to LLM-only generations, highlighting their vulnerability\nto error propagation due to linguistic shifts. These findings highlight the\nneed for improved robustness techniques to enhance reliability in diverse user\ninteractions."
                },
                "authors": [
                    {
                        "name": "Tianyu Cao"
                    },
                    {
                        "name": "Neel Bhandari"
                    },
                    {
                        "name": "Akhila Yerukola"
                    },
                    {
                        "name": "Akari Asai"
                    },
                    {
                        "name": "Maarten Sap"
                    }
                ],
                "author_detail": {
                    "name": "Maarten Sap"
                },
                "author": "Maarten Sap",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08231v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08231v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10137v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10137v2",
                "updated": "2025-04-11T03:17:52Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    3,
                    17,
                    52,
                    4,
                    101,
                    0
                ],
                "published": "2024-12-13T13:38:41Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    13,
                    38,
                    41,
                    4,
                    348,
                    0
                ],
                "title": "Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous\n  Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous\n  Environments"
                },
                "summary": "We address the task of Vision-Language Navigation in Continuous Environments\n(VLN-CE) under the zero-shot setting. Zero-shot VLN-CE is particularly\nchallenging due to the absence of expert demonstrations for training and\nminimal environment structural prior to guide navigation. To confront these\nchallenges, we propose a Constraint-Aware Navigator (CA-Nav), which reframes\nzero-shot VLN-CE as a sequential, constraint-aware sub-instruction completion\nprocess. CA-Nav continuously translates sub-instructions into navigation plans\nusing two core modules: the Constraint-Aware Sub-instruction Manager (CSM) and\nthe Constraint-Aware Value Mapper (CVM). CSM defines the completion criteria\nfor decomposed sub-instructions as constraints and tracks navigation progress\nby switching sub-instructions in a constraint-aware manner. CVM, guided by\nCSM's constraints, generates a value map on the fly and refines it using\nsuperpixel clustering to improve navigation stability. CA-Nav achieves the\nstate-of-the-art performance on two VLN-CE benchmarks, surpassing the previous\nbest method by 12 percent and 13 percent in Success Rate on the validation\nunseen splits of R2R-CE and RxR-CE, respectively. Moreover, CA-Nav demonstrates\nits effectiveness in real-world robot deployments across various indoor scenes\nand instructions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the task of Vision-Language Navigation in Continuous Environments\n(VLN-CE) under the zero-shot setting. Zero-shot VLN-CE is particularly\nchallenging due to the absence of expert demonstrations for training and\nminimal environment structural prior to guide navigation. To confront these\nchallenges, we propose a Constraint-Aware Navigator (CA-Nav), which reframes\nzero-shot VLN-CE as a sequential, constraint-aware sub-instruction completion\nprocess. CA-Nav continuously translates sub-instructions into navigation plans\nusing two core modules: the Constraint-Aware Sub-instruction Manager (CSM) and\nthe Constraint-Aware Value Mapper (CVM). CSM defines the completion criteria\nfor decomposed sub-instructions as constraints and tracks navigation progress\nby switching sub-instructions in a constraint-aware manner. CVM, guided by\nCSM's constraints, generates a value map on the fly and refines it using\nsuperpixel clustering to improve navigation stability. CA-Nav achieves the\nstate-of-the-art performance on two VLN-CE benchmarks, surpassing the previous\nbest method by 12 percent and 13 percent in Success Rate on the validation\nunseen splits of R2R-CE and RxR-CE, respectively. Moreover, CA-Nav demonstrates\nits effectiveness in real-world robot deployments across various indoor scenes\nand instructions."
                },
                "authors": [
                    {
                        "name": "Kehan Chen"
                    },
                    {
                        "name": "Dong An"
                    },
                    {
                        "name": "Yan Huang"
                    },
                    {
                        "name": "Rongtao Xu"
                    },
                    {
                        "name": "Yifei Su"
                    },
                    {
                        "name": "Yonggen Ling"
                    },
                    {
                        "name": "Ian Reid"
                    },
                    {
                        "name": "Liang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Liang Wang"
                },
                "author": "Liang Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10137v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10137v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08225v1",
                "updated": "2025-04-11T03:16:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    3,
                    16,
                    3,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T03:16:03Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    3,
                    16,
                    3,
                    4,
                    101,
                    0
                ],
                "title": "A Hybrid Cloud Management Plane for Data Processing Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid Cloud Management Plane for Data Processing Pipelines"
                },
                "summary": "As organizations increasingly rely on data-driven insights, the ability to\nrun data intensive applications seamlessly across multiple cloud environments\nbecomes critical for tapping into cloud innovations while complying with\nvarious security and regulatory requirements. However, big data application\ndevelopment and deployment remain challenging to accomplish in such\nenvironments. With the increasing containerization and modernization of big\ndata applications, we argue that a unified control/management plane now makes\nsense for running these applications in hybrid cloud environments. To this end,\nwe study the problem of building a generic hybrid-cloud management plane to\nradically simplify managing big data applications. A generic architecture for\nhybrid-cloud management, called Titchener, is proposed in this paper. Titchener\ncomprises of independent and loosely coupled local control planes interacting\nwith a highly available public cloud hosted global management plane. We\ndescribe a possible instantiation of Titchener based on Kubernetes and address\nissues related to global service discovery, network connectivity and access\ncontrol enforcement. We also validate our proposed designs with a real\nmanagement plane implementation based on a popular big data workflow\norchestration in hybrid-cloud environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As organizations increasingly rely on data-driven insights, the ability to\nrun data intensive applications seamlessly across multiple cloud environments\nbecomes critical for tapping into cloud innovations while complying with\nvarious security and regulatory requirements. However, big data application\ndevelopment and deployment remain challenging to accomplish in such\nenvironments. With the increasing containerization and modernization of big\ndata applications, we argue that a unified control/management plane now makes\nsense for running these applications in hybrid cloud environments. To this end,\nwe study the problem of building a generic hybrid-cloud management plane to\nradically simplify managing big data applications. A generic architecture for\nhybrid-cloud management, called Titchener, is proposed in this paper. Titchener\ncomprises of independent and loosely coupled local control planes interacting\nwith a highly available public cloud hosted global management plane. We\ndescribe a possible instantiation of Titchener based on Kubernetes and address\nissues related to global service discovery, network connectivity and access\ncontrol enforcement. We also validate our proposed designs with a real\nmanagement plane implementation based on a popular big data workflow\norchestration in hybrid-cloud environments."
                },
                "authors": [
                    {
                        "name": "Vignesh Babu"
                    },
                    {
                        "name": "Feng Lu"
                    },
                    {
                        "name": "Haotian Wu"
                    },
                    {
                        "name": "Cameron Moberg"
                    }
                ],
                "author_detail": {
                    "name": "Cameron Moberg"
                },
                "author": "Cameron Moberg",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08222v1",
                "updated": "2025-04-11T03:05:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    3,
                    5,
                    35,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T03:05:35Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    3,
                    5,
                    35,
                    4,
                    101,
                    0
                ],
                "title": "F$^3$Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F$^3$Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from\n  Videos"
                },
                "summary": "Analyzing Fast, Frequent, and Fine-grained (F$^3$) events presents a\nsignificant challenge in video analytics and multi-modal LLMs. Current methods\nstruggle to identify events that satisfy all the F$^3$ criteria with high\naccuracy due to challenges such as motion blur and subtle visual discrepancies.\nTo advance research in video understanding, we introduce F$^3$Set, a benchmark\nthat consists of video datasets for precise F$^3$ event detection. Datasets in\nF$^3$Set are characterized by their extensive scale and comprehensive detail,\nusually encompassing over 1,000 event types with precise timestamps and\nsupporting multi-level granularity. Currently, F$^3$Set contains several sports\ndatasets, and this framework may be extended to other applications as well. We\nevaluated popular temporal action understanding methods on F$^3$Set, revealing\nsubstantial challenges for existing techniques. Additionally, we propose a new\nmethod, F$^3$ED, for F$^3$ event detections, achieving superior performance.\nThe dataset, model, and benchmark code are available at\nhttps://github.com/F3Set/F3Set.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analyzing Fast, Frequent, and Fine-grained (F$^3$) events presents a\nsignificant challenge in video analytics and multi-modal LLMs. Current methods\nstruggle to identify events that satisfy all the F$^3$ criteria with high\naccuracy due to challenges such as motion blur and subtle visual discrepancies.\nTo advance research in video understanding, we introduce F$^3$Set, a benchmark\nthat consists of video datasets for precise F$^3$ event detection. Datasets in\nF$^3$Set are characterized by their extensive scale and comprehensive detail,\nusually encompassing over 1,000 event types with precise timestamps and\nsupporting multi-level granularity. Currently, F$^3$Set contains several sports\ndatasets, and this framework may be extended to other applications as well. We\nevaluated popular temporal action understanding methods on F$^3$Set, revealing\nsubstantial challenges for existing techniques. Additionally, we propose a new\nmethod, F$^3$ED, for F$^3$ event detections, achieving superior performance.\nThe dataset, model, and benchmark code are available at\nhttps://github.com/F3Set/F3Set."
                },
                "authors": [
                    {
                        "name": "Zhaoyu Liu"
                    },
                    {
                        "name": "Kan Jiang"
                    },
                    {
                        "name": "Murong Ma"
                    },
                    {
                        "name": "Zhe Hou"
                    },
                    {
                        "name": "Yun Lin"
                    },
                    {
                        "name": "Jin Song Dong"
                    }
                ],
                "author_detail": {
                    "name": "Jin Song Dong"
                },
                "author": "Jin Song Dong",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations\n  (ICLR 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11589v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11589v5",
                "updated": "2025-04-11T02:52:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    52,
                    59,
                    4,
                    101,
                    0
                ],
                "published": "2024-06-17T14:34:14Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    14,
                    34,
                    14,
                    0,
                    169,
                    0
                ],
                "title": "CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with\n  Test-Driven Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoSQA+: Pioneering the Multi-Choice Code Search Benchmark with\n  Test-Driven Agents"
                },
                "summary": "Semantic code search, retrieving code that matches a given natural language\nquery, is an important task to improve productivity in software engineering.\nExisting code search datasets face limitations: they rely on human annotators\nwho assess code primarily through semantic understanding rather than functional\nverification, leading to potential inaccuracies and scalability issues.\nAdditionally, current evaluation metrics often overlook the multi-choice nature\nof code search. This paper introduces CoSQA+, pairing high-quality queries from\nCoSQA with multiple suitable codes. We develop an automated pipeline featuring\nmultiple model-based candidate selections and the novel test-driven agent\nannotation system. Among a single Large Language Model (LLM) annotator and\nPython expert annotators (without test-based verification), agents leverage\ntest-based verification and achieve the highest accuracy of 92.0%. Through\nextensive experiments, CoSQA+ has demonstrated superior quality over CoSQA.\nModels trained on CoSQA+ exhibit improved performance. We provide the code and\ndata at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic code search, retrieving code that matches a given natural language\nquery, is an important task to improve productivity in software engineering.\nExisting code search datasets face limitations: they rely on human annotators\nwho assess code primarily through semantic understanding rather than functional\nverification, leading to potential inaccuracies and scalability issues.\nAdditionally, current evaluation metrics often overlook the multi-choice nature\nof code search. This paper introduces CoSQA+, pairing high-quality queries from\nCoSQA with multiple suitable codes. We develop an automated pipeline featuring\nmultiple model-based candidate selections and the novel test-driven agent\nannotation system. Among a single Large Language Model (LLM) annotator and\nPython expert annotators (without test-based verification), agents leverage\ntest-based verification and achieve the highest accuracy of 92.0%. Through\nextensive experiments, CoSQA+ has demonstrated superior quality over CoSQA.\nModels trained on CoSQA+ exhibit improved performance. We provide the code and\ndata at https://github.com/DeepSoftwareAnalytics/CoSQA_Plus."
                },
                "authors": [
                    {
                        "name": "Jing Gong"
                    },
                    {
                        "name": "Yanghui Wu"
                    },
                    {
                        "name": "Linxi Liang"
                    },
                    {
                        "name": "Yanlin Wang"
                    },
                    {
                        "name": "Jiachi Chen"
                    },
                    {
                        "name": "Mingwei Liu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "arxiv_comment": "15 pages, 5 figures, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11589v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11589v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; D.2.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07158v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07158v2",
                "updated": "2025-04-11T02:47:17Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    47,
                    17,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-09T11:24:32Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    11,
                    24,
                    32,
                    2,
                    99,
                    0
                ],
                "title": "Holistic Capability Preservation: Towards Compact Yet Comprehensive\n  Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Holistic Capability Preservation: Towards Compact Yet Comprehensive\n  Reasoning Models"
                },
                "summary": "This technical report presents Ring-Lite-Distill, a lightweight reasoning\nmodel derived from our open-source Mixture-of-Experts (MoE) Large Language\nModels (LLMs) Ling-Lite. This study demonstrates that through meticulous\nhigh-quality data curation and ingenious training paradigms, the compact MoE\nmodel Ling-Lite can be further trained to achieve exceptional reasoning\ncapabilities, while maintaining its parameter-efficient architecture with only\n2.75 billion activated parameters, establishing an efficient lightweight\nreasoning architecture. In particular, in constructing this model, we have not\nmerely focused on enhancing advanced reasoning capabilities, exemplified by\nhigh-difficulty mathematical problem solving, but rather aimed to develop a\nreasoning model with more comprehensive competency coverage. Our approach\nensures coverage across reasoning tasks of varying difficulty levels while\npreserving generic capabilities, such as instruction following, tool use, and\nknowledge retention. We show that, Ring-Lite-Distill's reasoning ability\nreaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general\ncapabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The\nmodels are accessible at https://huggingface.co/inclusionAI",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Ring-Lite-Distill, a lightweight reasoning\nmodel derived from our open-source Mixture-of-Experts (MoE) Large Language\nModels (LLMs) Ling-Lite. This study demonstrates that through meticulous\nhigh-quality data curation and ingenious training paradigms, the compact MoE\nmodel Ling-Lite can be further trained to achieve exceptional reasoning\ncapabilities, while maintaining its parameter-efficient architecture with only\n2.75 billion activated parameters, establishing an efficient lightweight\nreasoning architecture. In particular, in constructing this model, we have not\nmerely focused on enhancing advanced reasoning capabilities, exemplified by\nhigh-difficulty mathematical problem solving, but rather aimed to develop a\nreasoning model with more comprehensive competency coverage. Our approach\nensures coverage across reasoning tasks of varying difficulty levels while\npreserving generic capabilities, such as instruction following, tool use, and\nknowledge retention. We show that, Ring-Lite-Distill's reasoning ability\nreaches a level comparable to DeepSeek-R1-Distill-Qwen-7B, while its general\ncapabilities significantly surpass those of DeepSeek-R1-Distill-Qwen-7B. The\nmodels are accessible at https://huggingface.co/inclusionAI"
                },
                "authors": [
                    {
                        "name": "Ling Team"
                    },
                    {
                        "name": "Caizhi Tang"
                    },
                    {
                        "name": "Chilin Fu"
                    },
                    {
                        "name": "Chunwei Wu"
                    },
                    {
                        "name": "Jia Guo"
                    },
                    {
                        "name": "Jianwen Wang"
                    },
                    {
                        "name": "Jingyu Hu"
                    },
                    {
                        "name": "Liang Jiang"
                    },
                    {
                        "name": "Meng Li"
                    },
                    {
                        "name": "Peng Jiao"
                    },
                    {
                        "name": "Pingping Liu"
                    },
                    {
                        "name": "Shaomian Zheng"
                    },
                    {
                        "name": "Shiwei Liang"
                    },
                    {
                        "name": "Shuaicheng Li"
                    },
                    {
                        "name": "Yalin Zhang"
                    },
                    {
                        "name": "Yingting Wu"
                    },
                    {
                        "name": "Yongkang Liu"
                    },
                    {
                        "name": "Zhenyu Huang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenyu Huang"
                },
                "author": "Zhenyu Huang",
                "arxiv_comment": "Based on the further discussion of the working group, the current\n  version is deemed unsuitable for release. We are currently undertaking\n  further work that is expected to involve significant revisions, but this\n  process will require some additional time. We plan to proceed with the\n  release once these updates have been fully implemented",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07158v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07158v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06514v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06514v2",
                "updated": "2025-04-11T02:36:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    36,
                    28,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-09T01:25:27Z",
                "published_parsed": [
                    2025,
                    4,
                    9,
                    1,
                    25,
                    27,
                    2,
                    99,
                    0
                ],
                "title": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Missing Premise exacerbates Overthinking: Are Reasoning Models losing\n  Critical Thinking Skill?"
                },
                "summary": "We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We find that the response length of reasoning LLMs, whether trained by\nreinforcement learning or supervised learning, drastically increases for\nill-posed questions with missing premises (MiP), ending up with redundant and\nineffective thinking. This newly introduced scenario exacerbates the general\noverthinking issue to a large extent, which we name as the MiP-Overthinking.\nSuch failures are against the ``test-time scaling law'' but have been widely\nobserved on multiple datasets we curated with MiP, indicating the harm of cheap\noverthinking and a lack of critical thinking. Surprisingly, LLMs not\nspecifically trained for reasoning exhibit much better performance on the MiP\nscenario, producing much shorter responses that quickly identify ill-posed\nqueries. This implies a critical flaw of the current training recipe for\nreasoning LLMs, which does not encourage efficient thinking adequately, leading\nto the abuse of thinking patterns. To further investigate the reasons behind\nsuch failures, we conduct fine-grained analyses of the reasoning length,\noverthinking patterns, and location of critical thinking on different types of\nLLMs. Moreover, our extended ablation study reveals that the overthinking is\ncontagious through the distillation of reasoning models' responses. These\nresults improve the understanding of overthinking and shed novel insights into\nmitigating the problem."
                },
                "authors": [
                    {
                        "name": "Chenrui Fan"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Tianyi Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Tianyi Zhou"
                },
                "author": "Tianyi Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06514v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06514v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08211v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08211v1",
                "updated": "2025-04-11T02:34:39Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    34,
                    39,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:34:39Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    34,
                    39,
                    4,
                    101,
                    0
                ],
                "title": "LLM for Comparative Narrative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM for Comparative Narrative Analysis"
                },
                "summary": "In this paper, we conducted a Multi-Perspective Comparative Narrative\nAnalysis (CNA) on three prominent LLMs: GPT-3.5, PaLM2, and Llama2. We applied\nidentical prompts and evaluated their outputs on specific tasks, ensuring an\nequitable and unbiased comparison between various LLMs. Our study revealed that\nthe three LLMs generated divergent responses to the same prompt, indicating\nnotable discrepancies in their ability to comprehend and analyze the given\ntask. Human evaluation was used as the gold standard, evaluating four\nperspectives to analyze differences in LLM performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we conducted a Multi-Perspective Comparative Narrative\nAnalysis (CNA) on three prominent LLMs: GPT-3.5, PaLM2, and Llama2. We applied\nidentical prompts and evaluated their outputs on specific tasks, ensuring an\nequitable and unbiased comparison between various LLMs. Our study revealed that\nthe three LLMs generated divergent responses to the same prompt, indicating\nnotable discrepancies in their ability to comprehend and analyze the given\ntask. Human evaluation was used as the gold standard, evaluating four\nperspectives to analyze differences in LLM performance."
                },
                "authors": [
                    {
                        "name": "Leo Kampen"
                    },
                    {
                        "name": "Carlos Rabat Villarreal"
                    },
                    {
                        "name": "Louis Yu"
                    },
                    {
                        "name": "Santu Karmaker"
                    },
                    {
                        "name": "Dongji Feng"
                    }
                ],
                "author_detail": {
                    "name": "Dongji Feng"
                },
                "author": "Dongji Feng",
                "arxiv_comment": "5 pages, 4 figures, Appendix included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08211v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08211v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08208v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08208v1",
                "updated": "2025-04-11T02:19:26Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    19,
                    26,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:19:26Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    19,
                    26,
                    4,
                    101,
                    0
                ],
                "title": "How Good Are Large Language Models for Course Recommendation in MOOCs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Good Are Large Language Models for Course Recommendation in MOOCs?"
                },
                "summary": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and are increasingly being integrated into recommendation\nsystems. However, their potential in educational recommendation systems has yet\nto be fully explored. This paper investigates the use of LLMs as a\ngeneral-purpose recommendation model, leveraging their vast knowledge derived\nfrom large-scale corpora for course recommendation tasks. We explore a variety\nof approaches, ranging from prompt-based methods to more advanced fine-tuning\ntechniques, and compare their performance against traditional recommendation\nmodels. Extensive experiments were conducted on a real-world MOOC dataset,\nevaluating using LLMs as course recommendation systems across key dimensions\nsuch as accuracy, diversity, and novelty. Our results demonstrate that LLMs can\nachieve good performance comparable to traditional models, highlighting their\npotential to enhance educational recommendation systems. These findings pave\nthe way for further exploration and development of LLM-based approaches in the\ncontext of educational recommendations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have made significant strides in natural\nlanguage processing and are increasingly being integrated into recommendation\nsystems. However, their potential in educational recommendation systems has yet\nto be fully explored. This paper investigates the use of LLMs as a\ngeneral-purpose recommendation model, leveraging their vast knowledge derived\nfrom large-scale corpora for course recommendation tasks. We explore a variety\nof approaches, ranging from prompt-based methods to more advanced fine-tuning\ntechniques, and compare their performance against traditional recommendation\nmodels. Extensive experiments were conducted on a real-world MOOC dataset,\nevaluating using LLMs as course recommendation systems across key dimensions\nsuch as accuracy, diversity, and novelty. Our results demonstrate that LLMs can\nachieve good performance comparable to traditional models, highlighting their\npotential to enhance educational recommendation systems. These findings pave\nthe way for further exploration and development of LLM-based approaches in the\ncontext of educational recommendations."
                },
                "authors": [
                    {
                        "name": "Boxuan Ma"
                    },
                    {
                        "name": "Md Akib Zabed Khan"
                    },
                    {
                        "name": "Tianyuan Yang"
                    },
                    {
                        "name": "Agoritsa Polyzou"
                    },
                    {
                        "name": "Shin'ichi Konomi"
                    }
                ],
                "author_detail": {
                    "name": "Shin'ichi Konomi"
                },
                "author": "Shin'ichi Konomi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08208v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08208v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08207v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08207v1",
                "updated": "2025-04-11T02:19:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    19,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T02:19:01Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    19,
                    1,
                    4,
                    101,
                    0
                ],
                "title": "DRAFT-ing Architectural Design Decisions using LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DRAFT-ing Architectural Design Decisions using LLMs"
                },
                "summary": "Architectural Knowledge Management (AKM) is crucial for software development\nbut remains challenging due to the lack of standardization and high manual\neffort. Architecture Decision Records (ADRs) provide a structured approach to\ncapture Architecture Design Decisions (ADDs), but their adoption is limited due\nto the manual effort involved and insufficient tool support. Our previous work\nhas shown that Large Language Models (LLMs) can assist in generating ADDs.\nHowever, simply prompting the LLM does not produce quality ADDs. Moreover,\nusing third-party LLMs raises privacy concerns, while self-hosting them poses\nresource challenges.\n  To this end, we experimented with different approaches like few-shot,\nretrieval-augmented generation (RAG) and fine-tuning to enhance LLM's ability\nto generate ADDs. Our results show that both techniques improve effectiveness.\nBuilding on this, we propose Domain Specific Retreival Augumented Few Shot Fine\nTuninng, DRAFT, which combines the strengths of all these three approaches for\nmore effective ADD generation. DRAFT operates in two phases: an offline phase\nthat fine-tunes an LLM on generating ADDs augmented with retrieved examples and\nan online phase that generates ADDs by leveraging retrieved ADRs and the\nfine-tuned model.\n  We evaluated DRAFT against existing approaches on a dataset of 4,911 ADRs and\nvarious LLMs and analyzed them using automated metrics and human evaluations.\nResults show DRAFT outperforms all other approaches in effectiveness while\nmaintaining efficiency. Our findings indicate that DRAFT can aid architects in\ndrafting ADDs while addressing privacy and resource constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Architectural Knowledge Management (AKM) is crucial for software development\nbut remains challenging due to the lack of standardization and high manual\neffort. Architecture Decision Records (ADRs) provide a structured approach to\ncapture Architecture Design Decisions (ADDs), but their adoption is limited due\nto the manual effort involved and insufficient tool support. Our previous work\nhas shown that Large Language Models (LLMs) can assist in generating ADDs.\nHowever, simply prompting the LLM does not produce quality ADDs. Moreover,\nusing third-party LLMs raises privacy concerns, while self-hosting them poses\nresource challenges.\n  To this end, we experimented with different approaches like few-shot,\nretrieval-augmented generation (RAG) and fine-tuning to enhance LLM's ability\nto generate ADDs. Our results show that both techniques improve effectiveness.\nBuilding on this, we propose Domain Specific Retreival Augumented Few Shot Fine\nTuninng, DRAFT, which combines the strengths of all these three approaches for\nmore effective ADD generation. DRAFT operates in two phases: an offline phase\nthat fine-tunes an LLM on generating ADDs augmented with retrieved examples and\nan online phase that generates ADDs by leveraging retrieved ADRs and the\nfine-tuned model.\n  We evaluated DRAFT against existing approaches on a dataset of 4,911 ADRs and\nvarious LLMs and analyzed them using automated metrics and human evaluations.\nResults show DRAFT outperforms all other approaches in effectiveness while\nmaintaining efficiency. Our findings indicate that DRAFT can aid architects in\ndrafting ADDs while addressing privacy and resource constraints."
                },
                "authors": [
                    {
                        "name": "Rudra Dhar"
                    },
                    {
                        "name": "Adyansh Kakran"
                    },
                    {
                        "name": "Amey Karan"
                    },
                    {
                        "name": "Karthik Vaidhyanathan"
                    },
                    {
                        "name": "Vasudeva Varma"
                    }
                ],
                "author_detail": {
                    "name": "Vasudeva Varma"
                },
                "author": "Vasudeva Varma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08207v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08207v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.07596v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.07596v2",
                "updated": "2025-04-11T02:05:01Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    2,
                    5,
                    1,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-10T09:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    9,
                    48,
                    56,
                    3,
                    100,
                    0
                ],
                "title": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Boosting Universal LLM Reward Design through Heuristic Reward\n  Observation Space Evolution"
                },
                "summary": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are emerging as promising tools for automated\nreinforcement learning (RL) reward design, owing to their robust capabilities\nin commonsense reasoning and code generation. By engaging in dialogues with RL\nagents, LLMs construct a Reward Observation Space (ROS) by selecting relevant\nenvironment states and defining their internal operations. However, existing\nframeworks have not effectively leveraged historical exploration data or manual\ntask descriptions to iteratively evolve this space. In this paper, we propose a\nnovel heuristic framework that enhances LLM-driven reward design by evolving\nthe ROS through a table-based exploration caching mechanism and a text-code\nreconciliation strategy. Our framework introduces a state execution table,\nwhich tracks the historical usage and success rates of environment states,\novercoming the Markovian constraint typically found in LLM dialogues and\nfacilitating more effective exploration. Furthermore, we reconcile\nuser-provided task descriptions with expert-defined success criteria using\nstructured prompts, ensuring alignment in reward design objectives.\nComprehensive evaluations on benchmark RL tasks demonstrate the effectiveness\nand stability of the proposed framework. Code and video demos are available at\njingjjjjjie.github.io/LLM2Reward."
                },
                "authors": [
                    {
                        "name": "Zen Kit Heng"
                    },
                    {
                        "name": "Zimeng Zhao"
                    },
                    {
                        "name": "Tianhao Wu"
                    },
                    {
                        "name": "Yuanfei Wang"
                    },
                    {
                        "name": "Mingdong Wu"
                    },
                    {
                        "name": "Yangang Wang"
                    },
                    {
                        "name": "Hao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Hao Dong"
                },
                "author": "Hao Dong",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.07596v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.07596v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10370v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10370v2",
                "updated": "2025-04-11T01:52:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    1,
                    52,
                    15,
                    4,
                    101,
                    0
                ],
                "published": "2024-10-14T10:50:16Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    10,
                    50,
                    16,
                    0,
                    288,
                    0
                ],
                "title": "Innovative Thinking, Infinite Humor: Humor Research of Large Language\n  Models through Structured Thought Leaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Innovative Thinking, Infinite Humor: Humor Research of Large Language\n  Models through Structured Thought Leaps"
                },
                "summary": "Humor is previously regarded as a gift exclusive to humans for the following\nreasons. Humor is a culturally nuanced aspect of human language, presenting\nchallenges for its understanding and generation. Humor generation necessitates\na multi-hop reasoning process, with each hop founded on proper rationales.\nAlthough many studies, such as those related to GPT-o1, focus on logical\nreasoning with reflection and correction, they still fall short in humor\ngeneration. Due to the sparsity of the knowledge graph in creative thinking, it\nis arduous to achieve multi-hop reasoning. Consequently, in this paper, we\npropose a more robust framework for addressing the humor reasoning task, named\nLoL. LoL aims to inject external information to mitigate the sparsity of the\nknowledge graph, thereby enabling multi-hop reasoning. In the first stage of\nLoL, we put forward an automatic instruction-evolution method to incorporate\nthe deeper and broader thinking processes underlying humor. Judgment-oriented\ninstructions are devised to enhance the model's judgment capability,\ndynamically supplementing and updating the sparse knowledge graph.\nSubsequently, through reinforcement learning, the reasoning logic for each\nonline-generated response is extracted using GPT-4o. In this process, external\nknowledge is re-introduced to aid the model in logical reasoning and the\nlearning of human preferences. Finally, experimental results indicate that the\ncombination of these two processes can enhance both the model's judgment\nability and its generative capacity. These findings deepen our comprehension of\nthe creative capabilities of large language models (LLMs) and offer approaches\nto boost LLMs' creative abilities for cross-domain innovative applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humor is previously regarded as a gift exclusive to humans for the following\nreasons. Humor is a culturally nuanced aspect of human language, presenting\nchallenges for its understanding and generation. Humor generation necessitates\na multi-hop reasoning process, with each hop founded on proper rationales.\nAlthough many studies, such as those related to GPT-o1, focus on logical\nreasoning with reflection and correction, they still fall short in humor\ngeneration. Due to the sparsity of the knowledge graph in creative thinking, it\nis arduous to achieve multi-hop reasoning. Consequently, in this paper, we\npropose a more robust framework for addressing the humor reasoning task, named\nLoL. LoL aims to inject external information to mitigate the sparsity of the\nknowledge graph, thereby enabling multi-hop reasoning. In the first stage of\nLoL, we put forward an automatic instruction-evolution method to incorporate\nthe deeper and broader thinking processes underlying humor. Judgment-oriented\ninstructions are devised to enhance the model's judgment capability,\ndynamically supplementing and updating the sparse knowledge graph.\nSubsequently, through reinforcement learning, the reasoning logic for each\nonline-generated response is extracted using GPT-4o. In this process, external\nknowledge is re-introduced to aid the model in logical reasoning and the\nlearning of human preferences. Finally, experimental results indicate that the\ncombination of these two processes can enhance both the model's judgment\nability and its generative capacity. These findings deepen our comprehension of\nthe creative capabilities of large language models (LLMs) and offer approaches\nto boost LLMs' creative abilities for cross-domain innovative applications."
                },
                "authors": [
                    {
                        "name": "Han Wang"
                    },
                    {
                        "name": "Yilin Zhao"
                    },
                    {
                        "name": "Dian Li"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Gang Liu"
                    },
                    {
                        "name": "Xuguang Lan"
                    },
                    {
                        "name": "Hui Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hui Wang"
                },
                "author": "Hui Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10370v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10370v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06736v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06736v5",
                "updated": "2025-04-11T01:35:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    1,
                    35,
                    36,
                    4,
                    101,
                    0
                ],
                "published": "2024-11-11T06:04:53Z",
                "published_parsed": [
                    2024,
                    11,
                    11,
                    6,
                    4,
                    53,
                    0,
                    316,
                    0
                ],
                "title": "MrSteve: Instruction-Following Agents in Minecraft with What-Where-When\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MrSteve: Instruction-Following Agents in Minecraft with What-Where-When\n  Memory"
                },
                "summary": "Significant advances have been made in developing general-purpose embodied AI\nin environments like Minecraft through the adoption of LLM-augmented\nhierarchical approaches. While these approaches, which combine high-level\nplanners with low-level controllers, show promise, low-level controllers\nfrequently become performance bottlenecks due to repeated failures. In this\npaper, we argue that the primary cause of failure in many low-level controllers\nis the absence of an episodic memory system. To address this, we introduce\nMrSteve (Memory Recall Steve), a novel low-level controller equipped with Place\nEvent Memory (PEM), a form of episodic memory that captures what, where, and\nwhen information from episodes. This directly addresses the main limitation of\nthe popular low-level controller, Steve-1. Unlike previous models that rely on\nshort-term memory, PEM organizes spatial and event-based data, enabling\nefficient recall and navigation in long-horizon tasks. Additionally, we propose\nan Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing\nagents to alternate between exploration and task-solving based on recalled\nevents. Our approach significantly improves task-solving and exploration\nefficiency compared to existing methods. We will release our code and demos on\nthe project page: https://sites.google.com/view/mr-steve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Significant advances have been made in developing general-purpose embodied AI\nin environments like Minecraft through the adoption of LLM-augmented\nhierarchical approaches. While these approaches, which combine high-level\nplanners with low-level controllers, show promise, low-level controllers\nfrequently become performance bottlenecks due to repeated failures. In this\npaper, we argue that the primary cause of failure in many low-level controllers\nis the absence of an episodic memory system. To address this, we introduce\nMrSteve (Memory Recall Steve), a novel low-level controller equipped with Place\nEvent Memory (PEM), a form of episodic memory that captures what, where, and\nwhen information from episodes. This directly addresses the main limitation of\nthe popular low-level controller, Steve-1. Unlike previous models that rely on\nshort-term memory, PEM organizes spatial and event-based data, enabling\nefficient recall and navigation in long-horizon tasks. Additionally, we propose\nan Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing\nagents to alternate between exploration and task-solving based on recalled\nevents. Our approach significantly improves task-solving and exploration\nefficiency compared to existing methods. We will release our code and demos on\nthe project page: https://sites.google.com/view/mr-steve."
                },
                "authors": [
                    {
                        "name": "Junyeong Park"
                    },
                    {
                        "name": "Junmo Cho"
                    },
                    {
                        "name": "Sungjin Ahn"
                    }
                ],
                "author_detail": {
                    "name": "Sungjin Ahn"
                },
                "author": "Sungjin Ahn",
                "arxiv_comment": "Accepted to ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06736v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06736v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08192v1",
                "updated": "2025-04-11T01:24:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    1,
                    24,
                    3,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T01:24:03Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    1,
                    24,
                    3,
                    4,
                    101,
                    0
                ],
                "title": "SAEs $\\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder\n  Guardrails for Precision Unlearning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAEs $\\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder\n  Guardrails for Precision Unlearning in LLMs"
                },
                "summary": "Machine unlearning is a promising approach to improve LLM safety by removing\nunwanted knowledge from the model. However, prevailing gradient-based\nunlearning methods suffer from issues such as high computational costs,\nhyperparameter instability, poor sequential unlearning capability,\nvulnerability to relearning attacks, low data efficiency, and lack of\ninterpretability. While Sparse Autoencoders are well-suited to improve these\naspects by enabling targeted activation-based unlearning, prior approaches\nunderperform gradient-based methods. This work demonstrates that, contrary to\nthese earlier findings, SAEs can significantly improve unlearning when employed\ndynamically. We introduce $\\textbf{Dynamic DAE Guardrails}$ (DSG), a novel\nmethod for precision unlearning that leverages principled feature selection and\na dynamic classifier. Our experiments show DSG substantially outperforms\nleading unlearning methods, achieving superior forget-utility trade-offs. DSG\naddresses key drawbacks of gradient-based approaches for unlearning -- offering\nenhanced computational efficiency and stability, robust performance in\nsequential unlearning, stronger resistance to relearning attacks, better data\nefficiency including zero-shot settings, and more interpretable unlearning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine unlearning is a promising approach to improve LLM safety by removing\nunwanted knowledge from the model. However, prevailing gradient-based\nunlearning methods suffer from issues such as high computational costs,\nhyperparameter instability, poor sequential unlearning capability,\nvulnerability to relearning attacks, low data efficiency, and lack of\ninterpretability. While Sparse Autoencoders are well-suited to improve these\naspects by enabling targeted activation-based unlearning, prior approaches\nunderperform gradient-based methods. This work demonstrates that, contrary to\nthese earlier findings, SAEs can significantly improve unlearning when employed\ndynamically. We introduce $\\textbf{Dynamic DAE Guardrails}$ (DSG), a novel\nmethod for precision unlearning that leverages principled feature selection and\na dynamic classifier. Our experiments show DSG substantially outperforms\nleading unlearning methods, achieving superior forget-utility trade-offs. DSG\naddresses key drawbacks of gradient-based approaches for unlearning -- offering\nenhanced computational efficiency and stability, robust performance in\nsequential unlearning, stronger resistance to relearning attacks, better data\nefficiency including zero-shot settings, and more interpretable unlearning."
                },
                "authors": [
                    {
                        "name": "Aashiq Muhamed"
                    },
                    {
                        "name": "Jacopo Bonato"
                    },
                    {
                        "name": "Mona Diab"
                    },
                    {
                        "name": "Virginia Smith"
                    }
                ],
                "author_detail": {
                    "name": "Virginia Smith"
                },
                "author": "Virginia Smith",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16557v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16557v2",
                "updated": "2025-04-11T01:12:00Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    1,
                    12,
                    0,
                    4,
                    101,
                    0
                ],
                "published": "2024-07-23T15:12:14Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    15,
                    12,
                    14,
                    1,
                    205,
                    0
                ],
                "title": "Patched RTC: evaluating LLMs for diverse software development tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patched RTC: evaluating LLMs for diverse software development tasks"
                },
                "summary": "This paper introduces Patched Round-Trip Correctness (Patched RTC), a novel\nevaluation technique for Large Language Models (LLMs) applied to diverse\nsoftware development tasks, particularly focusing on \"outer loop\" activities\nsuch as bug fixing, code review, and documentation updates. Patched RTC extends\nthe original Round-Trip Correctness method to work with any LLM and downstream\ntask, offering a self-evaluating framework that measures consistency and\nrobustness of model responses without human intervention. The study\ndemonstrates a correlation between Patched RTC scores and task-specific\naccuracy metrics, presenting it as an alternative to the LLM-as-Judge paradigm\nfor open-domain task evaluation. We implement Patched RTC in an open-source\nframework called patchwork, allowing for transparent evaluation during\ninference across various patchflows. Experiments comparing GPT-3.5 and GPT-4\nmodels across different software development tasks reveal that Patched RTC\neffectively distinguishes model performance and task difficulty. The paper also\nexplores the impact of consistency prompts on improving model accuracy,\nsuggesting that Patched RTC can guide prompt refinement and model selection for\ncomplex software development workflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Patched Round-Trip Correctness (Patched RTC), a novel\nevaluation technique for Large Language Models (LLMs) applied to diverse\nsoftware development tasks, particularly focusing on \"outer loop\" activities\nsuch as bug fixing, code review, and documentation updates. Patched RTC extends\nthe original Round-Trip Correctness method to work with any LLM and downstream\ntask, offering a self-evaluating framework that measures consistency and\nrobustness of model responses without human intervention. The study\ndemonstrates a correlation between Patched RTC scores and task-specific\naccuracy metrics, presenting it as an alternative to the LLM-as-Judge paradigm\nfor open-domain task evaluation. We implement Patched RTC in an open-source\nframework called patchwork, allowing for transparent evaluation during\ninference across various patchflows. Experiments comparing GPT-3.5 and GPT-4\nmodels across different software development tasks reveal that Patched RTC\neffectively distinguishes model performance and task difficulty. The paper also\nexplores the impact of consistency prompts on improving model accuracy,\nsuggesting that Patched RTC can guide prompt refinement and model selection for\ncomplex software development workflows."
                },
                "authors": [
                    {
                        "name": "Asankhaya Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Asankhaya Sharma"
                },
                "author": "Asankhaya Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16557v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16557v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.18521v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.18521v3",
                "updated": "2025-04-11T01:10:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    1,
                    10,
                    50,
                    4,
                    101,
                    0
                ],
                "published": "2024-07-26T05:34:34Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    5,
                    34,
                    34,
                    4,
                    208,
                    0
                ],
                "title": "Patched MOA: optimizing inference for diverse software development tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Patched MOA: optimizing inference for diverse software development tasks"
                },
                "summary": "This paper introduces Patched MOA (Mixture of Agents), an inference\noptimization technique that significantly enhances the performance of large\nlanguage models (LLMs) across diverse software development tasks. We evaluate\nthree inference optimization algorithms - Best of N, Mixture of Agents, and\nMonte Carlo Tree Search and demonstrate that Patched MOA can boost the\nperformance of smaller models to surpass that of larger, more expensive models.\nNotably, our approach improves the gpt-4o-mini model's performance on the\nArena-Hard-Auto benchmark by 15.52%, outperforming gpt-4-turbo at a fraction of\nthe cost. We also apply Patched MOA to various software development workflows,\nshowing consistent improvements in task completion rates. Our method is\nmodel-agnostic, transparent to end-users, and can be easily integrated into\nexisting LLM pipelines. This work contributes to the growing field of LLM\noptimization, offering a cost-effective solution for enhancing model\nperformance without the need for fine-tuning or larger models. Our\nimplementation is open-source and available at\nhttps://github.com/codelion/optillm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Patched MOA (Mixture of Agents), an inference\noptimization technique that significantly enhances the performance of large\nlanguage models (LLMs) across diverse software development tasks. We evaluate\nthree inference optimization algorithms - Best of N, Mixture of Agents, and\nMonte Carlo Tree Search and demonstrate that Patched MOA can boost the\nperformance of smaller models to surpass that of larger, more expensive models.\nNotably, our approach improves the gpt-4o-mini model's performance on the\nArena-Hard-Auto benchmark by 15.52%, outperforming gpt-4-turbo at a fraction of\nthe cost. We also apply Patched MOA to various software development workflows,\nshowing consistent improvements in task completion rates. Our method is\nmodel-agnostic, transparent to end-users, and can be easily integrated into\nexisting LLM pipelines. This work contributes to the growing field of LLM\noptimization, offering a cost-effective solution for enhancing model\nperformance without the need for fine-tuning or larger models. Our\nimplementation is open-source and available at\nhttps://github.com/codelion/optillm."
                },
                "authors": [
                    {
                        "name": "Asankhaya Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Asankhaya Sharma"
                },
                "author": "Asankhaya Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.18521v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.18521v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08180v1",
                "updated": "2025-04-11T00:39:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    0,
                    39,
                    50,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T00:39:50Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    0,
                    39,
                    50,
                    4,
                    101,
                    0
                ],
                "title": "A Vulnerability Code Intent Summary Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Vulnerability Code Intent Summary Dataset"
                },
                "summary": "In the era of Large Language Models (LLMs), the code summarization technique\nboosts a lot, along with the emergence of many new significant works. However,\nthe potential of code summarization in the Computer Security Area still remains\nexplored. Can we generate a code summary of a code snippet for its security\nintention? Thus, this work proposes an innovative large-scale multi-perspective\nCode Intent Summary Dataset named BADS , aiming to increase the understanding\nof a given code snippet and reduce the risk in the code developing process. The\nprocedure of establishing a dataset can be divided into four steps: First, we\ncollect samples of codes with known vulnerabilities as well as code generated\nby AI from multiple sources. Second, we do the data clean and format\nunification, then do the data combination. Third, we utilize the LLM to\nautomatically Annotate the code snippet. Last, We do the human evaluation to\ndouble-check. The dataset contains X code examples which cover Y categories of\nvulnerability. Our data are from Z open-source projects and CVE entries, and\ncompared to existing work, our dataset not only contains original code but also\ncode function summary and security intent summary, providing context\ninformation for research in code security analysis. All information is in CSV\nformat. The contributions of this paper are four-fold: the establishment of a\nhigh-quality, multi-perspective Code Intent Summary Dataset; an innovative\nmethod in data collection and processing; A new multi-perspective code analysis\nframework that promotes cross-disciplinary research in the fields of software\nengineering and cybersecurity; improving the practicality and scalability of\nthe research outcomes by considering the code length limitations in real-world\napplications. Our dataset and related tools have been publicly released on\nGitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of Large Language Models (LLMs), the code summarization technique\nboosts a lot, along with the emergence of many new significant works. However,\nthe potential of code summarization in the Computer Security Area still remains\nexplored. Can we generate a code summary of a code snippet for its security\nintention? Thus, this work proposes an innovative large-scale multi-perspective\nCode Intent Summary Dataset named BADS , aiming to increase the understanding\nof a given code snippet and reduce the risk in the code developing process. The\nprocedure of establishing a dataset can be divided into four steps: First, we\ncollect samples of codes with known vulnerabilities as well as code generated\nby AI from multiple sources. Second, we do the data clean and format\nunification, then do the data combination. Third, we utilize the LLM to\nautomatically Annotate the code snippet. Last, We do the human evaluation to\ndouble-check. The dataset contains X code examples which cover Y categories of\nvulnerability. Our data are from Z open-source projects and CVE entries, and\ncompared to existing work, our dataset not only contains original code but also\ncode function summary and security intent summary, providing context\ninformation for research in code security analysis. All information is in CSV\nformat. The contributions of this paper are four-fold: the establishment of a\nhigh-quality, multi-perspective Code Intent Summary Dataset; an innovative\nmethod in data collection and processing; A new multi-perspective code analysis\nframework that promotes cross-disciplinary research in the fields of software\nengineering and cybersecurity; improving the practicality and scalability of\nthe research outcomes by considering the code length limitations in real-world\napplications. Our dataset and related tools have been publicly released on\nGitHub."
                },
                "authors": [
                    {
                        "name": "Yifan Huang"
                    },
                    {
                        "name": "Weisong Sun"
                    },
                    {
                        "name": "Yubin Qu"
                    }
                ],
                "author_detail": {
                    "name": "Yubin Qu"
                },
                "author": "Yubin Qu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01869v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01869v2",
                "updated": "2025-04-11T00:37:11Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    0,
                    37,
                    11,
                    4,
                    101,
                    0
                ],
                "published": "2025-02-25T21:50:46Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    21,
                    50,
                    46,
                    1,
                    56,
                    0
                ],
                "title": "From Small to Large Language Models: Revisiting the Federalist Papers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Small to Large Language Models: Revisiting the Federalist Papers"
                },
                "summary": "For a long time, the authorship of the Federalist Papers had been a subject\nof inquiry and debate, not only by linguists and historians but also by\nstatisticians. In what was arguably the first Bayesian case study, Mosteller\nand Wallace (1963) provided the first statistical evidence for attributing all\ndisputed papers to Madison. Our paper revisits this historical dataset but from\na lens of modern language models, both small and large. We review some of the\nmore popular Large Language Model (LLM) tools and examine them from a\nstatistical point of view in the context of text classification. We investigate\nwhether, without any attempt to fine-tune, the general embedding constructs can\nbe useful for stylometry and attribution. We explain differences between\nvarious word/phrase embeddings and discuss how to aggregate them in a document.\nContrary to our expectations, we exemplify that dimension expansion with word\nembeddings may not always be beneficial for attribution relative to dimension\nreduction with topic embeddings. Our experiments demonstrate that default LLM\nembeddings (even after manual fine-tuning) may not consistently improve\nauthorship attribution accuracy. Instead, Bayesian analysis with topic\nembeddings trained on ``function words\" yields superior out-of-sample\nclassification performance. This suggests that traditional (small) statistical\nlanguage models, with their interpretability and solid theoretical foundation,\ncan offer significant advantages in authorship attribution tasks. The code used\nin this analysis is available at github.com/sowonjeong/slm-to-llm",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "For a long time, the authorship of the Federalist Papers had been a subject\nof inquiry and debate, not only by linguists and historians but also by\nstatisticians. In what was arguably the first Bayesian case study, Mosteller\nand Wallace (1963) provided the first statistical evidence for attributing all\ndisputed papers to Madison. Our paper revisits this historical dataset but from\na lens of modern language models, both small and large. We review some of the\nmore popular Large Language Model (LLM) tools and examine them from a\nstatistical point of view in the context of text classification. We investigate\nwhether, without any attempt to fine-tune, the general embedding constructs can\nbe useful for stylometry and attribution. We explain differences between\nvarious word/phrase embeddings and discuss how to aggregate them in a document.\nContrary to our expectations, we exemplify that dimension expansion with word\nembeddings may not always be beneficial for attribution relative to dimension\nreduction with topic embeddings. Our experiments demonstrate that default LLM\nembeddings (even after manual fine-tuning) may not consistently improve\nauthorship attribution accuracy. Instead, Bayesian analysis with topic\nembeddings trained on ``function words\" yields superior out-of-sample\nclassification performance. This suggests that traditional (small) statistical\nlanguage models, with their interpretability and solid theoretical foundation,\ncan offer significant advantages in authorship attribution tasks. The code used\nin this analysis is available at github.com/sowonjeong/slm-to-llm"
                },
                "authors": [
                    {
                        "name": "So Won Jeong"
                    },
                    {
                        "name": "Veronika Rokov"
                    }
                ],
                "author_detail": {
                    "name": "Veronika Rokov"
                },
                "author": "Veronika Rokov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01869v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01869v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08176v1",
                "updated": "2025-04-11T00:13:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    11,
                    0,
                    13,
                    59,
                    4,
                    101,
                    0
                ],
                "published": "2025-04-11T00:13:59Z",
                "published_parsed": [
                    2025,
                    4,
                    11,
                    0,
                    13,
                    59,
                    4,
                    101,
                    0
                ],
                "title": "GenXSS: an AI-Driven Framework for Automated Detection of XSS Attacks in\n  WAFs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenXSS: an AI-Driven Framework for Automated Detection of XSS Attacks in\n  WAFs"
                },
                "summary": "The increasing reliance on web services has led to a rise in cybersecurity\nthreats, particularly Cross-Site Scripting (XSS) attacks, which target\nclient-side layers of web applications by injecting malicious scripts.\nTraditional Web Application Firewalls (WAFs) struggle to detect highly\nobfuscated and complex attacks, as their rules require manual updates. This\npaper presents a novel generative AI framework that leverages Large Language\nModels (LLMs) to enhance XSS mitigation. The framework achieves two primary\nobjectives: (1) generating sophisticated and syntactically validated XSS\npayloads using in-context learning, and (2) automating defense mechanisms by\ntesting these attacks against a vulnerable application secured by a WAF,\nclassifying bypassing attacks, and generating effective WAF security rules.\nExperimental results using GPT-4o demonstrate the framework's effectiveness\ngenerating 264 XSS payloads, 83% of which were validated, with 80% bypassing\nModSecurity WAF equipped with an industry standard security rule set developed\nby the Open Web Application Security Project (OWASP) to protect against web\nvulnerabilities. Through rule generation, 86% of previously successful attacks\nwere blocked using only 15 new rules. In comparison, Google Gemini Pro achieved\na lower bypass rate of 63%, highlighting performance differences across LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing reliance on web services has led to a rise in cybersecurity\nthreats, particularly Cross-Site Scripting (XSS) attacks, which target\nclient-side layers of web applications by injecting malicious scripts.\nTraditional Web Application Firewalls (WAFs) struggle to detect highly\nobfuscated and complex attacks, as their rules require manual updates. This\npaper presents a novel generative AI framework that leverages Large Language\nModels (LLMs) to enhance XSS mitigation. The framework achieves two primary\nobjectives: (1) generating sophisticated and syntactically validated XSS\npayloads using in-context learning, and (2) automating defense mechanisms by\ntesting these attacks against a vulnerable application secured by a WAF,\nclassifying bypassing attacks, and generating effective WAF security rules.\nExperimental results using GPT-4o demonstrate the framework's effectiveness\ngenerating 264 XSS payloads, 83% of which were validated, with 80% bypassing\nModSecurity WAF equipped with an industry standard security rule set developed\nby the Open Web Application Security Project (OWASP) to protect against web\nvulnerabilities. Through rule generation, 86% of previously successful attacks\nwere blocked using only 15 new rules. In comparison, Google Gemini Pro achieved\na lower bypass rate of 63%, highlighting performance differences across LLMs."
                },
                "authors": [
                    {
                        "name": "Vahid Babaey"
                    },
                    {
                        "name": "Arun Ravindran"
                    }
                ],
                "author_detail": {
                    "name": "Arun Ravindran"
                },
                "author": "Arun Ravindran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04762v2",
                "updated": "2025-04-10T23:54:51Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    23,
                    54,
                    51,
                    3,
                    100,
                    0
                ],
                "published": "2025-01-08T18:08:48Z",
                "published_parsed": [
                    2025,
                    1,
                    8,
                    18,
                    8,
                    48,
                    2,
                    8,
                    0
                ],
                "title": "Efficient and Responsible Adaptation of Large Language Models for Robust\n  and Equitable Top-k Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient and Responsible Adaptation of Large Language Models for Robust\n  and Equitable Top-k Recommendations"
                },
                "summary": "Conventional recommendation systems (RSs) are typically optimized to enhance\nperformance metrics uniformly across all training samples, inadvertently\noverlooking the needs of diverse user populations. The performance disparity\namong various populations can harm the model's robustness to sub-populations\ndue to the varying user properties. While large language models (LLMs) show\npromise in enhancing RS performance, their practical applicability is hindered\nby high costs, inference latency, and degraded performance on long user\nqueries. To address these challenges, we propose a hybrid task allocation\nframework designed to promote social good by equitably serving all user groups.\nBy adopting a two-phase approach, we promote a strategic assignment of tasks\nfor efficient and responsible adaptation of LLMs. Our strategy works by first\nidentifying the weak and inactive users that receive a suboptimal ranking\nperformance by RSs. Next, we use an in-context learning approach for such\nusers, wherein each user interaction history is contextualized as a distinct\nranking task. We evaluate our hybrid framework by incorporating eight different\nrecommendation algorithms and three different LLMs -- both open and\nclose-sourced. Our results on three real-world datasets show a significant\nreduction in weak users and improved robustness to subpopulations without\ndisproportionately escalating costs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional recommendation systems (RSs) are typically optimized to enhance\nperformance metrics uniformly across all training samples, inadvertently\noverlooking the needs of diverse user populations. The performance disparity\namong various populations can harm the model's robustness to sub-populations\ndue to the varying user properties. While large language models (LLMs) show\npromise in enhancing RS performance, their practical applicability is hindered\nby high costs, inference latency, and degraded performance on long user\nqueries. To address these challenges, we propose a hybrid task allocation\nframework designed to promote social good by equitably serving all user groups.\nBy adopting a two-phase approach, we promote a strategic assignment of tasks\nfor efficient and responsible adaptation of LLMs. Our strategy works by first\nidentifying the weak and inactive users that receive a suboptimal ranking\nperformance by RSs. Next, we use an in-context learning approach for such\nusers, wherein each user interaction history is contextualized as a distinct\nranking task. We evaluate our hybrid framework by incorporating eight different\nrecommendation algorithms and three different LLMs -- both open and\nclose-sourced. Our results on three real-world datasets show a significant\nreduction in weak users and improved robustness to subpopulations without\ndisproportionately escalating costs."
                },
                "authors": [
                    {
                        "name": "Kirandeep Kaur"
                    },
                    {
                        "name": "Manya Chadha"
                    },
                    {
                        "name": "Vinayak Gupta"
                    },
                    {
                        "name": "Chirag Shah"
                    }
                ],
                "author_detail": {
                    "name": "Chirag Shah"
                },
                "author": "Chirag Shah",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2405.00824",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.17332v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.17332v3",
                "updated": "2025-04-10T23:50:28Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    23,
                    50,
                    28,
                    3,
                    100,
                    0
                ],
                "published": "2025-03-21T17:32:32Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    17,
                    32,
                    32,
                    4,
                    80,
                    0
                ],
                "title": "CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web\n  Application Vulnerabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVE-Bench: A Benchmark for AI Agents' Ability to Exploit Real-World Web\n  Application Vulnerabilities"
                },
                "summary": "Large language model (LLM) agents are increasingly capable of autonomously\nconducting cyberattacks, posing significant threats to existing applications.\nThis growing risk highlights the urgent need for a real-world benchmark to\nevaluate the ability of LLM agents to exploit web application vulnerabilities.\nHowever, existing benchmarks fall short as they are limited to abstracted\nCapture the Flag competitions or lack comprehensive coverage. Building a\nbenchmark for real-world vulnerabilities involves both specialized expertise to\nreproduce exploits and a systematic approach to evaluating unpredictable\nthreats. To address this challenge, we introduce CVE-Bench, a real-world\ncybersecurity benchmark based on critical-severity Common Vulnerabilities and\nExposures. In CVE-Bench, we design a sandbox framework that enables LLM agents\nto exploit vulnerable web applications in scenarios that mimic real-world\nconditions, while also providing effective evaluation of their exploits. Our\nevaluation shows that the state-of-the-art agent framework can resolve up to\n13% of vulnerabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) agents are increasingly capable of autonomously\nconducting cyberattacks, posing significant threats to existing applications.\nThis growing risk highlights the urgent need for a real-world benchmark to\nevaluate the ability of LLM agents to exploit web application vulnerabilities.\nHowever, existing benchmarks fall short as they are limited to abstracted\nCapture the Flag competitions or lack comprehensive coverage. Building a\nbenchmark for real-world vulnerabilities involves both specialized expertise to\nreproduce exploits and a systematic approach to evaluating unpredictable\nthreats. To address this challenge, we introduce CVE-Bench, a real-world\ncybersecurity benchmark based on critical-severity Common Vulnerabilities and\nExposures. In CVE-Bench, we design a sandbox framework that enables LLM agents\nto exploit vulnerable web applications in scenarios that mimic real-world\nconditions, while also providing effective evaluation of their exploits. Our\nevaluation shows that the state-of-the-art agent framework can resolve up to\n13% of vulnerabilities."
                },
                "authors": [
                    {
                        "name": "Yuxuan Zhu"
                    },
                    {
                        "name": "Antony Kellermann"
                    },
                    {
                        "name": "Dylan Bowman"
                    },
                    {
                        "name": "Philip Li"
                    },
                    {
                        "name": "Akul Gupta"
                    },
                    {
                        "name": "Adarsh Danda"
                    },
                    {
                        "name": "Richard Fang"
                    },
                    {
                        "name": "Conner Jensen"
                    },
                    {
                        "name": "Eric Ihli"
                    },
                    {
                        "name": "Jason Benn"
                    },
                    {
                        "name": "Jet Geronimo"
                    },
                    {
                        "name": "Avi Dhir"
                    },
                    {
                        "name": "Sudhit Rao"
                    },
                    {
                        "name": "Kaicheng Yu"
                    },
                    {
                        "name": "Twm Stone"
                    },
                    {
                        "name": "Daniel Kang"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kang"
                },
                "author": "Daniel Kang",
                "arxiv_comment": "15 pages, 4 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.17332v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.17332v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.05114v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.05114v4",
                "updated": "2025-04-10T23:50:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    23,
                    50,
                    14,
                    3,
                    100,
                    0
                ],
                "published": "2023-12-08T15:42:28Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    15,
                    42,
                    28,
                    4,
                    342,
                    0
                ],
                "title": "The Inadequacy of Similarity-based Privacy Metrics: Privacy Attacks\n  against \"Truly Anonymous\" Synthetic Datasets",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Inadequacy of Similarity-based Privacy Metrics: Privacy Attacks\n  against \"Truly Anonymous\" Synthetic Datasets"
                },
                "summary": "Generative models producing synthetic data are meant to provide a\nprivacy-friendly approach to releasing data. However, their privacy guarantees\nare only considered robust when models satisfy Differential Privacy (DP). Alas,\nthis is not a ubiquitous standard, as many leading companies (and, in fact,\nresearch papers) use ad-hoc privacy metrics based on testing the statistical\nsimilarity between synthetic and real data.\n  In this paper, we examine the privacy metrics used in real-world synthetic\ndata deployments and demonstrate their unreliability in several ways. First, we\nprovide counter-examples where severe privacy violations occur even if the\nprivacy tests pass and instantiate accurate membership and attribute inference\nattacks with minimal cost. We then introduce ReconSyn, a reconstruction attack\nthat generates multiple synthetic datasets that are considered private by the\nmetrics but actually leak information unique to individual records. We show\nthat ReconSyn recovers 78-100% of the outliers in the train data with only\nblack-box access to a single fitted generative model and the privacy metrics.\nIn the process, we show that applying DP only to the model does not mitigate\nthis attack, as using privacy metrics breaks the end-to-end DP pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models producing synthetic data are meant to provide a\nprivacy-friendly approach to releasing data. However, their privacy guarantees\nare only considered robust when models satisfy Differential Privacy (DP). Alas,\nthis is not a ubiquitous standard, as many leading companies (and, in fact,\nresearch papers) use ad-hoc privacy metrics based on testing the statistical\nsimilarity between synthetic and real data.\n  In this paper, we examine the privacy metrics used in real-world synthetic\ndata deployments and demonstrate their unreliability in several ways. First, we\nprovide counter-examples where severe privacy violations occur even if the\nprivacy tests pass and instantiate accurate membership and attribute inference\nattacks with minimal cost. We then introduce ReconSyn, a reconstruction attack\nthat generates multiple synthetic datasets that are considered private by the\nmetrics but actually leak information unique to individual records. We show\nthat ReconSyn recovers 78-100% of the outliers in the train data with only\nblack-box access to a single fitted generative model and the privacy metrics.\nIn the process, we show that applying DP only to the model does not mitigate\nthis attack, as using privacy metrics breaks the end-to-end DP pipeline."
                },
                "authors": [
                    {
                        "name": "Georgi Ganev"
                    },
                    {
                        "name": "Emiliano De Cristofaro"
                    }
                ],
                "author_detail": {
                    "name": "Emiliano De Cristofaro"
                },
                "author": "Emiliano De Cristofaro",
                "arxiv_comment": "Published in the Proceedings of the 46th IEEE Symposium on Security &\n  Privacy (IEEE S&P 2025). Please cite the S&P version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.05114v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.05114v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08167v1",
                "updated": "2025-04-10T23:25:19Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    23,
                    25,
                    19,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T23:25:19Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    23,
                    25,
                    19,
                    3,
                    100,
                    0
                ],
                "title": "Quantum-assured magnetic navigation achieves positioning accuracy better\n  than a strategic-grade INS in airborne and ground-based field trials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum-assured magnetic navigation achieves positioning accuracy better\n  than a strategic-grade INS in airborne and ground-based field trials"
                },
                "summary": "Modern navigation systems rely critically on GNSS, which in many cases is\nunavailable or unreliable (e.g. due to jamming or spoofing). For this reason\nthere is great interest in augmenting backup navigation systems such as\ninertial navigation systems (INS) with additional modalities that reduce\npositioning error in the absence of reliable GNSS. Magnetic-anomaly navigation\nis one such approach, providing passive, non-jammable navigation through\nperiodic position fixes obtained by comparing local measurements of Earth's\ncrustal field against known anomaly maps. Despite its potential, existing\nMagNav efforts have been limited by magnetometer performance and platform\nnoise; solutions addressing these problems have proven either too brittle or\nimpractical for realistic deployment. Here we demonstrate a quantum-assured\nMagNav solution based on proprietary quantum magnetometers with by a novel\ndenoising and map-matching algorithms. The system fits on fixed-wing drones or\nin the avionics bay of a commercial airliner. We present trials at altitudes up\nto 19000 feet, testing onboard and outboard quantum magnetometers comparing\nagainst a strategic-grade INS. Our MagNav solution achieves superior\nperformance, delivering up to 46x better positioning error than the\nvelocity-aided INS; the best final positioning accuracy we achieve is 22m or\n0.006% of the flight distance. Airborne trials consistently achieve at least\n11x advantage over the INS across varying conditions, altitudes, and flight\npatterns. The system learns model parameters online without special vehicle\nmaneuvers providing robustness to various configuration changes (e.g. changing\npayload or latitude). Our trials also include the first successful MagNav\nperformed in a ground vehicle using publicly-available anomaly maps, delivering\nbounded positioning error 7x lower than the INS, with both systems in strapdown\nconfiguration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern navigation systems rely critically on GNSS, which in many cases is\nunavailable or unreliable (e.g. due to jamming or spoofing). For this reason\nthere is great interest in augmenting backup navigation systems such as\ninertial navigation systems (INS) with additional modalities that reduce\npositioning error in the absence of reliable GNSS. Magnetic-anomaly navigation\nis one such approach, providing passive, non-jammable navigation through\nperiodic position fixes obtained by comparing local measurements of Earth's\ncrustal field against known anomaly maps. Despite its potential, existing\nMagNav efforts have been limited by magnetometer performance and platform\nnoise; solutions addressing these problems have proven either too brittle or\nimpractical for realistic deployment. Here we demonstrate a quantum-assured\nMagNav solution based on proprietary quantum magnetometers with by a novel\ndenoising and map-matching algorithms. The system fits on fixed-wing drones or\nin the avionics bay of a commercial airliner. We present trials at altitudes up\nto 19000 feet, testing onboard and outboard quantum magnetometers comparing\nagainst a strategic-grade INS. Our MagNav solution achieves superior\nperformance, delivering up to 46x better positioning error than the\nvelocity-aided INS; the best final positioning accuracy we achieve is 22m or\n0.006% of the flight distance. Airborne trials consistently achieve at least\n11x advantage over the INS across varying conditions, altitudes, and flight\npatterns. The system learns model parameters online without special vehicle\nmaneuvers providing robustness to various configuration changes (e.g. changing\npayload or latitude). Our trials also include the first successful MagNav\nperformed in a ground vehicle using publicly-available anomaly maps, delivering\nbounded positioning error 7x lower than the INS, with both systems in strapdown\nconfiguration."
                },
                "authors": [
                    {
                        "name": "Murat Muradoglu"
                    },
                    {
                        "name": "Mattias T. Johnsson"
                    },
                    {
                        "name": "Nathanial M. Wilson"
                    },
                    {
                        "name": "Yuval Cohen"
                    },
                    {
                        "name": "Dongki Shin"
                    },
                    {
                        "name": "Tomas Navickas"
                    },
                    {
                        "name": "Tadas Pyragius"
                    },
                    {
                        "name": "Divya Thomas"
                    },
                    {
                        "name": "Daniel Thompson"
                    },
                    {
                        "name": "Steven I. Moore"
                    },
                    {
                        "name": "Md Tanvir Rahman"
                    },
                    {
                        "name": "Adrian Walker"
                    },
                    {
                        "name": "Indranil Dutta"
                    },
                    {
                        "name": "Suraj Bijjahalli"
                    },
                    {
                        "name": "Jacob Berlocher"
                    },
                    {
                        "name": "Michael R. Hush"
                    },
                    {
                        "name": "Russell P. Anderson"
                    },
                    {
                        "name": "Stuart S. Szigeti"
                    },
                    {
                        "name": "Michael J. Biercuk"
                    }
                ],
                "author_detail": {
                    "name": "Michael J. Biercuk"
                },
                "author": "Michael J. Biercuk",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.23488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.23488v2",
                "updated": "2025-04-10T23:04:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    23,
                    4,
                    21,
                    3,
                    100,
                    0
                ],
                "published": "2024-10-30T22:43:47Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    22,
                    43,
                    47,
                    2,
                    304,
                    0
                ],
                "title": "PACER: Preference-conditioned All-terrain Costmap Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PACER: Preference-conditioned All-terrain Costmap Generation"
                },
                "summary": "In autonomous robot navigation, terrain cost assignment is typically\nperformed using a semantics-based paradigm in which terrain is first labeled\nusing a pre-trained semantic classifier and costs are then assigned according\nto a user-defined mapping between label and cost. While this approach is\nrapidly adaptable to changing user preferences, only preferences over the types\nof terrain that are already known by the semantic classifier can be expressed.\nIn this paper, we hypothesize that a machine-learning-based alternative to the\nsemantics-based paradigm above will allow for rapid cost assignment adaptation\nto preferences expressed over new terrains at deployment time without the need\nfor additional training. To investigate this hypothesis, we introduce and study\nPACER, a novel approach to costmap generation that accepts as input a single\nbirds-eye view (BEV) image of the surrounding area along with a user-specified\npreference context and generates a corresponding BEV costmap that aligns with\nthe preference context. Using both real and synthetic data along with a\ncombination of proposed training tasks, we find that PACER is able to adapt\nquickly to new user preferences while also exhibiting better generalization to\nnovel terrains compared to both semantics-based and representation-learning\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In autonomous robot navigation, terrain cost assignment is typically\nperformed using a semantics-based paradigm in which terrain is first labeled\nusing a pre-trained semantic classifier and costs are then assigned according\nto a user-defined mapping between label and cost. While this approach is\nrapidly adaptable to changing user preferences, only preferences over the types\nof terrain that are already known by the semantic classifier can be expressed.\nIn this paper, we hypothesize that a machine-learning-based alternative to the\nsemantics-based paradigm above will allow for rapid cost assignment adaptation\nto preferences expressed over new terrains at deployment time without the need\nfor additional training. To investigate this hypothesis, we introduce and study\nPACER, a novel approach to costmap generation that accepts as input a single\nbirds-eye view (BEV) image of the surrounding area along with a user-specified\npreference context and generates a corresponding BEV costmap that aligns with\nthe preference context. Using both real and synthetic data along with a\ncombination of proposed training tasks, we find that PACER is able to adapt\nquickly to new user preferences while also exhibiting better generalization to\nnovel terrains compared to both semantics-based and representation-learning\napproaches."
                },
                "authors": [
                    {
                        "name": "Luisa Mao"
                    },
                    {
                        "name": "Garrett Warnell"
                    },
                    {
                        "name": "Peter Stone"
                    },
                    {
                        "name": "Joydeep Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Joydeep Biswas"
                },
                "author": "Joydeep Biswas",
                "arxiv_doi": "10.1109/LRA.2025.3549645",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2025.3549645",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.23488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.23488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08154v1",
                "updated": "2025-04-10T22:37:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    22,
                    37,
                    27,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T22:37:27Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    22,
                    37,
                    27,
                    3,
                    100,
                    0
                ],
                "title": "Investigating Vision-Language Model for Point Cloud-based Vehicle\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Investigating Vision-Language Model for Point Cloud-based Vehicle\n  Classification"
                },
                "summary": "Heavy-duty trucks pose significant safety challenges due to their large size\nand limited maneuverability compared to passenger vehicles. A deeper\nunderstanding of truck characteristics is essential for enhancing the safety\nperspective of cooperative autonomous driving. Traditional LiDAR-based truck\nclassification methods rely on extensive manual annotations, which makes them\nlabor-intensive and costly. The rapid advancement of large language models\n(LLMs) trained on massive datasets presents an opportunity to leverage their\nfew-shot learning capabilities for truck classification. However, existing\nvision-language models (VLMs) are primarily trained on image datasets, which\nmakes it challenging to directly process point cloud data. This study\nintroduces a novel framework that integrates roadside LiDAR point cloud data\nwith VLMs to facilitate efficient and accurate truck classification, which\nsupports cooperative and safe driving environments. This study introduces three\nkey innovations: (1) leveraging real-world LiDAR datasets for model\ndevelopment, (2) designing a preprocessing pipeline to adapt point cloud data\nfor VLM input, including point cloud registration for dense 3D rendering and\nmathematical morphological techniques to enhance feature representation, and\n(3) utilizing in-context learning with few-shot prompting to enable vehicle\nclassification with minimally labeled training data. Experimental results\ndemonstrate encouraging performance of this method and present its potential to\nreduce annotation efforts while improving classification accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heavy-duty trucks pose significant safety challenges due to their large size\nand limited maneuverability compared to passenger vehicles. A deeper\nunderstanding of truck characteristics is essential for enhancing the safety\nperspective of cooperative autonomous driving. Traditional LiDAR-based truck\nclassification methods rely on extensive manual annotations, which makes them\nlabor-intensive and costly. The rapid advancement of large language models\n(LLMs) trained on massive datasets presents an opportunity to leverage their\nfew-shot learning capabilities for truck classification. However, existing\nvision-language models (VLMs) are primarily trained on image datasets, which\nmakes it challenging to directly process point cloud data. This study\nintroduces a novel framework that integrates roadside LiDAR point cloud data\nwith VLMs to facilitate efficient and accurate truck classification, which\nsupports cooperative and safe driving environments. This study introduces three\nkey innovations: (1) leveraging real-world LiDAR datasets for model\ndevelopment, (2) designing a preprocessing pipeline to adapt point cloud data\nfor VLM input, including point cloud registration for dense 3D rendering and\nmathematical morphological techniques to enhance feature representation, and\n(3) utilizing in-context learning with few-shot prompting to enable vehicle\nclassification with minimally labeled training data. Experimental results\ndemonstrate encouraging performance of this method and present its potential to\nreduce annotation efforts while improving classification accuracy."
                },
                "authors": [
                    {
                        "name": "Yiqiao Li"
                    },
                    {
                        "name": "Jie Wei"
                    },
                    {
                        "name": "Camille Kamga"
                    }
                ],
                "author_detail": {
                    "name": "Camille Kamga"
                },
                "author": "Camille Kamga",
                "arxiv_comment": "5 pages,3 figures, 1 table, CVPR DriveX workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08148v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08148v1",
                "updated": "2025-04-10T22:19:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    22,
                    19,
                    41,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T22:19:41Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    22,
                    19,
                    41,
                    3,
                    100,
                    0
                ],
                "title": "Orchestrating Agents and Data for Enterprise: A Blueprint Architecture\n  for Compound AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orchestrating Agents and Data for Enterprise: A Blueprint Architecture\n  for Compound AI"
                },
                "summary": "Large language models (LLMs) have gained significant interest in industry due\nto their impressive capabilities across a wide range of tasks. However, the\nwidespread adoption of LLMs presents several challenges, such as integration\ninto existing applications and infrastructure, utilization of company\nproprietary data, models, and APIs, and meeting cost, quality, responsiveness,\nand other requirements. To address these challenges, there is a notable shift\nfrom monolithic models to compound AI systems, with the premise of more\npowerful, versatile, and reliable applications. However, progress thus far has\nbeen piecemeal, with proposals for agentic workflows, programming models, and\nextended LLM capabilities, without a clear vision of an overall architecture.\nIn this paper, we propose a 'blueprint architecture' for compound AI systems\nfor orchestrating agents and data for enterprise applications. In our proposed\narchitecture the key orchestration concept is 'streams' to coordinate the flow\nof data and instructions among agents. Existing proprietary models and APIs in\nthe enterprise are mapped to 'agents', defined in an 'agent registry' that\nserves agent metadata and learned representations for search and planning.\nAgents can utilize proprietary data through a 'data registry' that similarly\nregisters enterprise data of various modalities. Tying it all together, data\nand task 'planners' break down, map, and optimize tasks and queries for given\nquality of service (QoS) requirements such as cost, accuracy, and latency. We\nillustrate an implementation of the architecture for a use-case in the HR\ndomain and discuss opportunities and challenges for 'agentic AI' in the\nenterprise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have gained significant interest in industry due\nto their impressive capabilities across a wide range of tasks. However, the\nwidespread adoption of LLMs presents several challenges, such as integration\ninto existing applications and infrastructure, utilization of company\nproprietary data, models, and APIs, and meeting cost, quality, responsiveness,\nand other requirements. To address these challenges, there is a notable shift\nfrom monolithic models to compound AI systems, with the premise of more\npowerful, versatile, and reliable applications. However, progress thus far has\nbeen piecemeal, with proposals for agentic workflows, programming models, and\nextended LLM capabilities, without a clear vision of an overall architecture.\nIn this paper, we propose a 'blueprint architecture' for compound AI systems\nfor orchestrating agents and data for enterprise applications. In our proposed\narchitecture the key orchestration concept is 'streams' to coordinate the flow\nof data and instructions among agents. Existing proprietary models and APIs in\nthe enterprise are mapped to 'agents', defined in an 'agent registry' that\nserves agent metadata and learned representations for search and planning.\nAgents can utilize proprietary data through a 'data registry' that similarly\nregisters enterprise data of various modalities. Tying it all together, data\nand task 'planners' break down, map, and optimize tasks and queries for given\nquality of service (QoS) requirements such as cost, accuracy, and latency. We\nillustrate an implementation of the architecture for a use-case in the HR\ndomain and discuss opportunities and challenges for 'agentic AI' in the\nenterprise."
                },
                "authors": [
                    {
                        "name": "Eser Kandogan"
                    },
                    {
                        "name": "Nikita Bhutani"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Rafael Li Chen"
                    },
                    {
                        "name": "Sairam Gurajada"
                    },
                    {
                        "name": "Estevam Hruschka"
                    }
                ],
                "author_detail": {
                    "name": "Estevam Hruschka"
                },
                "author": "Estevam Hruschka",
                "arxiv_journal_ref": "First Workshop on Data-AI Systems (DAIS), ICDE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08148v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08148v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08137v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08137v1",
                "updated": "2025-04-10T21:33:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    21,
                    33,
                    15,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T21:33:15Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    21,
                    33,
                    15,
                    3,
                    100,
                    0
                ],
                "title": "Empowering Vector Architectures for ML: The CAMP Architecture for Matrix\n  Multiplication",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering Vector Architectures for ML: The CAMP Architecture for Matrix\n  Multiplication"
                },
                "summary": "This study presents the Cartesian Accumulative Matrix Pipeline (CAMP)\narchitecture, a novel approach designed to enhance matrix multiplication in\nVector Architectures (VAs) and Single Instruction Multiple Data (SIMD) units.\nCAMP improves the processing efficiency of Quantized Neural Networks (QNNs).\nMatrix multiplication is a cornerstone of machine learning applications, and\nits quantized versions are increasingly popular for more efficient operations.\nUnfortunately, existing VAs and SIMD-support units struggle to efficiently\nhandle these quantized formats. In this work, we propose CAMP, a simple yet\neffective architecture that leverages a hybrid multiplier. The CAMP\narchitecture significantly advances the performance of vector architectures in\nhandling quantized data, enabling more efficient execution of matrix\nmultiplication across various platforms, specifically targeting the ARMv8\nScalable Vector Extension (SVE) and edge RISC-V SIMD-based architectures. In\naddition to increasing throughput, CAMP's architectural design also contributes\nto energy efficiency, making it an effective solution for low-power\napplications. Evaluations on a range of Large Language Models (LLMs) and\nConvolutional Neural Networks (CNNs) demonstrate that matrix multiplication\noperations using the proposed micro-architecture achieve up to 17$\\times$ and\n23$\\times$ performance improvements compared to their respective baselines, the\nARM A64FX core and a RISC-V-based edge System-on-Chip (SoC). Furthermore,\nsynthesis and place-and-route (PnR) of the CAMP micro-architecture using\nSynopsys tools -- targeting ARM TSMC 7nm for A64FX and GlobalFoundries 22nm for\nthe RISC-V SoC -- add only 1\\% and 4\\% area overhead, respectively, compared to\nthe baseline designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study presents the Cartesian Accumulative Matrix Pipeline (CAMP)\narchitecture, a novel approach designed to enhance matrix multiplication in\nVector Architectures (VAs) and Single Instruction Multiple Data (SIMD) units.\nCAMP improves the processing efficiency of Quantized Neural Networks (QNNs).\nMatrix multiplication is a cornerstone of machine learning applications, and\nits quantized versions are increasingly popular for more efficient operations.\nUnfortunately, existing VAs and SIMD-support units struggle to efficiently\nhandle these quantized formats. In this work, we propose CAMP, a simple yet\neffective architecture that leverages a hybrid multiplier. The CAMP\narchitecture significantly advances the performance of vector architectures in\nhandling quantized data, enabling more efficient execution of matrix\nmultiplication across various platforms, specifically targeting the ARMv8\nScalable Vector Extension (SVE) and edge RISC-V SIMD-based architectures. In\naddition to increasing throughput, CAMP's architectural design also contributes\nto energy efficiency, making it an effective solution for low-power\napplications. Evaluations on a range of Large Language Models (LLMs) and\nConvolutional Neural Networks (CNNs) demonstrate that matrix multiplication\noperations using the proposed micro-architecture achieve up to 17$\\times$ and\n23$\\times$ performance improvements compared to their respective baselines, the\nARM A64FX core and a RISC-V-based edge System-on-Chip (SoC). Furthermore,\nsynthesis and place-and-route (PnR) of the CAMP micro-architecture using\nSynopsys tools -- targeting ARM TSMC 7nm for A64FX and GlobalFoundries 22nm for\nthe RISC-V SoC -- add only 1\\% and 4\\% area overhead, respectively, compared to\nthe baseline designs."
                },
                "authors": [
                    {
                        "name": "Mohammadreza Esmali Nojehdeh"
                    },
                    {
                        "name": "Hossein Mokhtarnia"
                    },
                    {
                        "name": "Julian Pavon Rivera"
                    },
                    {
                        "name": "Narcis Rodas Quiroga"
                    },
                    {
                        "name": "Roger Figueras Bagu"
                    },
                    {
                        "name": "Enrico Reggiani"
                    },
                    {
                        "name": "Miquel Moreto"
                    },
                    {
                        "name": "Osman Unsal"
                    },
                    {
                        "name": "Adrian Cristal"
                    },
                    {
                        "name": "Eduard Ayguade"
                    }
                ],
                "author_detail": {
                    "name": "Eduard Ayguade"
                },
                "author": "Eduard Ayguade",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08137v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08137v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.12622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.12622v2",
                "updated": "2025-04-10T21:14:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    21,
                    14,
                    2,
                    3,
                    100,
                    0
                ],
                "published": "2024-08-14T10:32:06Z",
                "published_parsed": [
                    2024,
                    8,
                    14,
                    10,
                    32,
                    6,
                    2,
                    227,
                    0
                ],
                "title": "The AI Risk Repository: A Comprehensive Meta-Review, Database, and\n  Taxonomy of Risks From Artificial Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The AI Risk Repository: A Comprehensive Meta-Review, Database, and\n  Taxonomy of Risks From Artificial Intelligence"
                },
                "summary": "The risks posed by Artificial Intelligence (AI) are of considerable concern\nto academics, auditors, policymakers, AI companies, and the public. However, a\nlack of shared understanding of AI risks can impede our ability to\ncomprehensively discuss, research, and react to them. This paper addresses this\ngap by creating an AI Risk Repository to serve as a common frame of reference.\nThis comprises a living database of 777 risks extracted from 43 taxonomies,\nwhich can be filtered based on two overarching taxonomies and easily accessed,\nmodified, and updated via our website and online spreadsheets. We construct our\nRepository with a systematic review of taxonomies and other structured\nclassifications of AI risk followed by an expert consultation. We develop our\ntaxonomies of AI risk using a best-fit framework synthesis. Our high-level\nCausal Taxonomy of AI Risks classifies each risk by its causal factors (1)\nEntity: Human, AI; (2) Intentionality: Intentional, Unintentional; and (3)\nTiming: Pre-deployment; Post-deployment. Our mid-level Domain Taxonomy of AI\nRisks classifies risks into seven AI risk domains: (1) Discrimination &\ntoxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors &\nmisuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and\n(7) AI system safety, failures, & limitations. These are further divided into\n23 subdomains. The AI Risk Repository is, to our knowledge, the first attempt\nto rigorously curate, analyze, and extract AI risk frameworks into a publicly\naccessible, comprehensive, extensible, and categorized risk database. This\ncreates a foundation for a more coordinated, coherent, and complete approach to\ndefining, auditing, and managing the risks posed by AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The risks posed by Artificial Intelligence (AI) are of considerable concern\nto academics, auditors, policymakers, AI companies, and the public. However, a\nlack of shared understanding of AI risks can impede our ability to\ncomprehensively discuss, research, and react to them. This paper addresses this\ngap by creating an AI Risk Repository to serve as a common frame of reference.\nThis comprises a living database of 777 risks extracted from 43 taxonomies,\nwhich can be filtered based on two overarching taxonomies and easily accessed,\nmodified, and updated via our website and online spreadsheets. We construct our\nRepository with a systematic review of taxonomies and other structured\nclassifications of AI risk followed by an expert consultation. We develop our\ntaxonomies of AI risk using a best-fit framework synthesis. Our high-level\nCausal Taxonomy of AI Risks classifies each risk by its causal factors (1)\nEntity: Human, AI; (2) Intentionality: Intentional, Unintentional; and (3)\nTiming: Pre-deployment; Post-deployment. Our mid-level Domain Taxonomy of AI\nRisks classifies risks into seven AI risk domains: (1) Discrimination &\ntoxicity, (2) Privacy & security, (3) Misinformation, (4) Malicious actors &\nmisuse, (5) Human-computer interaction, (6) Socioeconomic & environmental, and\n(7) AI system safety, failures, & limitations. These are further divided into\n23 subdomains. The AI Risk Repository is, to our knowledge, the first attempt\nto rigorously curate, analyze, and extract AI risk frameworks into a publicly\naccessible, comprehensive, extensible, and categorized risk database. This\ncreates a foundation for a more coordinated, coherent, and complete approach to\ndefining, auditing, and managing the risks posed by AI systems."
                },
                "authors": [
                    {
                        "name": "Peter Slattery"
                    },
                    {
                        "name": "Alexander K. Saeri"
                    },
                    {
                        "name": "Emily A. C. Grundy"
                    },
                    {
                        "name": "Jess Graham"
                    },
                    {
                        "name": "Michael Noetel"
                    },
                    {
                        "name": "Risto Uuk"
                    },
                    {
                        "name": "James Dao"
                    },
                    {
                        "name": "Soroush Pour"
                    },
                    {
                        "name": "Stephen Casper"
                    },
                    {
                        "name": "Neil Thompson"
                    }
                ],
                "author_detail": {
                    "name": "Neil Thompson"
                },
                "author": "Neil Thompson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.12622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.12622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.0; K.4.1; K.4.1; K.4.2; K.4.3; K.6.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.01995v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.01995v2",
                "updated": "2025-04-10T20:43:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    20,
                    43,
                    23,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-01T00:10:10Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    0,
                    10,
                    10,
                    1,
                    91,
                    0
                ],
                "title": "Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brains vs. Bytes: Evaluating LLM Proficiency in Olympiad Mathematics"
                },
                "summary": "Recent advances in large language models (LLMs) have shown impressive\nprogress in mathematical reasoning tasks. However, current evaluation\nbenchmarks predominantly focus on the accuracy of final answers, often\noverlooking the crucial logical rigor for mathematical problem solving. The\nclaim that state-of-the-art LLMs can solve Math Olympiad-level problems\nrequires closer examination. To explore this, we conducted both qualitative and\nquantitative human evaluations of proofs generated by LLMs, and developed a\nschema for automatically assessing their reasoning capabilities. Our study\nreveals that current LLMs fall significantly short of solving challenging\nOlympiad-level problems and frequently fail to distinguish correct mathematical\nreasoning from clearly flawed solutions. Our analyses demonstrate that the\noccasional correct final answers provided by LLMs often result from pattern\nrecognition or heuristic shortcuts rather than genuine mathematical reasoning.\nThese findings underscore the substantial gap between LLM performance and human\nexpertise in advanced mathematical reasoning and highlight the importance of\ndeveloping benchmarks that prioritize the soundness of the reasoning used to\narrive at an answer rather than the mere correctness of the final answers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have shown impressive\nprogress in mathematical reasoning tasks. However, current evaluation\nbenchmarks predominantly focus on the accuracy of final answers, often\noverlooking the crucial logical rigor for mathematical problem solving. The\nclaim that state-of-the-art LLMs can solve Math Olympiad-level problems\nrequires closer examination. To explore this, we conducted both qualitative and\nquantitative human evaluations of proofs generated by LLMs, and developed a\nschema for automatically assessing their reasoning capabilities. Our study\nreveals that current LLMs fall significantly short of solving challenging\nOlympiad-level problems and frequently fail to distinguish correct mathematical\nreasoning from clearly flawed solutions. Our analyses demonstrate that the\noccasional correct final answers provided by LLMs often result from pattern\nrecognition or heuristic shortcuts rather than genuine mathematical reasoning.\nThese findings underscore the substantial gap between LLM performance and human\nexpertise in advanced mathematical reasoning and highlight the importance of\ndeveloping benchmarks that prioritize the soundness of the reasoning used to\narrive at an answer rather than the mere correctness of the final answers."
                },
                "authors": [
                    {
                        "name": "Hamed Mahdavi"
                    },
                    {
                        "name": "Alireza Hashemi"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Pegah Mohammadipour"
                    },
                    {
                        "name": "Alireza Farhadi"
                    },
                    {
                        "name": "Samira Malek"
                    },
                    {
                        "name": "Yekta Yazdanifard"
                    },
                    {
                        "name": "Amir Khasahmadi"
                    },
                    {
                        "name": "Vasant Honavar"
                    }
                ],
                "author_detail": {
                    "name": "Vasant Honavar"
                },
                "author": "Vasant Honavar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.01995v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.01995v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08120v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08120v1",
                "updated": "2025-04-10T20:39:18Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    20,
                    39,
                    18,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T20:39:18Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    20,
                    39,
                    18,
                    3,
                    100,
                    0
                ],
                "title": "DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and\n  Summarization?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeepSeek vs. o3-mini: How Well can Reasoning LLMs Evaluate MT and\n  Summarization?"
                },
                "summary": "Reasoning-enabled large language models (LLMs) have recently demonstrated\nimpressive performance in complex logical and mathematical tasks, yet their\neffectiveness in evaluating natural language generation remains unexplored.\nThis study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI\no3) with their non-reasoning counterparts across machine translation (MT) and\ntext summarization (TS) evaluation tasks. We evaluate eight models across three\narchitectural categories, including state-of-the-art reasoning models, their\ndistilled variants (ranging from 8B to 70B parameters), and equivalent\nconventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval\nbenchmarks reveal that the benefits of reasoning capabilities are highly model\nand task-dependent: while OpenAI o3-mini models show consistent performance\nimprovements with increased reasoning intensity, DeepSeek-R1 underperforms\ncompared to its non-reasoning variant, with exception to certain aspects of TS\nevaluation. Correlation analysis demonstrates that increased reasoning token\nusage positively correlates with evaluation quality in o3-mini models.\nFurthermore, our results show that distillation of reasoning capabilities\nmaintains reasonable performance in medium-sized models (32B) but degrades\nsubstantially in smaller variants (8B). This work provides the first\ncomprehensive assessment of reasoning LLMs for NLG evaluation and offers\ninsights into their practical use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning-enabled large language models (LLMs) have recently demonstrated\nimpressive performance in complex logical and mathematical tasks, yet their\neffectiveness in evaluating natural language generation remains unexplored.\nThis study systematically compares reasoning-based LLMs (DeepSeek-R1 and OpenAI\no3) with their non-reasoning counterparts across machine translation (MT) and\ntext summarization (TS) evaluation tasks. We evaluate eight models across three\narchitectural categories, including state-of-the-art reasoning models, their\ndistilled variants (ranging from 8B to 70B parameters), and equivalent\nconventional, non-reasoning LLMs. Our experiments on WMT23 and SummEval\nbenchmarks reveal that the benefits of reasoning capabilities are highly model\nand task-dependent: while OpenAI o3-mini models show consistent performance\nimprovements with increased reasoning intensity, DeepSeek-R1 underperforms\ncompared to its non-reasoning variant, with exception to certain aspects of TS\nevaluation. Correlation analysis demonstrates that increased reasoning token\nusage positively correlates with evaluation quality in o3-mini models.\nFurthermore, our results show that distillation of reasoning capabilities\nmaintains reasonable performance in medium-sized models (32B) but degrades\nsubstantially in smaller variants (8B). This work provides the first\ncomprehensive assessment of reasoning LLMs for NLG evaluation and offers\ninsights into their practical use."
                },
                "authors": [
                    {
                        "name": "Daniil Larionov"
                    },
                    {
                        "name": "Sotaro Takeshita"
                    },
                    {
                        "name": "Ran Zhang"
                    },
                    {
                        "name": "Yanran Chen"
                    },
                    {
                        "name": "Christoph Leiter"
                    },
                    {
                        "name": "Zhipin Wang"
                    },
                    {
                        "name": "Christian Greisinger"
                    },
                    {
                        "name": "Steffen Eger"
                    }
                ],
                "author_detail": {
                    "name": "Steffen Eger"
                },
                "author": "Steffen Eger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08120v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08120v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08113v1",
                "updated": "2025-04-10T20:19:50Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    20,
                    19,
                    50,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T20:19:50Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    20,
                    19,
                    50,
                    3,
                    100,
                    0
                ],
                "title": "Test Amplification for REST APIs via Single and Multi-Agent LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Amplification for REST APIs via Single and Multi-Agent LLM Systems"
                },
                "summary": "REST APIs (Representational State Transfer Application Programming\nInterfaces) are essential to modern cloud-native applications. Strong and\nautomated test cases are crucial to expose lurking bugs in the API. However,\ncreating automated tests for REST APIs is difficult, and it requires test cases\nthat explore the protocol's boundary conditions. In this paper, we investigate\nhow single-agent and multi-agent LLM (Large Language Model) systems can amplify\na REST API test suite. Our evaluation demonstrates increased API coverage,\nidentification of numerous bugs in the API under test, and insights into the\ncomputational cost and energy consumption of both approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REST APIs (Representational State Transfer Application Programming\nInterfaces) are essential to modern cloud-native applications. Strong and\nautomated test cases are crucial to expose lurking bugs in the API. However,\ncreating automated tests for REST APIs is difficult, and it requires test cases\nthat explore the protocol's boundary conditions. In this paper, we investigate\nhow single-agent and multi-agent LLM (Large Language Model) systems can amplify\na REST API test suite. Our evaluation demonstrates increased API coverage,\nidentification of numerous bugs in the API under test, and insights into the\ncomputational cost and energy consumption of both approaches."
                },
                "authors": [
                    {
                        "name": "Robbe Nooyens"
                    },
                    {
                        "name": "Tolgahan Bardakci"
                    },
                    {
                        "name": "Mutlu Beyazit"
                    },
                    {
                        "name": "Serge Demeyer"
                    }
                ],
                "author_detail": {
                    "name": "Serge Demeyer"
                },
                "author": "Serge Demeyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08112v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08112v1",
                "updated": "2025-04-10T20:19:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    10,
                    20,
                    19,
                    20,
                    3,
                    100,
                    0
                ],
                "published": "2025-04-10T20:19:20Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    20,
                    19,
                    20,
                    3,
                    100,
                    0
                ],
                "title": "Scaling Laws of Graph Neural Networks for Atomistic Materials Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Laws of Graph Neural Networks for Atomistic Materials Modeling"
                },
                "summary": "Atomistic materials modeling is a critical task with wide-ranging\napplications, from drug discovery to materials science, where accurate\npredictions of the target material property can lead to significant\nadvancements in scientific discovery. Graph Neural Networks (GNNs) represent\nthe state-of-the-art approach for modeling atomistic material data thanks to\ntheir capacity to capture complex relational structures. While machine learning\nperformance has historically improved with larger models and datasets, GNNs for\natomistic materials modeling remain relatively small compared to large language\nmodels (LLMs), which leverage billions of parameters and terabyte-scale\ndatasets to achieve remarkable performance in their respective domains. To\naddress this gap, we explore the scaling limits of GNNs for atomistic materials\nmodeling by developing a foundational model with billions of parameters,\ntrained on extensive datasets in terabyte-scale. Our approach incorporates\ntechniques from LLM libraries to efficiently manage large-scale data and\nmodels, enabling both effective training and deployment of these large-scale\nGNN models. This work addresses three fundamental questions in scaling GNNs:\nthe potential for scaling GNN model architectures, the effect of dataset size\non model accuracy, and the applicability of LLM-inspired techniques to GNN\narchitectures. Specifically, the outcomes of this study include (1) insights\ninto the scaling laws for GNNs, highlighting the relationship between model\nsize, dataset volume, and accuracy, (2) a foundational GNN model optimized for\natomistic materials modeling, and (3) a GNN codebase enhanced with advanced\nLLM-based training techniques. Our findings lay the groundwork for large-scale\nGNNs with billions of parameters and terabyte-scale datasets, establishing a\nscalable pathway for future advancements in atomistic materials modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Atomistic materials modeling is a critical task with wide-ranging\napplications, from drug discovery to materials science, where accurate\npredictions of the target material property can lead to significant\nadvancements in scientific discovery. Graph Neural Networks (GNNs) represent\nthe state-of-the-art approach for modeling atomistic material data thanks to\ntheir capacity to capture complex relational structures. While machine learning\nperformance has historically improved with larger models and datasets, GNNs for\natomistic materials modeling remain relatively small compared to large language\nmodels (LLMs), which leverage billions of parameters and terabyte-scale\ndatasets to achieve remarkable performance in their respective domains. To\naddress this gap, we explore the scaling limits of GNNs for atomistic materials\nmodeling by developing a foundational model with billions of parameters,\ntrained on extensive datasets in terabyte-scale. Our approach incorporates\ntechniques from LLM libraries to efficiently manage large-scale data and\nmodels, enabling both effective training and deployment of these large-scale\nGNN models. This work addresses three fundamental questions in scaling GNNs:\nthe potential for scaling GNN model architectures, the effect of dataset size\non model accuracy, and the applicability of LLM-inspired techniques to GNN\narchitectures. Specifically, the outcomes of this study include (1) insights\ninto the scaling laws for GNNs, highlighting the relationship between model\nsize, dataset volume, and accuracy, (2) a foundational GNN model optimized for\natomistic materials modeling, and (3) a GNN codebase enhanced with advanced\nLLM-based training techniques. Our findings lay the groundwork for large-scale\nGNNs with billions of parameters and terabyte-scale datasets, establishing a\nscalable pathway for future advancements in atomistic materials modeling."
                },
                "authors": [
                    {
                        "name": "Chaojian Li"
                    },
                    {
                        "name": "Zhifan Ye"
                    },
                    {
                        "name": "Massimiliano Lupo Pasini"
                    },
                    {
                        "name": "Jong Youl Choi"
                    },
                    {
                        "name": "Cheng Wan"
                    },
                    {
                        "name": "Yingyan Celine Lin"
                    },
                    {
                        "name": "Prasanna Balaprakash"
                    }
                ],
                "author_detail": {
                    "name": "Prasanna Balaprakash"
                },
                "author": "Prasanna Balaprakash",
                "arxiv_comment": "Accepted by DAC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08112v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08112v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]